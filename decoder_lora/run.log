03/21/2024 20:45:58 - WARNING - lora.log - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: False
03/21/2024 20:45:58 - INFO - lora.log - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=True,
bf16_full_eval=True,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=False,
ddp_timeout=18000000,
debug=[],
deepspeed=None,
disable_tqdm=True,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=100,
evaluation_strategy=steps,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=8,
gradient_checkpointing=True,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=True,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=0.0001,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=./output/logs,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=10,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=10.0,
optim=adamw_torch,
optim_args=None,
output_dir=./output,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=1,
per_device_train_batch_size=1,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['tensorboard'],
resume_from_checkpoint=None,
run_name=./output,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=100,
save_strategy=steps,
save_total_limit=100,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=400,
weight_decay=0.0,
)
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/transformers/models/auto/configuration_auto.py:1096: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
[INFO|configuration_utils.py:726] 2024-03-21 20:45:58,986 >> loading configuration file ../model/Atom-7B/config.json
[INFO|configuration_utils.py:791] 2024-03-21 20:45:58,987 >> Model config LlamaConfig {
  "_name_or_path": "../model/Atom-7B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "auto_map": {
    "AutoConfig": "configuration_atom.LlamaConfig",
    "AutoModel": "model_atom.LlamaForCausalLM",
    "AutoModelForCausalLM": "model_atom.LlamaForCausalLM",
    "AutoModelForSequenceClassification": "model_atom.LlamaForSequenceClassification"
  },
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_length": 4096,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 2,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "type": "dynamic"
  },
  "rope_theta": 500000,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.38.2",
  "use_cache": true,
  "vocab_size": 65000
}

/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/transformers/models/auto/tokenization_auto.py:720: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
[INFO|tokenization_utils_base.py:2044] 2024-03-21 20:45:58,988 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:2044] 2024-03-21 20:45:58,988 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2044] 2024-03-21 20:45:58,988 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2044] 2024-03-21 20:45:58,988 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2044] 2024-03-21 20:45:58,988 >> loading file tokenizer.json
</s>
03/21/2024 20:45:59 - INFO - lora.log - lora配置: LoraConfig(peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, task_type='CAUSAL_LM', inference_mode=False, r=8, target_modules={'v_proj', 'o_proj', 'gate_proj', 'down_proj', 'q_proj', 'up_proj', 'k_proj'}, lora_alpha=16, lora_dropout=0.05, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', loftq_config={}, use_dora=False)
03/21/2024 20:45:59 - INFO - lora.log - torch_dtype: torch.float16
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py:466: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
[INFO|modeling_utils.py:3254] 2024-03-21 20:45:59,071 >> loading weights file ../model/Atom-7B/model.safetensors.index.json
[INFO|modeling_utils.py:1400] 2024-03-21 20:45:59,071 >> Instantiating LlamaForCausalLM model under default dtype torch.float16.
[WARNING|logging.py:329] 2024-03-21 20:45:59,071 >> The model was loaded with use_flash_attention_2=True, which is deprecated and may be removed in a future release. Please use `attn_implementation="flash_attention_2"` instead.
[INFO|configuration_utils.py:845] 2024-03-21 20:45:59,073 >> Generate config GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2,
  "max_length": 4096,
  "pad_token_id": 2
}

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:01<00:03,  1.75s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:02<00:01,  1.39s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:03<00:00,  1.17s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:03<00:00,  1.26s/it]
[INFO|modeling_utils.py:3992] 2024-03-21 20:46:03,102 >> All model checkpoint weights were used when initializing LlamaForCausalLM.

[INFO|modeling_utils.py:4000] 2024-03-21 20:46:03,102 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at ../model/Atom-7B.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.
[INFO|configuration_utils.py:798] 2024-03-21 20:46:03,104 >> loading configuration file ../model/Atom-7B/generation_config.json
[INFO|configuration_utils.py:845] 2024-03-21 20:46:03,105 >> Generate config GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2,
  "max_length": 4096,
  "pad_token_id": 2
}

/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/datasets/load.py:2516: FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.
You can remove this warning by passing 'token=<use_auth_token>' instead.
  warnings.warn(
Using custom data configuration default-b8b142b6dd81b35c
trainable params: 19,988,480 || all params: 7,028,740,096 || trainable%: 0.284382118658439
03/21/2024 20:46:04 - INFO - datasets.builder - Using custom data configuration default-b8b142b6dd81b35c
Loading Dataset Infos from /home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/datasets/packaged_modules/csv
03/21/2024 20:46:04 - INFO - datasets.info - Loading Dataset Infos from /home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/datasets/packaged_modules/csv
Generating dataset csv (/home/asus/intelligent-test/lora/output/dataset_cache/csv/default-b8b142b6dd81b35c/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6)
03/21/2024 20:46:04 - INFO - datasets.builder - Generating dataset csv (/home/asus/intelligent-test/lora/output/dataset_cache/csv/default-b8b142b6dd81b35c/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6)
Downloading and preparing dataset csv/default to /home/asus/intelligent-test/lora/output/dataset_cache/csv/default-b8b142b6dd81b35c/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6...
03/21/2024 20:46:04 - INFO - datasets.builder - Downloading and preparing dataset csv/default to /home/asus/intelligent-test/lora/output/dataset_cache/csv/default-b8b142b6dd81b35c/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6...
Downloading took 0.0 min
03/21/2024 20:46:04 - INFO - datasets.download.download_manager - Downloading took 0.0 min
Checksum Computation took 0.0 min
03/21/2024 20:46:04 - INFO - datasets.download.download_manager - Checksum Computation took 0.0 min
Generating train split
03/21/2024 20:46:04 - INFO - datasets.builder - Generating train split
Generating train split: 0 examples [00:00, ? examples/s]Generating train split: 10000 examples [00:00, 89343.73 examples/s]Generating train split: 20000 examples [00:00, 88964.28 examples/s]Generating train split: 40000 examples [00:00, 100186.55 examples/s]Generating train split: 44005 examples [00:00, 97566.96 examples/s] 
Generating validation split
03/21/2024 20:46:05 - INFO - datasets.builder - Generating validation split
Generating validation split: 0 examples [00:00, ? examples/s]Generating validation split: 638 examples [00:00, 122795.79 examples/s]
Unable to verify splits sizes.
03/21/2024 20:46:05 - INFO - datasets.utils.info_utils - Unable to verify splits sizes.
Dataset csv downloaded and prepared to /home/asus/intelligent-test/lora/output/dataset_cache/csv/default-b8b142b6dd81b35c/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6. Subsequent calls will reuse this data.
03/21/2024 20:46:05 - INFO - datasets.builder - Dataset csv downloaded and prepared to /home/asus/intelligent-test/lora/output/dataset_cache/csv/default-b8b142b6dd81b35c/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6. Subsequent calls will reuse this data.
Process #0 will write at /home/asus/intelligent-test/lora/output/dataset_cache/csv/default-b8b142b6dd81b35c/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-87848bfe3065c0e2_00000_of_00010.arrow
数据集中是否输入输出在同一列: True
03/21/2024 20:46:05 - INFO - datasets.arrow_dataset - Process #0 will write at /home/asus/intelligent-test/lora/output/dataset_cache/csv/default-b8b142b6dd81b35c/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-87848bfe3065c0e2_00000_of_00010.arrow
Process #1 will write at /home/asus/intelligent-test/lora/output/dataset_cache/csv/default-b8b142b6dd81b35c/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-87848bfe3065c0e2_00001_of_00010.arrow
03/21/2024 20:46:05 - INFO - datasets.arrow_dataset - Process #1 will write at /home/asus/intelligent-test/lora/output/dataset_cache/csv/default-b8b142b6dd81b35c/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-87848bfe3065c0e2_00001_of_00010.arrow
Process #2 will write at /home/asus/intelligent-test/lora/output/dataset_cache/csv/default-b8b142b6dd81b35c/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-87848bfe3065c0e2_00002_of_00010.arrow
03/21/2024 20:46:05 - INFO - datasets.arrow_dataset - Process #2 will write at /home/asus/intelligent-test/lora/output/dataset_cache/csv/default-b8b142b6dd81b35c/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-87848bfe3065c0e2_00002_of_00010.arrow
Process #3 will write at /home/asus/intelligent-test/lora/output/dataset_cache/csv/default-b8b142b6dd81b35c/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-87848bfe3065c0e2_00003_of_00010.arrow
03/21/2024 20:46:05 - INFO - datasets.arrow_dataset - Process #3 will write at /home/asus/intelligent-test/lora/output/dataset_cache/csv/default-b8b142b6dd81b35c/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-87848bfe3065c0e2_00003_of_00010.arrow
Process #4 will write at /home/asus/intelligent-test/lora/output/dataset_cache/csv/default-b8b142b6dd81b35c/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-87848bfe3065c0e2_00004_of_00010.arrow
03/21/2024 20:46:05 - INFO - datasets.arrow_dataset - Process #4 will write at /home/asus/intelligent-test/lora/output/dataset_cache/csv/default-b8b142b6dd81b35c/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-87848bfe3065c0e2_00004_of_00010.arrow
Process #5 will write at /home/asus/intelligent-test/lora/output/dataset_cache/csv/default-b8b142b6dd81b35c/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-87848bfe3065c0e2_00005_of_00010.arrow
03/21/2024 20:46:05 - INFO - datasets.arrow_dataset - Process #5 will write at /home/asus/intelligent-test/lora/output/dataset_cache/csv/default-b8b142b6dd81b35c/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-87848bfe3065c0e2_00005_of_00010.arrow
Process #6 will write at /home/asus/intelligent-test/lora/output/dataset_cache/csv/default-b8b142b6dd81b35c/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-87848bfe3065c0e2_00006_of_00010.arrow
03/21/2024 20:46:05 - INFO - datasets.arrow_dataset - Process #6 will write at /home/asus/intelligent-test/lora/output/dataset_cache/csv/default-b8b142b6dd81b35c/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-87848bfe3065c0e2_00006_of_00010.arrow
Process #7 will write at /home/asus/intelligent-test/lora/output/dataset_cache/csv/default-b8b142b6dd81b35c/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-87848bfe3065c0e2_00007_of_00010.arrow
03/21/2024 20:46:05 - INFO - datasets.arrow_dataset - Process #7 will write at /home/asus/intelligent-test/lora/output/dataset_cache/csv/default-b8b142b6dd81b35c/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-87848bfe3065c0e2_00007_of_00010.arrow
Process #8 will write at /home/asus/intelligent-test/lora/output/dataset_cache/csv/default-b8b142b6dd81b35c/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-87848bfe3065c0e2_00008_of_00010.arrow
03/21/2024 20:46:05 - INFO - datasets.arrow_dataset - Process #8 will write at /home/asus/intelligent-test/lora/output/dataset_cache/csv/default-b8b142b6dd81b35c/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-87848bfe3065c0e2_00008_of_00010.arrow
Process #9 will write at /home/asus/intelligent-test/lora/output/dataset_cache/csv/default-b8b142b6dd81b35c/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-87848bfe3065c0e2_00009_of_00010.arrow
03/21/2024 20:46:05 - INFO - datasets.arrow_dataset - Process #9 will write at /home/asus/intelligent-test/lora/output/dataset_cache/csv/default-b8b142b6dd81b35c/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-87848bfe3065c0e2_00009_of_00010.arrow
Spawning 10 processes
03/21/2024 20:46:05 - INFO - datasets.arrow_dataset - Spawning 10 processes
Running tokenizer on dataset (num_proc=10):   0%|          | 0/44005 [00:00<?, ? examples/s]Caching processed dataset at /home/asus/intelligent-test/lora/output/dataset_cache/csv/default-b8b142b6dd81b35c/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-87848bfe3065c0e2_00006_of_00010.arrow
03/21/2024 20:46:05 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/asus/intelligent-test/lora/output/dataset_cache/csv/default-b8b142b6dd81b35c/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-87848bfe3065c0e2_00006_of_00010.arrow
Running tokenizer on dataset (num_proc=10):   2%|▏         | 1000/44005 [00:00<00:16, 2532.80 examples/s]Caching processed dataset at /home/asus/intelligent-test/lora/output/dataset_cache/csv/default-b8b142b6dd81b35c/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-87848bfe3065c0e2_00005_of_00010.arrow
03/21/2024 20:46:05 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/asus/intelligent-test/lora/output/dataset_cache/csv/default-b8b142b6dd81b35c/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-87848bfe3065c0e2_00005_of_00010.arrow
Caching processed dataset at /home/asus/intelligent-test/lora/output/dataset_cache/csv/default-b8b142b6dd81b35c/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-87848bfe3065c0e2_00000_of_00010.arrow
03/21/2024 20:46:05 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/asus/intelligent-test/lora/output/dataset_cache/csv/default-b8b142b6dd81b35c/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-87848bfe3065c0e2_00000_of_00010.arrow
Caching processed dataset at /home/asus/intelligent-test/lora/output/dataset_cache/csv/default-b8b142b6dd81b35c/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-87848bfe3065c0e2_00004_of_00010.arrow
03/21/2024 20:46:06 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/asus/intelligent-test/lora/output/dataset_cache/csv/default-b8b142b6dd81b35c/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-87848bfe3065c0e2_00004_of_00010.arrow
Caching processed dataset at /home/asus/intelligent-test/lora/output/dataset_cache/csv/default-b8b142b6dd81b35c/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-87848bfe3065c0e2_00001_of_00010.arrow
03/21/2024 20:46:06 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/asus/intelligent-test/lora/output/dataset_cache/csv/default-b8b142b6dd81b35c/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-87848bfe3065c0e2_00001_of_00010.arrow
Caching processed dataset at /home/asus/intelligent-test/lora/output/dataset_cache/csv/default-b8b142b6dd81b35c/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-87848bfe3065c0e2_00002_of_00010.arrow
03/21/2024 20:46:06 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/asus/intelligent-test/lora/output/dataset_cache/csv/default-b8b142b6dd81b35c/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-87848bfe3065c0e2_00002_of_00010.arrow
Caching processed dataset at /home/asus/intelligent-test/lora/output/dataset_cache/csv/default-b8b142b6dd81b35c/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-87848bfe3065c0e2_00009_of_00010.arrow
03/21/2024 20:46:06 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/asus/intelligent-test/lora/output/dataset_cache/csv/default-b8b142b6dd81b35c/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-87848bfe3065c0e2_00009_of_00010.arrow
Running tokenizer on dataset (num_proc=10):   9%|▉         | 4000/44005 [00:00<00:04, 9672.99 examples/s]Caching processed dataset at /home/asus/intelligent-test/lora/output/dataset_cache/csv/default-b8b142b6dd81b35c/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-87848bfe3065c0e2_00003_of_00010.arrow
03/21/2024 20:46:06 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/asus/intelligent-test/lora/output/dataset_cache/csv/default-b8b142b6dd81b35c/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-87848bfe3065c0e2_00003_of_00010.arrow
Caching processed dataset at /home/asus/intelligent-test/lora/output/dataset_cache/csv/default-b8b142b6dd81b35c/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-87848bfe3065c0e2_00008_of_00010.arrow
03/21/2024 20:46:06 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/asus/intelligent-test/lora/output/dataset_cache/csv/default-b8b142b6dd81b35c/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-87848bfe3065c0e2_00008_of_00010.arrow
Caching processed dataset at /home/asus/intelligent-test/lora/output/dataset_cache/csv/default-b8b142b6dd81b35c/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-87848bfe3065c0e2_00007_of_00010.arrow
03/21/2024 20:46:06 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/asus/intelligent-test/lora/output/dataset_cache/csv/default-b8b142b6dd81b35c/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-87848bfe3065c0e2_00007_of_00010.arrow
Running tokenizer on dataset (num_proc=10):  23%|██▎       | 10000/44005 [00:00<00:01, 22942.62 examples/s]Running tokenizer on dataset (num_proc=10):  32%|███▏      | 14000/44005 [00:00<00:01, 18800.80 examples/s]Running tokenizer on dataset (num_proc=10):  45%|████▌     | 20000/44005 [00:01<00:00, 26104.86 examples/s]Running tokenizer on dataset (num_proc=10):  55%|█████▍    | 24000/44005 [00:01<00:00, 22025.08 examples/s]Running tokenizer on dataset (num_proc=10):  61%|██████▏   | 27000/44005 [00:01<00:00, 22978.89 examples/s]Running tokenizer on dataset (num_proc=10):  70%|███████   | 31000/44005 [00:01<00:00, 22264.13 examples/s]Running tokenizer on dataset (num_proc=10):  80%|███████▉  | 35000/44005 [00:01<00:00, 24141.06 examples/s]Running tokenizer on dataset (num_proc=10):  91%|█████████ | 40002/44005 [00:01<00:00, 26515.03 examples/s]Running tokenizer on dataset (num_proc=10):  98%|█████████▊| 43204/44005 [00:01<00:00, 24889.51 examples/s]Running tokenizer on dataset (num_proc=10): 100%|██████████| 44005/44005 [00:02<00:00, 21208.28 examples/s]
Concatenating 10 shards
03/21/2024 20:46:07 - INFO - datasets.arrow_dataset - Concatenating 10 shards
Process #0 will write at /home/asus/intelligent-test/lora/output/dataset_cache/csv/default-b8b142b6dd81b35c/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-f91eba2517e7dd3c_00000_of_00010.arrow
03/21/2024 20:46:07 - INFO - datasets.arrow_dataset - Process #0 will write at /home/asus/intelligent-test/lora/output/dataset_cache/csv/default-b8b142b6dd81b35c/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-f91eba2517e7dd3c_00000_of_00010.arrow
Process #1 will write at /home/asus/intelligent-test/lora/output/dataset_cache/csv/default-b8b142b6dd81b35c/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-f91eba2517e7dd3c_00001_of_00010.arrow
03/21/2024 20:46:07 - INFO - datasets.arrow_dataset - Process #1 will write at /home/asus/intelligent-test/lora/output/dataset_cache/csv/default-b8b142b6dd81b35c/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-f91eba2517e7dd3c_00001_of_00010.arrow
Process #2 will write at /home/asus/intelligent-test/lora/output/dataset_cache/csv/default-b8b142b6dd81b35c/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-f91eba2517e7dd3c_00002_of_00010.arrow
03/21/2024 20:46:07 - INFO - datasets.arrow_dataset - Process #2 will write at /home/asus/intelligent-test/lora/output/dataset_cache/csv/default-b8b142b6dd81b35c/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-f91eba2517e7dd3c_00002_of_00010.arrow
Process #3 will write at /home/asus/intelligent-test/lora/output/dataset_cache/csv/default-b8b142b6dd81b35c/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-f91eba2517e7dd3c_00003_of_00010.arrow
03/21/2024 20:46:07 - INFO - datasets.arrow_dataset - Process #3 will write at /home/asus/intelligent-test/lora/output/dataset_cache/csv/default-b8b142b6dd81b35c/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-f91eba2517e7dd3c_00003_of_00010.arrow
Process #4 will write at /home/asus/intelligent-test/lora/output/dataset_cache/csv/default-b8b142b6dd81b35c/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-f91eba2517e7dd3c_00004_of_00010.arrow
03/21/2024 20:46:07 - INFO - datasets.arrow_dataset - Process #4 will write at /home/asus/intelligent-test/lora/output/dataset_cache/csv/default-b8b142b6dd81b35c/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-f91eba2517e7dd3c_00004_of_00010.arrow
Process #5 will write at /home/asus/intelligent-test/lora/output/dataset_cache/csv/default-b8b142b6dd81b35c/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-f91eba2517e7dd3c_00005_of_00010.arrow
03/21/2024 20:46:07 - INFO - datasets.arrow_dataset - Process #5 will write at /home/asus/intelligent-test/lora/output/dataset_cache/csv/default-b8b142b6dd81b35c/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-f91eba2517e7dd3c_00005_of_00010.arrow
Process #6 will write at /home/asus/intelligent-test/lora/output/dataset_cache/csv/default-b8b142b6dd81b35c/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-f91eba2517e7dd3c_00006_of_00010.arrow
03/21/2024 20:46:07 - INFO - datasets.arrow_dataset - Process #6 will write at /home/asus/intelligent-test/lora/output/dataset_cache/csv/default-b8b142b6dd81b35c/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-f91eba2517e7dd3c_00006_of_00010.arrow
Process #7 will write at /home/asus/intelligent-test/lora/output/dataset_cache/csv/default-b8b142b6dd81b35c/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-f91eba2517e7dd3c_00007_of_00010.arrow
03/21/2024 20:46:07 - INFO - datasets.arrow_dataset - Process #7 will write at /home/asus/intelligent-test/lora/output/dataset_cache/csv/default-b8b142b6dd81b35c/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-f91eba2517e7dd3c_00007_of_00010.arrow
Process #8 will write at /home/asus/intelligent-test/lora/output/dataset_cache/csv/default-b8b142b6dd81b35c/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-f91eba2517e7dd3c_00008_of_00010.arrow
03/21/2024 20:46:07 - INFO - datasets.arrow_dataset - Process #8 will write at /home/asus/intelligent-test/lora/output/dataset_cache/csv/default-b8b142b6dd81b35c/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-f91eba2517e7dd3c_00008_of_00010.arrow
Process #9 will write at /home/asus/intelligent-test/lora/output/dataset_cache/csv/default-b8b142b6dd81b35c/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-f91eba2517e7dd3c_00009_of_00010.arrow
03/21/2024 20:46:07 - INFO - datasets.arrow_dataset - Process #9 will write at /home/asus/intelligent-test/lora/output/dataset_cache/csv/default-b8b142b6dd81b35c/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-f91eba2517e7dd3c_00009_of_00010.arrow
Spawning 10 processes
03/21/2024 20:46:07 - INFO - datasets.arrow_dataset - Spawning 10 processes
Running tokenizer on dataset (num_proc=10):   0%|          | 0/638 [00:00<?, ? examples/s]Caching processed dataset at /home/asus/intelligent-test/lora/output/dataset_cache/csv/default-b8b142b6dd81b35c/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-f91eba2517e7dd3c_00000_of_00010.arrow
03/21/2024 20:46:07 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/asus/intelligent-test/lora/output/dataset_cache/csv/default-b8b142b6dd81b35c/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-f91eba2517e7dd3c_00000_of_00010.arrow
Caching processed dataset at /home/asus/intelligent-test/lora/output/dataset_cache/csv/default-b8b142b6dd81b35c/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-f91eba2517e7dd3c_00001_of_00010.arrow
03/21/2024 20:46:07 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/asus/intelligent-test/lora/output/dataset_cache/csv/default-b8b142b6dd81b35c/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-f91eba2517e7dd3c_00001_of_00010.arrow
Caching processed dataset at /home/asus/intelligent-test/lora/output/dataset_cache/csv/default-b8b142b6dd81b35c/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-f91eba2517e7dd3c_00002_of_00010.arrow
03/21/2024 20:46:07 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/asus/intelligent-test/lora/output/dataset_cache/csv/default-b8b142b6dd81b35c/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-f91eba2517e7dd3c_00002_of_00010.arrow
Caching processed dataset at /home/asus/intelligent-test/lora/output/dataset_cache/csv/default-b8b142b6dd81b35c/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-f91eba2517e7dd3c_00003_of_00010.arrow
03/21/2024 20:46:07 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/asus/intelligent-test/lora/output/dataset_cache/csv/default-b8b142b6dd81b35c/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-f91eba2517e7dd3c_00003_of_00010.arrow
Caching processed dataset at /home/asus/intelligent-test/lora/output/dataset_cache/csv/default-b8b142b6dd81b35c/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-f91eba2517e7dd3c_00004_of_00010.arrow
03/21/2024 20:46:07 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/asus/intelligent-test/lora/output/dataset_cache/csv/default-b8b142b6dd81b35c/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-f91eba2517e7dd3c_00004_of_00010.arrow
Caching processed dataset at /home/asus/intelligent-test/lora/output/dataset_cache/csv/default-b8b142b6dd81b35c/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-f91eba2517e7dd3c_00005_of_00010.arrow
03/21/2024 20:46:07 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/asus/intelligent-test/lora/output/dataset_cache/csv/default-b8b142b6dd81b35c/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-f91eba2517e7dd3c_00005_of_00010.arrow
Caching processed dataset at /home/asus/intelligent-test/lora/output/dataset_cache/csv/default-b8b142b6dd81b35c/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-f91eba2517e7dd3c_00006_of_00010.arrow
03/21/2024 20:46:07 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/asus/intelligent-test/lora/output/dataset_cache/csv/default-b8b142b6dd81b35c/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-f91eba2517e7dd3c_00006_of_00010.arrow
Running tokenizer on dataset (num_proc=10):  60%|██████    | 384/638 [00:00<00:00, 3606.91 examples/s]Caching processed dataset at /home/asus/intelligent-test/lora/output/dataset_cache/csv/default-b8b142b6dd81b35c/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-f91eba2517e7dd3c_00007_of_00010.arrow
03/21/2024 20:46:07 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/asus/intelligent-test/lora/output/dataset_cache/csv/default-b8b142b6dd81b35c/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-f91eba2517e7dd3c_00007_of_00010.arrow
Caching processed dataset at /home/asus/intelligent-test/lora/output/dataset_cache/csv/default-b8b142b6dd81b35c/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-f91eba2517e7dd3c_00008_of_00010.arrow
03/21/2024 20:46:07 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/asus/intelligent-test/lora/output/dataset_cache/csv/default-b8b142b6dd81b35c/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-f91eba2517e7dd3c_00008_of_00010.arrow
Caching processed dataset at /home/asus/intelligent-test/lora/output/dataset_cache/csv/default-b8b142b6dd81b35c/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-f91eba2517e7dd3c_00009_of_00010.arrow
03/21/2024 20:46:07 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/asus/intelligent-test/lora/output/dataset_cache/csv/default-b8b142b6dd81b35c/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-f91eba2517e7dd3c_00009_of_00010.arrow
Running tokenizer on dataset (num_proc=10): 100%|██████████| 638/638 [00:00<00:00, 3287.42 examples/s]
Concatenating 10 shards
03/21/2024 20:46:07 - INFO - datasets.arrow_dataset - Concatenating 10 shards
03/21/2024 20:46:07 - INFO - lora.log - 训练集的采样41905: {'input_ids': [1, 12968, 29901, 29871, 38857, 32949, 33304, 30214, 40339, 38276, 32471, 53824, 33304, 32135, 33690, 32066, 52072, 32513, 30267, 13, 33304, 29901, 29871, 30690, 33664, 32537, 2544, 29943, 42150, 34568, 54696, 43788, 30346, 30744, 33201, 57884, 33707, 32009, 2544, 29943, 42150, 34568, 54696, 39738, 30767, 32471, 32400, 32214, 32131, 32193, 33304, 30214, 33404, 42363, 32154, 36280, 32407, 30214, 57908, 31149, 37813, 32407, 42363, 31900, 30214, 32004, 34573, 2544, 29943, 32653, 31033, 31300, 30684, 30267, 32407, 42363, 31900, 30210, 35544, 35045, 31272, 32846, 50149, 32911, 32711, 30267, 13, 2, 1, 4007, 22137, 29901, 29871, 32406, 30313, 29901, 29871, 30690, 33664, 32537, 2544, 29943, 42150, 34568, 54696, 43788, 30346, 30744, 33201, 29892, 29871, 32406, 30313, 29901, 29871, 33707, 32009, 2544, 29943, 42150, 34568, 54696, 39738, 30767, 29892, 29871, 32473, 33606, 29901, 29871, 34573, 2544, 29943, 29892, 29871, 32406, 30313, 29901, 29871, 32846, 50149, 13, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [1, 12968, 29901, 29871, 38857, 32949, 33304, 30214, 40339, 38276, 32471, 53824, 33304, 32135, 33690, 32066, 52072, 32513, 30267, 13, 33304, 29901, 29871, 30690, 33664, 32537, 2544, 29943, 42150, 34568, 54696, 43788, 30346, 30744, 33201, 57884, 33707, 32009, 2544, 29943, 42150, 34568, 54696, 39738, 30767, 32471, 32400, 32214, 32131, 32193, 33304, 30214, 33404, 42363, 32154, 36280, 32407, 30214, 57908, 31149, 37813, 32407, 42363, 31900, 30214, 32004, 34573, 2544, 29943, 32653, 31033, 31300, 30684, 30267, 32407, 42363, 31900, 30210, 35544, 35045, 31272, 32846, 50149, 32911, 32711, 30267, 13, 2, 1, 4007, 22137, 29901, 29871, 32406, 30313, 29901, 29871, 30690, 33664, 32537, 2544, 29943, 42150, 34568, 54696, 43788, 30346, 30744, 33201, 29892, 29871, 32406, 30313, 29901, 29871, 33707, 32009, 2544, 29943, 42150, 34568, 54696, 39738, 30767, 29892, 29871, 32473, 33606, 29901, 29871, 34573, 2544, 29943, 29892, 29871, 32406, 30313, 29901, 29871, 32846, 50149, 13, 2]}
Caching indices mapping at /home/asus/intelligent-test/lora/output/dataset_cache/csv/default-b8b142b6dd81b35c/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-cc69ef9bbd896b3d.arrow
03/21/2024 20:46:07 - INFO - datasets.arrow_dataset - Caching indices mapping at /home/asus/intelligent-test/lora/output/dataset_cache/csv/default-b8b142b6dd81b35c/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-cc69ef9bbd896b3d.arrow
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
  warnings.warn(
[INFO|trainer.py:601] 2024-03-21 20:46:08,043 >> Using auto half precision backend
[INFO|trainer.py:1812] 2024-03-21 20:46:08,284 >> ***** Running training *****
[INFO|trainer.py:1813] 2024-03-21 20:46:08,284 >>   Num examples = 44,005
[INFO|trainer.py:1814] 2024-03-21 20:46:08,284 >>   Num Epochs = 10
[INFO|trainer.py:1815] 2024-03-21 20:46:08,284 >>   Instantaneous batch size per device = 1
[INFO|trainer.py:1818] 2024-03-21 20:46:08,284 >>   Total train batch size (w. parallel, distributed & accumulation) = 8
[INFO|trainer.py:1819] 2024-03-21 20:46:08,284 >>   Gradient Accumulation steps = 8
[INFO|trainer.py:1820] 2024-03-21 20:46:08,284 >>   Total optimization steps = 55,000
[INFO|trainer.py:1821] 2024-03-21 20:46:08,287 >>   Number of trainable parameters = 19,988,480
03/21/2024 20:46:08 - WARNING - transformers_modules.Atom-7B.model_atom - `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
03/21/2024 20:46:08 - WARNING - transformers_modules.Atom-7B.model_atom - The input hidden states seems to be silently casted in float32, this might be related to the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in torch.float16.
[INFO|trainer.py:3376] 2024-03-21 20:54:34,674 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-21 20:54:34,674 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-21 20:54:34,674 >>   Batch size = 1
[WARNING|trainer.py:2492] 2024-03-21 20:56:42,869 >> Checkpoint destination directory ./output/checkpoint-100 already exists and is non-empty. Saving will proceed but saved results may be invalid.
[INFO|trainer.py:3067] 2024-03-21 20:56:42,869 >> Saving model checkpoint to ./output/checkpoint-100
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-21 20:56:43,027 >> tokenizer config file saved in ./output/checkpoint-100/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-21 20:56:43,027 >> Special tokens file saved in ./output/checkpoint-100/special_tokens_map.json
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-21 21:05:16,731 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-21 21:05:16,732 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-21 21:05:16,732 >>   Batch size = 1
[WARNING|trainer.py:2492] 2024-03-21 21:07:24,741 >> Checkpoint destination directory ./output/checkpoint-200 already exists and is non-empty. Saving will proceed but saved results may be invalid.
[INFO|trainer.py:3067] 2024-03-21 21:07:24,741 >> Saving model checkpoint to ./output/checkpoint-200
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-21 21:07:24,891 >> tokenizer config file saved in ./output/checkpoint-200/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-21 21:07:24,892 >> Special tokens file saved in ./output/checkpoint-200/special_tokens_map.json
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-21 21:15:52,294 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-21 21:15:52,294 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-21 21:15:52,294 >>   Batch size = 1
[WARNING|trainer.py:2492] 2024-03-21 21:18:00,184 >> Checkpoint destination directory ./output/checkpoint-300 already exists and is non-empty. Saving will proceed but saved results may be invalid.
[INFO|trainer.py:3067] 2024-03-21 21:18:00,185 >> Saving model checkpoint to ./output/checkpoint-300
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-21 21:18:00,326 >> tokenizer config file saved in ./output/checkpoint-300/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-21 21:18:00,327 >> Special tokens file saved in ./output/checkpoint-300/special_tokens_map.json
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-21 21:26:30,556 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-21 21:26:30,556 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-21 21:26:30,556 >>   Batch size = 1
[WARNING|trainer.py:2492] 2024-03-21 21:28:38,487 >> Checkpoint destination directory ./output/checkpoint-400 already exists and is non-empty. Saving will proceed but saved results may be invalid.
[INFO|trainer.py:3067] 2024-03-21 21:28:38,487 >> Saving model checkpoint to ./output/checkpoint-400
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-21 21:28:38,758 >> tokenizer config file saved in ./output/checkpoint-400/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-21 21:28:38,758 >> Special tokens file saved in ./output/checkpoint-400/special_tokens_map.json
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
********************on step end call back********************
Step 10 finish
{'loss': 2.6881, 'grad_norm': 1.5933634042739868, 'learning_rate': 2.5e-06, 'epoch': 0.0}
********************on step end call back********************
Step 20 finish
{'loss': 2.5713, 'grad_norm': 1.7091033458709717, 'learning_rate': 5e-06, 'epoch': 0.0}
********************on step end call back********************
Step 30 finish
{'loss': 2.6596, 'grad_norm': 2.0622992515563965, 'learning_rate': 7.5e-06, 'epoch': 0.01}
********************on step end call back********************
Step 40 finish
{'loss': 2.5415, 'grad_norm': 1.382068157196045, 'learning_rate': 1e-05, 'epoch': 0.01}
********************on step end call back********************
Step 50 finish
{'loss': 2.3282, 'grad_norm': 1.4908679723739624, 'learning_rate': 1.25e-05, 'epoch': 0.01}
********************on step end call back********************
Step 60 finish
{'loss': 2.0502, 'grad_norm': 1.3395971059799194, 'learning_rate': 1.5e-05, 'epoch': 0.01}
********************on step end call back********************
Step 70 finish
{'loss': 1.831, 'grad_norm': 0.9188601970672607, 'learning_rate': 1.75e-05, 'epoch': 0.01}
********************on step end call back********************
Step 80 finish
{'loss': 1.7402, 'grad_norm': 1.4681968688964844, 'learning_rate': 2e-05, 'epoch': 0.01}
********************on step end call back********************
Step 90 finish
{'loss': 1.6749, 'grad_norm': 1.5083723068237305, 'learning_rate': 2.25e-05, 'epoch': 0.02}
********************on step end call back********************
Step 100 finish
{'loss': 1.5425, 'grad_norm': 1.8574849367141724, 'learning_rate': 2.5e-05, 'epoch': 0.02}
{'eval_loss': 1.1228742599487305, 'eval_accuracy': 0.8125, 'eval_runtime': 128.1948, 'eval_samples_per_second': 4.977, 'eval_steps_per_second': 4.977, 'epoch': 0.02}
********************save call back********************
********************on step end call back********************
Step 110 finish
{'loss': 1.4759, 'grad_norm': 1.7271842956542969, 'learning_rate': 2.7500000000000004e-05, 'epoch': 0.02}
********************on step end call back********************
Step 120 finish
{'loss': 1.4023, 'grad_norm': 0.9909939765930176, 'learning_rate': 3e-05, 'epoch': 0.02}
********************on step end call back********************
Step 130 finish
{'loss': 1.3138, 'grad_norm': 1.011919379234314, 'learning_rate': 3.2500000000000004e-05, 'epoch': 0.02}
********************on step end call back********************
Step 140 finish
{'loss': 1.2653, 'grad_norm': 1.3903332948684692, 'learning_rate': 3.5e-05, 'epoch': 0.03}
********************on step end call back********************
Step 150 finish
{'loss': 1.302, 'grad_norm': 0.9955399036407471, 'learning_rate': 3.7500000000000003e-05, 'epoch': 0.03}
********************on step end call back********************
Step 160 finish
{'loss': 1.3406, 'grad_norm': 1.1826692819595337, 'learning_rate': 4e-05, 'epoch': 0.03}
********************on step end call back********************
Step 170 finish
{'loss': 1.263, 'grad_norm': 1.1507214307785034, 'learning_rate': 4.25e-05, 'epoch': 0.03}
********************on step end call back********************
Step 180 finish
{'loss': 1.2393, 'grad_norm': 1.7683024406433105, 'learning_rate': 4.5e-05, 'epoch': 0.03}
********************on step end call back********************
Step 190 finish
{'loss': 1.1757, 'grad_norm': 0.9971316456794739, 'learning_rate': 4.75e-05, 'epoch': 0.03}
********************on step end call back********************
Step 200 finish
{'loss': 1.2559, 'grad_norm': 1.1539263725280762, 'learning_rate': 5e-05, 'epoch': 0.04}
{'eval_loss': 0.8577756285667419, 'eval_accuracy': 0.84375, 'eval_runtime': 128.0089, 'eval_samples_per_second': 4.984, 'eval_steps_per_second': 4.984, 'epoch': 0.04}
********************save call back********************
********************on step end call back********************
Step 210 finish
{'loss': 1.1783, 'grad_norm': 1.154973030090332, 'learning_rate': 5.25e-05, 'epoch': 0.04}
********************on step end call back********************
Step 220 finish
{'loss': 1.215, 'grad_norm': 1.2729926109313965, 'learning_rate': 5.500000000000001e-05, 'epoch': 0.04}
********************on step end call back********************
Step 230 finish
{'loss': 1.1959, 'grad_norm': 1.1793599128723145, 'learning_rate': 5.7499999999999995e-05, 'epoch': 0.04}
********************on step end call back********************
Step 240 finish
{'loss': 1.2223, 'grad_norm': 1.1909912824630737, 'learning_rate': 6e-05, 'epoch': 0.04}
********************on step end call back********************
Step 250 finish
{'loss': 1.1733, 'grad_norm': 1.2296215295791626, 'learning_rate': 6.25e-05, 'epoch': 0.05}
********************on step end call back********************
Step 260 finish
{'loss': 1.2022, 'grad_norm': 1.124962568283081, 'learning_rate': 6.500000000000001e-05, 'epoch': 0.05}
********************on step end call back********************
Step 270 finish
{'loss': 1.1121, 'grad_norm': 1.412685513496399, 'learning_rate': 6.750000000000001e-05, 'epoch': 0.05}
********************on step end call back********************
Step 280 finish
{'loss': 1.2526, 'grad_norm': 1.4993046522140503, 'learning_rate': 7e-05, 'epoch': 0.05}
********************on step end call back********************
Step 290 finish
{'loss': 1.1419, 'grad_norm': 1.3525892496109009, 'learning_rate': 7.25e-05, 'epoch': 0.05}
********************on step end call back********************
Step 300 finish
{'loss': 1.201, 'grad_norm': 1.4669052362442017, 'learning_rate': 7.500000000000001e-05, 'epoch': 0.05}
{'eval_loss': 0.7600902915000916, 'eval_accuracy': 0.8541666666666666, 'eval_runtime': 127.8898, 'eval_samples_per_second': 4.989, 'eval_steps_per_second': 4.989, 'epoch': 0.05}
********************save call back********************
********************on step end call back********************
Step 310 finish
{'loss': 1.2314, 'grad_norm': 1.3426185846328735, 'learning_rate': 7.75e-05, 'epoch': 0.06}
********************on step end call back********************
Step 320 finish
{'loss': 1.1147, 'grad_norm': 1.2683342695236206, 'learning_rate': 8e-05, 'epoch': 0.06}
********************on step end call back********************
Step 330 finish
{'loss': 1.0872, 'grad_norm': 1.407589316368103, 'learning_rate': 8.25e-05, 'epoch': 0.06}
********************on step end call back********************
Step 340 finish
{'loss': 1.0419, 'grad_norm': 1.2115098237991333, 'learning_rate': 8.5e-05, 'epoch': 0.06}
********************on step end call back********************
Step 350 finish
{'loss': 1.1413, 'grad_norm': 0.9931526184082031, 'learning_rate': 8.75e-05, 'epoch': 0.06}
********************on step end call back********************
Step 360 finish
{'loss': 1.0733, 'grad_norm': 1.2118418216705322, 'learning_rate': 9e-05, 'epoch': 0.07}
********************on step end call back********************
Step 370 finish
{'loss': 1.1696, 'grad_norm': 1.401659369468689, 'learning_rate': 9.250000000000001e-05, 'epoch': 0.07}
********************on step end call back********************
Step 380 finish
{'loss': 1.0897, 'grad_norm': 1.3471466302871704, 'learning_rate': 9.5e-05, 'epoch': 0.07}
********************on step end call back********************
Step 390 finish
{'loss': 1.1077, 'grad_norm': 1.5654160976409912, 'learning_rate': 9.75e-05, 'epoch': 0.07}
********************on step end call back********************
Step 400 finish
{'loss': 1.0423, 'grad_norm': 1.3647671937942505, 'learning_rate': 0.0001, 'epoch': 0.07}
{'eval_loss': 0.7208843231201172, 'eval_accuracy': 0.8541666666666666, 'eval_runtime': 127.9308, 'eval_samples_per_second': 4.987, 'eval_steps_per_second': 4.987, 'epoch': 0.07}
********************save call back********************
********************on step end call back********************
Step 410 finish
{'loss': 1.1207, 'grad_norm': 1.5235310792922974, 'learning_rate': 9.998168498168498e-05, 'epoch': 0.07}
********************on step end call back********************
Step 420 finish
{'loss': 1.055, 'grad_norm': 1.357619285583496, 'learning_rate': 9.996336996336996e-05, 'epoch': 0.08}
********************on step end call back********************[INFO|trainer.py:3376] 2024-03-21 21:37:11,266 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-21 21:37:11,266 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-21 21:37:11,266 >>   Batch size = 1
[WARNING|trainer.py:2492] 2024-03-21 21:39:19,302 >> Checkpoint destination directory ./output/checkpoint-500 already exists and is non-empty. Saving will proceed but saved results may be invalid.
[INFO|trainer.py:3067] 2024-03-21 21:39:19,302 >> Saving model checkpoint to ./output/checkpoint-500
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-21 21:39:19,616 >> tokenizer config file saved in ./output/checkpoint-500/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-21 21:39:19,616 >> Special tokens file saved in ./output/checkpoint-500/special_tokens_map.json
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-21 21:47:54,824 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-21 21:47:54,824 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-21 21:47:54,824 >>   Batch size = 1
[WARNING|trainer.py:2492] 2024-03-21 21:50:02,802 >> Checkpoint destination directory ./output/checkpoint-600 already exists and is non-empty. Saving will proceed but saved results may be invalid.
[INFO|trainer.py:3067] 2024-03-21 21:50:02,802 >> Saving model checkpoint to ./output/checkpoint-600
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-21 21:50:03,113 >> tokenizer config file saved in ./output/checkpoint-600/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-21 21:50:03,113 >> Special tokens file saved in ./output/checkpoint-600/special_tokens_map.json
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-21 21:58:36,203 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-21 21:58:36,203 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-21 21:58:36,203 >>   Batch size = 1
[WARNING|trainer.py:2492] 2024-03-21 22:00:44,751 >> Checkpoint destination directory ./output/checkpoint-700 already exists and is non-empty. Saving will proceed but saved results may be invalid.
[INFO|trainer.py:3067] 2024-03-21 22:00:44,752 >> Saving model checkpoint to ./output/checkpoint-700
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-21 22:00:45,051 >> tokenizer config file saved in ./output/checkpoint-700/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-21 22:00:45,051 >> Special tokens file saved in ./output/checkpoint-700/special_tokens_map.json
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-21 22:09:11,323 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-21 22:09:11,323 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-21 22:09:11,323 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-21 22:11:19,920 >> Saving model checkpoint to ./output/tmp-checkpoint-800
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-21 22:11:20,294 >> tokenizer config file saved in ./output/tmp-checkpoint-800/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-21 22:11:20,294 >> Special tokens file saved in ./output/tmp-checkpoint-800/special_tokens_map.json
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(

Step 430 finish
{'loss': 1.0478, 'grad_norm': 1.3923250436782837, 'learning_rate': 9.994505494505495e-05, 'epoch': 0.08}
********************on step end call back********************
Step 440 finish
{'loss': 1.037, 'grad_norm': 1.6555299758911133, 'learning_rate': 9.992673992673993e-05, 'epoch': 0.08}
********************on step end call back********************
Step 450 finish
{'loss': 0.9695, 'grad_norm': 1.5543514490127563, 'learning_rate': 9.990842490842491e-05, 'epoch': 0.08}
********************on step end call back********************
Step 460 finish
{'loss': 1.0014, 'grad_norm': 1.463805913925171, 'learning_rate': 9.989010989010989e-05, 'epoch': 0.08}
********************on step end call back********************
Step 470 finish
{'loss': 1.0316, 'grad_norm': 1.3850724697113037, 'learning_rate': 9.987179487179488e-05, 'epoch': 0.09}
********************on step end call back********************
Step 480 finish
{'loss': 0.9766, 'grad_norm': 1.3891358375549316, 'learning_rate': 9.985347985347986e-05, 'epoch': 0.09}
********************on step end call back********************
Step 490 finish
{'loss': 1.0678, 'grad_norm': 1.3856712579727173, 'learning_rate': 9.983516483516484e-05, 'epoch': 0.09}
********************on step end call back********************
Step 500 finish
{'loss': 1.0225, 'grad_norm': 1.4855632781982422, 'learning_rate': 9.981684981684982e-05, 'epoch': 0.09}
{'eval_loss': 0.6960034966468811, 'eval_accuracy': 0.8645833333333334, 'eval_runtime': 128.0353, 'eval_samples_per_second': 4.983, 'eval_steps_per_second': 4.983, 'epoch': 0.09}
********************save call back********************
********************on step end call back********************
Step 510 finish
{'loss': 1.0303, 'grad_norm': 1.5271496772766113, 'learning_rate': 9.97985347985348e-05, 'epoch': 0.09}
********************on step end call back********************
Step 520 finish
{'loss': 1.0425, 'grad_norm': 1.363293170928955, 'learning_rate': 9.978021978021979e-05, 'epoch': 0.09}
********************on step end call back********************
Step 530 finish
{'loss': 1.0795, 'grad_norm': 1.1336166858673096, 'learning_rate': 9.976190476190477e-05, 'epoch': 0.1}
********************on step end call back********************
Step 540 finish
{'loss': 1.002, 'grad_norm': 1.2903295755386353, 'learning_rate': 9.974358974358975e-05, 'epoch': 0.1}
********************on step end call back********************
Step 550 finish
{'loss': 1.0399, 'grad_norm': 1.5737348794937134, 'learning_rate': 9.972527472527473e-05, 'epoch': 0.1}
********************on step end call back********************
Step 560 finish
{'loss': 0.995, 'grad_norm': 1.5680325031280518, 'learning_rate': 9.970695970695972e-05, 'epoch': 0.1}
********************on step end call back********************
Step 570 finish
{'loss': 0.9955, 'grad_norm': 1.7336722612380981, 'learning_rate': 9.96886446886447e-05, 'epoch': 0.1}
********************on step end call back********************
Step 580 finish
{'loss': 1.0275, 'grad_norm': 1.4176838397979736, 'learning_rate': 9.967032967032968e-05, 'epoch': 0.11}
********************on step end call back********************
Step 590 finish
{'loss': 0.9985, 'grad_norm': 2.0261597633361816, 'learning_rate': 9.965201465201466e-05, 'epoch': 0.11}
********************on step end call back********************
Step 600 finish
{'loss': 1.0247, 'grad_norm': 1.597387433052063, 'learning_rate': 9.963369963369963e-05, 'epoch': 0.11}
{'eval_loss': 0.643058717250824, 'eval_accuracy': 0.8854166666666666, 'eval_runtime': 127.9776, 'eval_samples_per_second': 4.985, 'eval_steps_per_second': 4.985, 'epoch': 0.11}
********************save call back********************
********************on step end call back********************
Step 610 finish
{'loss': 0.9555, 'grad_norm': 1.3920750617980957, 'learning_rate': 9.961538461538463e-05, 'epoch': 0.11}
********************on step end call back********************
Step 620 finish
{'loss': 1.0099, 'grad_norm': 1.4567766189575195, 'learning_rate': 9.95970695970696e-05, 'epoch': 0.11}
********************on step end call back********************
Step 630 finish
{'loss': 0.976, 'grad_norm': 1.4304451942443848, 'learning_rate': 9.957875457875458e-05, 'epoch': 0.11}
********************on step end call back********************
Step 640 finish
{'loss': 1.0226, 'grad_norm': 1.5040189027786255, 'learning_rate': 9.956043956043956e-05, 'epoch': 0.12}
********************on step end call back********************
Step 650 finish
{'loss': 0.9341, 'grad_norm': 1.352020502090454, 'learning_rate': 9.954212454212456e-05, 'epoch': 0.12}
********************on step end call back********************
Step 660 finish
{'loss': 1.039, 'grad_norm': 1.7528554201126099, 'learning_rate': 9.952380952380953e-05, 'epoch': 0.12}
********************on step end call back********************
Step 670 finish
{'loss': 0.9561, 'grad_norm': 1.71627676486969, 'learning_rate': 9.950549450549451e-05, 'epoch': 0.12}
********************on step end call back********************
Step 680 finish
{'loss': 0.9899, 'grad_norm': 1.5163685083389282, 'learning_rate': 9.948717948717949e-05, 'epoch': 0.12}
********************on step end call back********************
Step 690 finish
{'loss': 0.939, 'grad_norm': 1.4067462682724, 'learning_rate': 9.946886446886447e-05, 'epoch': 0.13}
********************on step end call back********************
Step 700 finish
{'loss': 0.966, 'grad_norm': 1.2994194030761719, 'learning_rate': 9.945054945054946e-05, 'epoch': 0.13}
{'eval_loss': 0.6201809644699097, 'eval_accuracy': 0.8854166666666666, 'eval_runtime': 128.5476, 'eval_samples_per_second': 4.963, 'eval_steps_per_second': 4.963, 'epoch': 0.13}
********************save call back********************
********************on step end call back********************
Step 710 finish
{'loss': 0.907, 'grad_norm': 1.4708882570266724, 'learning_rate': 9.943223443223444e-05, 'epoch': 0.13}
********************on step end call back********************
Step 720 finish
{'loss': 0.9456, 'grad_norm': 1.5397419929504395, 'learning_rate': 9.941391941391942e-05, 'epoch': 0.13}
********************on step end call back********************
Step 730 finish
{'loss': 0.8837, 'grad_norm': 1.4833104610443115, 'learning_rate': 9.93956043956044e-05, 'epoch': 0.13}
********************on step end call back********************
Step 740 finish
{'loss': 0.9827, 'grad_norm': 1.254668116569519, 'learning_rate': 9.937728937728939e-05, 'epoch': 0.13}
********************on step end call back********************
Step 750 finish
{'loss': 0.8685, 'grad_norm': 1.476963996887207, 'learning_rate': 9.935897435897437e-05, 'epoch': 0.14}
********************on step end call back********************
Step 760 finish
{'loss': 0.914, 'grad_norm': 1.44670832157135, 'learning_rate': 9.934065934065935e-05, 'epoch': 0.14}
********************on step end call back********************
Step 770 finish
{'loss': 0.8578, 'grad_norm': 1.564819574356079, 'learning_rate': 9.932234432234433e-05, 'epoch': 0.14}
********************on step end call back********************
Step 780 finish
{'loss': 0.9314, 'grad_norm': 1.4396312236785889, 'learning_rate': 9.93040293040293e-05, 'epoch': 0.14}
********************on step end call back********************
Step 790 finish
{'loss': 0.9153, 'grad_norm': 1.518792986869812, 'learning_rate': 9.92857142857143e-05, 'epoch': 0.14}
********************on step end call back********************
Step 800 finish
{'loss': 0.8594, 'grad_norm': 1.372074007987976, 'learning_rate': 9.926739926739928e-05, 'epoch': 0.15}
{'eval_loss': 0.6087630987167358, 'eval_accuracy': 0.84375, 'eval_runtime': 128.5962, 'eval_samples_per_second': 4.961, 'eval_steps_per_second': 4.961, 'epoch': 0.15}
********************save call back********************
********************on step end call back********************
Step 810 finish
{'loss': 0.8644, 'grad_norm': 1.0809898376464844, 'learning_rate': 9.924908424908426e-05, 'epoch': 0.15}
********************on step end call back********************
Step 820 finish
{'loss': 0.9186, 'grad_norm': 1.197169542312622, 'learning_rate': 9.923076923076923e-05, 'epoch': 0.15}
[INFO|trainer.py:3376] 2024-03-21 22:19:57,918 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-21 22:19:57,919 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-21 22:19:57,919 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-21 22:22:06,897 >> Saving model checkpoint to ./output/tmp-checkpoint-900
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-21 22:22:07,037 >> tokenizer config file saved in ./output/tmp-checkpoint-900/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-21 22:22:07,037 >> Special tokens file saved in ./output/tmp-checkpoint-900/special_tokens_map.json
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-21 22:30:53,297 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-21 22:30:53,298 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-21 22:30:53,298 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-21 22:33:04,438 >> Saving model checkpoint to ./output/tmp-checkpoint-1000
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-21 22:33:04,594 >> tokenizer config file saved in ./output/tmp-checkpoint-1000/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-21 22:33:04,594 >> Special tokens file saved in ./output/tmp-checkpoint-1000/special_tokens_map.json
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-21 22:41:48,628 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-21 22:41:48,628 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-21 22:41:48,628 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-21 22:43:57,696 >> Saving model checkpoint to ./output/tmp-checkpoint-1100
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-21 22:43:57,843 >> tokenizer config file saved in ./output/tmp-checkpoint-1100/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-21 22:43:57,843 >> Special tokens file saved in ./output/tmp-checkpoint-1100/special_tokens_map.json
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-21 22:52:37,587 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-21 22:52:37,587 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-21 22:52:37,587 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-21 22:54:47,857 >> Saving model checkpoint to ./output/tmp-checkpoint-1200
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-21 22:54:47,996 >> tokenizer config file saved in ./output/tmp-checkpoint-1200/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-21 22:54:47,996 >> Special tokens file saved in ./output/tmp-checkpoint-1200/special_tokens_map.json
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
********************on step end call back********************
Step 830 finish
{'loss': 0.9389, 'grad_norm': 1.2552655935287476, 'learning_rate': 9.921245421245421e-05, 'epoch': 0.15}
********************on step end call back********************
Step 840 finish
{'loss': 0.9324, 'grad_norm': 1.5459365844726562, 'learning_rate': 9.91941391941392e-05, 'epoch': 0.15}
********************on step end call back********************
Step 850 finish
{'loss': 0.8898, 'grad_norm': 1.3512688875198364, 'learning_rate': 9.917582417582418e-05, 'epoch': 0.15}
********************on step end call back********************
Step 860 finish
{'loss': 0.891, 'grad_norm': 1.6140403747558594, 'learning_rate': 9.915750915750916e-05, 'epoch': 0.16}
********************on step end call back********************
Step 870 finish
{'loss': 0.8524, 'grad_norm': 1.1902592182159424, 'learning_rate': 9.913919413919414e-05, 'epoch': 0.16}
********************on step end call back********************
Step 880 finish
{'loss': 0.9058, 'grad_norm': 1.6506656408309937, 'learning_rate': 9.912087912087913e-05, 'epoch': 0.16}
********************on step end call back********************
Step 890 finish
{'loss': 0.9212, 'grad_norm': 1.574151635169983, 'learning_rate': 9.910256410256411e-05, 'epoch': 0.16}
********************on step end call back********************
Step 900 finish
{'loss': 0.8609, 'grad_norm': 1.3672194480895996, 'learning_rate': 9.908424908424909e-05, 'epoch': 0.16}
{'eval_loss': 0.5652119517326355, 'eval_accuracy': 0.8645833333333334, 'eval_runtime': 128.9773, 'eval_samples_per_second': 4.947, 'eval_steps_per_second': 4.947, 'epoch': 0.16}
********************save call back********************
********************on step end call back********************
Step 910 finish
{'loss': 0.7625, 'grad_norm': 1.5030694007873535, 'learning_rate': 9.906593406593407e-05, 'epoch': 0.17}
********************on step end call back********************
Step 920 finish
{'loss': 0.8494, 'grad_norm': 1.2627875804901123, 'learning_rate': 9.904761904761905e-05, 'epoch': 0.17}
********************on step end call back********************
Step 930 finish
{'loss': 0.8405, 'grad_norm': 1.3702443838119507, 'learning_rate': 9.902930402930404e-05, 'epoch': 0.17}
********************on step end call back********************
Step 940 finish
{'loss': 0.8405, 'grad_norm': 1.5472091436386108, 'learning_rate': 9.901098901098902e-05, 'epoch': 0.17}
********************on step end call back********************
Step 950 finish
{'loss': 0.8539, 'grad_norm': 1.7554380893707275, 'learning_rate': 9.8992673992674e-05, 'epoch': 0.17}
********************on step end call back********************
Step 960 finish
{'loss': 0.9085, 'grad_norm': 1.6446163654327393, 'learning_rate': 9.897435897435898e-05, 'epoch': 0.17}
********************on step end call back********************
Step 970 finish
{'loss': 0.8152, 'grad_norm': 1.471215844154358, 'learning_rate': 9.895604395604397e-05, 'epoch': 0.18}
********************on step end call back********************
Step 980 finish
{'loss': 0.8859, 'grad_norm': 1.636286735534668, 'learning_rate': 9.893772893772895e-05, 'epoch': 0.18}
********************on step end call back********************
Step 990 finish
{'loss': 0.8983, 'grad_norm': 1.5395270586013794, 'learning_rate': 9.891941391941393e-05, 'epoch': 0.18}
********************on step end call back********************
Step 1000 finish
{'loss': 0.8251, 'grad_norm': 1.6998085975646973, 'learning_rate': 9.89010989010989e-05, 'epoch': 0.18}
{'eval_loss': 0.5391366481781006, 'eval_accuracy': 0.875, 'eval_runtime': 131.14, 'eval_samples_per_second': 4.865, 'eval_steps_per_second': 4.865, 'epoch': 0.18}
********************save call back********************
********************on step end call back********************
Step 1010 finish
{'loss': 0.8811, 'grad_norm': 1.6753805875778198, 'learning_rate': 9.888278388278389e-05, 'epoch': 0.18}
********************on step end call back********************
Step 1020 finish
{'loss': 0.8689, 'grad_norm': 2.0472962856292725, 'learning_rate': 9.886446886446888e-05, 'epoch': 0.19}
********************on step end call back********************
Step 1030 finish
{'loss': 0.923, 'grad_norm': 1.6155749559402466, 'learning_rate': 9.884615384615386e-05, 'epoch': 0.19}
********************on step end call back********************
Step 1040 finish
{'loss': 0.8583, 'grad_norm': 1.541020154953003, 'learning_rate': 9.882783882783884e-05, 'epoch': 0.19}
********************on step end call back********************
Step 1050 finish
{'loss': 0.7975, 'grad_norm': 1.5423195362091064, 'learning_rate': 9.880952380952381e-05, 'epoch': 0.19}
********************on step end call back********************
Step 1060 finish
{'loss': 0.8625, 'grad_norm': 1.8794487714767456, 'learning_rate': 9.87912087912088e-05, 'epoch': 0.19}
********************on step end call back********************
Step 1070 finish
{'loss': 0.8403, 'grad_norm': 1.4111977815628052, 'learning_rate': 9.877289377289378e-05, 'epoch': 0.19}
********************on step end call back********************
Step 1080 finish
{'loss': 0.8699, 'grad_norm': 1.5125612020492554, 'learning_rate': 9.875457875457876e-05, 'epoch': 0.2}
********************on step end call back********************
Step 1090 finish
{'loss': 0.8864, 'grad_norm': 1.7130913734436035, 'learning_rate': 9.873626373626374e-05, 'epoch': 0.2}
********************on step end call back********************
Step 1100 finish
{'loss': 0.8597, 'grad_norm': 1.2374354600906372, 'learning_rate': 9.871794871794872e-05, 'epoch': 0.2}
{'eval_loss': 0.5241586565971375, 'eval_accuracy': 0.8645833333333334, 'eval_runtime': 129.0674, 'eval_samples_per_second': 4.943, 'eval_steps_per_second': 4.943, 'epoch': 0.2}
********************save call back********************
********************on step end call back********************
Step 1110 finish
{'loss': 0.7724, 'grad_norm': 1.5494250059127808, 'learning_rate': 9.869963369963371e-05, 'epoch': 0.2}
********************on step end call back********************
Step 1120 finish
{'loss': 0.7695, 'grad_norm': 1.8835569620132446, 'learning_rate': 9.868131868131869e-05, 'epoch': 0.2}
********************on step end call back********************
Step 1130 finish
{'loss': 0.8276, 'grad_norm': 1.4455859661102295, 'learning_rate': 9.866300366300367e-05, 'epoch': 0.21}
********************on step end call back********************
Step 1140 finish
{'loss': 0.8118, 'grad_norm': 1.6360825300216675, 'learning_rate': 9.864468864468865e-05, 'epoch': 0.21}
********************on step end call back********************
Step 1150 finish
{'loss': 0.8206, 'grad_norm': 1.7267180681228638, 'learning_rate': 9.862637362637364e-05, 'epoch': 0.21}
********************on step end call back********************
Step 1160 finish
{'loss': 0.8286, 'grad_norm': 1.7505290508270264, 'learning_rate': 9.860805860805862e-05, 'epoch': 0.21}
********************on step end call back********************
Step 1170 finish
{'loss': 0.807, 'grad_norm': 1.6333855390548706, 'learning_rate': 9.85897435897436e-05, 'epoch': 0.21}
********************on step end call back********************
Step 1180 finish
{'loss': 0.8184, 'grad_norm': 1.5664173364639282, 'learning_rate': 9.857142857142858e-05, 'epoch': 0.21}
********************on step end call back********************
Step 1190 finish
{'loss': 0.8253, 'grad_norm': 1.7031223773956299, 'learning_rate': 9.855311355311356e-05, 'epoch': 0.22}
********************on step end call back********************
Step 1200 finish
{'loss': 0.8238, 'grad_norm': 1.424160122871399, 'learning_rate': 9.853479853479855e-05, 'epoch': 0.22}
{'eval_loss': 0.5150759816169739, 'eval_accuracy': 0.8854166666666666, 'eval_runtime': 130.2685, 'eval_samples_per_second': 4.898, 'eval_steps_per_second': 4.898, 'epoch': 0.22}
********************save call back********************
********************on step end call back********************
Step 1210 finish
{'loss': 0.7054, 'grad_norm': 1.8214850425720215, 'learning_rate': 9.851648351648353e-05, 'epoch': 0.22}
********************on step end call back********************
Step 1220 finish
[INFO|trainer.py:3376] 2024-03-21 23:03:23,962 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-21 23:03:23,962 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-21 23:03:23,962 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-21 23:05:34,114 >> Saving model checkpoint to ./output/tmp-checkpoint-1300
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-21 23:05:34,261 >> tokenizer config file saved in ./output/tmp-checkpoint-1300/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-21 23:05:34,262 >> Special tokens file saved in ./output/tmp-checkpoint-1300/special_tokens_map.json
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-21 23:14:15,835 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-21 23:14:15,836 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-21 23:14:15,836 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-21 23:16:26,090 >> Saving model checkpoint to ./output/tmp-checkpoint-1400
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-21 23:16:26,236 >> tokenizer config file saved in ./output/tmp-checkpoint-1400/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-21 23:16:26,236 >> Special tokens file saved in ./output/tmp-checkpoint-1400/special_tokens_map.json
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-21 23:25:10,087 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-21 23:25:10,087 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-21 23:25:10,087 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-21 23:27:20,401 >> Saving model checkpoint to ./output/tmp-checkpoint-1500
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-21 23:27:20,661 >> tokenizer config file saved in ./output/tmp-checkpoint-1500/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-21 23:27:20,662 >> Special tokens file saved in ./output/tmp-checkpoint-1500/special_tokens_map.json
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-21 23:36:15,072 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-21 23:36:15,072 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-21 23:36:15,072 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-21 23:38:24,956 >> Saving model checkpoint to ./output/tmp-checkpoint-1600
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-21 23:38:25,097 >> tokenizer config file saved in ./output/tmp-checkpoint-1600/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-21 23:38:25,098 >> Special tokens file saved in ./output/tmp-checkpoint-1600/special_tokens_map.json
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
{'loss': 0.8578, 'grad_norm': 1.6332134008407593, 'learning_rate': 9.849816849816851e-05, 'epoch': 0.22}
********************on step end call back********************
Step 1230 finish
{'loss': 0.8083, 'grad_norm': 1.6727532148361206, 'learning_rate': 9.847985347985349e-05, 'epoch': 0.22}
********************on step end call back********************
Step 1240 finish
{'loss': 0.8596, 'grad_norm': 1.9848133325576782, 'learning_rate': 9.846153846153848e-05, 'epoch': 0.23}
********************on step end call back********************
Step 1250 finish
{'loss': 0.7717, 'grad_norm': 1.9233835935592651, 'learning_rate': 9.844322344322346e-05, 'epoch': 0.23}
********************on step end call back********************
Step 1260 finish
{'loss': 0.7954, 'grad_norm': 1.8256772756576538, 'learning_rate': 9.842490842490842e-05, 'epoch': 0.23}
********************on step end call back********************
Step 1270 finish
{'loss': 0.804, 'grad_norm': 1.4232176542282104, 'learning_rate': 9.84065934065934e-05, 'epoch': 0.23}
********************on step end call back********************
Step 1280 finish
{'loss': 0.7704, 'grad_norm': 1.9200059175491333, 'learning_rate': 9.838827838827839e-05, 'epoch': 0.23}
********************on step end call back********************
Step 1290 finish
{'loss': 0.8322, 'grad_norm': 1.730605959892273, 'learning_rate': 9.836996336996337e-05, 'epoch': 0.23}
********************on step end call back********************
Step 1300 finish
{'loss': 0.7784, 'grad_norm': 1.4850245714187622, 'learning_rate': 9.835164835164835e-05, 'epoch': 0.24}
{'eval_loss': 0.4957825839519501, 'eval_accuracy': 0.8645833333333334, 'eval_runtime': 130.1506, 'eval_samples_per_second': 4.902, 'eval_steps_per_second': 4.902, 'epoch': 0.24}
********************save call back********************
********************on step end call back********************
Step 1310 finish
{'loss': 0.7774, 'grad_norm': 1.5476019382476807, 'learning_rate': 9.833333333333333e-05, 'epoch': 0.24}
********************on step end call back********************
Step 1320 finish
{'loss': 0.8154, 'grad_norm': 1.5421693325042725, 'learning_rate': 9.831501831501831e-05, 'epoch': 0.24}
********************on step end call back********************
Step 1330 finish
{'loss': 0.8098, 'grad_norm': 1.9724950790405273, 'learning_rate': 9.82967032967033e-05, 'epoch': 0.24}
********************on step end call back********************
Step 1340 finish
{'loss': 0.8641, 'grad_norm': 1.7422410249710083, 'learning_rate': 9.827838827838828e-05, 'epoch': 0.24}
********************on step end call back********************
Step 1350 finish
{'loss': 0.8042, 'grad_norm': 1.6581635475158691, 'learning_rate': 9.826007326007326e-05, 'epoch': 0.25}
********************on step end call back********************
Step 1360 finish
{'loss': 0.8235, 'grad_norm': 2.007215976715088, 'learning_rate': 9.824175824175824e-05, 'epoch': 0.25}
********************on step end call back********************
Step 1370 finish
{'loss': 0.843, 'grad_norm': 1.4511988162994385, 'learning_rate': 9.822344322344323e-05, 'epoch': 0.25}
********************on step end call back********************
Step 1380 finish
{'loss': 0.7928, 'grad_norm': 1.5451997518539429, 'learning_rate': 9.820512820512821e-05, 'epoch': 0.25}
********************on step end call back********************
Step 1390 finish
{'loss': 0.7539, 'grad_norm': 1.7425252199172974, 'learning_rate': 9.818681318681319e-05, 'epoch': 0.25}
********************on step end call back********************
Step 1400 finish
{'loss': 0.7184, 'grad_norm': 1.4500982761383057, 'learning_rate': 9.816849816849817e-05, 'epoch': 0.25}
{'eval_loss': 0.4788456857204437, 'eval_accuracy': 0.8541666666666666, 'eval_runtime': 130.2531, 'eval_samples_per_second': 4.898, 'eval_steps_per_second': 4.898, 'epoch': 0.25}
********************save call back********************
********************on step end call back********************
Step 1410 finish
{'loss': 0.8688, 'grad_norm': 1.8514931201934814, 'learning_rate': 9.815018315018314e-05, 'epoch': 0.26}
********************on step end call back********************
Step 1420 finish
{'loss': 0.7466, 'grad_norm': 1.554579257965088, 'learning_rate': 9.813186813186814e-05, 'epoch': 0.26}
********************on step end call back********************
Step 1430 finish
{'loss': 0.7475, 'grad_norm': 1.9553064107894897, 'learning_rate': 9.811355311355311e-05, 'epoch': 0.26}
********************on step end call back********************
Step 1440 finish
{'loss': 0.7515, 'grad_norm': 1.3751294612884521, 'learning_rate': 9.80952380952381e-05, 'epoch': 0.26}
********************on step end call back********************
Step 1450 finish
{'loss': 0.8281, 'grad_norm': 1.628180980682373, 'learning_rate': 9.807692307692307e-05, 'epoch': 0.26}
********************on step end call back********************
Step 1460 finish
{'loss': 0.7339, 'grad_norm': 1.8444491624832153, 'learning_rate': 9.805860805860806e-05, 'epoch': 0.27}
********************on step end call back********************
Step 1470 finish
{'loss': 0.807, 'grad_norm': 1.532536506652832, 'learning_rate': 9.804029304029304e-05, 'epoch': 0.27}
********************on step end call back********************
Step 1480 finish
{'loss': 0.852, 'grad_norm': 1.5573660135269165, 'learning_rate': 9.802197802197802e-05, 'epoch': 0.27}
********************on step end call back********************
Step 1490 finish
{'loss': 0.7819, 'grad_norm': 1.2519618272781372, 'learning_rate': 9.8003663003663e-05, 'epoch': 0.27}
********************on step end call back********************
Step 1500 finish
{'loss': 0.7438, 'grad_norm': 1.3996219635009766, 'learning_rate': 9.798534798534798e-05, 'epoch': 0.27}
{'eval_loss': 0.4765193462371826, 'eval_accuracy': 0.8645833333333334, 'eval_runtime': 130.3131, 'eval_samples_per_second': 4.896, 'eval_steps_per_second': 4.896, 'epoch': 0.27}
********************save call back********************
********************on step end call back********************
Step 1510 finish
{'loss': 0.711, 'grad_norm': 1.5385942459106445, 'learning_rate': 9.796703296703297e-05, 'epoch': 0.27}
********************on step end call back********************
Step 1520 finish
{'loss': 0.8768, 'grad_norm': 1.7240067720413208, 'learning_rate': 9.794871794871795e-05, 'epoch': 0.28}
********************on step end call back********************
Step 1530 finish
{'loss': 0.7631, 'grad_norm': 1.6354305744171143, 'learning_rate': 9.793040293040293e-05, 'epoch': 0.28}
********************on step end call back********************
Step 1540 finish
{'loss': 0.7649, 'grad_norm': 1.4665584564208984, 'learning_rate': 9.791208791208791e-05, 'epoch': 0.28}
********************on step end call back********************
Step 1550 finish
{'loss': 0.7689, 'grad_norm': 1.7471059560775757, 'learning_rate': 9.78937728937729e-05, 'epoch': 0.28}
********************on step end call back********************
Step 1560 finish
{'loss': 0.7355, 'grad_norm': 1.603691577911377, 'learning_rate': 9.787545787545788e-05, 'epoch': 0.28}
********************on step end call back********************
Step 1570 finish
{'loss': 0.7428, 'grad_norm': 1.651698350906372, 'learning_rate': 9.785714285714286e-05, 'epoch': 0.29}
********************on step end call back********************
Step 1580 finish
{'loss': 0.7834, 'grad_norm': 1.4722672700881958, 'learning_rate': 9.783882783882784e-05, 'epoch': 0.29}
********************on step end call back********************
Step 1590 finish
{'loss': 0.7829, 'grad_norm': 1.7632564306259155, 'learning_rate': 9.782051282051282e-05, 'epoch': 0.29}
********************on step end call back********************
Step 1600 finish
{'loss': 0.7604, 'grad_norm': 1.9754868745803833, 'learning_rate': 9.780219780219781e-05, 'epoch': 0.29}
{'eval_loss': 0.4580686688423157, 'eval_accuracy': 0.84375, 'eval_runtime': 129.8827, 'eval_samples_per_second': 4.912, 'eval_steps_per_second': 4.912, 'epoch': 0.29}
********************save call back********************
********************on step end call back********************
Step 1610 finish
{'loss': 0.7827, 'grad_norm': 1.2288296222686768, 'learning_rate': 9.778388278388279e-05, 'epoch': 0.29}
[INFO|trainer.py:3376] 2024-03-21 23:47:09,709 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-21 23:47:09,709 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-21 23:47:09,709 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-21 23:49:19,665 >> Saving model checkpoint to ./output/tmp-checkpoint-1700
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-21 23:49:19,810 >> tokenizer config file saved in ./output/tmp-checkpoint-1700/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-21 23:49:19,810 >> Special tokens file saved in ./output/tmp-checkpoint-1700/special_tokens_map.json
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-21 23:58:16,043 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-21 23:58:16,043 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-21 23:58:16,043 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-22 00:00:26,872 >> Saving model checkpoint to ./output/tmp-checkpoint-1800
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-22 00:00:27,017 >> tokenizer config file saved in ./output/tmp-checkpoint-1800/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-22 00:00:27,018 >> Special tokens file saved in ./output/tmp-checkpoint-1800/special_tokens_map.json
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-22 00:09:05,756 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-22 00:09:05,756 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-22 00:09:05,756 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-22 00:11:15,968 >> Saving model checkpoint to ./output/tmp-checkpoint-1900
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-22 00:11:16,144 >> tokenizer config file saved in ./output/tmp-checkpoint-1900/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-22 00:11:16,144 >> Special tokens file saved in ./output/tmp-checkpoint-1900/special_tokens_map.json
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-22 00:19:55,506 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-22 00:19:55,506 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-22 00:19:55,506 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-22 00:22:05,717 >> Saving model checkpoint to ./output/tmp-checkpoint-2000
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-22 00:22:05,858 >> tokenizer config file saved in ./output/tmp-checkpoint-2000/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-22 00:22:05,858 >> Special tokens file saved in ./output/tmp-checkpoint-2000/special_tokens_map.json
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
********************on step end call back********************
Step 1620 finish
{'loss': 0.7307, 'grad_norm': 1.7500392198562622, 'learning_rate': 9.776556776556777e-05, 'epoch': 0.29}
********************on step end call back********************
Step 1630 finish
{'loss': 0.7709, 'grad_norm': 1.3134777545928955, 'learning_rate': 9.774725274725274e-05, 'epoch': 0.3}
********************on step end call back********************
Step 1640 finish
{'loss': 0.745, 'grad_norm': 1.301792860031128, 'learning_rate': 9.772893772893774e-05, 'epoch': 0.3}
********************on step end call back********************
Step 1650 finish
{'loss': 0.7795, 'grad_norm': 1.3659013509750366, 'learning_rate': 9.771062271062272e-05, 'epoch': 0.3}
********************on step end call back********************
Step 1660 finish
{'loss': 0.7963, 'grad_norm': 1.4253778457641602, 'learning_rate': 9.76923076923077e-05, 'epoch': 0.3}
********************on step end call back********************
Step 1670 finish
{'loss': 0.7064, 'grad_norm': 1.7368320226669312, 'learning_rate': 9.767399267399267e-05, 'epoch': 0.3}
********************on step end call back********************
Step 1680 finish
{'loss': 0.7422, 'grad_norm': 1.3425185680389404, 'learning_rate': 9.765567765567765e-05, 'epoch': 0.31}
********************on step end call back********************
Step 1690 finish
{'loss': 0.6842, 'grad_norm': 1.2836540937423706, 'learning_rate': 9.763736263736264e-05, 'epoch': 0.31}
********************on step end call back********************
Step 1700 finish
{'loss': 0.7035, 'grad_norm': 1.9614053964614868, 'learning_rate': 9.761904761904762e-05, 'epoch': 0.31}
{'eval_loss': 0.4491974115371704, 'eval_accuracy': 0.8645833333333334, 'eval_runtime': 129.9554, 'eval_samples_per_second': 4.909, 'eval_steps_per_second': 4.909, 'epoch': 0.31}
********************save call back********************
********************on step end call back********************
Step 1710 finish
{'loss': 0.6216, 'grad_norm': 1.8280837535858154, 'learning_rate': 9.76007326007326e-05, 'epoch': 0.31}
********************on step end call back********************
Step 1720 finish
{'loss': 0.7592, 'grad_norm': 1.4160574674606323, 'learning_rate': 9.758241758241758e-05, 'epoch': 0.31}
********************on step end call back********************
Step 1730 finish
{'loss': 0.7517, 'grad_norm': 1.5128064155578613, 'learning_rate': 9.756410256410257e-05, 'epoch': 0.31}
********************on step end call back********************
Step 1740 finish
{'loss': 0.7488, 'grad_norm': 2.3279497623443604, 'learning_rate': 9.754578754578755e-05, 'epoch': 0.32}
********************on step end call back********************
Step 1750 finish
{'loss': 0.7111, 'grad_norm': 1.426074504852295, 'learning_rate': 9.752747252747253e-05, 'epoch': 0.32}
********************on step end call back********************
Step 1760 finish
{'loss': 0.7159, 'grad_norm': 1.777578592300415, 'learning_rate': 9.750915750915751e-05, 'epoch': 0.32}
********************on step end call back********************
Step 1770 finish
{'loss': 0.7257, 'grad_norm': 2.0907137393951416, 'learning_rate': 9.749084249084249e-05, 'epoch': 0.32}
********************on step end call back********************
Step 1780 finish
{'loss': 0.6877, 'grad_norm': 1.565946340560913, 'learning_rate': 9.747252747252748e-05, 'epoch': 0.32}
********************on step end call back********************
Step 1790 finish
{'loss': 0.8204, 'grad_norm': 1.8214776515960693, 'learning_rate': 9.745421245421246e-05, 'epoch': 0.33}
********************on step end call back********************
Step 1800 finish
{'loss': 0.7543, 'grad_norm': 1.911080002784729, 'learning_rate': 9.743589743589744e-05, 'epoch': 0.33}
{'eval_loss': 0.44056037068367004, 'eval_accuracy': 0.8541666666666666, 'eval_runtime': 130.8282, 'eval_samples_per_second': 4.877, 'eval_steps_per_second': 4.877, 'epoch': 0.33}
********************save call back********************
********************on step end call back********************
Step 1810 finish
{'loss': 0.7797, 'grad_norm': 1.5446617603302002, 'learning_rate': 9.741758241758242e-05, 'epoch': 0.33}
********************on step end call back********************
Step 1820 finish
{'loss': 0.6947, 'grad_norm': 1.4284427165985107, 'learning_rate': 9.739926739926741e-05, 'epoch': 0.33}
********************on step end call back********************
Step 1830 finish
{'loss': 0.6763, 'grad_norm': 1.1718381643295288, 'learning_rate': 9.738095238095239e-05, 'epoch': 0.33}
********************on step end call back********************
Step 1840 finish
{'loss': 0.7333, 'grad_norm': 1.5356944799423218, 'learning_rate': 9.736263736263737e-05, 'epoch': 0.33}
********************on step end call back********************
Step 1850 finish
{'loss': 0.7056, 'grad_norm': 1.595446228981018, 'learning_rate': 9.734432234432234e-05, 'epoch': 0.34}
********************on step end call back********************
Step 1860 finish
{'loss': 0.7109, 'grad_norm': 1.5451916456222534, 'learning_rate': 9.732600732600732e-05, 'epoch': 0.34}
********************on step end call back********************
Step 1870 finish
{'loss': 0.701, 'grad_norm': 1.9824589490890503, 'learning_rate': 9.730769230769232e-05, 'epoch': 0.34}
********************on step end call back********************
Step 1880 finish
{'loss': 0.7096, 'grad_norm': 1.522093653678894, 'learning_rate': 9.72893772893773e-05, 'epoch': 0.34}
********************on step end call back********************
Step 1890 finish
{'loss': 0.7934, 'grad_norm': 1.3266537189483643, 'learning_rate': 9.727106227106227e-05, 'epoch': 0.34}
********************on step end call back********************
Step 1900 finish
{'loss': 0.6993, 'grad_norm': 1.5963982343673706, 'learning_rate': 9.725274725274725e-05, 'epoch': 0.35}
{'eval_loss': 0.43157002329826355, 'eval_accuracy': 0.8541666666666666, 'eval_runtime': 130.2109, 'eval_samples_per_second': 4.9, 'eval_steps_per_second': 4.9, 'epoch': 0.35}
********************save call back********************
********************on step end call back********************
Step 1910 finish
{'loss': 0.703, 'grad_norm': 1.8575806617736816, 'learning_rate': 9.723443223443224e-05, 'epoch': 0.35}
********************on step end call back********************
Step 1920 finish
{'loss': 0.7452, 'grad_norm': 1.6665586233139038, 'learning_rate': 9.721611721611722e-05, 'epoch': 0.35}
********************on step end call back********************
Step 1930 finish
{'loss': 0.6444, 'grad_norm': 1.732970118522644, 'learning_rate': 9.71978021978022e-05, 'epoch': 0.35}
********************on step end call back********************
Step 1940 finish
{'loss': 0.74, 'grad_norm': 1.3418022394180298, 'learning_rate': 9.717948717948718e-05, 'epoch': 0.35}
********************on step end call back********************
Step 1950 finish
{'loss': 0.6997, 'grad_norm': 1.4374597072601318, 'learning_rate': 9.716117216117216e-05, 'epoch': 0.35}
********************on step end call back********************
Step 1960 finish
{'loss': 0.7211, 'grad_norm': 1.5420639514923096, 'learning_rate': 9.714285714285715e-05, 'epoch': 0.36}
********************on step end call back********************
Step 1970 finish
{'loss': 0.6784, 'grad_norm': 1.5560145378112793, 'learning_rate': 9.712454212454213e-05, 'epoch': 0.36}
********************on step end call back********************
Step 1980 finish
{'loss': 0.7539, 'grad_norm': 1.4798564910888672, 'learning_rate': 9.710622710622711e-05, 'epoch': 0.36}
********************on step end call back********************
Step 1990 finish
{'loss': 0.7257, 'grad_norm': 1.910423755645752, 'learning_rate': 9.708791208791209e-05, 'epoch': 0.36}
********************on step end call back********************
Step 2000 finish
{'loss': 0.6711, 'grad_norm': 1.5180447101593018, 'learning_rate': 9.706959706959707e-05, 'epoch': 0.36}
{'eval_loss': 0.42135360836982727, 'eval_accuracy': 0.8645833333333334, 'eval_runtime': 130.2106, 'eval_samples_per_second': 4.9, 'eval_steps_per_second': 4.9, 'epoch': 0.36}
********************save call back********************
********************on step end call back********************
Step 2010 finish
[INFO|trainer.py:3376] 2024-03-22 00:30:52,528 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-22 00:30:52,529 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-22 00:30:52,529 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-22 00:33:03,058 >> Saving model checkpoint to ./output/tmp-checkpoint-2100
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-22 00:33:03,222 >> tokenizer config file saved in ./output/tmp-checkpoint-2100/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-22 00:33:03,223 >> Special tokens file saved in ./output/tmp-checkpoint-2100/special_tokens_map.json
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-22 00:41:42,773 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-22 00:41:42,773 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-22 00:41:42,773 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-22 00:43:52,843 >> Saving model checkpoint to ./output/tmp-checkpoint-2200
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-22 00:43:52,986 >> tokenizer config file saved in ./output/tmp-checkpoint-2200/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-22 00:43:52,986 >> Special tokens file saved in ./output/tmp-checkpoint-2200/special_tokens_map.json
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-22 00:52:41,204 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-22 00:52:41,204 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-22 00:52:41,205 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-22 00:54:50,803 >> Saving model checkpoint to ./output/tmp-checkpoint-2300
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-22 00:54:50,965 >> tokenizer config file saved in ./output/tmp-checkpoint-2300/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-22 00:54:50,965 >> Special tokens file saved in ./output/tmp-checkpoint-2300/special_tokens_map.json
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-22 01:03:34,613 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-22 01:03:34,613 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-22 01:03:34,613 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-22 01:05:43,993 >> Saving model checkpoint to ./output/tmp-checkpoint-2400
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-22 01:05:44,136 >> tokenizer config file saved in ./output/tmp-checkpoint-2400/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-22 01:05:44,136 >> Special tokens file saved in ./output/tmp-checkpoint-2400/special_tokens_map.json
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
{'loss': 0.7637, 'grad_norm': 1.5284738540649414, 'learning_rate': 9.705128205128206e-05, 'epoch': 0.37}
********************on step end call back********************
Step 2020 finish
{'loss': 0.7089, 'grad_norm': 1.3720409870147705, 'learning_rate': 9.703296703296704e-05, 'epoch': 0.37}
********************on step end call back********************
Step 2030 finish
{'loss': 0.6416, 'grad_norm': 1.2916843891143799, 'learning_rate': 9.701465201465202e-05, 'epoch': 0.37}
********************on step end call back********************
Step 2040 finish
{'loss': 0.6875, 'grad_norm': 1.8547604084014893, 'learning_rate': 9.6996336996337e-05, 'epoch': 0.37}
********************on step end call back********************
Step 2050 finish
{'loss': 0.7507, 'grad_norm': 1.8586946725845337, 'learning_rate': 9.697802197802199e-05, 'epoch': 0.37}
********************on step end call back********************
Step 2060 finish
{'loss': 0.7068, 'grad_norm': 1.5576252937316895, 'learning_rate': 9.695970695970697e-05, 'epoch': 0.37}
********************on step end call back********************
Step 2070 finish
{'loss': 0.5874, 'grad_norm': 1.5091475248336792, 'learning_rate': 9.694139194139195e-05, 'epoch': 0.38}
********************on step end call back********************
Step 2080 finish
{'loss': 0.6919, 'grad_norm': 1.3155393600463867, 'learning_rate': 9.692307692307692e-05, 'epoch': 0.38}
********************on step end call back********************
Step 2090 finish
{'loss': 0.6826, 'grad_norm': 1.372460126876831, 'learning_rate': 9.69047619047619e-05, 'epoch': 0.38}
********************on step end call back********************
Step 2100 finish
{'loss': 0.7135, 'grad_norm': 1.3447734117507935, 'learning_rate': 9.68864468864469e-05, 'epoch': 0.38}
{'eval_loss': 0.4171878397464752, 'eval_accuracy': 0.8541666666666666, 'eval_runtime': 130.5282, 'eval_samples_per_second': 4.888, 'eval_steps_per_second': 4.888, 'epoch': 0.38}
********************save call back********************
********************on step end call back********************
Step 2110 finish
{'loss': 0.6749, 'grad_norm': 1.823194146156311, 'learning_rate': 9.686813186813187e-05, 'epoch': 0.38}
********************on step end call back********************
Step 2120 finish
{'loss': 0.6396, 'grad_norm': 1.5462135076522827, 'learning_rate': 9.684981684981685e-05, 'epoch': 0.39}
********************on step end call back********************
Step 2130 finish
{'loss': 0.7468, 'grad_norm': 1.1474214792251587, 'learning_rate': 9.683150183150183e-05, 'epoch': 0.39}
********************on step end call back********************
Step 2140 finish
{'loss': 0.6731, 'grad_norm': 1.5162004232406616, 'learning_rate': 9.681318681318682e-05, 'epoch': 0.39}
********************on step end call back********************
Step 2150 finish
{'loss': 0.6986, 'grad_norm': 1.5586862564086914, 'learning_rate': 9.67948717948718e-05, 'epoch': 0.39}
********************on step end call back********************
Step 2160 finish
{'loss': 0.7329, 'grad_norm': 1.403663992881775, 'learning_rate': 9.677655677655678e-05, 'epoch': 0.39}
********************on step end call back********************
Step 2170 finish
{'loss': 0.6317, 'grad_norm': 1.3529564142227173, 'learning_rate': 9.675824175824176e-05, 'epoch': 0.39}
********************on step end call back********************
Step 2180 finish
{'loss': 0.6994, 'grad_norm': 1.5034117698669434, 'learning_rate': 9.673992673992674e-05, 'epoch': 0.4}
********************on step end call back********************
Step 2190 finish
{'loss': 0.6049, 'grad_norm': 1.5190272331237793, 'learning_rate': 9.672161172161173e-05, 'epoch': 0.4}
********************on step end call back********************
Step 2200 finish
{'loss': 0.644, 'grad_norm': 1.619096040725708, 'learning_rate': 9.670329670329671e-05, 'epoch': 0.4}
{'eval_loss': 0.40443655848503113, 'eval_accuracy': 0.8645833333333334, 'eval_runtime': 130.0684, 'eval_samples_per_second': 4.905, 'eval_steps_per_second': 4.905, 'epoch': 0.4}
********************save call back********************
********************on step end call back********************
Step 2210 finish
{'loss': 0.6039, 'grad_norm': 1.329345703125, 'learning_rate': 9.668498168498169e-05, 'epoch': 0.4}
********************on step end call back********************
Step 2220 finish
{'loss': 0.7135, 'grad_norm': 1.6190937757492065, 'learning_rate': 9.666666666666667e-05, 'epoch': 0.4}
********************on step end call back********************
Step 2230 finish
{'loss': 0.6665, 'grad_norm': 1.2794209718704224, 'learning_rate': 9.664835164835166e-05, 'epoch': 0.41}
********************on step end call back********************
Step 2240 finish
{'loss': 0.6513, 'grad_norm': 1.344990849494934, 'learning_rate': 9.663003663003664e-05, 'epoch': 0.41}
********************on step end call back********************
Step 2250 finish
{'loss': 0.64, 'grad_norm': 1.639695644378662, 'learning_rate': 9.661172161172162e-05, 'epoch': 0.41}
********************on step end call back********************
Step 2260 finish
{'loss': 0.6439, 'grad_norm': 1.3485406637191772, 'learning_rate': 9.65934065934066e-05, 'epoch': 0.41}
********************on step end call back********************
Step 2270 finish
{'loss': 0.6657, 'grad_norm': 1.691146969795227, 'learning_rate': 9.657509157509157e-05, 'epoch': 0.41}
********************on step end call back********************
Step 2280 finish
{'loss': 0.6978, 'grad_norm': 1.522369623184204, 'learning_rate': 9.655677655677657e-05, 'epoch': 0.41}
********************on step end call back********************
Step 2290 finish
{'loss': 0.6416, 'grad_norm': 1.3360631465911865, 'learning_rate': 9.653846153846155e-05, 'epoch': 0.42}
********************on step end call back********************
Step 2300 finish
{'loss': 0.6759, 'grad_norm': 1.589147925376892, 'learning_rate': 9.652014652014652e-05, 'epoch': 0.42}
{'eval_loss': 0.4040014147758484, 'eval_accuracy': 0.8645833333333334, 'eval_runtime': 129.5974, 'eval_samples_per_second': 4.923, 'eval_steps_per_second': 4.923, 'epoch': 0.42}
********************save call back********************
********************on step end call back********************
Step 2310 finish
{'loss': 0.6793, 'grad_norm': 1.503994107246399, 'learning_rate': 9.65018315018315e-05, 'epoch': 0.42}
********************on step end call back********************
Step 2320 finish
{'loss': 0.6596, 'grad_norm': 1.332919716835022, 'learning_rate': 9.64835164835165e-05, 'epoch': 0.42}
********************on step end call back********************
Step 2330 finish
{'loss': 0.6157, 'grad_norm': 1.3782777786254883, 'learning_rate': 9.646520146520147e-05, 'epoch': 0.42}
********************on step end call back********************
Step 2340 finish
{'loss': 0.724, 'grad_norm': 1.2308977842330933, 'learning_rate': 9.644688644688645e-05, 'epoch': 0.43}
********************on step end call back********************
Step 2350 finish
{'loss': 0.6354, 'grad_norm': 1.2938563823699951, 'learning_rate': 9.642857142857143e-05, 'epoch': 0.43}
********************on step end call back********************
Step 2360 finish
{'loss': 0.6728, 'grad_norm': 1.5454869270324707, 'learning_rate': 9.641025641025641e-05, 'epoch': 0.43}
********************on step end call back********************
Step 2370 finish
{'loss': 0.6967, 'grad_norm': 1.5528137683868408, 'learning_rate': 9.63919413919414e-05, 'epoch': 0.43}
********************on step end call back********************
Step 2380 finish
{'loss': 0.655, 'grad_norm': 1.6320858001708984, 'learning_rate': 9.637362637362638e-05, 'epoch': 0.43}
********************on step end call back********************
Step 2390 finish
{'loss': 0.7036, 'grad_norm': 1.3441585302352905, 'learning_rate': 9.635531135531136e-05, 'epoch': 0.43}
********************on step end call back********************
Step 2400 finish
{'loss': 0.7568, 'grad_norm': 1.0756839513778687, 'learning_rate': 9.633699633699634e-05, 'epoch': 0.44}
{'eval_loss': 0.39619123935699463, 'eval_accuracy': 0.8645833333333334, 'eval_runtime': 129.379, 'eval_samples_per_second': 4.931, 'eval_steps_per_second': 4.931, 'epoch': 0.44}
********************save call back********************
[INFO|trainer.py:3376] 2024-03-22 01:14:18,401 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-22 01:14:18,402 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-22 01:14:18,402 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-22 01:16:28,489 >> Saving model checkpoint to ./output/tmp-checkpoint-2500
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-22 01:16:28,653 >> tokenizer config file saved in ./output/tmp-checkpoint-2500/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-22 01:16:28,653 >> Special tokens file saved in ./output/tmp-checkpoint-2500/special_tokens_map.json
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-22 01:25:09,265 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-22 01:25:09,265 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-22 01:25:09,265 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-22 01:27:19,539 >> Saving model checkpoint to ./output/tmp-checkpoint-2600
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-22 01:27:19,682 >> tokenizer config file saved in ./output/tmp-checkpoint-2600/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-22 01:27:19,682 >> Special tokens file saved in ./output/tmp-checkpoint-2600/special_tokens_map.json
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-22 01:36:02,761 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-22 01:36:02,762 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-22 01:36:02,762 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-22 01:38:12,869 >> Saving model checkpoint to ./output/tmp-checkpoint-2700
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-22 01:38:13,014 >> tokenizer config file saved in ./output/tmp-checkpoint-2700/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-22 01:38:13,015 >> Special tokens file saved in ./output/tmp-checkpoint-2700/special_tokens_map.json
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-22 01:46:54,942 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-22 01:46:54,942 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-22 01:46:54,942 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-22 01:49:05,367 >> Saving model checkpoint to ./output/tmp-checkpoint-2800
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-22 01:49:05,511 >> tokenizer config file saved in ./output/tmp-checkpoint-2800/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-22 01:49:05,511 >> Special tokens file saved in ./output/tmp-checkpoint-2800/special_tokens_map.json
********************on step end call back********************
Step 2410 finish
{'loss': 0.5836, 'grad_norm': 1.7670282125473022, 'learning_rate': 9.631868131868133e-05, 'epoch': 0.44}
********************on step end call back********************
Step 2420 finish
{'loss': 0.6384, 'grad_norm': 1.268011450767517, 'learning_rate': 9.630036630036631e-05, 'epoch': 0.44}
********************on step end call back********************
Step 2430 finish
{'loss': 0.6461, 'grad_norm': 1.4188305139541626, 'learning_rate': 9.628205128205129e-05, 'epoch': 0.44}
********************on step end call back********************
Step 2440 finish
{'loss': 0.6872, 'grad_norm': 1.4720569849014282, 'learning_rate': 9.626373626373627e-05, 'epoch': 0.44}
********************on step end call back********************
Step 2450 finish
{'loss': 0.6545, 'grad_norm': 1.613191843032837, 'learning_rate': 9.624542124542125e-05, 'epoch': 0.45}
********************on step end call back********************
Step 2460 finish
{'loss': 0.6442, 'grad_norm': 1.3029663562774658, 'learning_rate': 9.622710622710624e-05, 'epoch': 0.45}
********************on step end call back********************
Step 2470 finish
{'loss': 0.6758, 'grad_norm': 1.5015474557876587, 'learning_rate': 9.620879120879122e-05, 'epoch': 0.45}
********************on step end call back********************
Step 2480 finish
{'loss': 0.6441, 'grad_norm': 1.873458743095398, 'learning_rate': 9.61904761904762e-05, 'epoch': 0.45}
********************on step end call back********************
Step 2490 finish
{'loss': 0.6286, 'grad_norm': 1.7523964643478394, 'learning_rate': 9.617216117216118e-05, 'epoch': 0.45}
********************on step end call back********************
Step 2500 finish
{'loss': 0.7007, 'grad_norm': 1.6378875970840454, 'learning_rate': 9.615384615384617e-05, 'epoch': 0.45}
{'eval_loss': 0.3980960547924042, 'eval_accuracy': 0.875, 'eval_runtime': 130.0869, 'eval_samples_per_second': 4.904, 'eval_steps_per_second': 4.904, 'epoch': 0.45}
********************save call back********************
********************on step end call back********************
Step 2510 finish
{'loss': 0.6866, 'grad_norm': 1.3950916528701782, 'learning_rate': 9.613553113553115e-05, 'epoch': 0.46}
********************on step end call back********************
Step 2520 finish
{'loss': 0.6995, 'grad_norm': 1.7984293699264526, 'learning_rate': 9.611721611721612e-05, 'epoch': 0.46}
********************on step end call back********************
Step 2530 finish
{'loss': 0.6269, 'grad_norm': 1.6752381324768066, 'learning_rate': 9.60989010989011e-05, 'epoch': 0.46}
********************on step end call back********************
Step 2540 finish
{'loss': 0.6368, 'grad_norm': 1.3551404476165771, 'learning_rate': 9.608058608058608e-05, 'epoch': 0.46}
********************on step end call back********************
Step 2550 finish
{'loss': 0.644, 'grad_norm': 1.5037463903427124, 'learning_rate': 9.606227106227107e-05, 'epoch': 0.46}
********************on step end call back********************
Step 2560 finish
{'loss': 0.6982, 'grad_norm': 1.7235102653503418, 'learning_rate': 9.604395604395605e-05, 'epoch': 0.47}
********************on step end call back********************
Step 2570 finish
{'loss': 0.615, 'grad_norm': 1.36119544506073, 'learning_rate': 9.602564102564103e-05, 'epoch': 0.47}
********************on step end call back********************
Step 2580 finish
{'loss': 0.633, 'grad_norm': 1.5076768398284912, 'learning_rate': 9.600732600732601e-05, 'epoch': 0.47}
********************on step end call back********************
Step 2590 finish
{'loss': 0.6202, 'grad_norm': 1.395867109298706, 'learning_rate': 9.5989010989011e-05, 'epoch': 0.47}
********************on step end call back********************
Step 2600 finish
{'loss': 0.6772, 'grad_norm': 1.5269392728805542, 'learning_rate': 9.597069597069598e-05, 'epoch': 0.47}
{'eval_loss': 0.3842009902000427, 'eval_accuracy': 0.8645833333333334, 'eval_runtime': 130.273, 'eval_samples_per_second': 4.897, 'eval_steps_per_second': 4.897, 'epoch': 0.47}
********************save call back********************
********************on step end call back********************
Step 2610 finish
{'loss': 0.6166, 'grad_norm': 1.606832504272461, 'learning_rate': 9.595238095238096e-05, 'epoch': 0.47}
********************on step end call back********************
Step 2620 finish
{'loss': 0.6283, 'grad_norm': 1.5572335720062256, 'learning_rate': 9.593406593406594e-05, 'epoch': 0.48}
********************on step end call back********************
Step 2630 finish
{'loss': 0.6296, 'grad_norm': 1.3573076725006104, 'learning_rate': 9.591575091575092e-05, 'epoch': 0.48}
********************on step end call back********************
Step 2640 finish
{'loss': 0.6332, 'grad_norm': 1.5522351264953613, 'learning_rate': 9.589743589743591e-05, 'epoch': 0.48}
********************on step end call back********************
Step 2650 finish
{'loss': 0.6219, 'grad_norm': 1.3758349418640137, 'learning_rate': 9.587912087912089e-05, 'epoch': 0.48}
********************on step end call back********************
Step 2660 finish
{'loss': 0.6048, 'grad_norm': 1.052390217781067, 'learning_rate': 9.586080586080587e-05, 'epoch': 0.48}
********************on step end call back********************
Step 2670 finish
{'loss': 0.6192, 'grad_norm': 1.2594319581985474, 'learning_rate': 9.584249084249085e-05, 'epoch': 0.49}
********************on step end call back********************
Step 2680 finish
{'loss': 0.6083, 'grad_norm': 1.5398733615875244, 'learning_rate': 9.582417582417584e-05, 'epoch': 0.49}
********************on step end call back********************
Step 2690 finish
{'loss': 0.6808, 'grad_norm': 1.4080352783203125, 'learning_rate': 9.580586080586082e-05, 'epoch': 0.49}
********************on step end call back********************
Step 2700 finish
{'loss': 0.6808, 'grad_norm': 1.548014521598816, 'learning_rate': 9.57875457875458e-05, 'epoch': 0.49}
{'eval_loss': 0.38971012830734253, 'eval_accuracy': 0.875, 'eval_runtime': 130.1069, 'eval_samples_per_second': 4.904, 'eval_steps_per_second': 4.904, 'epoch': 0.49}
********************save call back********************
********************on step end call back********************
Step 2710 finish
{'loss': 0.6245, 'grad_norm': 1.370437741279602, 'learning_rate': 9.576923076923078e-05, 'epoch': 0.49}
********************on step end call back********************
Step 2720 finish
{'loss': 0.6553, 'grad_norm': 1.3737242221832275, 'learning_rate': 9.575091575091575e-05, 'epoch': 0.49}
********************on step end call back********************
Step 2730 finish
{'loss': 0.6341, 'grad_norm': 1.4666258096694946, 'learning_rate': 9.573260073260075e-05, 'epoch': 0.5}
********************on step end call back********************
Step 2740 finish
{'loss': 0.6244, 'grad_norm': 1.5764542818069458, 'learning_rate': 9.571428571428573e-05, 'epoch': 0.5}
********************on step end call back********************
Step 2750 finish
{'loss': 0.6065, 'grad_norm': 1.3011184930801392, 'learning_rate': 9.56959706959707e-05, 'epoch': 0.5}
********************on step end call back********************
Step 2760 finish
{'loss': 0.6259, 'grad_norm': 1.8440989255905151, 'learning_rate': 9.567765567765568e-05, 'epoch': 0.5}
********************on step end call back********************
Step 2770 finish
{'loss': 0.6999, 'grad_norm': 1.4835230112075806, 'learning_rate': 9.565934065934066e-05, 'epoch': 0.5}
********************on step end call back********************
Step 2780 finish
{'loss': 0.6871, 'grad_norm': 1.3304615020751953, 'learning_rate': 9.564102564102565e-05, 'epoch': 0.51}
********************on step end call back********************
Step 2790 finish
{'loss': 0.572, 'grad_norm': 1.5015968084335327, 'learning_rate': 9.562271062271063e-05, 'epoch': 0.51}
********************on step end call back********************
Step 2800 finish
{'loss': 0.6967, 'grad_norm': 1.8364276885986328, 'learning_rate': 9.560439560439561e-05, 'epoch': 0.51}
{'eval_loss': 0.3651435971260071, 'eval_accuracy': 0.84375, 'eval_runtime': 130.4247, 'eval_samples_per_second': 4.892, 'eval_steps_per_second': 4.892, 'epoch': 0.51}
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-22 01:57:48,981 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-22 01:57:48,982 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-22 01:57:48,982 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-22 01:59:59,564 >> Saving model checkpoint to ./output/tmp-checkpoint-2900
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-22 01:59:59,709 >> tokenizer config file saved in ./output/tmp-checkpoint-2900/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-22 01:59:59,709 >> Special tokens file saved in ./output/tmp-checkpoint-2900/special_tokens_map.json
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-22 02:08:35,524 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-22 02:08:35,524 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-22 02:08:35,524 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-22 02:10:45,939 >> Saving model checkpoint to ./output/tmp-checkpoint-3000
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-22 02:10:46,083 >> tokenizer config file saved in ./output/tmp-checkpoint-3000/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-22 02:10:46,083 >> Special tokens file saved in ./output/tmp-checkpoint-3000/special_tokens_map.json
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-22 02:19:18,739 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-22 02:19:18,739 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-22 02:19:18,739 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-22 02:21:29,354 >> Saving model checkpoint to ./output/tmp-checkpoint-3100
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-22 02:21:29,496 >> tokenizer config file saved in ./output/tmp-checkpoint-3100/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-22 02:21:29,496 >> Special tokens file saved in ./output/tmp-checkpoint-3100/special_tokens_map.json
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-22 02:30:11,932 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-22 02:30:11,932 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-22 02:30:11,932 >>   Batch size = 1
********************save call back********************
********************on step end call back********************
Step 2810 finish
{'loss': 0.6591, 'grad_norm': 1.1683188676834106, 'learning_rate': 9.558608058608059e-05, 'epoch': 0.51}
********************on step end call back********************
Step 2820 finish
{'loss': 0.6235, 'grad_norm': 1.2423545122146606, 'learning_rate': 9.556776556776558e-05, 'epoch': 0.51}
********************on step end call back********************
Step 2830 finish
{'loss': 0.6275, 'grad_norm': 1.5238869190216064, 'learning_rate': 9.554945054945056e-05, 'epoch': 0.51}
********************on step end call back********************
Step 2840 finish
{'loss': 0.5119, 'grad_norm': 1.3924869298934937, 'learning_rate': 9.553113553113554e-05, 'epoch': 0.52}
********************on step end call back********************
Step 2850 finish
{'loss': 0.6101, 'grad_norm': 1.2557697296142578, 'learning_rate': 9.551282051282052e-05, 'epoch': 0.52}
********************on step end call back********************
Step 2860 finish
{'loss': 0.6468, 'grad_norm': 1.2822964191436768, 'learning_rate': 9.54945054945055e-05, 'epoch': 0.52}
********************on step end call back********************
Step 2870 finish
{'loss': 0.6557, 'grad_norm': 1.6945592164993286, 'learning_rate': 9.547619047619049e-05, 'epoch': 0.52}
********************on step end call back********************
Step 2880 finish
{'loss': 0.702, 'grad_norm': 1.798399806022644, 'learning_rate': 9.545787545787547e-05, 'epoch': 0.52}
********************on step end call back********************
Step 2890 finish
{'loss': 0.5754, 'grad_norm': 1.6064072847366333, 'learning_rate': 9.543956043956045e-05, 'epoch': 0.53}
********************on step end call back********************
Step 2900 finish
{'loss': 0.6159, 'grad_norm': 1.0843089818954468, 'learning_rate': 9.542124542124543e-05, 'epoch': 0.53}
{'eval_loss': 0.3741602301597595, 'eval_accuracy': 0.8645833333333334, 'eval_runtime': 130.5821, 'eval_samples_per_second': 4.886, 'eval_steps_per_second': 4.886, 'epoch': 0.53}
********************save call back********************
********************on step end call back********************
Step 2910 finish
{'loss': 0.5686, 'grad_norm': 1.5159878730773926, 'learning_rate': 9.540293040293042e-05, 'epoch': 0.53}
********************on step end call back********************
Step 2920 finish
{'loss': 0.6011, 'grad_norm': 1.193367600440979, 'learning_rate': 9.53846153846154e-05, 'epoch': 0.53}
********************on step end call back********************
Step 2930 finish
{'loss': 0.6514, 'grad_norm': 1.222037672996521, 'learning_rate': 9.536630036630038e-05, 'epoch': 0.53}
********************on step end call back********************
Step 2940 finish
{'loss': 0.6912, 'grad_norm': 1.3521718978881836, 'learning_rate': 9.534798534798535e-05, 'epoch': 0.53}
********************on step end call back********************
Step 2950 finish
{'loss': 0.6099, 'grad_norm': 1.5470921993255615, 'learning_rate': 9.532967032967033e-05, 'epoch': 0.54}
********************on step end call back********************
Step 2960 finish
{'loss': 0.6379, 'grad_norm': 1.3409403562545776, 'learning_rate': 9.531135531135531e-05, 'epoch': 0.54}
********************on step end call back********************
Step 2970 finish
{'loss': 0.6826, 'grad_norm': 1.5492630004882812, 'learning_rate': 9.529304029304029e-05, 'epoch': 0.54}
********************on step end call back********************
Step 2980 finish
{'loss': 0.5709, 'grad_norm': 1.5470808744430542, 'learning_rate': 9.527472527472527e-05, 'epoch': 0.54}
********************on step end call back********************
Step 2990 finish
{'loss': 0.5827, 'grad_norm': 1.096369743347168, 'learning_rate': 9.525641025641026e-05, 'epoch': 0.54}
********************on step end call back********************
Step 3000 finish
{'loss': 0.6419, 'grad_norm': 1.3725013732910156, 'learning_rate': 9.523809523809524e-05, 'epoch': 0.55}
{'eval_loss': 0.37413397431373596, 'eval_accuracy': 0.8645833333333334, 'eval_runtime': 130.4133, 'eval_samples_per_second': 4.892, 'eval_steps_per_second': 4.892, 'epoch': 0.55}
********************save call back********************
********************on step end call back********************
Step 3010 finish
{'loss': 0.5512, 'grad_norm': 1.6036525964736938, 'learning_rate': 9.521978021978022e-05, 'epoch': 0.55}
********************on step end call back********************
Step 3020 finish
{'loss': 0.5824, 'grad_norm': 1.4598616361618042, 'learning_rate': 9.52014652014652e-05, 'epoch': 0.55}
********************on step end call back********************
Step 3030 finish
{'loss': 0.6208, 'grad_norm': 1.4464492797851562, 'learning_rate': 9.518315018315018e-05, 'epoch': 0.55}
********************on step end call back********************
Step 3040 finish
{'loss': 0.6501, 'grad_norm': 1.4794394969940186, 'learning_rate': 9.516483516483517e-05, 'epoch': 0.55}
********************on step end call back********************
Step 3050 finish
{'loss': 0.6548, 'grad_norm': 1.7709144353866577, 'learning_rate': 9.514652014652015e-05, 'epoch': 0.55}
********************on step end call back********************
Step 3060 finish
{'loss': 0.6503, 'grad_norm': 1.4866774082183838, 'learning_rate': 9.512820512820513e-05, 'epoch': 0.56}
********************on step end call back********************
Step 3070 finish
{'loss': 0.6215, 'grad_norm': 1.3815854787826538, 'learning_rate': 9.51098901098901e-05, 'epoch': 0.56}
********************on step end call back********************
Step 3080 finish
{'loss': 0.6536, 'grad_norm': 1.062637209892273, 'learning_rate': 9.50915750915751e-05, 'epoch': 0.56}
********************on step end call back********************
Step 3090 finish
{'loss': 0.5668, 'grad_norm': 1.332257866859436, 'learning_rate': 9.507326007326008e-05, 'epoch': 0.56}
********************on step end call back********************
Step 3100 finish
{'loss': 0.6714, 'grad_norm': 1.4818161725997925, 'learning_rate': 9.505494505494506e-05, 'epoch': 0.56}
{'eval_loss': 0.3750693202018738, 'eval_accuracy': 0.875, 'eval_runtime': 130.6141, 'eval_samples_per_second': 4.885, 'eval_steps_per_second': 4.885, 'epoch': 0.56}
********************save call back********************
********************on step end call back********************
Step 3110 finish
{'loss': 0.631, 'grad_norm': 1.6651560068130493, 'learning_rate': 9.503663003663003e-05, 'epoch': 0.57}
********************on step end call back********************
Step 3120 finish
{'loss': 0.5874, 'grad_norm': 1.4478977918624878, 'learning_rate': 9.501831501831501e-05, 'epoch': 0.57}
********************on step end call back********************
Step 3130 finish
{'loss': 0.6389, 'grad_norm': 1.2252585887908936, 'learning_rate': 9.5e-05, 'epoch': 0.57}
********************on step end call back********************
Step 3140 finish
{'loss': 0.6232, 'grad_norm': 1.2706292867660522, 'learning_rate': 9.498168498168498e-05, 'epoch': 0.57}
********************on step end call back********************
Step 3150 finish
{'loss': 0.5968, 'grad_norm': 1.2612824440002441, 'learning_rate': 9.496336996336996e-05, 'epoch': 0.57}
********************on step end call back********************
Step 3160 finish
{'loss': 0.624, 'grad_norm': 1.523000717163086, 'learning_rate': 9.494505494505494e-05, 'epoch': 0.57}
********************on step end call back********************
Step 3170 finish
{'loss': 0.6366, 'grad_norm': 1.4895801544189453, 'learning_rate': 9.492673992673993e-05, 'epoch': 0.58}
********************on step end call back********************
Step 3180 finish
{'loss': 0.6235, 'grad_norm': 1.4146898984909058, 'learning_rate': 9.490842490842491e-05, 'epoch': 0.58}
********************on step end call back********************
Step 3190 finish
{'loss': 0.5838, 'grad_norm': 1.5418479442596436, 'learning_rate': 9.489010989010989e-05, 'epoch': 0.58}
********************on step end call back********************
Step 3200 finish
{'loss': 0.6012, 'grad_norm': 1.6460216045379639, 'learning_rate': 9.487179487179487e-05, 'epoch': 0.58}
[INFO|trainer.py:3067] 2024-03-22 02:32:22,514 >> Saving model checkpoint to ./output/tmp-checkpoint-3200
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-22 02:32:22,661 >> tokenizer config file saved in ./output/tmp-checkpoint-3200/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-22 02:32:22,661 >> Special tokens file saved in ./output/tmp-checkpoint-3200/special_tokens_map.json
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-22 02:40:56,732 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-22 02:40:56,733 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-22 02:40:56,733 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-22 02:43:06,955 >> Saving model checkpoint to ./output/tmp-checkpoint-3300
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-22 02:43:07,113 >> tokenizer config file saved in ./output/tmp-checkpoint-3300/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-22 02:43:07,114 >> Special tokens file saved in ./output/tmp-checkpoint-3300/special_tokens_map.json
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-22 02:51:49,511 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-22 02:51:49,512 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-22 02:51:49,512 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-22 02:53:59,669 >> Saving model checkpoint to ./output/tmp-checkpoint-3400
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-22 02:53:59,820 >> tokenizer config file saved in ./output/tmp-checkpoint-3400/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-22 02:53:59,820 >> Special tokens file saved in ./output/tmp-checkpoint-3400/special_tokens_map.json
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-22 03:02:46,067 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-22 03:02:46,067 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-22 03:02:46,067 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-22 03:04:56,241 >> Saving model checkpoint to ./output/tmp-checkpoint-3500
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-22 03:04:56,386 >> tokenizer config file saved in ./output/tmp-checkpoint-3500/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-22 03:04:56,386 >> Special tokens file saved in ./output/tmp-checkpoint-3500/special_tokens_map.json
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
{'eval_loss': 0.3549214005470276, 'eval_accuracy': 0.8854166666666666, 'eval_runtime': 130.5817, 'eval_samples_per_second': 4.886, 'eval_steps_per_second': 4.886, 'epoch': 0.58}
********************save call back********************
********************on step end call back********************
Step 3210 finish
{'loss': 0.6186, 'grad_norm': 1.2833126783370972, 'learning_rate': 9.485347985347985e-05, 'epoch': 0.58}
********************on step end call back********************
Step 3220 finish
{'loss': 0.6204, 'grad_norm': 1.3543967008590698, 'learning_rate': 9.483516483516484e-05, 'epoch': 0.59}
********************on step end call back********************
Step 3230 finish
{'loss': 0.6041, 'grad_norm': 1.9052762985229492, 'learning_rate': 9.481684981684982e-05, 'epoch': 0.59}
********************on step end call back********************
Step 3240 finish
{'loss': 0.5807, 'grad_norm': 1.3915866613388062, 'learning_rate': 9.47985347985348e-05, 'epoch': 0.59}
********************on step end call back********************
Step 3250 finish
{'loss': 0.6321, 'grad_norm': 1.3106179237365723, 'learning_rate': 9.478021978021978e-05, 'epoch': 0.59}
********************on step end call back********************
Step 3260 finish
{'loss': 0.6033, 'grad_norm': 1.4056237936019897, 'learning_rate': 9.476190476190476e-05, 'epoch': 0.59}
********************on step end call back********************
Step 3270 finish
{'loss': 0.5924, 'grad_norm': 1.2984776496887207, 'learning_rate': 9.474358974358975e-05, 'epoch': 0.59}
********************on step end call back********************
Step 3280 finish
{'loss': 0.5853, 'grad_norm': 1.5665194988250732, 'learning_rate': 9.472527472527473e-05, 'epoch': 0.6}
********************on step end call back********************
Step 3290 finish
{'loss': 0.6497, 'grad_norm': 1.2336227893829346, 'learning_rate': 9.47069597069597e-05, 'epoch': 0.6}
********************on step end call back********************
Step 3300 finish
{'loss': 0.5816, 'grad_norm': 1.285330057144165, 'learning_rate': 9.468864468864468e-05, 'epoch': 0.6}
{'eval_loss': 0.3597533106803894, 'eval_accuracy': 0.8541666666666666, 'eval_runtime': 130.221, 'eval_samples_per_second': 4.899, 'eval_steps_per_second': 4.899, 'epoch': 0.6}
********************save call back********************
********************on step end call back********************
Step 3310 finish
{'loss': 0.62, 'grad_norm': 1.1551342010498047, 'learning_rate': 9.467032967032968e-05, 'epoch': 0.6}
********************on step end call back********************
Step 3320 finish
{'loss': 0.6111, 'grad_norm': 1.4755477905273438, 'learning_rate': 9.465201465201466e-05, 'epoch': 0.6}
********************on step end call back********************
Step 3330 finish
{'loss': 0.6196, 'grad_norm': 1.539066195487976, 'learning_rate': 9.463369963369963e-05, 'epoch': 0.61}
********************on step end call back********************
Step 3340 finish
{'loss': 0.5705, 'grad_norm': 1.339983582496643, 'learning_rate': 9.461538461538461e-05, 'epoch': 0.61}
********************on step end call back********************
Step 3350 finish
{'loss': 0.6172, 'grad_norm': 1.4565731287002563, 'learning_rate': 9.459706959706959e-05, 'epoch': 0.61}
********************on step end call back********************
Step 3360 finish
{'loss': 0.6094, 'grad_norm': 1.5021947622299194, 'learning_rate': 9.457875457875458e-05, 'epoch': 0.61}
********************on step end call back********************
Step 3370 finish
{'loss': 0.6594, 'grad_norm': 1.4068032503128052, 'learning_rate': 9.456043956043956e-05, 'epoch': 0.61}
********************on step end call back********************
Step 3380 finish
{'loss': 0.5496, 'grad_norm': 1.478756308555603, 'learning_rate': 9.454212454212454e-05, 'epoch': 0.61}
********************on step end call back********************
Step 3390 finish
{'loss': 0.5939, 'grad_norm': 1.3838444948196411, 'learning_rate': 9.452380952380952e-05, 'epoch': 0.62}
********************on step end call back********************
Step 3400 finish
{'loss': 0.5444, 'grad_norm': 1.5267962217330933, 'learning_rate': 9.450549450549451e-05, 'epoch': 0.62}
{'eval_loss': 0.3621023893356323, 'eval_accuracy': 0.875, 'eval_runtime': 130.1568, 'eval_samples_per_second': 4.902, 'eval_steps_per_second': 4.902, 'epoch': 0.62}
********************save call back********************
********************on step end call back********************
Step 3410 finish
{'loss': 0.6202, 'grad_norm': 2.0467751026153564, 'learning_rate': 9.448717948717949e-05, 'epoch': 0.62}
********************on step end call back********************
Step 3420 finish
{'loss': 0.5994, 'grad_norm': 1.328688144683838, 'learning_rate': 9.446886446886447e-05, 'epoch': 0.62}
********************on step end call back********************
Step 3430 finish
{'loss': 0.6553, 'grad_norm': 1.5718389749526978, 'learning_rate': 9.445054945054945e-05, 'epoch': 0.62}
********************on step end call back********************
Step 3440 finish
{'loss': 0.5976, 'grad_norm': 1.2702335119247437, 'learning_rate': 9.443223443223443e-05, 'epoch': 0.63}
********************on step end call back********************
Step 3450 finish
{'loss': 0.6402, 'grad_norm': 1.4435127973556519, 'learning_rate': 9.441391941391942e-05, 'epoch': 0.63}
********************on step end call back********************
Step 3460 finish
{'loss': 0.5477, 'grad_norm': 1.4183388948440552, 'learning_rate': 9.43956043956044e-05, 'epoch': 0.63}
********************on step end call back********************
Step 3470 finish
{'loss': 0.5774, 'grad_norm': 1.247890591621399, 'learning_rate': 9.437728937728938e-05, 'epoch': 0.63}
********************on step end call back********************
Step 3480 finish
{'loss': 0.6189, 'grad_norm': 1.186089038848877, 'learning_rate': 9.435897435897436e-05, 'epoch': 0.63}
********************on step end call back********************
Step 3490 finish
{'loss': 0.5988, 'grad_norm': 1.453096866607666, 'learning_rate': 9.434065934065935e-05, 'epoch': 0.63}
********************on step end call back********************
Step 3500 finish
{'loss': 0.5285, 'grad_norm': 1.3818624019622803, 'learning_rate': 9.432234432234433e-05, 'epoch': 0.64}
{'eval_loss': 0.3474993109703064, 'eval_accuracy': 0.8645833333333334, 'eval_runtime': 130.1733, 'eval_samples_per_second': 4.901, 'eval_steps_per_second': 4.901, 'epoch': 0.64}
********************save call back********************
********************on step end call back********************
Step 3510 finish
{'loss': 0.6334, 'grad_norm': 1.4392818212509155, 'learning_rate': 9.43040293040293e-05, 'epoch': 0.64}
********************on step end call back********************
Step 3520 finish
{'loss': 0.5471, 'grad_norm': 1.7055402994155884, 'learning_rate': 9.428571428571429e-05, 'epoch': 0.64}
********************on step end call back********************
Step 3530 finish
{'loss': 0.5927, 'grad_norm': 1.1432393789291382, 'learning_rate': 9.426739926739926e-05, 'epoch': 0.64}
********************on step end call back********************
Step 3540 finish
{'loss': 0.5753, 'grad_norm': 1.1194087266921997, 'learning_rate': 9.424908424908426e-05, 'epoch': 0.64}
********************on step end call back********************
Step 3550 finish
{'loss': 0.54, 'grad_norm': 1.5169727802276611, 'learning_rate': 9.423076923076924e-05, 'epoch': 0.65}
********************on step end call back********************
Step 3560 finish
{'loss': 0.549, 'grad_norm': 1.156270146369934, 'learning_rate': 9.421245421245421e-05, 'epoch': 0.65}
********************on step end call back********************
Step 3570 finish
{'loss': 0.5854, 'grad_norm': 1.5705726146697998, 'learning_rate': 9.419413919413919e-05, 'epoch': 0.65}
********************on step end call back********************
Step 3580 finish
{'loss': 0.6029, 'grad_norm': 1.4901190996170044, 'learning_rate': 9.417582417582418e-05, 'epoch': 0.65}
********************on step end call back********************
Step 3590 finish
{'loss': 0.6358, 'grad_norm': 1.482812523841858, 'learning_rate': 9.415750915750916e-05, 'epoch': 0.65}
********************on step end call back********************
Step 3600 finish
[INFO|trainer.py:3376] 2024-03-22 03:13:29,210 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-22 03:13:29,210 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-22 03:13:29,210 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-22 03:15:39,320 >> Saving model checkpoint to ./output/tmp-checkpoint-3600
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-22 03:15:39,469 >> tokenizer config file saved in ./output/tmp-checkpoint-3600/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-22 03:15:39,469 >> Special tokens file saved in ./output/tmp-checkpoint-3600/special_tokens_map.json
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-22 03:24:22,890 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-22 03:24:22,891 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-22 03:24:22,891 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-22 03:26:32,936 >> Saving model checkpoint to ./output/tmp-checkpoint-3700
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-22 03:26:33,086 >> tokenizer config file saved in ./output/tmp-checkpoint-3700/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-22 03:26:33,086 >> Special tokens file saved in ./output/tmp-checkpoint-3700/special_tokens_map.json
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-22 03:35:05,653 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-22 03:35:05,653 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-22 03:35:05,653 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-22 03:37:15,282 >> Saving model checkpoint to ./output/tmp-checkpoint-3800
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-22 03:37:15,427 >> tokenizer config file saved in ./output/tmp-checkpoint-3800/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-22 03:37:15,427 >> Special tokens file saved in ./output/tmp-checkpoint-3800/special_tokens_map.json
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-22 03:45:54,074 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-22 03:45:54,075 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-22 03:45:54,075 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-22 03:48:04,493 >> Saving model checkpoint to ./output/tmp-checkpoint-3900
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-22 03:48:04,639 >> tokenizer config file saved in ./output/tmp-checkpoint-3900/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-22 03:48:04,639 >> Special tokens file saved in ./output/tmp-checkpoint-3900/special_tokens_map.json
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
{'loss': 0.5404, 'grad_norm': 1.5499767065048218, 'learning_rate': 9.413919413919414e-05, 'epoch': 0.65}
{'eval_loss': 0.3557875156402588, 'eval_accuracy': 0.8645833333333334, 'eval_runtime': 130.1085, 'eval_samples_per_second': 4.904, 'eval_steps_per_second': 4.904, 'epoch': 0.65}
********************save call back********************
********************on step end call back********************
Step 3610 finish
{'loss': 0.585, 'grad_norm': 1.447317361831665, 'learning_rate': 9.412087912087912e-05, 'epoch': 0.66}
********************on step end call back********************
Step 3620 finish
{'loss': 0.5983, 'grad_norm': 1.254123568534851, 'learning_rate': 9.41025641025641e-05, 'epoch': 0.66}
********************on step end call back********************
Step 3630 finish
{'loss': 0.5873, 'grad_norm': 1.6576907634735107, 'learning_rate': 9.408424908424909e-05, 'epoch': 0.66}
********************on step end call back********************
Step 3640 finish
{'loss': 0.5756, 'grad_norm': 1.4185290336608887, 'learning_rate': 9.406593406593407e-05, 'epoch': 0.66}
********************on step end call back********************
Step 3650 finish
{'loss': 0.5894, 'grad_norm': 1.389841914176941, 'learning_rate': 9.404761904761905e-05, 'epoch': 0.66}
********************on step end call back********************
Step 3660 finish
{'loss': 0.5875, 'grad_norm': 1.2507647275924683, 'learning_rate': 9.402930402930403e-05, 'epoch': 0.67}
********************on step end call back********************
Step 3670 finish
{'loss': 0.527, 'grad_norm': 1.2635002136230469, 'learning_rate': 9.401098901098902e-05, 'epoch': 0.67}
********************on step end call back********************
Step 3680 finish
{'loss': 0.6352, 'grad_norm': 1.1809699535369873, 'learning_rate': 9.3992673992674e-05, 'epoch': 0.67}
********************on step end call back********************
Step 3690 finish
{'loss': 0.5652, 'grad_norm': 1.2369661331176758, 'learning_rate': 9.397435897435898e-05, 'epoch': 0.67}
********************on step end call back********************
Step 3700 finish
{'loss': 0.6001, 'grad_norm': 1.4660282135009766, 'learning_rate': 9.395604395604396e-05, 'epoch': 0.67}
{'eval_loss': 0.3499812185764313, 'eval_accuracy': 0.8645833333333334, 'eval_runtime': 130.0443, 'eval_samples_per_second': 4.906, 'eval_steps_per_second': 4.906, 'epoch': 0.67}
********************save call back********************
********************on step end call back********************
Step 3710 finish
{'loss': 0.5768, 'grad_norm': 1.1884099245071411, 'learning_rate': 9.393772893772894e-05, 'epoch': 0.67}
********************on step end call back********************
Step 3720 finish
{'loss': 0.6751, 'grad_norm': 1.2750189304351807, 'learning_rate': 9.391941391941393e-05, 'epoch': 0.68}
********************on step end call back********************
Step 3730 finish
{'loss': 0.5635, 'grad_norm': 1.2325482368469238, 'learning_rate': 9.390109890109891e-05, 'epoch': 0.68}
********************on step end call back********************
Step 3740 finish
{'loss': 0.6033, 'grad_norm': 1.5591174364089966, 'learning_rate': 9.388278388278389e-05, 'epoch': 0.68}
********************on step end call back********************
Step 3750 finish
{'loss': 0.595, 'grad_norm': 1.3003820180892944, 'learning_rate': 9.386446886446886e-05, 'epoch': 0.68}
********************on step end call back********************
Step 3760 finish
{'loss': 0.468, 'grad_norm': 1.535679578781128, 'learning_rate': 9.384615384615386e-05, 'epoch': 0.68}
********************on step end call back********************
Step 3770 finish
{'loss': 0.5489, 'grad_norm': 1.315879225730896, 'learning_rate': 9.382783882783884e-05, 'epoch': 0.69}
********************on step end call back********************
Step 3780 finish
{'loss': 0.6819, 'grad_norm': 1.3414366245269775, 'learning_rate': 9.380952380952381e-05, 'epoch': 0.69}
********************on step end call back********************
Step 3790 finish
{'loss': 0.568, 'grad_norm': 1.5565314292907715, 'learning_rate': 9.379120879120879e-05, 'epoch': 0.69}
********************on step end call back********************
Step 3800 finish
{'loss': 0.635, 'grad_norm': 1.4449411630630493, 'learning_rate': 9.377289377289377e-05, 'epoch': 0.69}
{'eval_loss': 0.3568250834941864, 'eval_accuracy': 0.8645833333333334, 'eval_runtime': 129.6278, 'eval_samples_per_second': 4.922, 'eval_steps_per_second': 4.922, 'epoch': 0.69}
********************save call back********************
********************on step end call back********************
Step 3810 finish
{'loss': 0.5505, 'grad_norm': 1.3326520919799805, 'learning_rate': 9.375457875457876e-05, 'epoch': 0.69}
********************on step end call back********************
Step 3820 finish
{'loss': 0.5952, 'grad_norm': 1.2601441144943237, 'learning_rate': 9.373626373626374e-05, 'epoch': 0.69}
********************on step end call back********************
Step 3830 finish
{'loss': 0.6483, 'grad_norm': 1.4165468215942383, 'learning_rate': 9.371794871794872e-05, 'epoch': 0.7}
********************on step end call back********************
Step 3840 finish
{'loss': 0.638, 'grad_norm': 1.519559621810913, 'learning_rate': 9.36996336996337e-05, 'epoch': 0.7}
********************on step end call back********************
Step 3850 finish
{'loss': 0.6469, 'grad_norm': 1.723737359046936, 'learning_rate': 9.368131868131869e-05, 'epoch': 0.7}
********************on step end call back********************
Step 3860 finish
{'loss': 0.5339, 'grad_norm': 1.4291383028030396, 'learning_rate': 9.366300366300367e-05, 'epoch': 0.7}
********************on step end call back********************
Step 3870 finish
{'loss': 0.5438, 'grad_norm': 1.4567006826400757, 'learning_rate': 9.364468864468865e-05, 'epoch': 0.7}
********************on step end call back********************
Step 3880 finish
{'loss': 0.5969, 'grad_norm': 1.1338003873825073, 'learning_rate': 9.362637362637363e-05, 'epoch': 0.71}
********************on step end call back********************
Step 3890 finish
{'loss': 0.573, 'grad_norm': 1.5341529846191406, 'learning_rate': 9.360805860805861e-05, 'epoch': 0.71}
********************on step end call back********************
Step 3900 finish
{'loss': 0.6018, 'grad_norm': 1.6842591762542725, 'learning_rate': 9.35897435897436e-05, 'epoch': 0.71}
{'eval_loss': 0.3588429391384125, 'eval_accuracy': 0.8645833333333334, 'eval_runtime': 130.4177, 'eval_samples_per_second': 4.892, 'eval_steps_per_second': 4.892, 'epoch': 0.71}
********************save call back********************
********************on step end call back********************
Step 3910 finish
{'loss': 0.5864, 'grad_norm': 1.536521315574646, 'learning_rate': 9.357142857142858e-05, 'epoch': 0.71}
********************on step end call back********************
Step 3920 finish
{'loss': 0.5935, 'grad_norm': 1.1414883136749268, 'learning_rate': 9.355311355311356e-05, 'epoch': 0.71}
********************on step end call back********************
Step 3930 finish
{'loss': 0.6405, 'grad_norm': 1.2472877502441406, 'learning_rate': 9.353479853479854e-05, 'epoch': 0.71}
********************on step end call back********************
Step 3940 finish
{'loss': 0.6355, 'grad_norm': 1.378675103187561, 'learning_rate': 9.351648351648353e-05, 'epoch': 0.72}
********************on step end call back********************
Step 3950 finish
{'loss': 0.5636, 'grad_norm': 1.569300889968872, 'learning_rate': 9.349816849816851e-05, 'epoch': 0.72}
********************on step end call back********************
Step 3960 finish
{'loss': 0.5944, 'grad_norm': 1.6459473371505737, 'learning_rate': 9.347985347985349e-05, 'epoch': 0.72}
********************on step end call back********************
Step 3970 finish
{'loss': 0.5636, 'grad_norm': 1.3252065181732178, 'learning_rate': 9.346153846153846e-05, 'epoch': 0.72}
********************on step end call back********************
Step 3980 finish
{'loss': 0.5217, 'grad_norm': 1.4361815452575684, 'learning_rate': 9.344322344322344e-05, 'epoch': 0.72}
********************on step end call back********************
Step 3990 finish
{'loss': 0.6433, 'grad_norm': 1.4887759685516357, 'learning_rate': 9.342490842490844e-05, 'epoch': 0.73}
[INFO|trainer.py:3376] 2024-03-22 03:56:43,073 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-22 03:56:43,073 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-22 03:56:43,073 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-22 03:58:53,637 >> Saving model checkpoint to ./output/tmp-checkpoint-4000
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-22 03:58:53,782 >> tokenizer config file saved in ./output/tmp-checkpoint-4000/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-22 03:58:53,782 >> Special tokens file saved in ./output/tmp-checkpoint-4000/special_tokens_map.json
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-22 04:07:35,038 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-22 04:07:35,039 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-22 04:07:35,039 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-22 04:09:45,901 >> Saving model checkpoint to ./output/tmp-checkpoint-4100
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-22 04:09:46,073 >> tokenizer config file saved in ./output/tmp-checkpoint-4100/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-22 04:09:46,073 >> Special tokens file saved in ./output/tmp-checkpoint-4100/special_tokens_map.json
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-22 04:18:26,194 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-22 04:18:26,194 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-22 04:18:26,194 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-22 04:20:37,131 >> Saving model checkpoint to ./output/tmp-checkpoint-4200
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-22 04:20:37,259 >> tokenizer config file saved in ./output/tmp-checkpoint-4200/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-22 04:20:37,259 >> Special tokens file saved in ./output/tmp-checkpoint-4200/special_tokens_map.json
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-22 04:29:19,016 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-22 04:29:19,016 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-22 04:29:19,016 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-22 04:31:29,171 >> Saving model checkpoint to ./output/tmp-checkpoint-4300
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-22 04:31:29,311 >> tokenizer config file saved in ./output/tmp-checkpoint-4300/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-22 04:31:29,311 >> Special tokens file saved in ./output/tmp-checkpoint-4300/special_tokens_map.json
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
********************on step end call back********************
Step 4000 finish
{'loss': 0.5391, 'grad_norm': 1.7023440599441528, 'learning_rate': 9.340659340659341e-05, 'epoch': 0.73}
{'eval_loss': 0.341436505317688, 'eval_accuracy': 0.8645833333333334, 'eval_runtime': 130.5624, 'eval_samples_per_second': 4.887, 'eval_steps_per_second': 4.887, 'epoch': 0.73}
********************save call back********************
********************on step end call back********************
Step 4010 finish
{'loss': 0.5649, 'grad_norm': 1.19392728805542, 'learning_rate': 9.33882783882784e-05, 'epoch': 0.73}
********************on step end call back********************
Step 4020 finish
{'loss': 0.5591, 'grad_norm': 1.3915430307388306, 'learning_rate': 9.336996336996337e-05, 'epoch': 0.73}
********************on step end call back********************
Step 4030 finish
{'loss': 0.6186, 'grad_norm': 1.2978790998458862, 'learning_rate': 9.335164835164835e-05, 'epoch': 0.73}
********************on step end call back********************
Step 4040 finish
{'loss': 0.5382, 'grad_norm': 1.3772292137145996, 'learning_rate': 9.333333333333334e-05, 'epoch': 0.73}
********************on step end call back********************
Step 4050 finish
{'loss': 0.5162, 'grad_norm': 1.7264320850372314, 'learning_rate': 9.331501831501832e-05, 'epoch': 0.74}
********************on step end call back********************
Step 4060 finish
{'loss': 0.5653, 'grad_norm': 1.274572730064392, 'learning_rate': 9.32967032967033e-05, 'epoch': 0.74}
********************on step end call back********************
Step 4070 finish
{'loss': 0.5295, 'grad_norm': 1.3332676887512207, 'learning_rate': 9.327838827838828e-05, 'epoch': 0.74}
********************on step end call back********************
Step 4080 finish
{'loss': 0.4888, 'grad_norm': 0.9892896413803101, 'learning_rate': 9.326007326007327e-05, 'epoch': 0.74}
********************on step end call back********************
Step 4090 finish
{'loss': 0.5565, 'grad_norm': 1.1100528240203857, 'learning_rate': 9.324175824175825e-05, 'epoch': 0.74}
********************on step end call back********************
Step 4100 finish
{'loss': 0.5432, 'grad_norm': 1.2974238395690918, 'learning_rate': 9.322344322344323e-05, 'epoch': 0.75}
{'eval_loss': 0.3440210223197937, 'eval_accuracy': 0.875, 'eval_runtime': 130.8618, 'eval_samples_per_second': 4.875, 'eval_steps_per_second': 4.875, 'epoch': 0.75}
********************save call back********************
********************on step end call back********************
Step 4110 finish
{'loss': 0.5748, 'grad_norm': 1.3393995761871338, 'learning_rate': 9.320512820512821e-05, 'epoch': 0.75}
********************on step end call back********************
Step 4120 finish
{'loss': 0.5215, 'grad_norm': 1.216455340385437, 'learning_rate': 9.318681318681319e-05, 'epoch': 0.75}
********************on step end call back********************
Step 4130 finish
{'loss': 0.5198, 'grad_norm': 1.1116979122161865, 'learning_rate': 9.316849816849818e-05, 'epoch': 0.75}
********************on step end call back********************
Step 4140 finish
{'loss': 0.5882, 'grad_norm': 1.5457587242126465, 'learning_rate': 9.315018315018316e-05, 'epoch': 0.75}
********************on step end call back********************
Step 4150 finish
{'loss': 0.5581, 'grad_norm': 1.2007663249969482, 'learning_rate': 9.313186813186814e-05, 'epoch': 0.75}
********************on step end call back********************
Step 4160 finish
{'loss': 0.552, 'grad_norm': 1.3715792894363403, 'learning_rate': 9.311355311355312e-05, 'epoch': 0.76}
********************on step end call back********************
Step 4170 finish
{'loss': 0.6339, 'grad_norm': 1.2760214805603027, 'learning_rate': 9.309523809523811e-05, 'epoch': 0.76}
********************on step end call back********************
Step 4180 finish
{'loss': 0.5189, 'grad_norm': 1.2039347887039185, 'learning_rate': 9.307692307692309e-05, 'epoch': 0.76}
********************on step end call back********************
Step 4190 finish
{'loss': 0.5673, 'grad_norm': 1.2246953248977661, 'learning_rate': 9.305860805860807e-05, 'epoch': 0.76}
********************on step end call back********************
Step 4200 finish
{'loss': 0.5733, 'grad_norm': 1.517141342163086, 'learning_rate': 9.304029304029304e-05, 'epoch': 0.76}
{'eval_loss': 0.3456399440765381, 'eval_accuracy': 0.8645833333333334, 'eval_runtime': 130.9357, 'eval_samples_per_second': 4.873, 'eval_steps_per_second': 4.873, 'epoch': 0.76}
********************save call back********************
********************on step end call back********************
Step 4210 finish
{'loss': 0.5904, 'grad_norm': 1.1678071022033691, 'learning_rate': 9.302197802197802e-05, 'epoch': 0.77}
********************on step end call back********************
Step 4220 finish
{'loss': 0.5427, 'grad_norm': 1.2146644592285156, 'learning_rate': 9.300366300366302e-05, 'epoch': 0.77}
********************on step end call back********************
Step 4230 finish
{'loss': 0.5591, 'grad_norm': 1.4462299346923828, 'learning_rate': 9.2985347985348e-05, 'epoch': 0.77}
********************on step end call back********************
Step 4240 finish
{'loss': 0.5908, 'grad_norm': 1.2752679586410522, 'learning_rate': 9.296703296703297e-05, 'epoch': 0.77}
********************on step end call back********************
Step 4250 finish
{'loss': 0.54, 'grad_norm': 1.6012412309646606, 'learning_rate': 9.294871794871795e-05, 'epoch': 0.77}
********************on step end call back********************
Step 4260 finish
{'loss': 0.6332, 'grad_norm': 1.1108064651489258, 'learning_rate': 9.293040293040294e-05, 'epoch': 0.77}
********************on step end call back********************
Step 4270 finish
{'loss': 0.5411, 'grad_norm': 1.3417311906814575, 'learning_rate': 9.291208791208792e-05, 'epoch': 0.78}
********************on step end call back********************
Step 4280 finish
{'loss': 0.5466, 'grad_norm': 1.4873363971710205, 'learning_rate': 9.28937728937729e-05, 'epoch': 0.78}
********************on step end call back********************
Step 4290 finish
{'loss': 0.4966, 'grad_norm': 1.2873802185058594, 'learning_rate': 9.287545787545788e-05, 'epoch': 0.78}
********************on step end call back********************
Step 4300 finish
{'loss': 0.551, 'grad_norm': 1.1518585681915283, 'learning_rate': 9.285714285714286e-05, 'epoch': 0.78}
{'eval_loss': 0.3411276340484619, 'eval_accuracy': 0.875, 'eval_runtime': 130.1539, 'eval_samples_per_second': 4.902, 'eval_steps_per_second': 4.902, 'epoch': 0.78}
********************save call back********************
********************on step end call back********************
Step 4310 finish
{'loss': 0.5735, 'grad_norm': 1.3100661039352417, 'learning_rate': 9.283882783882785e-05, 'epoch': 0.78}
********************on step end call back********************
Step 4320 finish
{'loss': 0.4771, 'grad_norm': 1.1808865070343018, 'learning_rate': 9.282051282051283e-05, 'epoch': 0.79}
********************on step end call back********************
Step 4330 finish
{'loss': 0.5213, 'grad_norm': 1.5336122512817383, 'learning_rate': 9.280219780219781e-05, 'epoch': 0.79}
********************on step end call back********************
Step 4340 finish
{'loss': 0.5167, 'grad_norm': 1.2337183952331543, 'learning_rate': 9.278388278388279e-05, 'epoch': 0.79}
********************on step end call back********************
Step 4350 finish
{'loss': 0.542, 'grad_norm': 1.6308791637420654, 'learning_rate': 9.276556776556778e-05, 'epoch': 0.79}
********************on step end call back********************
Step 4360 finish
{'loss': 0.5509, 'grad_norm': 1.5702639818191528, 'learning_rate': 9.274725274725276e-05, 'epoch': 0.79}
********************on step end call back********************
Step 4370 finish
{'loss': 0.5219, 'grad_norm': 0.8930760622024536, 'learning_rate': 9.272893772893774e-05, 'epoch': 0.79}
********************on step end call back********************
Step 4380 finish
{'loss': 0.57, 'grad_norm': 1.1060817241668701, 'learning_rate': 9.271062271062272e-05, 'epoch': 0.8}
********************on step end call back********************
Step 4390 finish
[INFO|trainer.py:3376] 2024-03-22 04:40:06,021 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-22 04:40:06,022 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-22 04:40:06,022 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-22 04:42:16,396 >> Saving model checkpoint to ./output/tmp-checkpoint-4400
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-22 04:42:16,544 >> tokenizer config file saved in ./output/tmp-checkpoint-4400/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-22 04:42:16,544 >> Special tokens file saved in ./output/tmp-checkpoint-4400/special_tokens_map.json
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-22 04:50:57,588 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-22 04:50:57,588 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-22 04:50:57,588 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-22 04:53:08,776 >> Saving model checkpoint to ./output/tmp-checkpoint-4500
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-22 04:53:08,918 >> tokenizer config file saved in ./output/tmp-checkpoint-4500/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-22 04:53:08,918 >> Special tokens file saved in ./output/tmp-checkpoint-4500/special_tokens_map.json
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-22 05:01:44,425 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-22 05:01:44,425 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-22 05:01:44,425 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-22 05:03:54,689 >> Saving model checkpoint to ./output/tmp-checkpoint-4600
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-22 05:03:55,090 >> tokenizer config file saved in ./output/tmp-checkpoint-4600/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-22 05:03:55,090 >> Special tokens file saved in ./output/tmp-checkpoint-4600/special_tokens_map.json
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-22 05:12:32,776 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-22 05:12:32,777 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-22 05:12:32,777 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-22 05:14:42,735 >> Saving model checkpoint to ./output/tmp-checkpoint-4700
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-22 05:14:42,858 >> tokenizer config file saved in ./output/tmp-checkpoint-4700/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-22 05:14:42,858 >> Special tokens file saved in ./output/tmp-checkpoint-4700/special_tokens_map.json
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
{'loss': 0.5056, 'grad_norm': 1.3632702827453613, 'learning_rate': 9.26923076923077e-05, 'epoch': 0.8}
********************on step end call back********************
Step 4400 finish
{'loss': 0.4897, 'grad_norm': 1.3703374862670898, 'learning_rate': 9.267399267399269e-05, 'epoch': 0.8}
{'eval_loss': 0.3336949944496155, 'eval_accuracy': 0.8541666666666666, 'eval_runtime': 130.3725, 'eval_samples_per_second': 4.894, 'eval_steps_per_second': 4.894, 'epoch': 0.8}
********************save call back********************
********************on step end call back********************
Step 4410 finish
{'loss': 0.5521, 'grad_norm': 1.1298891305923462, 'learning_rate': 9.265567765567767e-05, 'epoch': 0.8}
********************on step end call back********************
Step 4420 finish
{'loss': 0.6231, 'grad_norm': 1.1090658903121948, 'learning_rate': 9.263736263736264e-05, 'epoch': 0.8}
********************on step end call back********************
Step 4430 finish
{'loss': 0.5422, 'grad_norm': 1.3912073373794556, 'learning_rate': 9.261904761904762e-05, 'epoch': 0.81}
********************on step end call back********************
Step 4440 finish
{'loss': 0.5547, 'grad_norm': 1.0542869567871094, 'learning_rate': 9.260073260073262e-05, 'epoch': 0.81}
********************on step end call back********************
Step 4450 finish
{'loss': 0.5285, 'grad_norm': 1.383339762687683, 'learning_rate': 9.25824175824176e-05, 'epoch': 0.81}
********************on step end call back********************
Step 4460 finish
{'loss': 0.5598, 'grad_norm': 1.1755009889602661, 'learning_rate': 9.256410256410257e-05, 'epoch': 0.81}
********************on step end call back********************
Step 4470 finish
{'loss': 0.5725, 'grad_norm': 1.056693196296692, 'learning_rate': 9.254578754578755e-05, 'epoch': 0.81}
********************on step end call back********************
Step 4480 finish
{'loss': 0.4915, 'grad_norm': 1.0525280237197876, 'learning_rate': 9.252747252747253e-05, 'epoch': 0.81}
********************on step end call back********************
Step 4490 finish
{'loss': 0.6169, 'grad_norm': 1.2632043361663818, 'learning_rate': 9.250915750915752e-05, 'epoch': 0.82}
********************on step end call back********************
Step 4500 finish
{'loss': 0.5529, 'grad_norm': 1.2433700561523438, 'learning_rate': 9.24908424908425e-05, 'epoch': 0.82}
{'eval_loss': 0.3487786054611206, 'eval_accuracy': 0.8645833333333334, 'eval_runtime': 131.1868, 'eval_samples_per_second': 4.863, 'eval_steps_per_second': 4.863, 'epoch': 0.82}
********************save call back********************
********************on step end call back********************
Step 4510 finish
{'loss': 0.5488, 'grad_norm': 1.1371433734893799, 'learning_rate': 9.247252747252748e-05, 'epoch': 0.82}
********************on step end call back********************
Step 4520 finish
{'loss': 0.5829, 'grad_norm': 1.3106343746185303, 'learning_rate': 9.245421245421246e-05, 'epoch': 0.82}
********************on step end call back********************
Step 4530 finish
{'loss': 0.5163, 'grad_norm': 1.0926169157028198, 'learning_rate': 9.243589743589745e-05, 'epoch': 0.82}
********************on step end call back********************
Step 4540 finish
{'loss': 0.5232, 'grad_norm': 1.3084288835525513, 'learning_rate': 9.241758241758243e-05, 'epoch': 0.83}
********************on step end call back********************
Step 4550 finish
{'loss': 0.5479, 'grad_norm': 1.1783732175827026, 'learning_rate': 9.239926739926741e-05, 'epoch': 0.83}
********************on step end call back********************
Step 4560 finish
{'loss': 0.5416, 'grad_norm': 1.282858967781067, 'learning_rate': 9.238095238095239e-05, 'epoch': 0.83}
********************on step end call back********************
Step 4570 finish
{'loss': 0.583, 'grad_norm': 1.6783185005187988, 'learning_rate': 9.236263736263737e-05, 'epoch': 0.83}
********************on step end call back********************
Step 4580 finish
{'loss': 0.6444, 'grad_norm': 1.3214111328125, 'learning_rate': 9.234432234432236e-05, 'epoch': 0.83}
********************on step end call back********************
Step 4590 finish
{'loss': 0.5463, 'grad_norm': 1.2474079132080078, 'learning_rate': 9.232600732600734e-05, 'epoch': 0.83}
********************on step end call back********************
Step 4600 finish
{'loss': 0.5497, 'grad_norm': 1.311220407485962, 'learning_rate': 9.230769230769232e-05, 'epoch': 0.84}
{'eval_loss': 0.3404925763607025, 'eval_accuracy': 0.8541666666666666, 'eval_runtime': 130.2628, 'eval_samples_per_second': 4.898, 'eval_steps_per_second': 4.898, 'epoch': 0.84}
********************save call back********************
********************on step end call back********************
Step 4610 finish
{'loss': 0.5541, 'grad_norm': 1.2280056476593018, 'learning_rate': 9.22893772893773e-05, 'epoch': 0.84}
********************on step end call back********************
Step 4620 finish
{'loss': 0.5142, 'grad_norm': 1.1622000932693481, 'learning_rate': 9.227106227106229e-05, 'epoch': 0.84}
********************on step end call back********************
Step 4630 finish
{'loss': 0.5669, 'grad_norm': 1.6892726421356201, 'learning_rate': 9.225274725274727e-05, 'epoch': 0.84}
********************on step end call back********************
Step 4640 finish
{'loss': 0.5226, 'grad_norm': 1.0373508930206299, 'learning_rate': 9.223443223443224e-05, 'epoch': 0.84}
********************on step end call back********************
Step 4650 finish
{'loss': 0.5446, 'grad_norm': 1.1222195625305176, 'learning_rate': 9.221611721611722e-05, 'epoch': 0.85}
********************on step end call back********************
Step 4660 finish
{'loss': 0.491, 'grad_norm': 1.1134381294250488, 'learning_rate': 9.21978021978022e-05, 'epoch': 0.85}
********************on step end call back********************
Step 4670 finish
{'loss': 0.5005, 'grad_norm': 1.2971452474594116, 'learning_rate': 9.217948717948718e-05, 'epoch': 0.85}
********************on step end call back********************
Step 4680 finish
{'loss': 0.5604, 'grad_norm': 1.5273025035858154, 'learning_rate': 9.216117216117216e-05, 'epoch': 0.85}
********************on step end call back********************
Step 4690 finish
{'loss': 0.6112, 'grad_norm': 1.021766185760498, 'learning_rate': 9.214285714285714e-05, 'epoch': 0.85}
********************on step end call back********************
Step 4700 finish
{'loss': 0.5616, 'grad_norm': 1.0014092922210693, 'learning_rate': 9.212454212454212e-05, 'epoch': 0.85}
{'eval_loss': 0.3410874903202057, 'eval_accuracy': 0.875, 'eval_runtime': 129.9574, 'eval_samples_per_second': 4.909, 'eval_steps_per_second': 4.909, 'epoch': 0.85}
********************save call back********************
********************on step end call back********************
Step 4710 finish
{'loss': 0.555, 'grad_norm': 1.1957091093063354, 'learning_rate': 9.210622710622711e-05, 'epoch': 0.86}
********************on step end call back********************
Step 4720 finish
{'loss': 0.6087, 'grad_norm': 1.5752525329589844, 'learning_rate': 9.208791208791209e-05, 'epoch': 0.86}
********************on step end call back********************
Step 4730 finish
{'loss': 0.6052, 'grad_norm': 1.1292572021484375, 'learning_rate': 9.206959706959707e-05, 'epoch': 0.86}
********************on step end call back********************
Step 4740 finish
{'loss': 0.5524, 'grad_norm': 1.1314318180084229, 'learning_rate': 9.205128205128205e-05, 'epoch': 0.86}
********************on step end call back********************
Step 4750 finish
{'loss': 0.5818, 'grad_norm': 1.4252773523330688, 'learning_rate': 9.203296703296704e-05, 'epoch': 0.86}
********************on step end call back********************
Step 4760 finish
{'loss': 0.5505, 'grad_norm': 1.085267424583435, 'learning_rate': 9.201465201465202e-05, 'epoch': 0.87}
********************on step end call back********************
Step 4770 finish
{'loss': 0.561, 'grad_norm': 1.4270089864730835, 'learning_rate': 9.1996336996337e-05, 'epoch': 0.87}
********************on step end call back********************
Step 4780 finish
{'loss': 0.5409, 'grad_norm': 1.3053280115127563, 'learning_rate': 9.197802197802197e-05, 'epoch': 0.87}
[INFO|trainer.py:3376] 2024-03-22 05:23:20,400 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-22 05:23:20,400 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-22 05:23:20,400 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-22 05:25:30,447 >> Saving model checkpoint to ./output/tmp-checkpoint-4800
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-22 05:25:30,587 >> tokenizer config file saved in ./output/tmp-checkpoint-4800/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-22 05:25:30,587 >> Special tokens file saved in ./output/tmp-checkpoint-4800/special_tokens_map.json
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-22 05:34:07,820 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-22 05:34:07,820 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-22 05:34:07,820 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-22 05:36:17,673 >> Saving model checkpoint to ./output/tmp-checkpoint-4900
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-22 05:36:17,815 >> tokenizer config file saved in ./output/tmp-checkpoint-4900/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-22 05:36:17,815 >> Special tokens file saved in ./output/tmp-checkpoint-4900/special_tokens_map.json
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-22 05:44:49,646 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-22 05:44:49,646 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-22 05:44:49,646 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-22 05:46:59,794 >> Saving model checkpoint to ./output/tmp-checkpoint-5000
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-22 05:46:59,937 >> tokenizer config file saved in ./output/tmp-checkpoint-5000/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-22 05:46:59,937 >> Special tokens file saved in ./output/tmp-checkpoint-5000/special_tokens_map.json
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-22 05:55:35,556 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-22 05:55:35,556 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-22 05:55:35,556 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-22 05:57:45,644 >> Saving model checkpoint to ./output/tmp-checkpoint-5100
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-22 05:57:45,783 >> tokenizer config file saved in ./output/tmp-checkpoint-5100/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-22 05:57:45,783 >> Special tokens file saved in ./output/tmp-checkpoint-5100/special_tokens_map.json
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
********************on step end call back********************
Step 4790 finish
{'loss': 0.4878, 'grad_norm': 0.9370599389076233, 'learning_rate': 9.195970695970695e-05, 'epoch': 0.87}
********************on step end call back********************
Step 4800 finish
{'loss': 0.5573, 'grad_norm': 1.3819873332977295, 'learning_rate': 9.194139194139195e-05, 'epoch': 0.87}
{'eval_loss': 0.3344615697860718, 'eval_accuracy': 0.8645833333333334, 'eval_runtime': 130.0457, 'eval_samples_per_second': 4.906, 'eval_steps_per_second': 4.906, 'epoch': 0.87}
********************save call back********************
********************on step end call back********************
Step 4810 finish
{'loss': 0.607, 'grad_norm': 1.2550677061080933, 'learning_rate': 9.192307692307692e-05, 'epoch': 0.87}
********************on step end call back********************
Step 4820 finish
{'loss': 0.5664, 'grad_norm': 0.9771403074264526, 'learning_rate': 9.19047619047619e-05, 'epoch': 0.88}
********************on step end call back********************
Step 4830 finish
{'loss': 0.4816, 'grad_norm': 1.2060205936431885, 'learning_rate': 9.188644688644688e-05, 'epoch': 0.88}
********************on step end call back********************
Step 4840 finish
{'loss': 0.5416, 'grad_norm': 1.2491103410720825, 'learning_rate': 9.186813186813187e-05, 'epoch': 0.88}
********************on step end call back********************
Step 4850 finish
{'loss': 0.5652, 'grad_norm': 1.2828693389892578, 'learning_rate': 9.184981684981685e-05, 'epoch': 0.88}
********************on step end call back********************
Step 4860 finish
{'loss': 0.55, 'grad_norm': 1.0402292013168335, 'learning_rate': 9.183150183150183e-05, 'epoch': 0.88}
********************on step end call back********************
Step 4870 finish
{'loss': 0.5646, 'grad_norm': 1.3131091594696045, 'learning_rate': 9.181318681318681e-05, 'epoch': 0.89}
********************on step end call back********************
Step 4880 finish
{'loss': 0.5691, 'grad_norm': 1.329066514968872, 'learning_rate': 9.179487179487179e-05, 'epoch': 0.89}
********************on step end call back********************
Step 4890 finish
{'loss': 0.5155, 'grad_norm': 1.312243938446045, 'learning_rate': 9.177655677655678e-05, 'epoch': 0.89}
********************on step end call back********************
Step 4900 finish
{'loss': 0.5624, 'grad_norm': 1.0900137424468994, 'learning_rate': 9.175824175824176e-05, 'epoch': 0.89}
{'eval_loss': 0.3302278220653534, 'eval_accuracy': 0.8645833333333334, 'eval_runtime': 129.8522, 'eval_samples_per_second': 4.913, 'eval_steps_per_second': 4.913, 'epoch': 0.89}
********************save call back********************
********************on step end call back********************
Step 4910 finish
{'loss': 0.5777, 'grad_norm': 1.1450389623641968, 'learning_rate': 9.173992673992674e-05, 'epoch': 0.89}
********************on step end call back********************
Step 4920 finish
{'loss': 0.522, 'grad_norm': 1.550856590270996, 'learning_rate': 9.172161172161172e-05, 'epoch': 0.89}
********************on step end call back********************
Step 4930 finish
{'loss': 0.4756, 'grad_norm': 1.1657527685165405, 'learning_rate': 9.170329670329671e-05, 'epoch': 0.9}
********************on step end call back********************
Step 4940 finish
{'loss': 0.4952, 'grad_norm': 1.0435376167297363, 'learning_rate': 9.168498168498169e-05, 'epoch': 0.9}
********************on step end call back********************
Step 4950 finish
{'loss': 0.5749, 'grad_norm': 1.371519684791565, 'learning_rate': 9.166666666666667e-05, 'epoch': 0.9}
********************on step end call back********************
Step 4960 finish
{'loss': 0.6171, 'grad_norm': 1.4470785856246948, 'learning_rate': 9.164835164835165e-05, 'epoch': 0.9}
********************on step end call back********************
Step 4970 finish
{'loss': 0.5571, 'grad_norm': 1.462218999862671, 'learning_rate': 9.163003663003663e-05, 'epoch': 0.9}
********************on step end call back********************
Step 4980 finish
{'loss': 0.5608, 'grad_norm': 1.8077576160430908, 'learning_rate': 9.161172161172162e-05, 'epoch': 0.91}
********************on step end call back********************
Step 4990 finish
{'loss': 0.567, 'grad_norm': 1.3919293880462646, 'learning_rate': 9.15934065934066e-05, 'epoch': 0.91}
********************on step end call back********************
Step 5000 finish
{'loss': 0.5248, 'grad_norm': 0.9897246956825256, 'learning_rate': 9.157509157509158e-05, 'epoch': 0.91}
{'eval_loss': 0.3372472822666168, 'eval_accuracy': 0.8645833333333334, 'eval_runtime': 130.147, 'eval_samples_per_second': 4.902, 'eval_steps_per_second': 4.902, 'epoch': 0.91}
********************save call back********************
********************on step end call back********************
Step 5010 finish
{'loss': 0.5134, 'grad_norm': 1.0436736345291138, 'learning_rate': 9.155677655677655e-05, 'epoch': 0.91}
********************on step end call back********************
Step 5020 finish
{'loss': 0.5629, 'grad_norm': 1.3416602611541748, 'learning_rate': 9.153846153846155e-05, 'epoch': 0.91}
********************on step end call back********************
Step 5030 finish
{'loss': 0.5169, 'grad_norm': 1.4136250019073486, 'learning_rate': 9.152014652014652e-05, 'epoch': 0.91}
********************on step end call back********************
Step 5040 finish
{'loss': 0.574, 'grad_norm': 1.4180755615234375, 'learning_rate': 9.15018315018315e-05, 'epoch': 0.92}
********************on step end call back********************
Step 5050 finish
{'loss': 0.5406, 'grad_norm': 1.120788812637329, 'learning_rate': 9.148351648351648e-05, 'epoch': 0.92}
********************on step end call back********************
Step 5060 finish
{'loss': 0.6279, 'grad_norm': 1.2164361476898193, 'learning_rate': 9.146520146520146e-05, 'epoch': 0.92}
********************on step end call back********************
Step 5070 finish
{'loss': 0.5272, 'grad_norm': 1.3203078508377075, 'learning_rate': 9.144688644688645e-05, 'epoch': 0.92}
********************on step end call back********************
Step 5080 finish
{'loss': 0.5463, 'grad_norm': 1.0684419870376587, 'learning_rate': 9.142857142857143e-05, 'epoch': 0.92}
********************on step end call back********************
Step 5090 finish
{'loss': 0.5504, 'grad_norm': 1.1509181261062622, 'learning_rate': 9.141025641025641e-05, 'epoch': 0.93}
********************on step end call back********************
Step 5100 finish
{'loss': 0.5401, 'grad_norm': 1.1939458847045898, 'learning_rate': 9.139194139194139e-05, 'epoch': 0.93}
{'eval_loss': 0.3312320113182068, 'eval_accuracy': 0.875, 'eval_runtime': 130.0868, 'eval_samples_per_second': 4.904, 'eval_steps_per_second': 4.904, 'epoch': 0.93}
********************save call back********************
********************on step end call back********************
Step 5110 finish
{'loss': 0.5153, 'grad_norm': 1.0542608499526978, 'learning_rate': 9.137362637362638e-05, 'epoch': 0.93}
********************on step end call back********************
Step 5120 finish
{'loss': 0.4878, 'grad_norm': 1.1441181898117065, 'learning_rate': 9.135531135531136e-05, 'epoch': 0.93}
********************on step end call back********************
Step 5130 finish
{'loss': 0.5874, 'grad_norm': 1.6233439445495605, 'learning_rate': 9.133699633699634e-05, 'epoch': 0.93}
********************on step end call back********************
Step 5140 finish
{'loss': 0.5489, 'grad_norm': 1.4945762157440186, 'learning_rate': 9.131868131868132e-05, 'epoch': 0.93}
********************on step end call back********************
Step 5150 finish
{'loss': 0.4864, 'grad_norm': 1.2139863967895508, 'learning_rate': 9.13003663003663e-05, 'epoch': 0.94}
********************on step end call back********************
Step 5160 finish
{'loss': 0.5939, 'grad_norm': 1.8506113290786743, 'learning_rate': 9.128205128205129e-05, 'epoch': 0.94}
********************on step end call back********************
Step 5170 finish
{'loss': 0.5567, 'grad_norm': 1.1677045822143555, 'learning_rate': 9.126373626373627e-05, 'epoch': 0.94}
********************on step end call back********************
Step 5180 finish
[INFO|trainer.py:3376] 2024-03-22 06:06:25,248 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-22 06:06:25,248 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-22 06:06:25,248 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-22 06:08:35,061 >> Saving model checkpoint to ./output/tmp-checkpoint-5200
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-22 06:08:35,207 >> tokenizer config file saved in ./output/tmp-checkpoint-5200/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-22 06:08:35,207 >> Special tokens file saved in ./output/tmp-checkpoint-5200/special_tokens_map.json
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-22 06:17:14,306 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-22 06:17:14,306 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-22 06:17:14,306 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-22 06:19:24,009 >> Saving model checkpoint to ./output/tmp-checkpoint-5300
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-22 06:19:24,155 >> tokenizer config file saved in ./output/tmp-checkpoint-5300/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-22 06:19:24,155 >> Special tokens file saved in ./output/tmp-checkpoint-5300/special_tokens_map.json
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-22 06:28:09,466 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-22 06:28:09,467 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-22 06:28:09,467 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-22 06:30:19,147 >> Saving model checkpoint to ./output/tmp-checkpoint-5400
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-22 06:30:19,288 >> tokenizer config file saved in ./output/tmp-checkpoint-5400/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-22 06:30:19,288 >> Special tokens file saved in ./output/tmp-checkpoint-5400/special_tokens_map.json
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-22 06:39:03,531 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-22 06:39:03,532 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-22 06:39:03,532 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-22 06:41:12,964 >> Saving model checkpoint to ./output/tmp-checkpoint-5500
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-22 06:41:13,104 >> tokenizer config file saved in ./output/tmp-checkpoint-5500/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-22 06:41:13,104 >> Special tokens file saved in ./output/tmp-checkpoint-5500/special_tokens_map.json
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
{'loss': 0.46, 'grad_norm': 1.1712902784347534, 'learning_rate': 9.124542124542125e-05, 'epoch': 0.94}
********************on step end call back********************
Step 5190 finish
{'loss': 0.5297, 'grad_norm': 1.491818904876709, 'learning_rate': 9.122710622710623e-05, 'epoch': 0.94}
********************on step end call back********************
Step 5200 finish
{'loss': 0.5295, 'grad_norm': 1.4021192789077759, 'learning_rate': 9.12087912087912e-05, 'epoch': 0.95}
{'eval_loss': 0.343340128660202, 'eval_accuracy': 0.8541666666666666, 'eval_runtime': 129.8116, 'eval_samples_per_second': 4.915, 'eval_steps_per_second': 4.915, 'epoch': 0.95}
********************save call back********************
********************on step end call back********************
Step 5210 finish
{'loss': 0.5153, 'grad_norm': 1.371235966682434, 'learning_rate': 9.11904761904762e-05, 'epoch': 0.95}
********************on step end call back********************
Step 5220 finish
{'loss': 0.5027, 'grad_norm': 1.4055230617523193, 'learning_rate': 9.117216117216118e-05, 'epoch': 0.95}
********************on step end call back********************
Step 5230 finish
{'loss': 0.5475, 'grad_norm': 0.9523614048957825, 'learning_rate': 9.115384615384615e-05, 'epoch': 0.95}
********************on step end call back********************
Step 5240 finish
{'loss': 0.538, 'grad_norm': 1.1024442911148071, 'learning_rate': 9.113553113553113e-05, 'epoch': 0.95}
********************on step end call back********************
Step 5250 finish
{'loss': 0.5549, 'grad_norm': 0.9926272630691528, 'learning_rate': 9.111721611721613e-05, 'epoch': 0.95}
********************on step end call back********************
Step 5260 finish
{'loss': 0.4782, 'grad_norm': 1.5582385063171387, 'learning_rate': 9.10989010989011e-05, 'epoch': 0.96}
********************on step end call back********************
Step 5270 finish
{'loss': 0.5297, 'grad_norm': 1.1698687076568604, 'learning_rate': 9.108058608058608e-05, 'epoch': 0.96}
********************on step end call back********************
Step 5280 finish
{'loss': 0.5324, 'grad_norm': 1.2318408489227295, 'learning_rate': 9.106227106227106e-05, 'epoch': 0.96}
********************on step end call back********************
Step 5290 finish
{'loss': 0.613, 'grad_norm': 1.0745854377746582, 'learning_rate': 9.104395604395604e-05, 'epoch': 0.96}
********************on step end call back********************
Step 5300 finish
{'loss': 0.5229, 'grad_norm': 1.3545117378234863, 'learning_rate': 9.102564102564103e-05, 'epoch': 0.96}
{'eval_loss': 0.33542922139167786, 'eval_accuracy': 0.90625, 'eval_runtime': 129.7024, 'eval_samples_per_second': 4.919, 'eval_steps_per_second': 4.919, 'epoch': 0.96}
********************save call back********************
********************on step end call back********************
Step 5310 finish
{'loss': 0.5077, 'grad_norm': 1.1659280061721802, 'learning_rate': 9.100732600732601e-05, 'epoch': 0.97}
********************on step end call back********************
Step 5320 finish
{'loss': 0.5845, 'grad_norm': 1.1223444938659668, 'learning_rate': 9.098901098901099e-05, 'epoch': 0.97}
********************on step end call back********************
Step 5330 finish
{'loss': 0.5203, 'grad_norm': 0.8908870220184326, 'learning_rate': 9.097069597069597e-05, 'epoch': 0.97}
********************on step end call back********************
Step 5340 finish
{'loss': 0.5425, 'grad_norm': 1.2929433584213257, 'learning_rate': 9.095238095238096e-05, 'epoch': 0.97}
********************on step end call back********************
Step 5350 finish
{'loss': 0.5268, 'grad_norm': 1.1148183345794678, 'learning_rate': 9.093406593406594e-05, 'epoch': 0.97}
********************on step end call back********************
Step 5360 finish
{'loss': 0.5248, 'grad_norm': 0.8390803933143616, 'learning_rate': 9.091575091575092e-05, 'epoch': 0.97}
********************on step end call back********************
Step 5370 finish
{'loss': 0.5774, 'grad_norm': 1.324987769126892, 'learning_rate': 9.08974358974359e-05, 'epoch': 0.98}
********************on step end call back********************
Step 5380 finish
{'loss': 0.4783, 'grad_norm': 1.1613657474517822, 'learning_rate': 9.087912087912088e-05, 'epoch': 0.98}
********************on step end call back********************
Step 5390 finish
{'loss': 0.5255, 'grad_norm': 1.2663190364837646, 'learning_rate': 9.086080586080587e-05, 'epoch': 0.98}
********************on step end call back********************
Step 5400 finish
{'loss': 0.5313, 'grad_norm': 1.4773144721984863, 'learning_rate': 9.084249084249085e-05, 'epoch': 0.98}
{'eval_loss': 0.32966411113739014, 'eval_accuracy': 0.875, 'eval_runtime': 129.6793, 'eval_samples_per_second': 4.92, 'eval_steps_per_second': 4.92, 'epoch': 0.98}
********************save call back********************
********************on step end call back********************
Step 5410 finish
{'loss': 0.4696, 'grad_norm': 1.161699891090393, 'learning_rate': 9.082417582417583e-05, 'epoch': 0.98}
********************on step end call back********************
Step 5420 finish
{'loss': 0.5205, 'grad_norm': 1.0212974548339844, 'learning_rate': 9.08058608058608e-05, 'epoch': 0.99}
********************on step end call back********************
Step 5430 finish
{'loss': 0.4965, 'grad_norm': 1.213659644126892, 'learning_rate': 9.07875457875458e-05, 'epoch': 0.99}
********************on step end call back********************
Step 5440 finish
{'loss': 0.561, 'grad_norm': 1.204361915588379, 'learning_rate': 9.076923076923078e-05, 'epoch': 0.99}
********************on step end call back********************
Step 5450 finish
{'loss': 0.5629, 'grad_norm': 1.1494839191436768, 'learning_rate': 9.075091575091575e-05, 'epoch': 0.99}
********************on step end call back********************
Step 5460 finish
{'loss': 0.5618, 'grad_norm': 1.2462502717971802, 'learning_rate': 9.073260073260073e-05, 'epoch': 0.99}
********************on step end call back********************
Step 5470 finish
{'loss': 0.5004, 'grad_norm': 1.1090748310089111, 'learning_rate': 9.071428571428571e-05, 'epoch': 0.99}
********************on step end call back********************
Step 5480 finish
{'loss': 0.5717, 'grad_norm': 1.4214245080947876, 'learning_rate': 9.06959706959707e-05, 'epoch': 1.0}
********************on step end call back********************
Step 5490 finish
{'loss': 0.4852, 'grad_norm': 1.233595371246338, 'learning_rate': 9.067765567765568e-05, 'epoch': 1.0}
********************on step end call back********************
Step 5500 finish
{'loss': 0.5508, 'grad_norm': 1.3220237493515015, 'learning_rate': 9.065934065934066e-05, 'epoch': 1.0}
{'eval_loss': 0.33498039841651917, 'eval_accuracy': 0.8958333333333334, 'eval_runtime': 129.4317, 'eval_samples_per_second': 4.929, 'eval_steps_per_second': 4.929, 'epoch': 1.0}
********************save call back********************
********************on epoch end call back********************
Epoch 0.9998863765481195 finish
********************on step end call back********************
Step 5510 finish
{'loss': 0.4627, 'grad_norm': 1.0888150930404663, 'learning_rate': 9.064102564102564e-05, 'epoch': 1.0}
********************on step end call back********************
Step 5520 finish
{'loss': 0.4821, 'grad_norm': 1.3104166984558105, 'learning_rate': 9.062271062271063e-05, 'epoch': 1.0}
********************on step end call back********************
Step 5530 finish
{'loss': 0.5273, 'grad_norm': 1.5170669555664062, 'learning_rate': 9.060439560439561e-05, 'epoch': 1.01}
********************on step end call back********************
Step 5540 finish
{'loss': 0.5038, 'grad_norm': 1.4034976959228516, 'learning_rate': 9.058608058608059e-05, 'epoch': 1.01}
********************on step end call back********************
Step 5550 finish
{'loss': 0.4662, 'grad_norm': 1.219955563545227, 'learning_rate': 9.056776556776557e-05, 'epoch': 1.01}
********************on step end call back********************
Step 5560 finish
{'loss': 0.4271, 'grad_norm': 1.1516852378845215, 'learning_rate': 9.054945054945055e-05, 'epoch': 1.01}
********************on step end call back********************
Step 5570 finish
[INFO|trainer.py:3376] 2024-03-22 06:49:47,150 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-22 06:49:47,150 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-22 06:49:47,150 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-22 06:51:56,574 >> Saving model checkpoint to ./output/tmp-checkpoint-5600
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-22 06:51:56,718 >> tokenizer config file saved in ./output/tmp-checkpoint-5600/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-22 06:51:56,718 >> Special tokens file saved in ./output/tmp-checkpoint-5600/special_tokens_map.json
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-22 07:00:29,794 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-22 07:00:29,794 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-22 07:00:29,794 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-22 07:02:39,306 >> Saving model checkpoint to ./output/tmp-checkpoint-5700
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-22 07:02:39,449 >> tokenizer config file saved in ./output/tmp-checkpoint-5700/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-22 07:02:39,449 >> Special tokens file saved in ./output/tmp-checkpoint-5700/special_tokens_map.json
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-22 07:11:19,100 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-22 07:11:19,100 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-22 07:11:19,100 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-22 07:13:28,736 >> Saving model checkpoint to ./output/tmp-checkpoint-5800
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-22 07:13:28,877 >> tokenizer config file saved in ./output/tmp-checkpoint-5800/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-22 07:13:28,877 >> Special tokens file saved in ./output/tmp-checkpoint-5800/special_tokens_map.json
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-22 07:22:06,314 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-22 07:22:06,314 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-22 07:22:06,314 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-22 07:24:15,990 >> Saving model checkpoint to ./output/tmp-checkpoint-5900
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-22 07:24:16,134 >> tokenizer config file saved in ./output/tmp-checkpoint-5900/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-22 07:24:16,134 >> Special tokens file saved in ./output/tmp-checkpoint-5900/special_tokens_map.json
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
{'loss': 0.4559, 'grad_norm': 1.0265878438949585, 'learning_rate': 9.053113553113554e-05, 'epoch': 1.01}
********************on step end call back********************
Step 5580 finish
{'loss': 0.4343, 'grad_norm': 1.0890849828720093, 'learning_rate': 9.051282051282052e-05, 'epoch': 1.01}
********************on step end call back********************
Step 5590 finish
{'loss': 0.509, 'grad_norm': 1.2198572158813477, 'learning_rate': 9.04945054945055e-05, 'epoch': 1.02}
********************on step end call back********************
Step 5600 finish
{'loss': 0.4302, 'grad_norm': 1.2566299438476562, 'learning_rate': 9.047619047619048e-05, 'epoch': 1.02}
{'eval_loss': 0.3377609848976135, 'eval_accuracy': 0.8645833333333334, 'eval_runtime': 129.4227, 'eval_samples_per_second': 4.93, 'eval_steps_per_second': 4.93, 'epoch': 1.02}
********************save call back********************
********************on step end call back********************
Step 5610 finish
{'loss': 0.4989, 'grad_norm': 1.8670583963394165, 'learning_rate': 9.045787545787547e-05, 'epoch': 1.02}
********************on step end call back********************
Step 5620 finish
{'loss': 0.4462, 'grad_norm': 1.3713767528533936, 'learning_rate': 9.043956043956045e-05, 'epoch': 1.02}
********************on step end call back********************
Step 5630 finish
{'loss': 0.5436, 'grad_norm': 1.2308982610702515, 'learning_rate': 9.042124542124543e-05, 'epoch': 1.02}
********************on step end call back********************
Step 5640 finish
{'loss': 0.4461, 'grad_norm': 1.1071053743362427, 'learning_rate': 9.04029304029304e-05, 'epoch': 1.03}
********************on step end call back********************
Step 5650 finish
{'loss': 0.4506, 'grad_norm': 1.2133948802947998, 'learning_rate': 9.038461538461538e-05, 'epoch': 1.03}
********************on step end call back********************
Step 5660 finish
{'loss': 0.447, 'grad_norm': 0.8740684390068054, 'learning_rate': 9.036630036630038e-05, 'epoch': 1.03}
********************on step end call back********************
Step 5670 finish
{'loss': 0.4838, 'grad_norm': 1.2572591304779053, 'learning_rate': 9.034798534798536e-05, 'epoch': 1.03}
********************on step end call back********************
Step 5680 finish
{'loss': 0.4277, 'grad_norm': 0.9845490455627441, 'learning_rate': 9.032967032967033e-05, 'epoch': 1.03}
********************on step end call back********************
Step 5690 finish
{'loss': 0.4678, 'grad_norm': 1.0083069801330566, 'learning_rate': 9.031135531135531e-05, 'epoch': 1.03}
********************on step end call back********************
Step 5700 finish
{'loss': 0.4665, 'grad_norm': 0.9527624249458313, 'learning_rate': 9.02930402930403e-05, 'epoch': 1.04}
{'eval_loss': 0.3306386172771454, 'eval_accuracy': 0.875, 'eval_runtime': 129.5111, 'eval_samples_per_second': 4.926, 'eval_steps_per_second': 4.926, 'epoch': 1.04}
********************save call back********************
********************on step end call back********************
Step 5710 finish
{'loss': 0.423, 'grad_norm': 0.9267966151237488, 'learning_rate': 9.027472527472528e-05, 'epoch': 1.04}
********************on step end call back********************
Step 5720 finish
{'loss': 0.5091, 'grad_norm': 1.4213908910751343, 'learning_rate': 9.025641025641026e-05, 'epoch': 1.04}
********************on step end call back********************
Step 5730 finish
{'loss': 0.5376, 'grad_norm': 1.1782207489013672, 'learning_rate': 9.023809523809524e-05, 'epoch': 1.04}
********************on step end call back********************
Step 5740 finish
{'loss': 0.5204, 'grad_norm': 1.1185157299041748, 'learning_rate': 9.021978021978022e-05, 'epoch': 1.04}
********************on step end call back********************
Step 5750 finish
{'loss': 0.521, 'grad_norm': 1.3043911457061768, 'learning_rate': 9.020146520146521e-05, 'epoch': 1.05}
********************on step end call back********************
Step 5760 finish
{'loss': 0.4632, 'grad_norm': 1.0196832418441772, 'learning_rate': 9.018315018315019e-05, 'epoch': 1.05}
********************on step end call back********************
Step 5770 finish
{'loss': 0.4699, 'grad_norm': 1.2188167572021484, 'learning_rate': 9.016483516483517e-05, 'epoch': 1.05}
********************on step end call back********************
Step 5780 finish
{'loss': 0.4606, 'grad_norm': 1.001357913017273, 'learning_rate': 9.014652014652015e-05, 'epoch': 1.05}
********************on step end call back********************
Step 5790 finish
{'loss': 0.4235, 'grad_norm': 1.1452480554580688, 'learning_rate': 9.012820512820514e-05, 'epoch': 1.05}
********************on step end call back********************
Step 5800 finish
{'loss': 0.4402, 'grad_norm': 0.9975769519805908, 'learning_rate': 9.010989010989012e-05, 'epoch': 1.05}
{'eval_loss': 0.3289586305618286, 'eval_accuracy': 0.875, 'eval_runtime': 129.6351, 'eval_samples_per_second': 4.922, 'eval_steps_per_second': 4.922, 'epoch': 1.05}
********************save call back********************
********************on step end call back********************
Step 5810 finish
{'loss': 0.4695, 'grad_norm': 1.2022665739059448, 'learning_rate': 9.00915750915751e-05, 'epoch': 1.06}
********************on step end call back********************
Step 5820 finish
{'loss': 0.4668, 'grad_norm': 1.1057640314102173, 'learning_rate': 9.007326007326008e-05, 'epoch': 1.06}
********************on step end call back********************
Step 5830 finish
{'loss': 0.4142, 'grad_norm': 1.3176884651184082, 'learning_rate': 9.005494505494506e-05, 'epoch': 1.06}
********************on step end call back********************
Step 5840 finish
{'loss': 0.4764, 'grad_norm': 1.2945858240127563, 'learning_rate': 9.003663003663005e-05, 'epoch': 1.06}
********************on step end call back********************
Step 5850 finish
{'loss': 0.4727, 'grad_norm': 1.3623673915863037, 'learning_rate': 9.001831501831503e-05, 'epoch': 1.06}
********************on step end call back********************
Step 5860 finish
{'loss': 0.5327, 'grad_norm': 0.8742676973342896, 'learning_rate': 9e-05, 'epoch': 1.07}
********************on step end call back********************
Step 5870 finish
{'loss': 0.5302, 'grad_norm': 1.265980839729309, 'learning_rate': 8.998168498168498e-05, 'epoch': 1.07}
********************on step end call back********************
Step 5880 finish
{'loss': 0.4567, 'grad_norm': 1.329316258430481, 'learning_rate': 8.996336996336998e-05, 'epoch': 1.07}
********************on step end call back********************
Step 5890 finish
{'loss': 0.4607, 'grad_norm': 1.1081796884536743, 'learning_rate': 8.994505494505496e-05, 'epoch': 1.07}
********************on step end call back********************
Step 5900 finish
{'loss': 0.4629, 'grad_norm': 1.1835063695907593, 'learning_rate': 8.992673992673993e-05, 'epoch': 1.07}
{'eval_loss': 0.3195502758026123, 'eval_accuracy': 0.8854166666666666, 'eval_runtime': 129.6753, 'eval_samples_per_second': 4.92, 'eval_steps_per_second': 4.92, 'epoch': 1.07}
********************save call back********************
********************on step end call back********************
Step 5910 finish
{'loss': 0.4743, 'grad_norm': 1.3944190740585327, 'learning_rate': 8.990842490842491e-05, 'epoch': 1.07}
********************on step end call back********************
Step 5920 finish
{'loss': 0.4916, 'grad_norm': 1.1136530637741089, 'learning_rate': 8.989010989010989e-05, 'epoch': 1.08}
********************on step end call back********************
Step 5930 finish
{'loss': 0.5218, 'grad_norm': 1.1283591985702515, 'learning_rate': 8.987179487179488e-05, 'epoch': 1.08}
********************on step end call back********************
Step 5940 finish
{'loss': 0.4646, 'grad_norm': 1.1999846696853638, 'learning_rate': 8.985347985347986e-05, 'epoch': 1.08}
********************on step end call back********************
Step 5950 finish
{'loss': 0.4583, 'grad_norm': 1.4220517873764038, 'learning_rate': 8.983516483516484e-05, 'epoch': 1.08}
********************on step end call back********************
Step 5960 finish
{'loss': 0.4424, 'grad_norm': 1.220374345779419, 'learning_rate': 8.981684981684982e-05, 'epoch': 1.08}
[INFO|trainer.py:3376] 2024-03-22 07:32:57,645 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-22 07:32:57,645 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-22 07:32:57,645 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-22 07:35:07,413 >> Saving model checkpoint to ./output/tmp-checkpoint-6000
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-22 07:35:07,557 >> tokenizer config file saved in ./output/tmp-checkpoint-6000/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-22 07:35:07,558 >> Special tokens file saved in ./output/tmp-checkpoint-6000/special_tokens_map.json
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-22 07:43:39,548 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-22 07:43:39,548 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-22 07:43:39,548 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-22 07:45:48,867 >> Saving model checkpoint to ./output/tmp-checkpoint-6100
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-22 07:45:49,006 >> tokenizer config file saved in ./output/tmp-checkpoint-6100/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-22 07:45:49,007 >> Special tokens file saved in ./output/tmp-checkpoint-6100/special_tokens_map.json
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-22 07:54:26,950 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-22 07:54:26,950 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-22 07:54:26,950 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-22 07:56:36,605 >> Saving model checkpoint to ./output/tmp-checkpoint-6200
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-22 07:56:36,746 >> tokenizer config file saved in ./output/tmp-checkpoint-6200/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-22 07:56:36,747 >> Special tokens file saved in ./output/tmp-checkpoint-6200/special_tokens_map.json
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-22 08:05:15,171 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-22 08:05:15,171 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-22 08:05:15,171 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-22 08:07:24,667 >> Saving model checkpoint to ./output/tmp-checkpoint-6300
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-22 08:07:24,810 >> tokenizer config file saved in ./output/tmp-checkpoint-6300/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-22 08:07:24,811 >> Special tokens file saved in ./output/tmp-checkpoint-6300/special_tokens_map.json
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
********************on step end call back********************
Step 5970 finish
{'loss': 0.4883, 'grad_norm': 1.2149783372879028, 'learning_rate': 8.97985347985348e-05, 'epoch': 1.09}
********************on step end call back********************
Step 5980 finish
{'loss': 0.4482, 'grad_norm': 1.3436042070388794, 'learning_rate': 8.978021978021979e-05, 'epoch': 1.09}
********************on step end call back********************
Step 5990 finish
{'loss': 0.4762, 'grad_norm': 1.1369202136993408, 'learning_rate': 8.976190476190477e-05, 'epoch': 1.09}
********************on step end call back********************
Step 6000 finish
{'loss': 0.4874, 'grad_norm': 1.105979084968567, 'learning_rate': 8.974358974358975e-05, 'epoch': 1.09}
{'eval_loss': 0.32504284381866455, 'eval_accuracy': 0.8645833333333334, 'eval_runtime': 129.7673, 'eval_samples_per_second': 4.916, 'eval_steps_per_second': 4.916, 'epoch': 1.09}
********************save call back********************
********************on step end call back********************
Step 6010 finish
{'loss': 0.4202, 'grad_norm': 1.1466377973556519, 'learning_rate': 8.972527472527473e-05, 'epoch': 1.09}
********************on step end call back********************
Step 6020 finish
{'loss': 0.5025, 'grad_norm': 1.2097856998443604, 'learning_rate': 8.970695970695972e-05, 'epoch': 1.09}
********************on step end call back********************
Step 6030 finish
{'loss': 0.3825, 'grad_norm': 1.1654528379440308, 'learning_rate': 8.96886446886447e-05, 'epoch': 1.1}
********************on step end call back********************
Step 6040 finish
{'loss': 0.5074, 'grad_norm': 1.1879220008850098, 'learning_rate': 8.967032967032968e-05, 'epoch': 1.1}
********************on step end call back********************
Step 6050 finish
{'loss': 0.4681, 'grad_norm': 1.128355622291565, 'learning_rate': 8.965201465201466e-05, 'epoch': 1.1}
********************on step end call back********************
Step 6060 finish
{'loss': 0.4728, 'grad_norm': 1.2488977909088135, 'learning_rate': 8.963369963369964e-05, 'epoch': 1.1}
********************on step end call back********************
Step 6070 finish
{'loss': 0.547, 'grad_norm': 1.3586658239364624, 'learning_rate': 8.961538461538463e-05, 'epoch': 1.1}
********************on step end call back********************
Step 6080 finish
{'loss': 0.4923, 'grad_norm': 1.0028878450393677, 'learning_rate': 8.95970695970696e-05, 'epoch': 1.11}
********************on step end call back********************
Step 6090 finish
{'loss': 0.4677, 'grad_norm': 1.1015123128890991, 'learning_rate': 8.957875457875458e-05, 'epoch': 1.11}
********************on step end call back********************
Step 6100 finish
{'loss': 0.5258, 'grad_norm': 1.1460320949554443, 'learning_rate': 8.956043956043956e-05, 'epoch': 1.11}
{'eval_loss': 0.3214147686958313, 'eval_accuracy': 0.875, 'eval_runtime': 129.3174, 'eval_samples_per_second': 4.934, 'eval_steps_per_second': 4.934, 'epoch': 1.11}
********************save call back********************
********************on step end call back********************
Step 6110 finish
{'loss': 0.5485, 'grad_norm': 1.3939496278762817, 'learning_rate': 8.954212454212456e-05, 'epoch': 1.11}
********************on step end call back********************
Step 6120 finish
{'loss': 0.4743, 'grad_norm': 1.1346575021743774, 'learning_rate': 8.952380952380953e-05, 'epoch': 1.11}
********************on step end call back********************
Step 6130 finish
{'loss': 0.4503, 'grad_norm': 0.9942449331283569, 'learning_rate': 8.950549450549451e-05, 'epoch': 1.11}
********************on step end call back********************
Step 6140 finish
{'loss': 0.5084, 'grad_norm': 1.1487058401107788, 'learning_rate': 8.948717948717949e-05, 'epoch': 1.12}
********************on step end call back********************
Step 6150 finish
{'loss': 0.5353, 'grad_norm': 1.2001157999038696, 'learning_rate': 8.946886446886447e-05, 'epoch': 1.12}
********************on step end call back********************
Step 6160 finish
{'loss': 0.4543, 'grad_norm': 1.0993974208831787, 'learning_rate': 8.945054945054946e-05, 'epoch': 1.12}
********************on step end call back********************
Step 6170 finish
{'loss': 0.4681, 'grad_norm': 1.412984013557434, 'learning_rate': 8.943223443223444e-05, 'epoch': 1.12}
********************on step end call back********************
Step 6180 finish
{'loss': 0.4428, 'grad_norm': 1.3603886365890503, 'learning_rate': 8.941391941391942e-05, 'epoch': 1.12}
********************on step end call back********************
Step 6190 finish
{'loss': 0.4851, 'grad_norm': 1.4243593215942383, 'learning_rate': 8.93956043956044e-05, 'epoch': 1.13}
********************on step end call back********************
Step 6200 finish
{'loss': 0.4158, 'grad_norm': 1.1976333856582642, 'learning_rate': 8.937728937728939e-05, 'epoch': 1.13}
{'eval_loss': 0.3237896263599396, 'eval_accuracy': 0.8958333333333334, 'eval_runtime': 129.6538, 'eval_samples_per_second': 4.921, 'eval_steps_per_second': 4.921, 'epoch': 1.13}
********************save call back********************
********************on step end call back********************
Step 6210 finish
{'loss': 0.5239, 'grad_norm': 1.015271782875061, 'learning_rate': 8.935897435897437e-05, 'epoch': 1.13}
********************on step end call back********************
Step 6220 finish
{'loss': 0.4341, 'grad_norm': 1.2976109981536865, 'learning_rate': 8.934065934065935e-05, 'epoch': 1.13}
********************on step end call back********************
Step 6230 finish
{'loss': 0.5524, 'grad_norm': 1.0214664936065674, 'learning_rate': 8.932234432234433e-05, 'epoch': 1.13}
********************on step end call back********************
Step 6240 finish
{'loss': 0.4364, 'grad_norm': 1.1946094036102295, 'learning_rate': 8.930402930402931e-05, 'epoch': 1.13}
********************on step end call back********************
Step 6250 finish
{'loss': 0.4363, 'grad_norm': 0.9640882015228271, 'learning_rate': 8.92857142857143e-05, 'epoch': 1.14}
********************on step end call back********************
Step 6260 finish
{'loss': 0.5085, 'grad_norm': 0.9976345300674438, 'learning_rate': 8.926739926739928e-05, 'epoch': 1.14}
********************on step end call back********************
Step 6270 finish
{'loss': 0.4759, 'grad_norm': 1.36513090133667, 'learning_rate': 8.924908424908426e-05, 'epoch': 1.14}
********************on step end call back********************
Step 6280 finish
{'loss': 0.4356, 'grad_norm': 1.2432773113250732, 'learning_rate': 8.923076923076924e-05, 'epoch': 1.14}
********************on step end call back********************
Step 6290 finish
{'loss': 0.4982, 'grad_norm': 1.2147389650344849, 'learning_rate': 8.921245421245423e-05, 'epoch': 1.14}
********************on step end call back********************
Step 6300 finish
{'loss': 0.491, 'grad_norm': 1.292709469795227, 'learning_rate': 8.91941391941392e-05, 'epoch': 1.15}
{'eval_loss': 0.32656481862068176, 'eval_accuracy': 0.8958333333333334, 'eval_runtime': 129.4954, 'eval_samples_per_second': 4.927, 'eval_steps_per_second': 4.927, 'epoch': 1.15}
********************save call back********************
********************on step end call back********************
Step 6310 finish
{'loss': 0.4705, 'grad_norm': 1.261817455291748, 'learning_rate': 8.917582417582419e-05, 'epoch': 1.15}
********************on step end call back********************
Step 6320 finish
{'loss': 0.5282, 'grad_norm': 1.0196958780288696, 'learning_rate': 8.915750915750916e-05, 'epoch': 1.15}
********************on step end call back********************
Step 6330 finish
{'loss': 0.4936, 'grad_norm': 1.2722759246826172, 'learning_rate': 8.913919413919414e-05, 'epoch': 1.15}
********************on step end call back********************
Step 6340 finish
{'loss': 0.4686, 'grad_norm': 1.3235836029052734, 'learning_rate': 8.912087912087914e-05, 'epoch': 1.15}
********************on step end call back********************
Step 6350 finish
{'loss': 0.4208, 'grad_norm': 1.4765676259994507, 'learning_rate': 8.910256410256411e-05, 'epoch': 1.15}
********************on step end call back********************
Step 6360 finish
[INFO|trainer.py:3376] 2024-03-22 08:15:57,791 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-22 08:15:57,791 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-22 08:15:57,791 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-22 08:18:07,139 >> Saving model checkpoint to ./output/tmp-checkpoint-6400
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-22 08:18:07,278 >> tokenizer config file saved in ./output/tmp-checkpoint-6400/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-22 08:18:07,278 >> Special tokens file saved in ./output/tmp-checkpoint-6400/special_tokens_map.json
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-22 08:26:48,898 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-22 08:26:48,898 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-22 08:26:48,898 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-22 08:28:58,624 >> Saving model checkpoint to ./output/tmp-checkpoint-6500
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-22 08:28:58,769 >> tokenizer config file saved in ./output/tmp-checkpoint-6500/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-22 08:28:58,769 >> Special tokens file saved in ./output/tmp-checkpoint-6500/special_tokens_map.json
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-22 08:37:34,985 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-22 08:37:34,985 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-22 08:37:34,985 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-22 08:39:44,735 >> Saving model checkpoint to ./output/tmp-checkpoint-6600
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-22 08:39:45,030 >> tokenizer config file saved in ./output/tmp-checkpoint-6600/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-22 08:39:45,030 >> Special tokens file saved in ./output/tmp-checkpoint-6600/special_tokens_map.json
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-22 08:48:16,629 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-22 08:48:16,629 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-22 08:48:16,629 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-22 08:50:26,365 >> Saving model checkpoint to ./output/tmp-checkpoint-6700
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-22 08:50:26,511 >> tokenizer config file saved in ./output/tmp-checkpoint-6700/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-22 08:50:26,511 >> Special tokens file saved in ./output/tmp-checkpoint-6700/special_tokens_map.json
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
{'loss': 0.4532, 'grad_norm': 1.1624575853347778, 'learning_rate': 8.908424908424909e-05, 'epoch': 1.16}
********************on step end call back********************
Step 6370 finish
{'loss': 0.4495, 'grad_norm': 1.2853875160217285, 'learning_rate': 8.906593406593407e-05, 'epoch': 1.16}
********************on step end call back********************
Step 6380 finish
{'loss': 0.5202, 'grad_norm': 1.1164389848709106, 'learning_rate': 8.904761904761905e-05, 'epoch': 1.16}
********************on step end call back********************
Step 6390 finish
{'loss': 0.4731, 'grad_norm': 1.6218414306640625, 'learning_rate': 8.902930402930403e-05, 'epoch': 1.16}
********************on step end call back********************
Step 6400 finish
{'loss': 0.4362, 'grad_norm': 1.0320806503295898, 'learning_rate': 8.901098901098901e-05, 'epoch': 1.16}
{'eval_loss': 0.3257540464401245, 'eval_accuracy': 0.8854166666666666, 'eval_runtime': 129.3479, 'eval_samples_per_second': 4.932, 'eval_steps_per_second': 4.932, 'epoch': 1.16}
********************save call back********************
********************on step end call back********************
Step 6410 finish
{'loss': 0.5167, 'grad_norm': 1.1812598705291748, 'learning_rate': 8.899267399267399e-05, 'epoch': 1.17}
********************on step end call back********************
Step 6420 finish
{'loss': 0.438, 'grad_norm': 1.511479377746582, 'learning_rate': 8.897435897435898e-05, 'epoch': 1.17}
********************on step end call back********************
Step 6430 finish
{'loss': 0.4483, 'grad_norm': 1.2155557870864868, 'learning_rate': 8.895604395604396e-05, 'epoch': 1.17}
********************on step end call back********************
Step 6440 finish
{'loss': 0.4957, 'grad_norm': 1.4644813537597656, 'learning_rate': 8.893772893772894e-05, 'epoch': 1.17}
********************on step end call back********************
Step 6450 finish
{'loss': 0.4273, 'grad_norm': 1.083286166191101, 'learning_rate': 8.891941391941391e-05, 'epoch': 1.17}
********************on step end call back********************
Step 6460 finish
{'loss': 0.4985, 'grad_norm': 1.3799856901168823, 'learning_rate': 8.89010989010989e-05, 'epoch': 1.17}
********************on step end call back********************
Step 6470 finish
{'loss': 0.4623, 'grad_norm': 1.1612457036972046, 'learning_rate': 8.888278388278389e-05, 'epoch': 1.18}
********************on step end call back********************
Step 6480 finish
{'loss': 0.5131, 'grad_norm': 1.3443783521652222, 'learning_rate': 8.886446886446886e-05, 'epoch': 1.18}
********************on step end call back********************
Step 6490 finish
{'loss': 0.4976, 'grad_norm': 1.110344409942627, 'learning_rate': 8.884615384615384e-05, 'epoch': 1.18}
********************on step end call back********************
Step 6500 finish
{'loss': 0.418, 'grad_norm': 0.920836329460144, 'learning_rate': 8.882783882783882e-05, 'epoch': 1.18}
{'eval_loss': 0.3288321793079376, 'eval_accuracy': 0.875, 'eval_runtime': 129.7251, 'eval_samples_per_second': 4.918, 'eval_steps_per_second': 4.918, 'epoch': 1.18}
********************save call back********************
********************on step end call back********************
Step 6510 finish
{'loss': 0.4671, 'grad_norm': 1.181730031967163, 'learning_rate': 8.880952380952381e-05, 'epoch': 1.18}
********************on step end call back********************
Step 6520 finish
{'loss': 0.4396, 'grad_norm': 1.4884644746780396, 'learning_rate': 8.87912087912088e-05, 'epoch': 1.19}
********************on step end call back********************
Step 6530 finish
{'loss': 0.5516, 'grad_norm': 1.063341498374939, 'learning_rate': 8.877289377289377e-05, 'epoch': 1.19}
********************on step end call back********************
Step 6540 finish
{'loss': 0.4497, 'grad_norm': 0.9660506844520569, 'learning_rate': 8.875457875457875e-05, 'epoch': 1.19}
********************on step end call back********************
Step 6550 finish
{'loss': 0.4843, 'grad_norm': 1.1738399267196655, 'learning_rate': 8.873626373626373e-05, 'epoch': 1.19}
********************on step end call back********************
Step 6560 finish
{'loss': 0.5075, 'grad_norm': 1.4315698146820068, 'learning_rate': 8.871794871794872e-05, 'epoch': 1.19}
********************on step end call back********************
Step 6570 finish
{'loss': 0.4769, 'grad_norm': 0.9307923316955566, 'learning_rate': 8.86996336996337e-05, 'epoch': 1.19}
********************on step end call back********************
Step 6580 finish
{'loss': 0.4833, 'grad_norm': 1.0387531518936157, 'learning_rate': 8.868131868131868e-05, 'epoch': 1.2}
********************on step end call back********************
Step 6590 finish
{'loss': 0.4939, 'grad_norm': 1.4016371965408325, 'learning_rate': 8.866300366300366e-05, 'epoch': 1.2}
********************on step end call back********************
Step 6600 finish
{'loss': 0.4513, 'grad_norm': 1.098260760307312, 'learning_rate': 8.864468864468865e-05, 'epoch': 1.2}
{'eval_loss': 0.3344445824623108, 'eval_accuracy': 0.8541666666666666, 'eval_runtime': 129.7487, 'eval_samples_per_second': 4.917, 'eval_steps_per_second': 4.917, 'epoch': 1.2}
********************save call back********************
********************on step end call back********************
Step 6610 finish
{'loss': 0.4204, 'grad_norm': 1.298400640487671, 'learning_rate': 8.862637362637363e-05, 'epoch': 1.2}
********************on step end call back********************
Step 6620 finish
{'loss': 0.4188, 'grad_norm': 1.2367782592773438, 'learning_rate': 8.860805860805861e-05, 'epoch': 1.2}
********************on step end call back********************
Step 6630 finish
{'loss': 0.4555, 'grad_norm': 1.2244696617126465, 'learning_rate': 8.858974358974359e-05, 'epoch': 1.21}
********************on step end call back********************
Step 6640 finish
{'loss': 0.4855, 'grad_norm': 1.2003931999206543, 'learning_rate': 8.857142857142857e-05, 'epoch': 1.21}
********************on step end call back********************
Step 6650 finish
{'loss': 0.4509, 'grad_norm': 0.9631105661392212, 'learning_rate': 8.855311355311356e-05, 'epoch': 1.21}
********************on step end call back********************
Step 6660 finish
{'loss': 0.4769, 'grad_norm': 1.2265739440917969, 'learning_rate': 8.853479853479854e-05, 'epoch': 1.21}
********************on step end call back********************
Step 6670 finish
{'loss': 0.4421, 'grad_norm': 1.4920892715454102, 'learning_rate': 8.851648351648352e-05, 'epoch': 1.21}
********************on step end call back********************
Step 6680 finish
{'loss': 0.4939, 'grad_norm': 1.2938573360443115, 'learning_rate': 8.84981684981685e-05, 'epoch': 1.21}
********************on step end call back********************
Step 6690 finish
{'loss': 0.4435, 'grad_norm': 1.0019532442092896, 'learning_rate': 8.847985347985349e-05, 'epoch': 1.22}
********************on step end call back********************
Step 6700 finish
{'loss': 0.4458, 'grad_norm': 0.8074877858161926, 'learning_rate': 8.846153846153847e-05, 'epoch': 1.22}
{'eval_loss': 0.32428276538848877, 'eval_accuracy': 0.8645833333333334, 'eval_runtime': 129.7345, 'eval_samples_per_second': 4.918, 'eval_steps_per_second': 4.918, 'epoch': 1.22}
********************save call back********************
********************on step end call back********************
Step 6710 finish
{'loss': 0.4775, 'grad_norm': 1.3001333475112915, 'learning_rate': 8.844322344322344e-05, 'epoch': 1.22}
********************on step end call back********************
Step 6720 finish
{'loss': 0.464, 'grad_norm': 0.9536634683609009, 'learning_rate': 8.842490842490842e-05, 'epoch': 1.22}
********************on step end call back********************
Step 6730 finish
{'loss': 0.4749, 'grad_norm': 1.3142926692962646, 'learning_rate': 8.84065934065934e-05, 'epoch': 1.22}
********************on step end call back********************
Step 6740 finish
{'loss': 0.4887, 'grad_norm': 1.398590087890625, 'learning_rate': 8.83882783882784e-05, 'epoch': 1.23}
********************on step end call back********************
Step 6750 finish
{'loss': 0.4617, 'grad_norm': 1.2733304500579834, 'learning_rate': 8.836996336996337e-05, 'epoch': 1.23}
[INFO|trainer.py:3376] 2024-03-22 08:59:02,704 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-22 08:59:02,705 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-22 08:59:02,705 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-22 09:01:12,449 >> Saving model checkpoint to ./output/tmp-checkpoint-6800
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-22 09:01:12,593 >> tokenizer config file saved in ./output/tmp-checkpoint-6800/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-22 09:01:12,594 >> Special tokens file saved in ./output/tmp-checkpoint-6800/special_tokens_map.json
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-22 09:09:44,673 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-22 09:09:44,673 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-22 09:09:44,673 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-22 09:11:54,508 >> Saving model checkpoint to ./output/tmp-checkpoint-6900
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-22 09:11:54,651 >> tokenizer config file saved in ./output/tmp-checkpoint-6900/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-22 09:11:54,651 >> Special tokens file saved in ./output/tmp-checkpoint-6900/special_tokens_map.json
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-22 09:20:34,586 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-22 09:20:34,586 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-22 09:20:34,586 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-22 09:22:44,496 >> Saving model checkpoint to ./output/tmp-checkpoint-7000
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-22 09:22:44,641 >> tokenizer config file saved in ./output/tmp-checkpoint-7000/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-22 09:22:44,641 >> Special tokens file saved in ./output/tmp-checkpoint-7000/special_tokens_map.json
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-22 09:31:14,933 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-22 09:31:14,933 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-22 09:31:14,933 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-22 09:33:24,907 >> Saving model checkpoint to ./output/tmp-checkpoint-7100
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-22 09:33:25,053 >> tokenizer config file saved in ./output/tmp-checkpoint-7100/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-22 09:33:25,053 >> Special tokens file saved in ./output/tmp-checkpoint-7100/special_tokens_map.json
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
********************on step end call back********************
Step 6760 finish
{'loss': 0.5507, 'grad_norm': 1.1321607828140259, 'learning_rate': 8.835164835164835e-05, 'epoch': 1.23}
********************on step end call back********************
Step 6770 finish
{'loss': 0.4613, 'grad_norm': 1.2813047170639038, 'learning_rate': 8.833333333333333e-05, 'epoch': 1.23}
********************on step end call back********************
Step 6780 finish
{'loss': 0.4865, 'grad_norm': 1.0238722562789917, 'learning_rate': 8.831501831501832e-05, 'epoch': 1.23}
********************on step end call back********************
Step 6790 finish
{'loss': 0.603, 'grad_norm': 1.3947244882583618, 'learning_rate': 8.82967032967033e-05, 'epoch': 1.23}
********************on step end call back********************
Step 6800 finish
{'loss': 0.4602, 'grad_norm': 1.1837280988693237, 'learning_rate': 8.827838827838828e-05, 'epoch': 1.24}
{'eval_loss': 0.32336580753326416, 'eval_accuracy': 0.8645833333333334, 'eval_runtime': 129.7435, 'eval_samples_per_second': 4.917, 'eval_steps_per_second': 4.917, 'epoch': 1.24}
********************save call back********************
********************on step end call back********************
Step 6810 finish
{'loss': 0.4774, 'grad_norm': 1.183967113494873, 'learning_rate': 8.826007326007326e-05, 'epoch': 1.24}
********************on step end call back********************
Step 6820 finish
{'loss': 0.5146, 'grad_norm': 1.0511351823806763, 'learning_rate': 8.824175824175824e-05, 'epoch': 1.24}
********************on step end call back********************
Step 6830 finish
{'loss': 0.4306, 'grad_norm': 1.3728246688842773, 'learning_rate': 8.822344322344323e-05, 'epoch': 1.24}
********************on step end call back********************
Step 6840 finish
{'loss': 0.4804, 'grad_norm': 1.3146508932113647, 'learning_rate': 8.820512820512821e-05, 'epoch': 1.24}
********************on step end call back********************
Step 6850 finish
{'loss': 0.4778, 'grad_norm': 1.2145028114318848, 'learning_rate': 8.818681318681319e-05, 'epoch': 1.25}
********************on step end call back********************
Step 6860 finish
{'loss': 0.4197, 'grad_norm': 1.0848182439804077, 'learning_rate': 8.816849816849817e-05, 'epoch': 1.25}
********************on step end call back********************
Step 6870 finish
{'loss': 0.4338, 'grad_norm': 1.3797703981399536, 'learning_rate': 8.815018315018316e-05, 'epoch': 1.25}
********************on step end call back********************
Step 6880 finish
{'loss': 0.4396, 'grad_norm': 1.2641215324401855, 'learning_rate': 8.813186813186814e-05, 'epoch': 1.25}
********************on step end call back********************
Step 6890 finish
{'loss': 0.4527, 'grad_norm': 1.0361040830612183, 'learning_rate': 8.811355311355312e-05, 'epoch': 1.25}
********************on step end call back********************
Step 6900 finish
{'loss': 0.4621, 'grad_norm': 1.0215216875076294, 'learning_rate': 8.80952380952381e-05, 'epoch': 1.25}
{'eval_loss': 0.32885023951530457, 'eval_accuracy': 0.8854166666666666, 'eval_runtime': 129.8333, 'eval_samples_per_second': 4.914, 'eval_steps_per_second': 4.914, 'epoch': 1.25}
********************save call back********************
********************on step end call back********************
Step 6910 finish
{'loss': 0.422, 'grad_norm': 0.8856333494186401, 'learning_rate': 8.807692307692307e-05, 'epoch': 1.26}
********************on step end call back********************
Step 6920 finish
{'loss': 0.4625, 'grad_norm': 1.3179283142089844, 'learning_rate': 8.805860805860807e-05, 'epoch': 1.26}
********************on step end call back********************
Step 6930 finish
{'loss': 0.4514, 'grad_norm': 1.2877261638641357, 'learning_rate': 8.804029304029304e-05, 'epoch': 1.26}
********************on step end call back********************
Step 6940 finish
{'loss': 0.4833, 'grad_norm': 1.0711815357208252, 'learning_rate': 8.802197802197802e-05, 'epoch': 1.26}
********************on step end call back********************
Step 6950 finish
{'loss': 0.4871, 'grad_norm': 1.2961905002593994, 'learning_rate': 8.8003663003663e-05, 'epoch': 1.26}
********************on step end call back********************
Step 6960 finish
{'loss': 0.4194, 'grad_norm': 1.2306958436965942, 'learning_rate': 8.7985347985348e-05, 'epoch': 1.27}
********************on step end call back********************
Step 6970 finish
{'loss': 0.4289, 'grad_norm': 0.9745946526527405, 'learning_rate': 8.796703296703297e-05, 'epoch': 1.27}
********************on step end call back********************
Step 6980 finish
{'loss': 0.5291, 'grad_norm': 1.151229977607727, 'learning_rate': 8.794871794871795e-05, 'epoch': 1.27}
********************on step end call back********************
Step 6990 finish
{'loss': 0.461, 'grad_norm': 1.2161515951156616, 'learning_rate': 8.793040293040293e-05, 'epoch': 1.27}
********************on step end call back********************
Step 7000 finish
{'loss': 0.4275, 'grad_norm': 0.8368428945541382, 'learning_rate': 8.791208791208791e-05, 'epoch': 1.27}
{'eval_loss': 0.3159715533256531, 'eval_accuracy': 0.8854166666666666, 'eval_runtime': 129.9085, 'eval_samples_per_second': 4.911, 'eval_steps_per_second': 4.911, 'epoch': 1.27}
********************save call back********************
********************on step end call back********************
Step 7010 finish
{'loss': 0.4817, 'grad_norm': 1.32390296459198, 'learning_rate': 8.78937728937729e-05, 'epoch': 1.27}
********************on step end call back********************
Step 7020 finish
{'loss': 0.4504, 'grad_norm': 1.0090117454528809, 'learning_rate': 8.787545787545788e-05, 'epoch': 1.28}
********************on step end call back********************
Step 7030 finish
{'loss': 0.4224, 'grad_norm': 0.9948376417160034, 'learning_rate': 8.785714285714286e-05, 'epoch': 1.28}
********************on step end call back********************
Step 7040 finish
{'loss': 0.4536, 'grad_norm': 1.0260045528411865, 'learning_rate': 8.783882783882784e-05, 'epoch': 1.28}
********************on step end call back********************
Step 7050 finish
{'loss': 0.4334, 'grad_norm': 1.4632655382156372, 'learning_rate': 8.782051282051283e-05, 'epoch': 1.28}
********************on step end call back********************
Step 7060 finish
{'loss': 0.4291, 'grad_norm': 1.6521328687667847, 'learning_rate': 8.780219780219781e-05, 'epoch': 1.28}
********************on step end call back********************
Step 7070 finish
{'loss': 0.4015, 'grad_norm': 1.2012035846710205, 'learning_rate': 8.778388278388279e-05, 'epoch': 1.29}
********************on step end call back********************
Step 7080 finish
{'loss': 0.4384, 'grad_norm': 1.1754271984100342, 'learning_rate': 8.776556776556777e-05, 'epoch': 1.29}
********************on step end call back********************
Step 7090 finish
{'loss': 0.3815, 'grad_norm': 1.0668792724609375, 'learning_rate': 8.774725274725275e-05, 'epoch': 1.29}
********************on step end call back********************
Step 7100 finish
{'loss': 0.4453, 'grad_norm': 1.1411080360412598, 'learning_rate': 8.772893772893774e-05, 'epoch': 1.29}
{'eval_loss': 0.3264586925506592, 'eval_accuracy': 0.8645833333333334, 'eval_runtime': 129.9732, 'eval_samples_per_second': 4.909, 'eval_steps_per_second': 4.909, 'epoch': 1.29}
********************save call back********************
********************on step end call back********************
Step 7110 finish
{'loss': 0.4312, 'grad_norm': 1.3868435621261597, 'learning_rate': 8.771062271062272e-05, 'epoch': 1.29}
********************on step end call back********************
Step 7120 finish
{'loss': 0.4913, 'grad_norm': 1.1117007732391357, 'learning_rate': 8.76923076923077e-05, 'epoch': 1.29}
********************on step end call back********************
Step 7130 finish
{'loss': 0.447, 'grad_norm': 1.33681321144104, 'learning_rate': 8.767399267399267e-05, 'epoch': 1.3}
********************on step end call back********************
Step 7140 finish
{'loss': 0.4088, 'grad_norm': 0.9888585805892944, 'learning_rate': 8.765567765567765e-05, 'epoch': 1.3}
********************on step end call back********************
Step 7150 finish
[INFO|trainer.py:3376] 2024-03-22 09:42:06,715 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-22 09:42:06,715 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-22 09:42:06,715 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-22 09:44:16,517 >> Saving model checkpoint to ./output/tmp-checkpoint-7200
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-22 09:44:16,659 >> tokenizer config file saved in ./output/tmp-checkpoint-7200/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-22 09:44:16,659 >> Special tokens file saved in ./output/tmp-checkpoint-7200/special_tokens_map.json
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-22 09:52:59,854 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-22 09:52:59,854 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-22 09:52:59,854 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-22 09:55:09,905 >> Saving model checkpoint to ./output/tmp-checkpoint-7300
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-22 09:55:10,047 >> tokenizer config file saved in ./output/tmp-checkpoint-7300/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-22 09:55:10,047 >> Special tokens file saved in ./output/tmp-checkpoint-7300/special_tokens_map.json
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-22 10:03:49,967 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-22 10:03:49,967 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-22 10:03:49,967 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-22 10:06:00,070 >> Saving model checkpoint to ./output/tmp-checkpoint-7400
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-22 10:06:00,216 >> tokenizer config file saved in ./output/tmp-checkpoint-7400/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-22 10:06:00,216 >> Special tokens file saved in ./output/tmp-checkpoint-7400/special_tokens_map.json
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-22 10:14:36,939 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-22 10:14:36,939 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-22 10:14:36,940 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-22 10:16:46,989 >> Saving model checkpoint to ./output/tmp-checkpoint-7500
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-22 10:16:47,139 >> tokenizer config file saved in ./output/tmp-checkpoint-7500/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-22 10:16:47,139 >> Special tokens file saved in ./output/tmp-checkpoint-7500/special_tokens_map.json
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
{'loss': 0.4704, 'grad_norm': 0.9301528334617615, 'learning_rate': 8.763736263736264e-05, 'epoch': 1.3}
********************on step end call back********************
Step 7160 finish
{'loss': 0.4896, 'grad_norm': 1.0356168746948242, 'learning_rate': 8.761904761904762e-05, 'epoch': 1.3}
********************on step end call back********************
Step 7170 finish
{'loss': 0.4427, 'grad_norm': 1.0737500190734863, 'learning_rate': 8.76007326007326e-05, 'epoch': 1.3}
********************on step end call back********************
Step 7180 finish
{'loss': 0.4457, 'grad_norm': 0.9828976988792419, 'learning_rate': 8.758241758241758e-05, 'epoch': 1.31}
********************on step end call back********************
Step 7190 finish
{'loss': 0.4266, 'grad_norm': 1.3386290073394775, 'learning_rate': 8.756410256410257e-05, 'epoch': 1.31}
********************on step end call back********************
Step 7200 finish
{'loss': 0.4322, 'grad_norm': 1.22187340259552, 'learning_rate': 8.754578754578755e-05, 'epoch': 1.31}
{'eval_loss': 0.32646384835243225, 'eval_accuracy': 0.8645833333333334, 'eval_runtime': 129.8008, 'eval_samples_per_second': 4.915, 'eval_steps_per_second': 4.915, 'epoch': 1.31}
********************save call back********************
********************on step end call back********************
Step 7210 finish
{'loss': 0.4656, 'grad_norm': 1.100588321685791, 'learning_rate': 8.752747252747253e-05, 'epoch': 1.31}
********************on step end call back********************
Step 7220 finish
{'loss': 0.4657, 'grad_norm': 1.3954353332519531, 'learning_rate': 8.750915750915751e-05, 'epoch': 1.31}
********************on step end call back********************
Step 7230 finish
{'loss': 0.4155, 'grad_norm': 1.4224201440811157, 'learning_rate': 8.749084249084249e-05, 'epoch': 1.31}
********************on step end call back********************
Step 7240 finish
{'loss': 0.4563, 'grad_norm': 0.9609143733978271, 'learning_rate': 8.747252747252748e-05, 'epoch': 1.32}
********************on step end call back********************
Step 7250 finish
{'loss': 0.4122, 'grad_norm': 0.877392590045929, 'learning_rate': 8.745421245421246e-05, 'epoch': 1.32}
********************on step end call back********************
Step 7260 finish
{'loss': 0.4588, 'grad_norm': 1.1655385494232178, 'learning_rate': 8.743589743589744e-05, 'epoch': 1.32}
********************on step end call back********************
Step 7270 finish
{'loss': 0.4541, 'grad_norm': 0.963355302810669, 'learning_rate': 8.741758241758242e-05, 'epoch': 1.32}
********************on step end call back********************
Step 7280 finish
{'loss': 0.4355, 'grad_norm': 1.0904711484909058, 'learning_rate': 8.739926739926741e-05, 'epoch': 1.32}
********************on step end call back********************
Step 7290 finish
{'loss': 0.4674, 'grad_norm': 1.0939433574676514, 'learning_rate': 8.738095238095239e-05, 'epoch': 1.33}
********************on step end call back********************
Step 7300 finish
{'loss': 0.4917, 'grad_norm': 1.293845534324646, 'learning_rate': 8.736263736263737e-05, 'epoch': 1.33}
{'eval_loss': 0.322400838136673, 'eval_accuracy': 0.8854166666666666, 'eval_runtime': 130.05, 'eval_samples_per_second': 4.906, 'eval_steps_per_second': 4.906, 'epoch': 1.33}
********************save call back********************
********************on step end call back********************
Step 7310 finish
{'loss': 0.4622, 'grad_norm': 1.19974946975708, 'learning_rate': 8.734432234432235e-05, 'epoch': 1.33}
********************on step end call back********************
Step 7320 finish
{'loss': 0.4418, 'grad_norm': 1.107670783996582, 'learning_rate': 8.732600732600732e-05, 'epoch': 1.33}
********************on step end call back********************
Step 7330 finish
{'loss': 0.5177, 'grad_norm': 1.0488317012786865, 'learning_rate': 8.730769230769232e-05, 'epoch': 1.33}
********************on step end call back********************
Step 7340 finish
{'loss': 0.4451, 'grad_norm': 1.3440501689910889, 'learning_rate': 8.72893772893773e-05, 'epoch': 1.33}
********************on step end call back********************
Step 7350 finish
{'loss': 0.4417, 'grad_norm': 1.3056646585464478, 'learning_rate': 8.727106227106227e-05, 'epoch': 1.34}
********************on step end call back********************
Step 7360 finish
{'loss': 0.4839, 'grad_norm': 1.1205140352249146, 'learning_rate': 8.725274725274725e-05, 'epoch': 1.34}
********************on step end call back********************
Step 7370 finish
{'loss': 0.4573, 'grad_norm': 0.9407082200050354, 'learning_rate': 8.723443223443225e-05, 'epoch': 1.34}
********************on step end call back********************
Step 7380 finish
{'loss': 0.511, 'grad_norm': 1.3319478034973145, 'learning_rate': 8.721611721611722e-05, 'epoch': 1.34}
********************on step end call back********************
Step 7390 finish
{'loss': 0.5013, 'grad_norm': 0.8431925773620605, 'learning_rate': 8.71978021978022e-05, 'epoch': 1.34}
********************on step end call back********************
Step 7400 finish
{'loss': 0.4466, 'grad_norm': 1.190002202987671, 'learning_rate': 8.717948717948718e-05, 'epoch': 1.35}
{'eval_loss': 0.31746533513069153, 'eval_accuracy': 0.8854166666666666, 'eval_runtime': 130.102, 'eval_samples_per_second': 4.904, 'eval_steps_per_second': 4.904, 'epoch': 1.35}
********************save call back********************
********************on step end call back********************
Step 7410 finish
{'loss': 0.4249, 'grad_norm': 1.376720666885376, 'learning_rate': 8.716117216117216e-05, 'epoch': 1.35}
********************on step end call back********************
Step 7420 finish
{'loss': 0.4714, 'grad_norm': 1.1919912099838257, 'learning_rate': 8.714285714285715e-05, 'epoch': 1.35}
********************on step end call back********************
Step 7430 finish
{'loss': 0.4493, 'grad_norm': 1.0823193788528442, 'learning_rate': 8.712454212454213e-05, 'epoch': 1.35}
********************on step end call back********************
Step 7440 finish
{'loss': 0.5287, 'grad_norm': 1.4328864812850952, 'learning_rate': 8.710622710622711e-05, 'epoch': 1.35}
********************on step end call back********************
Step 7450 finish
{'loss': 0.408, 'grad_norm': 1.5108333826065063, 'learning_rate': 8.708791208791209e-05, 'epoch': 1.35}
********************on step end call back********************
Step 7460 finish
{'loss': 0.4841, 'grad_norm': 1.0893124341964722, 'learning_rate': 8.706959706959708e-05, 'epoch': 1.36}
********************on step end call back********************
Step 7470 finish
{'loss': 0.4231, 'grad_norm': 1.1669985055923462, 'learning_rate': 8.705128205128206e-05, 'epoch': 1.36}
********************on step end call back********************
Step 7480 finish
{'loss': 0.4376, 'grad_norm': 1.3438704013824463, 'learning_rate': 8.703296703296704e-05, 'epoch': 1.36}
********************on step end call back********************
Step 7490 finish
{'loss': 0.4952, 'grad_norm': 1.05716872215271, 'learning_rate': 8.701465201465202e-05, 'epoch': 1.36}
********************on step end call back********************
Step 7500 finish
{'loss': 0.4634, 'grad_norm': 1.1453742980957031, 'learning_rate': 8.6996336996337e-05, 'epoch': 1.36}
{'eval_loss': 0.31484144926071167, 'eval_accuracy': 0.8854166666666666, 'eval_runtime': 130.049, 'eval_samples_per_second': 4.906, 'eval_steps_per_second': 4.906, 'epoch': 1.36}
********************save call back********************
********************on step end call back********************
Step 7510 finish
{'loss': 0.5081, 'grad_norm': 1.3797708749771118, 'learning_rate': 8.697802197802199e-05, 'epoch': 1.37}
********************on step end call back********************
Step 7520 finish
{'loss': 0.4623, 'grad_norm': 0.9342484474182129, 'learning_rate': 8.695970695970697e-05, 'epoch': 1.37}
********************on step end call back********************
Step 7530 finish
{'loss': 0.472, 'grad_norm': 2.0973188877105713, 'learning_rate': 8.694139194139195e-05, 'epoch': 1.37}
********************on step end call back********************
Step 7540 finish
{'loss': 0.4219, 'grad_norm': 1.1504905223846436, 'learning_rate': 8.692307692307692e-05, 'epoch': 1.37}
[INFO|trainer.py:3376] 2024-03-22 10:25:24,365 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-22 10:25:24,365 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-22 10:25:24,365 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-22 10:27:34,134 >> Saving model checkpoint to ./output/tmp-checkpoint-7600
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-22 10:27:34,281 >> tokenizer config file saved in ./output/tmp-checkpoint-7600/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-22 10:27:34,281 >> Special tokens file saved in ./output/tmp-checkpoint-7600/special_tokens_map.json
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-22 10:36:18,054 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-22 10:36:18,055 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-22 10:36:18,055 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-22 10:38:27,776 >> Saving model checkpoint to ./output/tmp-checkpoint-7700
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-22 10:38:27,918 >> tokenizer config file saved in ./output/tmp-checkpoint-7700/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-22 10:38:27,919 >> Special tokens file saved in ./output/tmp-checkpoint-7700/special_tokens_map.json
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-22 10:47:15,841 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-22 10:47:15,841 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-22 10:47:15,841 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-22 10:49:25,541 >> Saving model checkpoint to ./output/tmp-checkpoint-7800
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-22 10:49:25,685 >> tokenizer config file saved in ./output/tmp-checkpoint-7800/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-22 10:49:25,685 >> Special tokens file saved in ./output/tmp-checkpoint-7800/special_tokens_map.json
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-22 10:58:05,379 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-22 10:58:05,379 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-22 10:58:05,380 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-22 11:00:15,053 >> Saving model checkpoint to ./output/tmp-checkpoint-7900
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-22 11:00:15,197 >> tokenizer config file saved in ./output/tmp-checkpoint-7900/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-22 11:00:15,197 >> Special tokens file saved in ./output/tmp-checkpoint-7900/special_tokens_map.json
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
********************on step end call back********************
Step 7550 finish
{'loss': 0.5376, 'grad_norm': 1.2822149991989136, 'learning_rate': 8.690476190476192e-05, 'epoch': 1.37}
********************on step end call back********************
Step 7560 finish
{'loss': 0.4824, 'grad_norm': 1.3473153114318848, 'learning_rate': 8.68864468864469e-05, 'epoch': 1.37}
********************on step end call back********************
Step 7570 finish
{'loss': 0.4258, 'grad_norm': 1.3571940660476685, 'learning_rate': 8.686813186813187e-05, 'epoch': 1.38}
********************on step end call back********************
Step 7580 finish
{'loss': 0.4564, 'grad_norm': 1.2068960666656494, 'learning_rate': 8.684981684981685e-05, 'epoch': 1.38}
********************on step end call back********************
Step 7590 finish
{'loss': 0.4722, 'grad_norm': 1.3859752416610718, 'learning_rate': 8.683150183150183e-05, 'epoch': 1.38}
********************on step end call back********************
Step 7600 finish
{'loss': 0.4575, 'grad_norm': 1.2984139919281006, 'learning_rate': 8.681318681318682e-05, 'epoch': 1.38}
{'eval_loss': 0.3153652250766754, 'eval_accuracy': 0.875, 'eval_runtime': 129.767, 'eval_samples_per_second': 4.917, 'eval_steps_per_second': 4.917, 'epoch': 1.38}
********************save call back********************
********************on step end call back********************
Step 7610 finish
{'loss': 0.4188, 'grad_norm': 1.2623521089553833, 'learning_rate': 8.67948717948718e-05, 'epoch': 1.38}
********************on step end call back********************
Step 7620 finish
{'loss': 0.4706, 'grad_norm': 1.51125168800354, 'learning_rate': 8.677655677655678e-05, 'epoch': 1.39}
********************on step end call back********************
Step 7630 finish
{'loss': 0.464, 'grad_norm': 1.2312242984771729, 'learning_rate': 8.675824175824176e-05, 'epoch': 1.39}
********************on step end call back********************
Step 7640 finish
{'loss': 0.4531, 'grad_norm': 1.2752090692520142, 'learning_rate': 8.673992673992675e-05, 'epoch': 1.39}
********************on step end call back********************
Step 7650 finish
{'loss': 0.4125, 'grad_norm': 1.328393816947937, 'learning_rate': 8.672161172161173e-05, 'epoch': 1.39}
********************on step end call back********************
Step 7660 finish
{'loss': 0.4488, 'grad_norm': 1.2526524066925049, 'learning_rate': 8.670329670329671e-05, 'epoch': 1.39}
********************on step end call back********************
Step 7670 finish
{'loss': 0.4877, 'grad_norm': 1.2284799814224243, 'learning_rate': 8.668498168498169e-05, 'epoch': 1.39}
********************on step end call back********************
Step 7680 finish
{'loss': 0.4947, 'grad_norm': 1.1583210229873657, 'learning_rate': 8.666666666666667e-05, 'epoch': 1.4}
********************on step end call back********************
Step 7690 finish
{'loss': 0.4847, 'grad_norm': 1.1852518320083618, 'learning_rate': 8.664835164835166e-05, 'epoch': 1.4}
********************on step end call back********************
Step 7700 finish
{'loss': 0.5136, 'grad_norm': 1.2053691148757935, 'learning_rate': 8.663003663003664e-05, 'epoch': 1.4}
{'eval_loss': 0.32038429379463196, 'eval_accuracy': 0.8854166666666666, 'eval_runtime': 129.7203, 'eval_samples_per_second': 4.918, 'eval_steps_per_second': 4.918, 'epoch': 1.4}
********************save call back********************
********************on step end call back********************
Step 7710 finish
{'loss': 0.4587, 'grad_norm': 1.147241234779358, 'learning_rate': 8.661172161172162e-05, 'epoch': 1.4}
********************on step end call back********************
Step 7720 finish
{'loss': 0.4365, 'grad_norm': 1.2615550756454468, 'learning_rate': 8.65934065934066e-05, 'epoch': 1.4}
********************on step end call back********************
Step 7730 finish
{'loss': 0.4618, 'grad_norm': 1.5023800134658813, 'learning_rate': 8.657509157509159e-05, 'epoch': 1.41}
********************on step end call back********************
Step 7740 finish
{'loss': 0.5056, 'grad_norm': 1.105111837387085, 'learning_rate': 8.655677655677657e-05, 'epoch': 1.41}
********************on step end call back********************
Step 7750 finish
{'loss': 0.4536, 'grad_norm': 1.217347502708435, 'learning_rate': 8.653846153846155e-05, 'epoch': 1.41}
********************on step end call back********************
Step 7760 finish
{'loss': 0.5103, 'grad_norm': 1.360647201538086, 'learning_rate': 8.652014652014653e-05, 'epoch': 1.41}
********************on step end call back********************
Step 7770 finish
{'loss': 0.4591, 'grad_norm': 1.089045524597168, 'learning_rate': 8.65018315018315e-05, 'epoch': 1.41}
********************on step end call back********************
Step 7780 finish
{'loss': 0.4595, 'grad_norm': 1.1511672735214233, 'learning_rate': 8.64835164835165e-05, 'epoch': 1.41}
********************on step end call back********************
Step 7790 finish
{'loss': 0.4398, 'grad_norm': 1.2005828619003296, 'learning_rate': 8.646520146520148e-05, 'epoch': 1.42}
********************on step end call back********************
Step 7800 finish
{'loss': 0.4614, 'grad_norm': 1.401473045349121, 'learning_rate': 8.644688644688645e-05, 'epoch': 1.42}
{'eval_loss': 0.3173640966415405, 'eval_accuracy': 0.8854166666666666, 'eval_runtime': 129.6991, 'eval_samples_per_second': 4.919, 'eval_steps_per_second': 4.919, 'epoch': 1.42}
********************save call back********************
********************on step end call back********************
Step 7810 finish
{'loss': 0.4414, 'grad_norm': 1.3065431118011475, 'learning_rate': 8.642857142857143e-05, 'epoch': 1.42}
********************on step end call back********************
Step 7820 finish
{'loss': 0.4877, 'grad_norm': 1.1388822793960571, 'learning_rate': 8.641025641025642e-05, 'epoch': 1.42}
********************on step end call back********************
Step 7830 finish
{'loss': 0.4787, 'grad_norm': 1.1425950527191162, 'learning_rate': 8.63919413919414e-05, 'epoch': 1.42}
********************on step end call back********************
Step 7840 finish
{'loss': 0.4741, 'grad_norm': 1.1782146692276, 'learning_rate': 8.637362637362638e-05, 'epoch': 1.43}
********************on step end call back********************
Step 7850 finish
{'loss': 0.4629, 'grad_norm': 1.3179819583892822, 'learning_rate': 8.635531135531136e-05, 'epoch': 1.43}
********************on step end call back********************
Step 7860 finish
{'loss': 0.44, 'grad_norm': 1.7315475940704346, 'learning_rate': 8.633699633699634e-05, 'epoch': 1.43}
********************on step end call back********************
Step 7870 finish
{'loss': 0.4406, 'grad_norm': 1.0645387172698975, 'learning_rate': 8.631868131868133e-05, 'epoch': 1.43}
********************on step end call back********************
Step 7880 finish
{'loss': 0.4366, 'grad_norm': 0.9946690201759338, 'learning_rate': 8.630036630036631e-05, 'epoch': 1.43}
********************on step end call back********************
Step 7890 finish
{'loss': 0.4787, 'grad_norm': 1.2798041105270386, 'learning_rate': 8.628205128205129e-05, 'epoch': 1.43}
********************on step end call back********************
Step 7900 finish
{'loss': 0.4894, 'grad_norm': 1.3758496046066284, 'learning_rate': 8.626373626373627e-05, 'epoch': 1.44}
{'eval_loss': 0.32006505131721497, 'eval_accuracy': 0.8854166666666666, 'eval_runtime': 129.6729, 'eval_samples_per_second': 4.92, 'eval_steps_per_second': 4.92, 'epoch': 1.44}
********************save call back********************
********************on step end call back********************
Step 7910 finish
{'loss': 0.4308, 'grad_norm': 1.1918931007385254, 'learning_rate': 8.624542124542125e-05, 'epoch': 1.44}
********************on step end call back********************
Step 7920 finish
{'loss': 0.4261, 'grad_norm': 0.8659995198249817, 'learning_rate': 8.622710622710624e-05, 'epoch': 1.44}
********************on step end call back********************
Step 7930 finish
{'loss': 0.4513, 'grad_norm': 1.2410253286361694, 'learning_rate': 8.620879120879122e-05, 'epoch': 1.44}
********************on step end call back********************
Step 7940 finish
[INFO|trainer.py:3376] 2024-03-22 11:08:50,223 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-22 11:08:50,223 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-22 11:08:50,223 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-22 11:10:59,815 >> Saving model checkpoint to ./output/tmp-checkpoint-8000
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-22 11:10:59,959 >> tokenizer config file saved in ./output/tmp-checkpoint-8000/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-22 11:10:59,959 >> Special tokens file saved in ./output/tmp-checkpoint-8000/special_tokens_map.json
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-22 11:19:37,652 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-22 11:19:37,652 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-22 11:19:37,652 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-22 11:21:47,673 >> Saving model checkpoint to ./output/tmp-checkpoint-8100
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-22 11:21:47,818 >> tokenizer config file saved in ./output/tmp-checkpoint-8100/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-22 11:21:47,818 >> Special tokens file saved in ./output/tmp-checkpoint-8100/special_tokens_map.json
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-22 11:30:24,855 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-22 11:30:24,855 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-22 11:30:24,855 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-22 11:32:34,881 >> Saving model checkpoint to ./output/tmp-checkpoint-8200
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-22 11:32:35,025 >> tokenizer config file saved in ./output/tmp-checkpoint-8200/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-22 11:32:35,025 >> Special tokens file saved in ./output/tmp-checkpoint-8200/special_tokens_map.json
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-22 11:41:17,001 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-22 11:41:17,001 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-22 11:41:17,001 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-22 11:43:27,035 >> Saving model checkpoint to ./output/tmp-checkpoint-8300
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-22 11:43:27,178 >> tokenizer config file saved in ./output/tmp-checkpoint-8300/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-22 11:43:27,178 >> Special tokens file saved in ./output/tmp-checkpoint-8300/special_tokens_map.json
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
{'loss': 0.4158, 'grad_norm': 1.556868553161621, 'learning_rate': 8.61904761904762e-05, 'epoch': 1.44}
********************on step end call back********************
Step 7950 finish
{'loss': 0.5118, 'grad_norm': 1.2186660766601562, 'learning_rate': 8.617216117216118e-05, 'epoch': 1.45}
********************on step end call back********************
Step 7960 finish
{'loss': 0.4776, 'grad_norm': 1.1691029071807861, 'learning_rate': 8.615384615384617e-05, 'epoch': 1.45}
********************on step end call back********************
Step 7970 finish
{'loss': 0.4662, 'grad_norm': 1.2213034629821777, 'learning_rate': 8.613553113553115e-05, 'epoch': 1.45}
********************on step end call back********************
Step 7980 finish
{'loss': 0.4623, 'grad_norm': 1.1866356134414673, 'learning_rate': 8.611721611721613e-05, 'epoch': 1.45}
********************on step end call back********************
Step 7990 finish
{'loss': 0.4428, 'grad_norm': 1.0384260416030884, 'learning_rate': 8.60989010989011e-05, 'epoch': 1.45}
********************on step end call back********************
Step 8000 finish
{'loss': 0.4669, 'grad_norm': 1.3877131938934326, 'learning_rate': 8.608058608058608e-05, 'epoch': 1.45}
{'eval_loss': 0.32961156964302063, 'eval_accuracy': 0.875, 'eval_runtime': 129.5901, 'eval_samples_per_second': 4.923, 'eval_steps_per_second': 4.923, 'epoch': 1.45}
********************save call back********************
********************on step end call back********************
Step 8010 finish
{'loss': 0.4753, 'grad_norm': 1.3617305755615234, 'learning_rate': 8.606227106227108e-05, 'epoch': 1.46}
********************on step end call back********************
Step 8020 finish
{'loss': 0.4589, 'grad_norm': 1.3942605257034302, 'learning_rate': 8.604395604395605e-05, 'epoch': 1.46}
********************on step end call back********************
Step 8030 finish
{'loss': 0.4388, 'grad_norm': 0.9234143495559692, 'learning_rate': 8.602564102564103e-05, 'epoch': 1.46}
********************on step end call back********************
Step 8040 finish
{'loss': 0.4847, 'grad_norm': 0.8729496002197266, 'learning_rate': 8.600732600732601e-05, 'epoch': 1.46}
********************on step end call back********************
Step 8050 finish
{'loss': 0.4157, 'grad_norm': 1.1765726804733276, 'learning_rate': 8.5989010989011e-05, 'epoch': 1.46}
********************on step end call back********************
Step 8060 finish
{'loss': 0.4796, 'grad_norm': 1.0933893918991089, 'learning_rate': 8.597069597069598e-05, 'epoch': 1.47}
********************on step end call back********************
Step 8070 finish
{'loss': 0.4464, 'grad_norm': 1.157232642173767, 'learning_rate': 8.595238095238096e-05, 'epoch': 1.47}
********************on step end call back********************
Step 8080 finish
{'loss': 0.4431, 'grad_norm': 1.1585429906845093, 'learning_rate': 8.593406593406593e-05, 'epoch': 1.47}
********************on step end call back********************
Step 8090 finish
{'loss': 0.4337, 'grad_norm': 1.1767802238464355, 'learning_rate': 8.591575091575092e-05, 'epoch': 1.47}
********************on step end call back********************
Step 8100 finish
{'loss': 0.475, 'grad_norm': 1.2359477281570435, 'learning_rate': 8.58974358974359e-05, 'epoch': 1.47}
{'eval_loss': 0.3189105689525604, 'eval_accuracy': 0.8645833333333334, 'eval_runtime': 130.0195, 'eval_samples_per_second': 4.907, 'eval_steps_per_second': 4.907, 'epoch': 1.47}
********************save call back********************
********************on step end call back********************
Step 8110 finish
{'loss': 0.4903, 'grad_norm': 1.203269600868225, 'learning_rate': 8.587912087912088e-05, 'epoch': 1.47}
********************on step end call back********************
Step 8120 finish
{'loss': 0.4848, 'grad_norm': 1.306145191192627, 'learning_rate': 8.586080586080586e-05, 'epoch': 1.48}
********************on step end call back********************
Step 8130 finish
{'loss': 0.4453, 'grad_norm': 1.1461848020553589, 'learning_rate': 8.584249084249085e-05, 'epoch': 1.48}
********************on step end call back********************
Step 8140 finish
{'loss': 0.4862, 'grad_norm': 1.1209309101104736, 'learning_rate': 8.582417582417583e-05, 'epoch': 1.48}
********************on step end call back********************
Step 8150 finish
{'loss': 0.5035, 'grad_norm': 1.1966744661331177, 'learning_rate': 8.58058608058608e-05, 'epoch': 1.48}
********************on step end call back********************
Step 8160 finish
{'loss': 0.5108, 'grad_norm': 1.10152006149292, 'learning_rate': 8.578754578754578e-05, 'epoch': 1.48}
********************on step end call back********************
Step 8170 finish
{'loss': 0.5076, 'grad_norm': 1.3034613132476807, 'learning_rate': 8.576923076923076e-05, 'epoch': 1.49}
********************on step end call back********************
Step 8180 finish
{'loss': 0.4141, 'grad_norm': 1.3259611129760742, 'learning_rate': 8.575091575091576e-05, 'epoch': 1.49}
********************on step end call back********************
Step 8190 finish
{'loss': 0.4911, 'grad_norm': 1.1961331367492676, 'learning_rate': 8.573260073260073e-05, 'epoch': 1.49}
********************on step end call back********************
Step 8200 finish
{'loss': 0.434, 'grad_norm': 1.3436189889907837, 'learning_rate': 8.571428571428571e-05, 'epoch': 1.49}
{'eval_loss': 0.3119679391384125, 'eval_accuracy': 0.875, 'eval_runtime': 130.0254, 'eval_samples_per_second': 4.907, 'eval_steps_per_second': 4.907, 'epoch': 1.49}
********************save call back********************
********************on step end call back********************
Step 8210 finish
{'loss': 0.4067, 'grad_norm': 1.1812949180603027, 'learning_rate': 8.569597069597069e-05, 'epoch': 1.49}
********************on step end call back********************
Step 8220 finish
{'loss': 0.4379, 'grad_norm': 1.2107633352279663, 'learning_rate': 8.567765567765568e-05, 'epoch': 1.49}
********************on step end call back********************
Step 8230 finish
{'loss': 0.4541, 'grad_norm': 1.2262687683105469, 'learning_rate': 8.565934065934066e-05, 'epoch': 1.5}
********************on step end call back********************
Step 8240 finish
{'loss': 0.4859, 'grad_norm': 0.8891283869743347, 'learning_rate': 8.564102564102564e-05, 'epoch': 1.5}
********************on step end call back********************
Step 8250 finish
{'loss': 0.427, 'grad_norm': 1.2553223371505737, 'learning_rate': 8.562271062271062e-05, 'epoch': 1.5}
********************on step end call back********************
Step 8260 finish
{'loss': 0.4539, 'grad_norm': 1.1841998100280762, 'learning_rate': 8.56043956043956e-05, 'epoch': 1.5}
********************on step end call back********************
Step 8270 finish
{'loss': 0.406, 'grad_norm': 1.0691401958465576, 'learning_rate': 8.558608058608059e-05, 'epoch': 1.5}
********************on step end call back********************
Step 8280 finish
{'loss': 0.4252, 'grad_norm': 1.039103627204895, 'learning_rate': 8.556776556776557e-05, 'epoch': 1.51}
********************on step end call back********************
Step 8290 finish
{'loss': 0.4441, 'grad_norm': 1.3699545860290527, 'learning_rate': 8.554945054945055e-05, 'epoch': 1.51}
********************on step end call back********************
Step 8300 finish
{'loss': 0.4675, 'grad_norm': 1.291865587234497, 'learning_rate': 8.553113553113553e-05, 'epoch': 1.51}
{'eval_loss': 0.31305935978889465, 'eval_accuracy': 0.8541666666666666, 'eval_runtime': 130.0338, 'eval_samples_per_second': 4.906, 'eval_steps_per_second': 4.906, 'epoch': 1.51}
********************save call back********************
********************on step end call back********************
Step 8310 finish
{'loss': 0.4265, 'grad_norm': 1.235485315322876, 'learning_rate': 8.551282051282052e-05, 'epoch': 1.51}
********************on step end call back********************
Step 8320 finish
{'loss': 0.4732, 'grad_norm': 1.7541096210479736, 'learning_rate': 8.54945054945055e-05, 'epoch': 1.51}
********************on step end call back********************
Step 8330 finish
{'loss': 0.4215, 'grad_norm': 1.4004132747650146, 'learning_rate': 8.547619047619048e-05, 'epoch': 1.51}
[INFO|trainer.py:3376] 2024-03-22 11:51:56,163 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-22 11:51:56,164 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-22 11:51:56,164 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-22 11:54:06,137 >> Saving model checkpoint to ./output/tmp-checkpoint-8400
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-22 11:54:06,278 >> tokenizer config file saved in ./output/tmp-checkpoint-8400/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-22 11:54:06,278 >> Special tokens file saved in ./output/tmp-checkpoint-8400/special_tokens_map.json
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-22 12:02:57,610 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-22 12:02:57,610 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-22 12:02:57,610 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-22 12:05:08,282 >> Saving model checkpoint to ./output/tmp-checkpoint-8500
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-22 12:05:08,432 >> tokenizer config file saved in ./output/tmp-checkpoint-8500/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-22 12:05:08,432 >> Special tokens file saved in ./output/tmp-checkpoint-8500/special_tokens_map.json
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-22 12:13:50,180 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-22 12:13:50,180 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-22 12:13:50,180 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-22 12:16:00,200 >> Saving model checkpoint to ./output/tmp-checkpoint-8600
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-22 12:16:00,343 >> tokenizer config file saved in ./output/tmp-checkpoint-8600/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-22 12:16:00,343 >> Special tokens file saved in ./output/tmp-checkpoint-8600/special_tokens_map.json
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-22 12:24:41,941 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-22 12:24:41,941 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-22 12:24:41,941 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-22 12:26:51,702 >> Saving model checkpoint to ./output/tmp-checkpoint-8700
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-22 12:26:51,849 >> tokenizer config file saved in ./output/tmp-checkpoint-8700/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-22 12:26:51,849 >> Special tokens file saved in ./output/tmp-checkpoint-8700/special_tokens_map.json
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
********************on step end call back********************
Step 8340 finish
{'loss': 0.4622, 'grad_norm': 1.1190072298049927, 'learning_rate': 8.545787545787546e-05, 'epoch': 1.52}
********************on step end call back********************
Step 8350 finish
{'loss': 0.4561, 'grad_norm': 1.501908540725708, 'learning_rate': 8.543956043956043e-05, 'epoch': 1.52}
********************on step end call back********************
Step 8360 finish
{'loss': 0.4506, 'grad_norm': 1.279104471206665, 'learning_rate': 8.542124542124543e-05, 'epoch': 1.52}
********************on step end call back********************
Step 8370 finish
{'loss': 0.5018, 'grad_norm': 0.8879432082176208, 'learning_rate': 8.54029304029304e-05, 'epoch': 1.52}
********************on step end call back********************
Step 8380 finish
{'loss': 0.4678, 'grad_norm': 0.930532693862915, 'learning_rate': 8.538461538461538e-05, 'epoch': 1.52}
********************on step end call back********************
Step 8390 finish
{'loss': 0.4791, 'grad_norm': 1.0519675016403198, 'learning_rate': 8.536630036630036e-05, 'epoch': 1.53}
********************on step end call back********************
Step 8400 finish
{'loss': 0.4899, 'grad_norm': 1.5199717283248901, 'learning_rate': 8.534798534798534e-05, 'epoch': 1.53}
{'eval_loss': 0.3153265118598938, 'eval_accuracy': 0.8541666666666666, 'eval_runtime': 129.972, 'eval_samples_per_second': 4.909, 'eval_steps_per_second': 4.909, 'epoch': 1.53}
********************save call back********************
********************on step end call back********************
Step 8410 finish
{'loss': 0.4841, 'grad_norm': 1.174500584602356, 'learning_rate': 8.532967032967033e-05, 'epoch': 1.53}
********************on step end call back********************
Step 8420 finish
{'loss': 0.4186, 'grad_norm': 0.9948628544807434, 'learning_rate': 8.531135531135531e-05, 'epoch': 1.53}
********************on step end call back********************
Step 8430 finish
{'loss': 0.4669, 'grad_norm': 1.0586061477661133, 'learning_rate': 8.529304029304029e-05, 'epoch': 1.53}
********************on step end call back********************
Step 8440 finish
{'loss': 0.4805, 'grad_norm': 1.2167978286743164, 'learning_rate': 8.527472527472527e-05, 'epoch': 1.53}
********************on step end call back********************
Step 8450 finish
{'loss': 0.4432, 'grad_norm': 1.303576946258545, 'learning_rate': 8.525641025641026e-05, 'epoch': 1.54}
********************on step end call back********************
Step 8460 finish
{'loss': 0.4384, 'grad_norm': 1.0042623281478882, 'learning_rate': 8.523809523809524e-05, 'epoch': 1.54}
********************on step end call back********************
Step 8470 finish
{'loss': 0.4085, 'grad_norm': 1.290344476699829, 'learning_rate': 8.521978021978022e-05, 'epoch': 1.54}
********************on step end call back********************
Step 8480 finish
{'loss': 0.4958, 'grad_norm': 1.1287330389022827, 'learning_rate': 8.52014652014652e-05, 'epoch': 1.54}
********************on step end call back********************
Step 8490 finish
{'loss': 0.4073, 'grad_norm': 1.1450294256210327, 'learning_rate': 8.518315018315018e-05, 'epoch': 1.54}
********************on step end call back********************
Step 8500 finish
{'loss': 0.4741, 'grad_norm': 1.1441104412078857, 'learning_rate': 8.516483516483517e-05, 'epoch': 1.55}
{'eval_loss': 0.3144054710865021, 'eval_accuracy': 0.875, 'eval_runtime': 130.6715, 'eval_samples_per_second': 4.882, 'eval_steps_per_second': 4.882, 'epoch': 1.55}
********************save call back********************
********************on step end call back********************
Step 8510 finish
{'loss': 0.4181, 'grad_norm': 0.95883709192276, 'learning_rate': 8.514652014652015e-05, 'epoch': 1.55}
********************on step end call back********************
Step 8520 finish
{'loss': 0.4769, 'grad_norm': 1.4055004119873047, 'learning_rate': 8.512820512820513e-05, 'epoch': 1.55}
********************on step end call back********************
Step 8530 finish
{'loss': 0.5004, 'grad_norm': 1.3060754537582397, 'learning_rate': 8.51098901098901e-05, 'epoch': 1.55}
********************on step end call back********************
Step 8540 finish
{'loss': 0.4423, 'grad_norm': 1.2837895154953003, 'learning_rate': 8.50915750915751e-05, 'epoch': 1.55}
********************on step end call back********************
Step 8550 finish
{'loss': 0.4411, 'grad_norm': 1.1310397386550903, 'learning_rate': 8.507326007326008e-05, 'epoch': 1.55}
********************on step end call back********************
Step 8560 finish
{'loss': 0.4191, 'grad_norm': 0.9874706268310547, 'learning_rate': 8.505494505494506e-05, 'epoch': 1.56}
********************on step end call back********************
Step 8570 finish
{'loss': 0.468, 'grad_norm': 1.026821494102478, 'learning_rate': 8.503663003663004e-05, 'epoch': 1.56}
********************on step end call back********************
Step 8580 finish
{'loss': 0.4238, 'grad_norm': 0.8149908185005188, 'learning_rate': 8.501831501831501e-05, 'epoch': 1.56}
********************on step end call back********************
Step 8590 finish
{'loss': 0.4552, 'grad_norm': 0.8854609727859497, 'learning_rate': 8.5e-05, 'epoch': 1.56}
********************on step end call back********************
Step 8600 finish
{'loss': 0.4373, 'grad_norm': 1.1982121467590332, 'learning_rate': 8.498168498168498e-05, 'epoch': 1.56}
{'eval_loss': 0.3182870149612427, 'eval_accuracy': 0.8958333333333334, 'eval_runtime': 130.0184, 'eval_samples_per_second': 4.907, 'eval_steps_per_second': 4.907, 'epoch': 1.56}
********************save call back********************
********************on step end call back********************
Step 8610 finish
{'loss': 0.4431, 'grad_norm': 1.159484624862671, 'learning_rate': 8.496336996336996e-05, 'epoch': 1.57}
********************on step end call back********************
Step 8620 finish
{'loss': 0.4486, 'grad_norm': 1.2445932626724243, 'learning_rate': 8.494505494505494e-05, 'epoch': 1.57}
********************on step end call back********************
Step 8630 finish
{'loss': 0.4427, 'grad_norm': 0.9604330062866211, 'learning_rate': 8.492673992673993e-05, 'epoch': 1.57}
********************on step end call back********************
Step 8640 finish
{'loss': 0.3639, 'grad_norm': 0.8770664930343628, 'learning_rate': 8.490842490842491e-05, 'epoch': 1.57}
********************on step end call back********************
Step 8650 finish
{'loss': 0.4473, 'grad_norm': 1.3248430490493774, 'learning_rate': 8.489010989010989e-05, 'epoch': 1.57}
********************on step end call back********************
Step 8660 finish
{'loss': 0.4422, 'grad_norm': 1.0310248136520386, 'learning_rate': 8.487179487179487e-05, 'epoch': 1.57}
********************on step end call back********************
Step 8670 finish
{'loss': 0.4704, 'grad_norm': 1.1987147331237793, 'learning_rate': 8.485347985347985e-05, 'epoch': 1.58}
********************on step end call back********************
Step 8680 finish
{'loss': 0.4592, 'grad_norm': 1.2964296340942383, 'learning_rate': 8.483516483516484e-05, 'epoch': 1.58}
********************on step end call back********************
Step 8690 finish
{'loss': 0.4077, 'grad_norm': 1.2221407890319824, 'learning_rate': 8.481684981684982e-05, 'epoch': 1.58}
********************on step end call back********************
Step 8700 finish
{'loss': 0.4763, 'grad_norm': 1.3183653354644775, 'learning_rate': 8.47985347985348e-05, 'epoch': 1.58}
{'eval_loss': 0.3151592016220093, 'eval_accuracy': 0.875, 'eval_runtime': 129.7604, 'eval_samples_per_second': 4.917, 'eval_steps_per_second': 4.917, 'epoch': 1.58}
********************save call back********************
********************on step end call back********************
Step 8710 finish
{'loss': 0.4483, 'grad_norm': 0.7403360605239868, 'learning_rate': 8.478021978021978e-05, 'epoch': 1.58}
********************on step end call back********************
Step 8720 finish
{'loss': 0.4874, 'grad_norm': 1.256354570388794, 'learning_rate': 8.476190476190477e-05, 'epoch': 1.59}
********************on step end call back********************
Step 8730 finish
[INFO|trainer.py:3376] 2024-03-22 12:35:23,449 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-22 12:35:23,449 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-22 12:35:23,449 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-22 12:37:33,046 >> Saving model checkpoint to ./output/tmp-checkpoint-8800
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-22 12:37:33,196 >> tokenizer config file saved in ./output/tmp-checkpoint-8800/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-22 12:37:33,196 >> Special tokens file saved in ./output/tmp-checkpoint-8800/special_tokens_map.json
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-22 12:46:16,468 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-22 12:46:16,468 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-22 12:46:16,468 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-22 12:48:26,011 >> Saving model checkpoint to ./output/tmp-checkpoint-8900
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-22 12:48:26,156 >> tokenizer config file saved in ./output/tmp-checkpoint-8900/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-22 12:48:26,156 >> Special tokens file saved in ./output/tmp-checkpoint-8900/special_tokens_map.json
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-22 12:57:07,556 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-22 12:57:07,556 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-22 12:57:07,556 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-22 12:59:17,108 >> Saving model checkpoint to ./output/tmp-checkpoint-9000
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-22 12:59:17,253 >> tokenizer config file saved in ./output/tmp-checkpoint-9000/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-22 12:59:17,254 >> Special tokens file saved in ./output/tmp-checkpoint-9000/special_tokens_map.json
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-22 13:07:52,018 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-22 13:07:52,018 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-22 13:07:52,018 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-22 13:10:01,710 >> Saving model checkpoint to ./output/tmp-checkpoint-9100
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-22 13:10:01,851 >> tokenizer config file saved in ./output/tmp-checkpoint-9100/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-22 13:10:01,851 >> Special tokens file saved in ./output/tmp-checkpoint-9100/special_tokens_map.json
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
{'loss': 0.4054, 'grad_norm': 1.4234964847564697, 'learning_rate': 8.474358974358975e-05, 'epoch': 1.59}
********************on step end call back********************
Step 8740 finish
{'loss': 0.4333, 'grad_norm': 1.646554946899414, 'learning_rate': 8.472527472527473e-05, 'epoch': 1.59}
********************on step end call back********************
Step 8750 finish
{'loss': 0.4784, 'grad_norm': 1.202270269393921, 'learning_rate': 8.470695970695971e-05, 'epoch': 1.59}
********************on step end call back********************
Step 8760 finish
{'loss': 0.5065, 'grad_norm': 1.2429859638214111, 'learning_rate': 8.468864468864469e-05, 'epoch': 1.59}
********************on step end call back********************
Step 8770 finish
{'loss': 0.4625, 'grad_norm': 1.2119070291519165, 'learning_rate': 8.467032967032968e-05, 'epoch': 1.59}
********************on step end call back********************
Step 8780 finish
{'loss': 0.4343, 'grad_norm': 1.0152374505996704, 'learning_rate': 8.465201465201466e-05, 'epoch': 1.6}
********************on step end call back********************
Step 8790 finish
{'loss': 0.4493, 'grad_norm': 1.2741055488586426, 'learning_rate': 8.463369963369964e-05, 'epoch': 1.6}
********************on step end call back********************
Step 8800 finish
{'loss': 0.3906, 'grad_norm': 1.3619515895843506, 'learning_rate': 8.461538461538461e-05, 'epoch': 1.6}
{'eval_loss': 0.31573471426963806, 'eval_accuracy': 0.8854166666666666, 'eval_runtime': 129.5965, 'eval_samples_per_second': 4.923, 'eval_steps_per_second': 4.923, 'epoch': 1.6}
********************save call back********************
********************on step end call back********************
Step 8810 finish
{'loss': 0.483, 'grad_norm': 0.9568046927452087, 'learning_rate': 8.45970695970696e-05, 'epoch': 1.6}
********************on step end call back********************
Step 8820 finish
{'loss': 0.4183, 'grad_norm': 1.286774754524231, 'learning_rate': 8.457875457875459e-05, 'epoch': 1.6}
********************on step end call back********************
Step 8830 finish
{'loss': 0.3977, 'grad_norm': 1.0025588274002075, 'learning_rate': 8.456043956043956e-05, 'epoch': 1.61}
********************on step end call back********************
Step 8840 finish
{'loss': 0.4584, 'grad_norm': 1.3047139644622803, 'learning_rate': 8.454212454212454e-05, 'epoch': 1.61}
********************on step end call back********************
Step 8850 finish
{'loss': 0.4164, 'grad_norm': 0.9211034774780273, 'learning_rate': 8.452380952380952e-05, 'epoch': 1.61}
********************on step end call back********************
Step 8860 finish
{'loss': 0.5046, 'grad_norm': 1.4891587495803833, 'learning_rate': 8.450549450549451e-05, 'epoch': 1.61}
********************on step end call back********************
Step 8870 finish
{'loss': 0.465, 'grad_norm': 1.2431864738464355, 'learning_rate': 8.448717948717949e-05, 'epoch': 1.61}
********************on step end call back********************
Step 8880 finish
{'loss': 0.489, 'grad_norm': 1.4492014646530151, 'learning_rate': 8.446886446886447e-05, 'epoch': 1.61}
********************on step end call back********************
Step 8890 finish
{'loss': 0.457, 'grad_norm': 1.033352255821228, 'learning_rate': 8.445054945054945e-05, 'epoch': 1.62}
********************on step end call back********************
Step 8900 finish
{'loss': 0.5012, 'grad_norm': 0.9543073773384094, 'learning_rate': 8.443223443223444e-05, 'epoch': 1.62}
{'eval_loss': 0.3171554505825043, 'eval_accuracy': 0.875, 'eval_runtime': 129.5417, 'eval_samples_per_second': 4.925, 'eval_steps_per_second': 4.925, 'epoch': 1.62}
********************save call back********************
********************on step end call back********************
Step 8910 finish
{'loss': 0.3783, 'grad_norm': 0.9876933693885803, 'learning_rate': 8.441391941391942e-05, 'epoch': 1.62}
********************on step end call back********************
Step 8920 finish
{'loss': 0.4583, 'grad_norm': 0.9205542802810669, 'learning_rate': 8.43956043956044e-05, 'epoch': 1.62}
********************on step end call back********************
Step 8930 finish
{'loss': 0.4385, 'grad_norm': 1.0905088186264038, 'learning_rate': 8.437728937728938e-05, 'epoch': 1.62}
********************on step end call back********************
Step 8940 finish
{'loss': 0.4571, 'grad_norm': 1.2146879434585571, 'learning_rate': 8.435897435897436e-05, 'epoch': 1.63}
********************on step end call back********************
Step 8950 finish
{'loss': 0.4261, 'grad_norm': 1.3756438493728638, 'learning_rate': 8.434065934065935e-05, 'epoch': 1.63}
********************on step end call back********************
Step 8960 finish
{'loss': 0.5178, 'grad_norm': 1.3045504093170166, 'learning_rate': 8.432234432234433e-05, 'epoch': 1.63}
********************on step end call back********************
Step 8970 finish
{'loss': 0.4143, 'grad_norm': 1.1258600950241089, 'learning_rate': 8.430402930402931e-05, 'epoch': 1.63}
********************on step end call back********************
Step 8980 finish
{'loss': 0.4553, 'grad_norm': 0.9580963253974915, 'learning_rate': 8.428571428571429e-05, 'epoch': 1.63}
********************on step end call back********************
Step 8990 finish
{'loss': 0.4557, 'grad_norm': 1.1470367908477783, 'learning_rate': 8.426739926739928e-05, 'epoch': 1.63}
********************on step end call back********************
Step 9000 finish
{'loss': 0.4582, 'grad_norm': 1.020196557044983, 'learning_rate': 8.424908424908426e-05, 'epoch': 1.64}
{'eval_loss': 0.3137598931789398, 'eval_accuracy': 0.8541666666666666, 'eval_runtime': 129.5512, 'eval_samples_per_second': 4.925, 'eval_steps_per_second': 4.925, 'epoch': 1.64}
********************save call back********************
********************on step end call back********************
Step 9010 finish
{'loss': 0.4482, 'grad_norm': 1.0271806716918945, 'learning_rate': 8.423076923076924e-05, 'epoch': 1.64}
********************on step end call back********************
Step 9020 finish
{'loss': 0.4255, 'grad_norm': 1.3068757057189941, 'learning_rate': 8.421245421245421e-05, 'epoch': 1.64}
********************on step end call back********************
Step 9030 finish
{'loss': 0.4175, 'grad_norm': 1.2164238691329956, 'learning_rate': 8.41941391941392e-05, 'epoch': 1.64}
********************on step end call back********************
Step 9040 finish
{'loss': 0.413, 'grad_norm': 1.1307568550109863, 'learning_rate': 8.417582417582419e-05, 'epoch': 1.64}
********************on step end call back********************
Step 9050 finish
{'loss': 0.459, 'grad_norm': 0.9689744710922241, 'learning_rate': 8.415750915750916e-05, 'epoch': 1.65}
********************on step end call back********************
Step 9060 finish
{'loss': 0.4344, 'grad_norm': 1.1524593830108643, 'learning_rate': 8.413919413919414e-05, 'epoch': 1.65}
********************on step end call back********************
Step 9070 finish
{'loss': 0.4569, 'grad_norm': 1.317954659461975, 'learning_rate': 8.412087912087912e-05, 'epoch': 1.65}
********************on step end call back********************
Step 9080 finish
{'loss': 0.452, 'grad_norm': 1.432842493057251, 'learning_rate': 8.410256410256411e-05, 'epoch': 1.65}
********************on step end call back********************
Step 9090 finish
{'loss': 0.4621, 'grad_norm': 0.9745445847511292, 'learning_rate': 8.408424908424909e-05, 'epoch': 1.65}
********************on step end call back********************
Step 9100 finish
{'loss': 0.5071, 'grad_norm': 1.128523826599121, 'learning_rate': 8.406593406593407e-05, 'epoch': 1.65}
{'eval_loss': 0.3209548890590668, 'eval_accuracy': 0.8541666666666666, 'eval_runtime': 129.6917, 'eval_samples_per_second': 4.919, 'eval_steps_per_second': 4.919, 'epoch': 1.65}
********************save call back********************
********************on step end call back********************
Step 9110 finish
{'loss': 0.4158, 'grad_norm': 1.0650097131729126, 'learning_rate': 8.404761904761905e-05, 'epoch': 1.66}
********************on step end call back********************
Step 9120 finish
{'loss': 0.4436, 'grad_norm': 1.232742190361023, 'learning_rate': 8.402930402930403e-05, 'epoch': 1.66}
[INFO|trainer.py:3376] 2024-03-22 13:18:38,289 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-22 13:18:38,289 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-22 13:18:38,289 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-22 13:20:47,883 >> Saving model checkpoint to ./output/tmp-checkpoint-9200
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-22 13:20:48,027 >> tokenizer config file saved in ./output/tmp-checkpoint-9200/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-22 13:20:48,027 >> Special tokens file saved in ./output/tmp-checkpoint-9200/special_tokens_map.json
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-22 13:29:33,194 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-22 13:29:33,195 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-22 13:29:33,195 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-22 13:31:42,828 >> Saving model checkpoint to ./output/tmp-checkpoint-9300
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-22 13:31:42,972 >> tokenizer config file saved in ./output/tmp-checkpoint-9300/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-22 13:31:42,972 >> Special tokens file saved in ./output/tmp-checkpoint-9300/special_tokens_map.json
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-22 13:40:19,404 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-22 13:40:19,404 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-22 13:40:19,404 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-22 13:42:29,219 >> Saving model checkpoint to ./output/tmp-checkpoint-9400
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-22 13:42:29,367 >> tokenizer config file saved in ./output/tmp-checkpoint-9400/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-22 13:42:29,367 >> Special tokens file saved in ./output/tmp-checkpoint-9400/special_tokens_map.json
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-22 13:51:09,338 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-22 13:51:09,339 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-22 13:51:09,339 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-22 13:53:19,090 >> Saving model checkpoint to ./output/tmp-checkpoint-9500
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-22 13:53:19,236 >> tokenizer config file saved in ./output/tmp-checkpoint-9500/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-22 13:53:19,236 >> Special tokens file saved in ./output/tmp-checkpoint-9500/special_tokens_map.json
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
********************on step end call back********************
Step 9130 finish
{'loss': 0.4584, 'grad_norm': 1.3271628618240356, 'learning_rate': 8.401098901098902e-05, 'epoch': 1.66}
********************on step end call back********************
Step 9140 finish
{'loss': 0.4486, 'grad_norm': 1.3040995597839355, 'learning_rate': 8.3992673992674e-05, 'epoch': 1.66}
********************on step end call back********************
Step 9150 finish
{'loss': 0.4655, 'grad_norm': 1.1264086961746216, 'learning_rate': 8.397435897435898e-05, 'epoch': 1.66}
********************on step end call back********************
Step 9160 finish
{'loss': 0.3603, 'grad_norm': 0.9378930926322937, 'learning_rate': 8.395604395604396e-05, 'epoch': 1.67}
********************on step end call back********************
Step 9170 finish
{'loss': 0.4316, 'grad_norm': 0.9628861546516418, 'learning_rate': 8.393772893772894e-05, 'epoch': 1.67}
********************on step end call back********************
Step 9180 finish
{'loss': 0.4201, 'grad_norm': 1.2425585985183716, 'learning_rate': 8.391941391941393e-05, 'epoch': 1.67}
********************on step end call back********************
Step 9190 finish
{'loss': 0.4009, 'grad_norm': 0.988864004611969, 'learning_rate': 8.390109890109891e-05, 'epoch': 1.67}
********************on step end call back********************
Step 9200 finish
{'loss': 0.4399, 'grad_norm': 1.059617519378662, 'learning_rate': 8.388278388278389e-05, 'epoch': 1.67}
{'eval_loss': 0.32049494981765747, 'eval_accuracy': 0.875, 'eval_runtime': 129.5931, 'eval_samples_per_second': 4.923, 'eval_steps_per_second': 4.923, 'epoch': 1.67}
********************save call back********************
********************on step end call back********************
Step 9210 finish
{'loss': 0.45, 'grad_norm': 1.1122909784317017, 'learning_rate': 8.386446886446887e-05, 'epoch': 1.67}
********************on step end call back********************
Step 9220 finish
{'loss': 0.4486, 'grad_norm': 1.4650102853775024, 'learning_rate': 8.384615384615386e-05, 'epoch': 1.68}
********************on step end call back********************
Step 9230 finish
{'loss': 0.447, 'grad_norm': 1.039294958114624, 'learning_rate': 8.382783882783884e-05, 'epoch': 1.68}
********************on step end call back********************
Step 9240 finish
{'loss': 0.447, 'grad_norm': 1.0327421426773071, 'learning_rate': 8.380952380952382e-05, 'epoch': 1.68}
********************on step end call back********************
Step 9250 finish
{'loss': 0.4344, 'grad_norm': 1.1518912315368652, 'learning_rate': 8.37912087912088e-05, 'epoch': 1.68}
********************on step end call back********************
Step 9260 finish
{'loss': 0.5067, 'grad_norm': 1.044359564781189, 'learning_rate': 8.377289377289377e-05, 'epoch': 1.68}
********************on step end call back********************
Step 9270 finish
{'loss': 0.4417, 'grad_norm': 1.3027838468551636, 'learning_rate': 8.375457875457876e-05, 'epoch': 1.69}
********************on step end call back********************
Step 9280 finish
{'loss': 0.5093, 'grad_norm': 1.0800364017486572, 'learning_rate': 8.373626373626374e-05, 'epoch': 1.69}
********************on step end call back********************
Step 9290 finish
{'loss': 0.4125, 'grad_norm': 1.145573377609253, 'learning_rate': 8.371794871794872e-05, 'epoch': 1.69}
********************on step end call back********************
Step 9300 finish
{'loss': 0.4162, 'grad_norm': 1.1044957637786865, 'learning_rate': 8.36996336996337e-05, 'epoch': 1.69}
{'eval_loss': 0.31414172053337097, 'eval_accuracy': 0.875, 'eval_runtime': 129.6331, 'eval_samples_per_second': 4.922, 'eval_steps_per_second': 4.922, 'epoch': 1.69}
********************save call back********************
********************on step end call back********************
Step 9310 finish
{'loss': 0.3705, 'grad_norm': 0.798892080783844, 'learning_rate': 8.36813186813187e-05, 'epoch': 1.69}
********************on step end call back********************
Step 9320 finish
{'loss': 0.441, 'grad_norm': 1.1741183996200562, 'learning_rate': 8.366300366300367e-05, 'epoch': 1.69}
********************on step end call back********************
Step 9330 finish
{'loss': 0.4723, 'grad_norm': 0.9610249400138855, 'learning_rate': 8.364468864468865e-05, 'epoch': 1.7}
********************on step end call back********************
Step 9340 finish
{'loss': 0.4761, 'grad_norm': 0.9043750166893005, 'learning_rate': 8.362637362637363e-05, 'epoch': 1.7}
********************on step end call back********************
Step 9350 finish
{'loss': 0.4298, 'grad_norm': 1.200789451599121, 'learning_rate': 8.360805860805861e-05, 'epoch': 1.7}
********************on step end call back********************
Step 9360 finish
{'loss': 0.4454, 'grad_norm': 0.9743874669075012, 'learning_rate': 8.35897435897436e-05, 'epoch': 1.7}
********************on step end call back********************
Step 9370 finish
{'loss': 0.4403, 'grad_norm': 1.09100341796875, 'learning_rate': 8.357142857142858e-05, 'epoch': 1.7}
********************on step end call back********************
Step 9380 finish
{'loss': 0.4028, 'grad_norm': 1.2182056903839111, 'learning_rate': 8.355311355311356e-05, 'epoch': 1.71}
********************on step end call back********************
Step 9390 finish
{'loss': 0.4658, 'grad_norm': 1.0948565006256104, 'learning_rate': 8.353479853479854e-05, 'epoch': 1.71}
********************on step end call back********************
Step 9400 finish
{'loss': 0.5367, 'grad_norm': 0.9861515164375305, 'learning_rate': 8.351648351648353e-05, 'epoch': 1.71}
{'eval_loss': 0.31934279203414917, 'eval_accuracy': 0.8645833333333334, 'eval_runtime': 129.8137, 'eval_samples_per_second': 4.915, 'eval_steps_per_second': 4.915, 'epoch': 1.71}
********************save call back********************
********************on step end call back********************
Step 9410 finish
{'loss': 0.4519, 'grad_norm': 1.0375267267227173, 'learning_rate': 8.349816849816851e-05, 'epoch': 1.71}
********************on step end call back********************
Step 9420 finish
{'loss': 0.4428, 'grad_norm': 1.391311526298523, 'learning_rate': 8.347985347985349e-05, 'epoch': 1.71}
********************on step end call back********************
Step 9430 finish
{'loss': 0.4196, 'grad_norm': 1.3470654487609863, 'learning_rate': 8.346153846153847e-05, 'epoch': 1.71}
********************on step end call back********************
Step 9440 finish
{'loss': 0.4786, 'grad_norm': 0.9799776673316956, 'learning_rate': 8.344322344322344e-05, 'epoch': 1.72}
********************on step end call back********************
Step 9450 finish
{'loss': 0.3941, 'grad_norm': 0.9466285109519958, 'learning_rate': 8.342490842490844e-05, 'epoch': 1.72}
********************on step end call back********************
Step 9460 finish
{'loss': 0.3971, 'grad_norm': 1.2521768808364868, 'learning_rate': 8.340659340659342e-05, 'epoch': 1.72}
********************on step end call back********************
Step 9470 finish
{'loss': 0.465, 'grad_norm': 1.03077232837677, 'learning_rate': 8.33882783882784e-05, 'epoch': 1.72}
********************on step end call back********************
Step 9480 finish
{'loss': 0.4239, 'grad_norm': 1.0931452512741089, 'learning_rate': 8.336996336996337e-05, 'epoch': 1.72}
********************on step end call back********************
Step 9490 finish
{'loss': 0.4375, 'grad_norm': 1.158473014831543, 'learning_rate': 8.335164835164837e-05, 'epoch': 1.73}
********************on step end call back********************
Step 9500 finish
{'loss': 0.4163, 'grad_norm': 1.969933032989502, 'learning_rate': 8.333333333333334e-05, 'epoch': 1.73}
{'eval_loss': 0.3104300796985626, 'eval_accuracy': 0.8541666666666666, 'eval_runtime': 129.751, 'eval_samples_per_second': 4.917, 'eval_steps_per_second': 4.917, 'epoch': 1.73}
********************save call back********************
********************on step end call back********************
Step 9510 finish
{'loss': 0.4587, 'grad_norm': 0.9856157302856445, 'learning_rate': 8.331501831501832e-05, 'epoch': 1.73}
********************on step end call back********************
Step 9520 finish
[INFO|trainer.py:3376] 2024-03-22 14:01:57,320 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-22 14:01:57,320 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-22 14:01:57,320 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-22 14:04:06,944 >> Saving model checkpoint to ./output/tmp-checkpoint-9600
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-22 14:04:07,087 >> tokenizer config file saved in ./output/tmp-checkpoint-9600/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-22 14:04:07,088 >> Special tokens file saved in ./output/tmp-checkpoint-9600/special_tokens_map.json
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-22 14:12:45,420 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-22 14:12:45,420 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-22 14:12:45,420 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-22 14:14:55,204 >> Saving model checkpoint to ./output/tmp-checkpoint-9700
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-22 14:14:55,350 >> tokenizer config file saved in ./output/tmp-checkpoint-9700/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-22 14:14:55,350 >> Special tokens file saved in ./output/tmp-checkpoint-9700/special_tokens_map.json
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-22 14:23:32,849 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-22 14:23:32,849 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-22 14:23:32,849 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-22 14:25:42,758 >> Saving model checkpoint to ./output/tmp-checkpoint-9800
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-22 14:25:42,899 >> tokenizer config file saved in ./output/tmp-checkpoint-9800/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-22 14:25:42,899 >> Special tokens file saved in ./output/tmp-checkpoint-9800/special_tokens_map.json
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-22 14:34:23,794 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-22 14:34:23,795 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-22 14:34:23,795 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-22 14:36:33,722 >> Saving model checkpoint to ./output/tmp-checkpoint-9900
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-22 14:36:33,865 >> tokenizer config file saved in ./output/tmp-checkpoint-9900/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-22 14:36:33,865 >> Special tokens file saved in ./output/tmp-checkpoint-9900/special_tokens_map.json
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
{'loss': 0.4087, 'grad_norm': 1.1045411825180054, 'learning_rate': 8.32967032967033e-05, 'epoch': 1.73}
********************on step end call back********************
Step 9530 finish
{'loss': 0.4802, 'grad_norm': 1.2266621589660645, 'learning_rate': 8.327838827838828e-05, 'epoch': 1.73}
********************on step end call back********************
Step 9540 finish
{'loss': 0.4034, 'grad_norm': 1.1106302738189697, 'learning_rate': 8.326007326007327e-05, 'epoch': 1.73}
********************on step end call back********************
Step 9550 finish
{'loss': 0.4513, 'grad_norm': 1.11173677444458, 'learning_rate': 8.324175824175825e-05, 'epoch': 1.74}
********************on step end call back********************
Step 9560 finish
{'loss': 0.4493, 'grad_norm': 0.9962456226348877, 'learning_rate': 8.322344322344323e-05, 'epoch': 1.74}
********************on step end call back********************
Step 9570 finish
{'loss': 0.4167, 'grad_norm': 1.1215691566467285, 'learning_rate': 8.320512820512821e-05, 'epoch': 1.74}
********************on step end call back********************
Step 9580 finish
{'loss': 0.4329, 'grad_norm': 1.690673828125, 'learning_rate': 8.31868131868132e-05, 'epoch': 1.74}
********************on step end call back********************
Step 9590 finish
{'loss': 0.4546, 'grad_norm': 0.972247838973999, 'learning_rate': 8.316849816849818e-05, 'epoch': 1.74}
********************on step end call back********************
Step 9600 finish
{'loss': 0.5219, 'grad_norm': 1.443625569343567, 'learning_rate': 8.315018315018316e-05, 'epoch': 1.75}
{'eval_loss': 0.31381288170814514, 'eval_accuracy': 0.8854166666666666, 'eval_runtime': 129.6227, 'eval_samples_per_second': 4.922, 'eval_steps_per_second': 4.922, 'epoch': 1.75}
********************save call back********************
********************on step end call back********************
Step 9610 finish
{'loss': 0.4403, 'grad_norm': 1.2714051008224487, 'learning_rate': 8.313186813186814e-05, 'epoch': 1.75}
********************on step end call back********************
Step 9620 finish
{'loss': 0.4233, 'grad_norm': 1.1355377435684204, 'learning_rate': 8.311355311355312e-05, 'epoch': 1.75}
********************on step end call back********************
Step 9630 finish
{'loss': 0.4851, 'grad_norm': 0.9427053332328796, 'learning_rate': 8.309523809523811e-05, 'epoch': 1.75}
********************on step end call back********************
Step 9640 finish
{'loss': 0.4298, 'grad_norm': 1.3152799606323242, 'learning_rate': 8.307692307692309e-05, 'epoch': 1.75}
********************on step end call back********************
Step 9650 finish
{'loss': 0.4467, 'grad_norm': 1.171242356300354, 'learning_rate': 8.305860805860807e-05, 'epoch': 1.75}
********************on step end call back********************
Step 9660 finish
{'loss': 0.3992, 'grad_norm': 1.1339870691299438, 'learning_rate': 8.304029304029304e-05, 'epoch': 1.76}
********************on step end call back********************
Step 9670 finish
{'loss': 0.4464, 'grad_norm': 1.3264875411987305, 'learning_rate': 8.302197802197804e-05, 'epoch': 1.76}
********************on step end call back********************
Step 9680 finish
{'loss': 0.4811, 'grad_norm': 1.0685786008834839, 'learning_rate': 8.300366300366302e-05, 'epoch': 1.76}
********************on step end call back********************
Step 9690 finish
{'loss': 0.4345, 'grad_norm': 1.5512921810150146, 'learning_rate': 8.2985347985348e-05, 'epoch': 1.76}
********************on step end call back********************
Step 9700 finish
{'loss': 0.4072, 'grad_norm': 0.94830721616745, 'learning_rate': 8.296703296703297e-05, 'epoch': 1.76}
{'eval_loss': 0.3140721321105957, 'eval_accuracy': 0.8854166666666666, 'eval_runtime': 129.7823, 'eval_samples_per_second': 4.916, 'eval_steps_per_second': 4.916, 'epoch': 1.76}
********************save call back********************
********************on step end call back********************
Step 9710 finish
{'loss': 0.5065, 'grad_norm': 1.2344865798950195, 'learning_rate': 8.294871794871795e-05, 'epoch': 1.77}
********************on step end call back********************
Step 9720 finish
{'loss': 0.4308, 'grad_norm': 0.9146045446395874, 'learning_rate': 8.293040293040294e-05, 'epoch': 1.77}
********************on step end call back********************
Step 9730 finish
{'loss': 0.4149, 'grad_norm': 0.9459481239318848, 'learning_rate': 8.291208791208792e-05, 'epoch': 1.77}
********************on step end call back********************
Step 9740 finish
{'loss': 0.4329, 'grad_norm': 1.239319920539856, 'learning_rate': 8.28937728937729e-05, 'epoch': 1.77}
********************on step end call back********************
Step 9750 finish
{'loss': 0.4308, 'grad_norm': 1.197270393371582, 'learning_rate': 8.287545787545788e-05, 'epoch': 1.77}
********************on step end call back********************
Step 9760 finish
{'loss': 0.4566, 'grad_norm': 1.6963058710098267, 'learning_rate': 8.285714285714287e-05, 'epoch': 1.77}
********************on step end call back********************
Step 9770 finish
{'loss': 0.4195, 'grad_norm': 1.0315444469451904, 'learning_rate': 8.283882783882785e-05, 'epoch': 1.78}
********************on step end call back********************
Step 9780 finish
{'loss': 0.452, 'grad_norm': 0.9317657947540283, 'learning_rate': 8.282051282051283e-05, 'epoch': 1.78}
********************on step end call back********************
Step 9790 finish
{'loss': 0.4369, 'grad_norm': 1.0291885137557983, 'learning_rate': 8.28021978021978e-05, 'epoch': 1.78}
********************on step end call back********************
Step 9800 finish
{'loss': 0.3767, 'grad_norm': 1.3396151065826416, 'learning_rate': 8.278388278388279e-05, 'epoch': 1.78}
{'eval_loss': 0.3139472007751465, 'eval_accuracy': 0.875, 'eval_runtime': 129.9079, 'eval_samples_per_second': 4.911, 'eval_steps_per_second': 4.911, 'epoch': 1.78}
********************save call back********************
********************on step end call back********************
Step 9810 finish
{'loss': 0.44, 'grad_norm': 1.3058792352676392, 'learning_rate': 8.276556776556777e-05, 'epoch': 1.78}
********************on step end call back********************
Step 9820 finish
{'loss': 0.4576, 'grad_norm': 0.9305180311203003, 'learning_rate': 8.274725274725275e-05, 'epoch': 1.79}
********************on step end call back********************
Step 9830 finish
{'loss': 0.4253, 'grad_norm': 0.9491142630577087, 'learning_rate': 8.272893772893772e-05, 'epoch': 1.79}
********************on step end call back********************
Step 9840 finish
{'loss': 0.5034, 'grad_norm': 1.1354559659957886, 'learning_rate': 8.27106227106227e-05, 'epoch': 1.79}
********************on step end call back********************
Step 9850 finish
{'loss': 0.3925, 'grad_norm': 1.0966250896453857, 'learning_rate': 8.26923076923077e-05, 'epoch': 1.79}
********************on step end call back********************
Step 9860 finish
{'loss': 0.4569, 'grad_norm': 1.2965115308761597, 'learning_rate': 8.267399267399267e-05, 'epoch': 1.79}
********************on step end call back********************
Step 9870 finish
{'loss': 0.4508, 'grad_norm': 1.0840343236923218, 'learning_rate': 8.265567765567765e-05, 'epoch': 1.79}
********************on step end call back********************
Step 9880 finish
{'loss': 0.4559, 'grad_norm': 0.9403659105300903, 'learning_rate': 8.263736263736263e-05, 'epoch': 1.8}
********************on step end call back********************
Step 9890 finish
{'loss': 0.4389, 'grad_norm': 0.9964683055877686, 'learning_rate': 8.261904761904762e-05, 'epoch': 1.8}
********************on step end call back********************
Step 9900 finish
{'loss': 0.4309, 'grad_norm': 1.114026427268982, 'learning_rate': 8.26007326007326e-05, 'epoch': 1.8}
{'eval_loss': 0.31545883417129517, 'eval_accuracy': 0.8958333333333334, 'eval_runtime': 129.9265, 'eval_samples_per_second': 4.91, 'eval_steps_per_second': 4.91, 'epoch': 1.8}
********************save call back********************
********************on step end call back********************
Step 9910 finish
{'loss': 0.4292, 'grad_norm': 1.2294590473175049, 'learning_rate': 8.258241758241758e-05, 'epoch': 1.8}
[INFO|trainer.py:3376] 2024-03-22 14:45:20,324 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-22 14:45:20,324 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-22 14:45:20,324 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-22 14:47:30,281 >> Saving model checkpoint to ./output/tmp-checkpoint-10000
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-22 14:47:30,426 >> tokenizer config file saved in ./output/tmp-checkpoint-10000/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-22 14:47:30,426 >> Special tokens file saved in ./output/tmp-checkpoint-10000/special_tokens_map.json
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-22 14:56:05,279 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-22 14:56:05,279 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-22 14:56:05,279 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-22 14:58:15,204 >> Saving model checkpoint to ./output/tmp-checkpoint-10100
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-22 14:58:15,347 >> tokenizer config file saved in ./output/tmp-checkpoint-10100/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-22 14:58:15,348 >> Special tokens file saved in ./output/tmp-checkpoint-10100/special_tokens_map.json
[INFO|trainer.py:3159] 2024-03-22 14:58:15,535 >> Deleting older checkpoint [output/checkpoint-100] due to args.save_total_limit
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-22 15:06:57,353 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-22 15:06:57,353 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-22 15:06:57,353 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-22 15:09:07,164 >> Saving model checkpoint to ./output/tmp-checkpoint-10200
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-22 15:09:07,432 >> tokenizer config file saved in ./output/tmp-checkpoint-10200/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-22 15:09:07,432 >> Special tokens file saved in ./output/tmp-checkpoint-10200/special_tokens_map.json
[INFO|trainer.py:3159] 2024-03-22 15:09:07,624 >> Deleting older checkpoint [output/checkpoint-200] due to args.save_total_limit
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-22 15:17:42,546 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-22 15:17:42,546 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-22 15:17:42,546 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-22 15:19:52,402 >> Saving model checkpoint to ./output/tmp-checkpoint-10300
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-22 15:19:52,689 >> tokenizer config file saved in ./output/tmp-checkpoint-10300/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-22 15:19:52,689 >> Special tokens file saved in ./output/tmp-checkpoint-10300/special_tokens_map.json
[INFO|trainer.py:3159] 2024-03-22 15:19:52,926 >> Deleting older checkpoint [output/checkpoint-300] due to args.save_total_limit
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
********************on step end call back********************
Step 9920 finish
{'loss': 0.4112, 'grad_norm': 1.04094660282135, 'learning_rate': 8.256410256410256e-05, 'epoch': 1.8}
********************on step end call back********************
Step 9930 finish
{'loss': 0.4844, 'grad_norm': 1.0334668159484863, 'learning_rate': 8.254578754578754e-05, 'epoch': 1.81}
********************on step end call back********************
Step 9940 finish
{'loss': 0.4458, 'grad_norm': 0.9488576650619507, 'learning_rate': 8.252747252747253e-05, 'epoch': 1.81}
********************on step end call back********************
Step 9950 finish
{'loss': 0.4616, 'grad_norm': 0.9403133988380432, 'learning_rate': 8.250915750915751e-05, 'epoch': 1.81}
********************on step end call back********************
Step 9960 finish
{'loss': 0.4122, 'grad_norm': 1.142663598060608, 'learning_rate': 8.249084249084249e-05, 'epoch': 1.81}
********************on step end call back********************
Step 9970 finish
{'loss': 0.4318, 'grad_norm': 1.294824242591858, 'learning_rate': 8.247252747252747e-05, 'epoch': 1.81}
********************on step end call back********************
Step 9980 finish
{'loss': 0.4425, 'grad_norm': 0.9978452920913696, 'learning_rate': 8.245421245421246e-05, 'epoch': 1.81}
********************on step end call back********************
Step 9990 finish
{'loss': 0.4628, 'grad_norm': 1.0028597116470337, 'learning_rate': 8.243589743589744e-05, 'epoch': 1.82}
********************on step end call back********************
Step 10000 finish
{'loss': 0.52, 'grad_norm': 0.9272074103355408, 'learning_rate': 8.241758241758242e-05, 'epoch': 1.82}
{'eval_loss': 0.31844016909599304, 'eval_accuracy': 0.8854166666666666, 'eval_runtime': 129.9565, 'eval_samples_per_second': 4.909, 'eval_steps_per_second': 4.909, 'epoch': 1.82}
********************save call back********************
********************on step end call back********************
Step 10010 finish
{'loss': 0.4078, 'grad_norm': 1.136863112449646, 'learning_rate': 8.23992673992674e-05, 'epoch': 1.82}
********************on step end call back********************
Step 10020 finish
{'loss': 0.4072, 'grad_norm': 1.3320404291152954, 'learning_rate': 8.238095238095238e-05, 'epoch': 1.82}
********************on step end call back********************
Step 10030 finish
{'loss': 0.4609, 'grad_norm': 1.195655107498169, 'learning_rate': 8.236263736263737e-05, 'epoch': 1.82}
********************on step end call back********************
Step 10040 finish
{'loss': 0.4675, 'grad_norm': 0.9570343494415283, 'learning_rate': 8.234432234432235e-05, 'epoch': 1.83}
********************on step end call back********************
Step 10050 finish
{'loss': 0.4083, 'grad_norm': 0.9770844578742981, 'learning_rate': 8.232600732600732e-05, 'epoch': 1.83}
********************on step end call back********************
Step 10060 finish
{'loss': 0.4596, 'grad_norm': 1.0546321868896484, 'learning_rate': 8.23076923076923e-05, 'epoch': 1.83}
********************on step end call back********************
Step 10070 finish
{'loss': 0.4226, 'grad_norm': 1.1618013381958008, 'learning_rate': 8.22893772893773e-05, 'epoch': 1.83}
********************on step end call back********************
Step 10080 finish
{'loss': 0.4954, 'grad_norm': 1.2733855247497559, 'learning_rate': 8.227106227106227e-05, 'epoch': 1.83}
********************on step end call back********************
Step 10090 finish
{'loss': 0.4414, 'grad_norm': 1.151996374130249, 'learning_rate': 8.225274725274725e-05, 'epoch': 1.83}
********************on step end call back********************
Step 10100 finish
{'loss': 0.4125, 'grad_norm': 1.0582431554794312, 'learning_rate': 8.223443223443223e-05, 'epoch': 1.84}
{'eval_loss': 0.31357869505882263, 'eval_accuracy': 0.8854166666666666, 'eval_runtime': 129.9234, 'eval_samples_per_second': 4.911, 'eval_steps_per_second': 4.911, 'epoch': 1.84}
********************save call back********************
********************on step end call back********************
Step 10110 finish
{'loss': 0.515, 'grad_norm': 1.3413808345794678, 'learning_rate': 8.221611721611721e-05, 'epoch': 1.84}
********************on step end call back********************
Step 10120 finish
{'loss': 0.4276, 'grad_norm': 0.8461529612541199, 'learning_rate': 8.21978021978022e-05, 'epoch': 1.84}
********************on step end call back********************
Step 10130 finish
{'loss': 0.4821, 'grad_norm': 1.2608987092971802, 'learning_rate': 8.217948717948718e-05, 'epoch': 1.84}
********************on step end call back********************
Step 10140 finish
{'loss': 0.4779, 'grad_norm': 1.2989188432693481, 'learning_rate': 8.216117216117216e-05, 'epoch': 1.84}
********************on step end call back********************
Step 10150 finish
{'loss': 0.4048, 'grad_norm': 0.7311856150627136, 'learning_rate': 8.214285714285714e-05, 'epoch': 1.85}
********************on step end call back********************
Step 10160 finish
{'loss': 0.4249, 'grad_norm': 1.2761330604553223, 'learning_rate': 8.212454212454213e-05, 'epoch': 1.85}
********************on step end call back********************
Step 10170 finish
{'loss': 0.4037, 'grad_norm': 1.3111944198608398, 'learning_rate': 8.210622710622711e-05, 'epoch': 1.85}
********************on step end call back********************
Step 10180 finish
{'loss': 0.4668, 'grad_norm': 0.8684600591659546, 'learning_rate': 8.208791208791209e-05, 'epoch': 1.85}
********************on step end call back********************
Step 10190 finish
{'loss': 0.4714, 'grad_norm': 1.0848963260650635, 'learning_rate': 8.206959706959707e-05, 'epoch': 1.85}
********************on step end call back********************
Step 10200 finish
{'loss': 0.4163, 'grad_norm': 0.7333943843841553, 'learning_rate': 8.205128205128205e-05, 'epoch': 1.85}
{'eval_loss': 0.3116309344768524, 'eval_accuracy': 0.8645833333333334, 'eval_runtime': 129.8099, 'eval_samples_per_second': 4.915, 'eval_steps_per_second': 4.915, 'epoch': 1.85}
********************save call back********************
********************on step end call back********************
Step 10210 finish
{'loss': 0.4208, 'grad_norm': 1.250280737876892, 'learning_rate': 8.203296703296704e-05, 'epoch': 1.86}
********************on step end call back********************
Step 10220 finish
{'loss': 0.4694, 'grad_norm': 1.327607274055481, 'learning_rate': 8.201465201465202e-05, 'epoch': 1.86}
********************on step end call back********************
Step 10230 finish
{'loss': 0.454, 'grad_norm': 1.3620727062225342, 'learning_rate': 8.1996336996337e-05, 'epoch': 1.86}
********************on step end call back********************
Step 10240 finish
{'loss': 0.4664, 'grad_norm': 1.252292275428772, 'learning_rate': 8.197802197802198e-05, 'epoch': 1.86}
********************on step end call back********************
Step 10250 finish
{'loss': 0.4938, 'grad_norm': 1.2379355430603027, 'learning_rate': 8.195970695970697e-05, 'epoch': 1.86}
********************on step end call back********************
Step 10260 finish
{'loss': 0.4156, 'grad_norm': 1.0989779233932495, 'learning_rate': 8.194139194139195e-05, 'epoch': 1.87}
********************on step end call back********************
Step 10270 finish
{'loss': 0.4593, 'grad_norm': 1.1488037109375, 'learning_rate': 8.192307692307693e-05, 'epoch': 1.87}
********************on step end call back********************
Step 10280 finish
{'loss': 0.4208, 'grad_norm': 1.0500084161758423, 'learning_rate': 8.19047619047619e-05, 'epoch': 1.87}
********************on step end call back********************
Step 10290 finish
{'loss': 0.388, 'grad_norm': 0.9312822818756104, 'learning_rate': 8.188644688644688e-05, 'epoch': 1.87}
********************on step end call back********************
Step 10300 finish
{'loss': 0.4112, 'grad_norm': 1.1879196166992188, 'learning_rate': 8.186813186813188e-05, 'epoch': 1.87}
{'eval_loss': 0.31397131085395813, 'eval_accuracy': 0.875, 'eval_runtime': 129.8548, 'eval_samples_per_second': 4.913, 'eval_steps_per_second': 4.913, 'epoch': 1.87}
********************save call back********************
********************on step end call back********************
Step 10310 finish
[INFO|trainer.py:3376] 2024-03-22 15:28:33,649 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-22 15:28:33,649 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-22 15:28:33,649 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-22 15:30:43,756 >> Saving model checkpoint to ./output/tmp-checkpoint-10400
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-22 15:30:43,926 >> tokenizer config file saved in ./output/tmp-checkpoint-10400/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-22 15:30:43,926 >> Special tokens file saved in ./output/tmp-checkpoint-10400/special_tokens_map.json
[INFO|trainer.py:3159] 2024-03-22 15:30:44,117 >> Deleting older checkpoint [output/checkpoint-400] due to args.save_total_limit
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-22 15:39:26,961 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-22 15:39:26,961 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-22 15:39:26,961 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-22 15:41:36,945 >> Saving model checkpoint to ./output/tmp-checkpoint-10500
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-22 15:41:37,090 >> tokenizer config file saved in ./output/tmp-checkpoint-10500/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-22 15:41:37,090 >> Special tokens file saved in ./output/tmp-checkpoint-10500/special_tokens_map.json
[INFO|trainer.py:3159] 2024-03-22 15:41:37,281 >> Deleting older checkpoint [output/checkpoint-500] due to args.save_total_limit
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-22 15:50:26,702 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-22 15:50:26,702 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-22 15:50:26,702 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-22 15:52:36,806 >> Saving model checkpoint to ./output/tmp-checkpoint-10600
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-22 15:52:37,084 >> tokenizer config file saved in ./output/tmp-checkpoint-10600/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-22 15:52:37,084 >> Special tokens file saved in ./output/tmp-checkpoint-10600/special_tokens_map.json
[INFO|trainer.py:3159] 2024-03-22 15:52:37,286 >> Deleting older checkpoint [output/checkpoint-600] due to args.save_total_limit
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-22 16:01:12,435 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-22 16:01:12,436 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-22 16:01:12,436 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-22 16:03:22,496 >> Saving model checkpoint to ./output/tmp-checkpoint-10700
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-22 16:03:22,808 >> tokenizer config file saved in ./output/tmp-checkpoint-10700/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-22 16:03:22,808 >> Special tokens file saved in ./output/tmp-checkpoint-10700/special_tokens_map.json
[INFO|trainer.py:3159] 2024-03-22 16:03:23,012 >> Deleting older checkpoint [output/checkpoint-700] due to args.save_total_limit
{'loss': 0.4303, 'grad_norm': 1.0697487592697144, 'learning_rate': 8.184981684981685e-05, 'epoch': 1.87}
********************on step end call back********************
Step 10320 finish
{'loss': 0.4508, 'grad_norm': 1.4890064001083374, 'learning_rate': 8.183150183150183e-05, 'epoch': 1.88}
********************on step end call back********************
Step 10330 finish
{'loss': 0.4411, 'grad_norm': 1.109182596206665, 'learning_rate': 8.181318681318681e-05, 'epoch': 1.88}
********************on step end call back********************
Step 10340 finish
{'loss': 0.4388, 'grad_norm': 1.071061134338379, 'learning_rate': 8.179487179487179e-05, 'epoch': 1.88}
********************on step end call back********************
Step 10350 finish
{'loss': 0.4476, 'grad_norm': 1.3840723037719727, 'learning_rate': 8.177655677655678e-05, 'epoch': 1.88}
********************on step end call back********************
Step 10360 finish
{'loss': 0.3792, 'grad_norm': 1.258905291557312, 'learning_rate': 8.175824175824176e-05, 'epoch': 1.88}
********************on step end call back********************
Step 10370 finish
{'loss': 0.4634, 'grad_norm': 1.2845278978347778, 'learning_rate': 8.173992673992674e-05, 'epoch': 1.89}
********************on step end call back********************
Step 10380 finish
{'loss': 0.4764, 'grad_norm': 1.320497989654541, 'learning_rate': 8.172161172161172e-05, 'epoch': 1.89}
********************on step end call back********************
Step 10390 finish
{'loss': 0.4395, 'grad_norm': 1.2002540826797485, 'learning_rate': 8.170329670329671e-05, 'epoch': 1.89}
********************on step end call back********************
Step 10400 finish
{'loss': 0.4906, 'grad_norm': 1.146066427230835, 'learning_rate': 8.168498168498169e-05, 'epoch': 1.89}
{'eval_loss': 0.3196764886379242, 'eval_accuracy': 0.8854166666666666, 'eval_runtime': 130.1057, 'eval_samples_per_second': 4.904, 'eval_steps_per_second': 4.904, 'epoch': 1.89}
********************save call back********************
********************on step end call back********************
Step 10410 finish
{'loss': 0.3798, 'grad_norm': 0.9296652674674988, 'learning_rate': 8.166666666666667e-05, 'epoch': 1.89}
********************on step end call back********************
Step 10420 finish
{'loss': 0.4029, 'grad_norm': 1.0153220891952515, 'learning_rate': 8.164835164835165e-05, 'epoch': 1.89}
********************on step end call back********************
Step 10430 finish
{'loss': 0.4485, 'grad_norm': 1.1064974069595337, 'learning_rate': 8.163003663003663e-05, 'epoch': 1.9}
********************on step end call back********************
Step 10440 finish
{'loss': 0.4534, 'grad_norm': 0.9973716139793396, 'learning_rate': 8.161172161172162e-05, 'epoch': 1.9}
********************on step end call back********************
Step 10450 finish
{'loss': 0.4695, 'grad_norm': 1.176973581314087, 'learning_rate': 8.15934065934066e-05, 'epoch': 1.9}
********************on step end call back********************
Step 10460 finish
{'loss': 0.4489, 'grad_norm': 1.022965669631958, 'learning_rate': 8.157509157509158e-05, 'epoch': 1.9}
********************on step end call back********************
Step 10470 finish
{'loss': 0.4095, 'grad_norm': 1.293868899345398, 'learning_rate': 8.155677655677655e-05, 'epoch': 1.9}
********************on step end call back********************
Step 10480 finish
{'loss': 0.3705, 'grad_norm': 0.8857311606407166, 'learning_rate': 8.153846153846155e-05, 'epoch': 1.91}
********************on step end call back********************
Step 10490 finish
{'loss': 0.4751, 'grad_norm': 0.976905882358551, 'learning_rate': 8.152014652014653e-05, 'epoch': 1.91}
********************on step end call back********************
Step 10500 finish
{'loss': 0.4938, 'grad_norm': 1.301984429359436, 'learning_rate': 8.15018315018315e-05, 'epoch': 1.91}
{'eval_loss': 0.3156987130641937, 'eval_accuracy': 0.8645833333333334, 'eval_runtime': 129.983, 'eval_samples_per_second': 4.908, 'eval_steps_per_second': 4.908, 'epoch': 1.91}
********************save call back********************
********************on step end call back********************
Step 10510 finish
{'loss': 0.4295, 'grad_norm': 1.173097848892212, 'learning_rate': 8.148351648351648e-05, 'epoch': 1.91}
********************on step end call back********************
Step 10520 finish
{'loss': 0.4375, 'grad_norm': 1.224421501159668, 'learning_rate': 8.146520146520146e-05, 'epoch': 1.91}
********************on step end call back********************
Step 10530 finish
{'loss': 0.4893, 'grad_norm': 1.0457029342651367, 'learning_rate': 8.144688644688645e-05, 'epoch': 1.91}
********************on step end call back********************
Step 10540 finish
{'loss': 0.4191, 'grad_norm': 1.224456548690796, 'learning_rate': 8.142857142857143e-05, 'epoch': 1.92}
********************on step end call back********************
Step 10550 finish
{'loss': 0.4256, 'grad_norm': 1.042089581489563, 'learning_rate': 8.141025641025641e-05, 'epoch': 1.92}
********************on step end call back********************
Step 10560 finish
{'loss': 0.499, 'grad_norm': 0.9885138869285583, 'learning_rate': 8.139194139194139e-05, 'epoch': 1.92}
********************on step end call back********************
Step 10570 finish
{'loss': 0.4277, 'grad_norm': 1.2037460803985596, 'learning_rate': 8.137362637362638e-05, 'epoch': 1.92}
********************on step end call back********************
Step 10580 finish
{'loss': 0.4645, 'grad_norm': 0.9417330622673035, 'learning_rate': 8.135531135531136e-05, 'epoch': 1.92}
********************on step end call back********************
Step 10590 finish
{'loss': 0.468, 'grad_norm': 1.2397644519805908, 'learning_rate': 8.133699633699634e-05, 'epoch': 1.93}
********************on step end call back********************
Step 10600 finish
{'loss': 0.4318, 'grad_norm': 1.2033354043960571, 'learning_rate': 8.131868131868132e-05, 'epoch': 1.93}
{'eval_loss': 0.3202672600746155, 'eval_accuracy': 0.8541666666666666, 'eval_runtime': 130.1028, 'eval_samples_per_second': 4.904, 'eval_steps_per_second': 4.904, 'epoch': 1.93}
********************save call back********************
********************on step end call back********************
Step 10610 finish
{'loss': 0.4411, 'grad_norm': 1.140127420425415, 'learning_rate': 8.13003663003663e-05, 'epoch': 1.93}
********************on step end call back********************
Step 10620 finish
{'loss': 0.4181, 'grad_norm': 1.221612811088562, 'learning_rate': 8.128205128205129e-05, 'epoch': 1.93}
********************on step end call back********************
Step 10630 finish
{'loss': 0.4483, 'grad_norm': 0.9555543065071106, 'learning_rate': 8.126373626373627e-05, 'epoch': 1.93}
********************on step end call back********************
Step 10640 finish
{'loss': 0.4496, 'grad_norm': 1.2253925800323486, 'learning_rate': 8.124542124542125e-05, 'epoch': 1.93}
********************on step end call back********************
Step 10650 finish
{'loss': 0.4548, 'grad_norm': 0.9514753222465515, 'learning_rate': 8.122710622710623e-05, 'epoch': 1.94}
********************on step end call back********************
Step 10660 finish
{'loss': 0.446, 'grad_norm': 0.7764595150947571, 'learning_rate': 8.120879120879122e-05, 'epoch': 1.94}
********************on step end call back********************
Step 10670 finish
{'loss': 0.3962, 'grad_norm': 0.9774523377418518, 'learning_rate': 8.11904761904762e-05, 'epoch': 1.94}
********************on step end call back********************
Step 10680 finish
{'loss': 0.4535, 'grad_norm': 1.0714619159698486, 'learning_rate': 8.117216117216118e-05, 'epoch': 1.94}
********************on step end call back********************
Step 10690 finish
{'loss': 0.3607, 'grad_norm': 1.0837796926498413, 'learning_rate': 8.115384615384616e-05, 'epoch': 1.94}
********************on step end call back********************
Step 10700 finish
{'loss': 0.459, 'grad_norm': 0.9937066435813904, 'learning_rate': 8.113553113553113e-05, 'epoch': 1.95}
{'eval_loss': 0.31356722116470337, 'eval_accuracy': 0.8854166666666666, 'eval_runtime': 130.06, 'eval_samples_per_second': 4.905, 'eval_steps_per_second': 4.905, 'epoch': 1.95}
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-22 16:12:07,653 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-22 16:12:07,654 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-22 16:12:07,654 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-22 16:14:17,801 >> Saving model checkpoint to ./output/tmp-checkpoint-10800
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-22 16:14:18,108 >> tokenizer config file saved in ./output/tmp-checkpoint-10800/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-22 16:14:18,108 >> Special tokens file saved in ./output/tmp-checkpoint-10800/special_tokens_map.json
[INFO|trainer.py:3159] 2024-03-22 16:14:18,316 >> Deleting older checkpoint [output/checkpoint-800] due to args.save_total_limit
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-22 16:23:00,083 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-22 16:23:00,083 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-22 16:23:00,083 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-22 16:25:09,747 >> Saving model checkpoint to ./output/tmp-checkpoint-10900
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-22 16:25:09,890 >> tokenizer config file saved in ./output/tmp-checkpoint-10900/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-22 16:25:09,890 >> Special tokens file saved in ./output/tmp-checkpoint-10900/special_tokens_map.json
[INFO|trainer.py:3159] 2024-03-22 16:25:10,084 >> Deleting older checkpoint [output/checkpoint-900] due to args.save_total_limit
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-22 16:33:44,665 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-22 16:33:44,666 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-22 16:33:44,666 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-22 16:35:54,602 >> Saving model checkpoint to ./output/tmp-checkpoint-11000
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-22 16:35:54,746 >> tokenizer config file saved in ./output/tmp-checkpoint-11000/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-22 16:35:54,746 >> Special tokens file saved in ./output/tmp-checkpoint-11000/special_tokens_map.json
[INFO|trainer.py:3159] 2024-03-22 16:35:54,937 >> Deleting older checkpoint [output/checkpoint-1000] due to args.save_total_limit
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
********************save call back********************
********************on step end call back********************
Step 10710 finish
{'loss': 0.4109, 'grad_norm': 1.3558954000473022, 'learning_rate': 8.111721611721613e-05, 'epoch': 1.95}
********************on step end call back********************
Step 10720 finish
{'loss': 0.4278, 'grad_norm': 1.2815107107162476, 'learning_rate': 8.10989010989011e-05, 'epoch': 1.95}
********************on step end call back********************
Step 10730 finish
{'loss': 0.4294, 'grad_norm': 0.9773537516593933, 'learning_rate': 8.108058608058608e-05, 'epoch': 1.95}
********************on step end call back********************
Step 10740 finish
{'loss': 0.4174, 'grad_norm': 1.1070425510406494, 'learning_rate': 8.106227106227106e-05, 'epoch': 1.95}
********************on step end call back********************
Step 10750 finish
{'loss': 0.4131, 'grad_norm': 1.319899320602417, 'learning_rate': 8.104395604395605e-05, 'epoch': 1.95}
********************on step end call back********************
Step 10760 finish
{'loss': 0.4536, 'grad_norm': 1.1842267513275146, 'learning_rate': 8.102564102564103e-05, 'epoch': 1.96}
********************on step end call back********************
Step 10770 finish
{'loss': 0.4639, 'grad_norm': 0.882484495639801, 'learning_rate': 8.100732600732601e-05, 'epoch': 1.96}
********************on step end call back********************
Step 10780 finish
{'loss': 0.4509, 'grad_norm': 1.0285539627075195, 'learning_rate': 8.098901098901099e-05, 'epoch': 1.96}
********************on step end call back********************
Step 10790 finish
{'loss': 0.4277, 'grad_norm': 1.014170527458191, 'learning_rate': 8.097069597069597e-05, 'epoch': 1.96}
********************on step end call back********************
Step 10800 finish
{'loss': 0.4646, 'grad_norm': 1.0974677801132202, 'learning_rate': 8.095238095238096e-05, 'epoch': 1.96}
{'eval_loss': 0.32339921593666077, 'eval_accuracy': 0.8854166666666666, 'eval_runtime': 130.1466, 'eval_samples_per_second': 4.902, 'eval_steps_per_second': 4.902, 'epoch': 1.96}
********************save call back********************
********************on step end call back********************
Step 10810 finish
{'loss': 0.3868, 'grad_norm': 1.1800665855407715, 'learning_rate': 8.093406593406594e-05, 'epoch': 1.97}
********************on step end call back********************
Step 10820 finish
{'loss': 0.4167, 'grad_norm': 0.8880515098571777, 'learning_rate': 8.091575091575092e-05, 'epoch': 1.97}
********************on step end call back********************
Step 10830 finish
{'loss': 0.4576, 'grad_norm': 1.035147786140442, 'learning_rate': 8.08974358974359e-05, 'epoch': 1.97}
********************on step end call back********************
Step 10840 finish
{'loss': 0.3603, 'grad_norm': 0.908829391002655, 'learning_rate': 8.087912087912089e-05, 'epoch': 1.97}
********************on step end call back********************
Step 10850 finish
{'loss': 0.4724, 'grad_norm': 1.0872323513031006, 'learning_rate': 8.086080586080587e-05, 'epoch': 1.97}
********************on step end call back********************
Step 10860 finish
{'loss': 0.4801, 'grad_norm': 1.0457998514175415, 'learning_rate': 8.084249084249085e-05, 'epoch': 1.97}
********************on step end call back********************
Step 10870 finish
{'loss': 0.4216, 'grad_norm': 1.2462716102600098, 'learning_rate': 8.082417582417583e-05, 'epoch': 1.98}
********************on step end call back********************
Step 10880 finish
{'loss': 0.4529, 'grad_norm': 0.9826880693435669, 'learning_rate': 8.08058608058608e-05, 'epoch': 1.98}
********************on step end call back********************
Step 10890 finish
{'loss': 0.4553, 'grad_norm': 1.1187294721603394, 'learning_rate': 8.07875457875458e-05, 'epoch': 1.98}
********************on step end call back********************
Step 10900 finish
{'loss': 0.4361, 'grad_norm': 1.1159063577651978, 'learning_rate': 8.076923076923078e-05, 'epoch': 1.98}
{'eval_loss': 0.32019731402397156, 'eval_accuracy': 0.875, 'eval_runtime': 129.6626, 'eval_samples_per_second': 4.92, 'eval_steps_per_second': 4.92, 'epoch': 1.98}
********************save call back********************
********************on step end call back********************
Step 10910 finish
{'loss': 0.3845, 'grad_norm': 0.9544954299926758, 'learning_rate': 8.075091575091576e-05, 'epoch': 1.98}
********************on step end call back********************
Step 10920 finish
{'loss': 0.4121, 'grad_norm': 1.1225215196609497, 'learning_rate': 8.073260073260073e-05, 'epoch': 1.99}
********************on step end call back********************
Step 10930 finish
{'loss': 0.4269, 'grad_norm': 0.9459001421928406, 'learning_rate': 8.071428571428573e-05, 'epoch': 1.99}
********************on step end call back********************
Step 10940 finish
{'loss': 0.3947, 'grad_norm': 1.225084662437439, 'learning_rate': 8.06959706959707e-05, 'epoch': 1.99}
********************on step end call back********************
Step 10950 finish
{'loss': 0.4591, 'grad_norm': 1.3686281442642212, 'learning_rate': 8.067765567765568e-05, 'epoch': 1.99}
********************on step end call back********************
Step 10960 finish
{'loss': 0.43, 'grad_norm': 0.784182071685791, 'learning_rate': 8.065934065934066e-05, 'epoch': 1.99}
********************on step end call back********************
Step 10970 finish
{'loss': 0.4018, 'grad_norm': 1.072095274925232, 'learning_rate': 8.064102564102564e-05, 'epoch': 1.99}
********************on step end call back********************
Step 10980 finish
{'loss': 0.4278, 'grad_norm': 1.0747750997543335, 'learning_rate': 8.062271062271063e-05, 'epoch': 2.0}
********************on step end call back********************
Step 10990 finish
{'loss': 0.4237, 'grad_norm': 1.3584864139556885, 'learning_rate': 8.060439560439561e-05, 'epoch': 2.0}
********************on step end call back********************
Step 11000 finish
{'loss': 0.4533, 'grad_norm': 1.120664358139038, 'learning_rate': 8.058608058608059e-05, 'epoch': 2.0}
{'eval_loss': 0.32407400012016296, 'eval_accuracy': 0.8645833333333334, 'eval_runtime': 129.9358, 'eval_samples_per_second': 4.91, 'eval_steps_per_second': 4.91, 'epoch': 2.0}
********************save call back********************
********************on epoch end call back********************
Epoch 1.9999545506192478 finish
********************on step end call back********************
Step 11010 finish
{'loss': 0.3443, 'grad_norm': 1.042870283126831, 'learning_rate': 8.056776556776557e-05, 'epoch': 2.0}
********************on step end call back********************
Step 11020 finish
{'loss': 0.3631, 'grad_norm': 1.0294103622436523, 'learning_rate': 8.054945054945056e-05, 'epoch': 2.0}
********************on step end call back********************
Step 11030 finish
{'loss': 0.3982, 'grad_norm': 1.089640736579895, 'learning_rate': 8.053113553113554e-05, 'epoch': 2.01}
********************on step end call back********************
Step 11040 finish
{'loss': 0.3641, 'grad_norm': 0.9128574132919312, 'learning_rate': 8.051282051282052e-05, 'epoch': 2.01}
********************on step end call back********************
Step 11050 finish
{'loss': 0.3751, 'grad_norm': 1.452924132347107, 'learning_rate': 8.04945054945055e-05, 'epoch': 2.01}
********************on step end call back********************
Step 11060 finish
{'loss': 0.3734, 'grad_norm': 0.8544328212738037, 'learning_rate': 8.047619047619048e-05, 'epoch': 2.01}
********************on step end call back********************
Step 11070 finish
{'loss': 0.3869, 'grad_norm': 1.2517176866531372, 'learning_rate': 8.045787545787547e-05, 'epoch': 2.01}
********************on step end call back********************
Step 11080 finish
{'loss': 0.3881, 'grad_norm': 1.2862653732299805, 'learning_rate': 8.043956043956045e-05, 'epoch': 2.01}
********************on step end call back********************
Step 11090 finish
{'loss': 0.3837, 'grad_norm': 1.1974339485168457, 'learning_rate': 8.042124542124543e-05, 'epoch': 2.02}
********************on step end call back********************
Step 11100 finish
[INFO|trainer.py:3376] 2024-03-22 16:44:34,253 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-22 16:44:34,253 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-22 16:44:34,253 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-22 16:46:43,864 >> Saving model checkpoint to ./output/tmp-checkpoint-11100
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-22 16:46:44,004 >> tokenizer config file saved in ./output/tmp-checkpoint-11100/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-22 16:46:44,004 >> Special tokens file saved in ./output/tmp-checkpoint-11100/special_tokens_map.json
[INFO|trainer.py:3159] 2024-03-22 16:46:44,192 >> Deleting older checkpoint [output/checkpoint-1100] due to args.save_total_limit
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-22 16:55:18,049 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-22 16:55:18,050 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-22 16:55:18,050 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-22 16:57:27,577 >> Saving model checkpoint to ./output/tmp-checkpoint-11200
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-22 16:57:27,719 >> tokenizer config file saved in ./output/tmp-checkpoint-11200/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-22 16:57:27,719 >> Special tokens file saved in ./output/tmp-checkpoint-11200/special_tokens_map.json
[INFO|trainer.py:3159] 2024-03-22 16:57:27,906 >> Deleting older checkpoint [output/checkpoint-1200] due to args.save_total_limit
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-22 17:06:08,952 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-22 17:06:08,952 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-22 17:06:08,952 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-22 17:08:18,545 >> Saving model checkpoint to ./output/tmp-checkpoint-11300
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-22 17:08:18,686 >> tokenizer config file saved in ./output/tmp-checkpoint-11300/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-22 17:08:18,687 >> Special tokens file saved in ./output/tmp-checkpoint-11300/special_tokens_map.json
[INFO|trainer.py:3159] 2024-03-22 17:08:18,881 >> Deleting older checkpoint [output/checkpoint-1300] due to args.save_total_limit
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-22 17:16:58,603 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-22 17:16:58,603 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-22 17:16:58,603 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-22 17:19:08,376 >> Saving model checkpoint to ./output/tmp-checkpoint-11400
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-22 17:19:08,517 >> tokenizer config file saved in ./output/tmp-checkpoint-11400/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-22 17:19:08,517 >> Special tokens file saved in ./output/tmp-checkpoint-11400/special_tokens_map.json
[INFO|trainer.py:3159] 2024-03-22 17:19:08,714 >> Deleting older checkpoint [output/checkpoint-1400] due to args.save_total_limit
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
{'loss': 0.3603, 'grad_norm': 1.021012783050537, 'learning_rate': 8.04029304029304e-05, 'epoch': 2.02}
{'eval_loss': 0.3231002688407898, 'eval_accuracy': 0.875, 'eval_runtime': 129.6103, 'eval_samples_per_second': 4.922, 'eval_steps_per_second': 4.922, 'epoch': 2.02}
********************save call back********************
********************on step end call back********************
Step 11110 finish
{'loss': 0.3321, 'grad_norm': 1.0881093740463257, 'learning_rate': 8.038461538461538e-05, 'epoch': 2.02}
********************on step end call back********************
Step 11120 finish
{'loss': 0.3682, 'grad_norm': 1.3749154806137085, 'learning_rate': 8.036630036630038e-05, 'epoch': 2.02}
********************on step end call back********************
Step 11130 finish
{'loss': 0.3574, 'grad_norm': 1.1173181533813477, 'learning_rate': 8.034798534798536e-05, 'epoch': 2.02}
********************on step end call back********************
Step 11140 finish
{'loss': 0.3176, 'grad_norm': 1.0954136848449707, 'learning_rate': 8.032967032967033e-05, 'epoch': 2.03}
********************on step end call back********************
Step 11150 finish
{'loss': 0.3852, 'grad_norm': 0.9360926151275635, 'learning_rate': 8.031135531135531e-05, 'epoch': 2.03}
********************on step end call back********************
Step 11160 finish
{'loss': 0.3471, 'grad_norm': 1.1920554637908936, 'learning_rate': 8.02930402930403e-05, 'epoch': 2.03}
********************on step end call back********************
Step 11170 finish
{'loss': 0.3732, 'grad_norm': 0.995753824710846, 'learning_rate': 8.027472527472528e-05, 'epoch': 2.03}
********************on step end call back********************
Step 11180 finish
{'loss': 0.3522, 'grad_norm': 1.233944296836853, 'learning_rate': 8.025641025641026e-05, 'epoch': 2.03}
********************on step end call back********************
Step 11190 finish
{'loss': 0.384, 'grad_norm': 1.0323845148086548, 'learning_rate': 8.023809523809524e-05, 'epoch': 2.03}
********************on step end call back********************
Step 11200 finish
{'loss': 0.3912, 'grad_norm': 1.096535325050354, 'learning_rate': 8.021978021978022e-05, 'epoch': 2.04}
{'eval_loss': 0.31584277749061584, 'eval_accuracy': 0.8645833333333334, 'eval_runtime': 129.5268, 'eval_samples_per_second': 4.926, 'eval_steps_per_second': 4.926, 'epoch': 2.04}
********************save call back********************
********************on step end call back********************
Step 11210 finish
{'loss': 0.3865, 'grad_norm': 1.1219555139541626, 'learning_rate': 8.020146520146521e-05, 'epoch': 2.04}
********************on step end call back********************
Step 11220 finish
{'loss': 0.3102, 'grad_norm': 1.0729765892028809, 'learning_rate': 8.018315018315019e-05, 'epoch': 2.04}
********************on step end call back********************
Step 11230 finish
{'loss': 0.3536, 'grad_norm': 1.207042932510376, 'learning_rate': 8.016483516483517e-05, 'epoch': 2.04}
********************on step end call back********************
Step 11240 finish
{'loss': 0.4028, 'grad_norm': 1.1581918001174927, 'learning_rate': 8.014652014652015e-05, 'epoch': 2.04}
********************on step end call back********************
Step 11250 finish
{'loss': 0.4409, 'grad_norm': 1.0490976572036743, 'learning_rate': 8.012820512820514e-05, 'epoch': 2.05}
********************on step end call back********************
Step 11260 finish
{'loss': 0.4317, 'grad_norm': 1.1477081775665283, 'learning_rate': 8.010989010989012e-05, 'epoch': 2.05}
********************on step end call back********************
Step 11270 finish
{'loss': 0.3975, 'grad_norm': 0.8670092821121216, 'learning_rate': 8.00915750915751e-05, 'epoch': 2.05}
********************on step end call back********************
Step 11280 finish
{'loss': 0.3577, 'grad_norm': 1.145194172859192, 'learning_rate': 8.007326007326008e-05, 'epoch': 2.05}
********************on step end call back********************
Step 11290 finish
{'loss': 0.3714, 'grad_norm': 1.275996446609497, 'learning_rate': 8.005494505494506e-05, 'epoch': 2.05}
********************on step end call back********************
Step 11300 finish
{'loss': 0.355, 'grad_norm': 1.072630524635315, 'learning_rate': 8.003663003663005e-05, 'epoch': 2.05}
{'eval_loss': 0.3215343654155731, 'eval_accuracy': 0.8645833333333334, 'eval_runtime': 129.5919, 'eval_samples_per_second': 4.923, 'eval_steps_per_second': 4.923, 'epoch': 2.05}
********************save call back********************
********************on step end call back********************
Step 11310 finish
{'loss': 0.3903, 'grad_norm': 1.0410044193267822, 'learning_rate': 8.001831501831503e-05, 'epoch': 2.06}
********************on step end call back********************
Step 11320 finish
{'loss': 0.3679, 'grad_norm': 1.1214829683303833, 'learning_rate': 8e-05, 'epoch': 2.06}
********************on step end call back********************
Step 11330 finish
{'loss': 0.345, 'grad_norm': 0.938948392868042, 'learning_rate': 7.998168498168499e-05, 'epoch': 2.06}
********************on step end call back********************
Step 11340 finish
{'loss': 0.3762, 'grad_norm': 0.9283105134963989, 'learning_rate': 7.996336996336998e-05, 'epoch': 2.06}
********************on step end call back********************
Step 11350 finish
{'loss': 0.3498, 'grad_norm': 0.9378148317337036, 'learning_rate': 7.994505494505496e-05, 'epoch': 2.06}
********************on step end call back********************
Step 11360 finish
{'loss': 0.3914, 'grad_norm': 0.8952274322509766, 'learning_rate': 7.992673992673994e-05, 'epoch': 2.07}
********************on step end call back********************
Step 11370 finish
{'loss': 0.3759, 'grad_norm': 1.2987806797027588, 'learning_rate': 7.990842490842491e-05, 'epoch': 2.07}
********************on step end call back********************
Step 11380 finish
{'loss': 0.3334, 'grad_norm': 1.3123337030410767, 'learning_rate': 7.989010989010989e-05, 'epoch': 2.07}
********************on step end call back********************
Step 11390 finish
{'loss': 0.4062, 'grad_norm': 1.069228172302246, 'learning_rate': 7.987179487179488e-05, 'epoch': 2.07}
********************on step end call back********************
Step 11400 finish
{'loss': 0.4106, 'grad_norm': 1.5003368854522705, 'learning_rate': 7.985347985347986e-05, 'epoch': 2.07}
{'eval_loss': 0.3283413052558899, 'eval_accuracy': 0.875, 'eval_runtime': 129.772, 'eval_samples_per_second': 4.916, 'eval_steps_per_second': 4.916, 'epoch': 2.07}
********************save call back********************
********************on step end call back********************
Step 11410 finish
{'loss': 0.3756, 'grad_norm': 1.1297258138656616, 'learning_rate': 7.983516483516484e-05, 'epoch': 2.07}
********************on step end call back********************
Step 11420 finish
{'loss': 0.3807, 'grad_norm': 1.0681829452514648, 'learning_rate': 7.981684981684982e-05, 'epoch': 2.08}
********************on step end call back********************
Step 11430 finish
{'loss': 0.3572, 'grad_norm': 0.9102795124053955, 'learning_rate': 7.979853479853481e-05, 'epoch': 2.08}
********************on step end call back********************
Step 11440 finish
{'loss': 0.3472, 'grad_norm': 1.439542531967163, 'learning_rate': 7.978021978021979e-05, 'epoch': 2.08}
********************on step end call back********************
Step 11450 finish
{'loss': 0.3892, 'grad_norm': 1.0612105131149292, 'learning_rate': 7.976190476190477e-05, 'epoch': 2.08}
********************on step end call back********************
Step 11460 finish
{'loss': 0.376, 'grad_norm': 1.0100665092468262, 'learning_rate': 7.974358974358975e-05, 'epoch': 2.08}
********************on step end call back********************
Step 11470 finish
{'loss': 0.339, 'grad_norm': 0.9055776596069336, 'learning_rate': 7.972527472527473e-05, 'epoch': 2.09}
********************on step end call back********************
Step 11480 finish
{'loss': 0.3958, 'grad_norm': 1.345856785774231, 'learning_rate': 7.970695970695972e-05, 'epoch': 2.09}
********************on step end call back********************
Step 11490 finish
{'loss': 0.3346, 'grad_norm': 0.9883369207382202, 'learning_rate': 7.96886446886447e-05, 'epoch': 2.09}
[INFO|trainer.py:3376] 2024-03-22 17:27:45,133 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-22 17:27:45,133 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-22 17:27:45,133 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-22 17:29:55,190 >> Saving model checkpoint to ./output/tmp-checkpoint-11500
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-22 17:29:55,346 >> tokenizer config file saved in ./output/tmp-checkpoint-11500/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-22 17:29:55,346 >> Special tokens file saved in ./output/tmp-checkpoint-11500/special_tokens_map.json
[INFO|trainer.py:3159] 2024-03-22 17:29:55,564 >> Deleting older checkpoint [output/checkpoint-1500] due to args.save_total_limit
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-22 17:38:30,592 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-22 17:38:30,593 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-22 17:38:30,593 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-22 17:40:40,710 >> Saving model checkpoint to ./output/tmp-checkpoint-11600
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-22 17:40:40,913 >> tokenizer config file saved in ./output/tmp-checkpoint-11600/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-22 17:40:40,913 >> Special tokens file saved in ./output/tmp-checkpoint-11600/special_tokens_map.json
[INFO|trainer.py:3159] 2024-03-22 17:40:41,166 >> Deleting older checkpoint [output/checkpoint-1600] due to args.save_total_limit
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-22 17:49:16,911 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-22 17:49:16,911 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-22 17:49:16,911 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-22 17:51:27,051 >> Saving model checkpoint to ./output/tmp-checkpoint-11700
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-22 17:51:27,199 >> tokenizer config file saved in ./output/tmp-checkpoint-11700/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-22 17:51:27,199 >> Special tokens file saved in ./output/tmp-checkpoint-11700/special_tokens_map.json
[INFO|trainer.py:3159] 2024-03-22 17:51:27,388 >> Deleting older checkpoint [output/checkpoint-1700] due to args.save_total_limit
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-22 17:59:58,596 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-22 17:59:58,596 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-22 17:59:58,596 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-22 18:02:08,748 >> Saving model checkpoint to ./output/tmp-checkpoint-11800
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-22 18:02:08,915 >> tokenizer config file saved in ./output/tmp-checkpoint-11800/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-22 18:02:08,915 >> Special tokens file saved in ./output/tmp-checkpoint-11800/special_tokens_map.json
[INFO|trainer.py:3159] 2024-03-22 18:02:09,117 >> Deleting older checkpoint [output/checkpoint-1800] due to args.save_total_limit
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
********************on step end call back********************
Step 11500 finish
{'loss': 0.3938, 'grad_norm': 1.104222059249878, 'learning_rate': 7.967032967032966e-05, 'epoch': 2.09}
{'eval_loss': 0.32223832607269287, 'eval_accuracy': 0.8645833333333334, 'eval_runtime': 130.0553, 'eval_samples_per_second': 4.906, 'eval_steps_per_second': 4.906, 'epoch': 2.09}
********************save call back********************
********************on step end call back********************
Step 11510 finish
{'loss': 0.3691, 'grad_norm': 0.9688548445701599, 'learning_rate': 7.965201465201466e-05, 'epoch': 2.09}
********************on step end call back********************
Step 11520 finish
{'loss': 0.3586, 'grad_norm': 1.3395063877105713, 'learning_rate': 7.963369963369964e-05, 'epoch': 2.09}
********************on step end call back********************
Step 11530 finish
{'loss': 0.3726, 'grad_norm': 1.2409731149673462, 'learning_rate': 7.961538461538461e-05, 'epoch': 2.1}
********************on step end call back********************
Step 11540 finish
{'loss': 0.3253, 'grad_norm': 1.0360991954803467, 'learning_rate': 7.95970695970696e-05, 'epoch': 2.1}
********************on step end call back********************
Step 11550 finish
{'loss': 0.3912, 'grad_norm': 1.1922732591629028, 'learning_rate': 7.957875457875457e-05, 'epoch': 2.1}
********************on step end call back********************
Step 11560 finish
{'loss': 0.3808, 'grad_norm': 0.9936719536781311, 'learning_rate': 7.956043956043956e-05, 'epoch': 2.1}
********************on step end call back********************
Step 11570 finish
{'loss': 0.3481, 'grad_norm': 1.2040716409683228, 'learning_rate': 7.954212454212454e-05, 'epoch': 2.1}
********************on step end call back********************
Step 11580 finish
{'loss': 0.3712, 'grad_norm': 1.2258176803588867, 'learning_rate': 7.952380952380952e-05, 'epoch': 2.11}
********************on step end call back********************
Step 11590 finish
{'loss': 0.3985, 'grad_norm': 1.6219640970230103, 'learning_rate': 7.95054945054945e-05, 'epoch': 2.11}
********************on step end call back********************
Step 11600 finish
{'loss': 0.3688, 'grad_norm': 1.2999006509780884, 'learning_rate': 7.948717948717948e-05, 'epoch': 2.11}
{'eval_loss': 0.324516236782074, 'eval_accuracy': 0.8645833333333334, 'eval_runtime': 130.116, 'eval_samples_per_second': 4.903, 'eval_steps_per_second': 4.903, 'epoch': 2.11}
********************save call back********************
********************on step end call back********************
Step 11610 finish
{'loss': 0.3842, 'grad_norm': 1.17399263381958, 'learning_rate': 7.946886446886447e-05, 'epoch': 2.11}
********************on step end call back********************
Step 11620 finish
{'loss': 0.3402, 'grad_norm': 1.1920233964920044, 'learning_rate': 7.945054945054945e-05, 'epoch': 2.11}
********************on step end call back********************
Step 11630 finish
{'loss': 0.3717, 'grad_norm': 1.3569378852844238, 'learning_rate': 7.943223443223443e-05, 'epoch': 2.11}
********************on step end call back********************
Step 11640 finish
{'loss': 0.342, 'grad_norm': 0.9817288517951965, 'learning_rate': 7.941391941391941e-05, 'epoch': 2.12}
********************on step end call back********************
Step 11650 finish
{'loss': 0.3976, 'grad_norm': 1.110744833946228, 'learning_rate': 7.93956043956044e-05, 'epoch': 2.12}
********************on step end call back********************
Step 11660 finish
{'loss': 0.4022, 'grad_norm': 1.1454418897628784, 'learning_rate': 7.937728937728938e-05, 'epoch': 2.12}
********************on step end call back********************
Step 11670 finish
{'loss': 0.3697, 'grad_norm': 0.8370205163955688, 'learning_rate': 7.935897435897436e-05, 'epoch': 2.12}
********************on step end call back********************
Step 11680 finish
{'loss': 0.3827, 'grad_norm': 1.495547890663147, 'learning_rate': 7.934065934065934e-05, 'epoch': 2.12}
********************on step end call back********************
Step 11690 finish
{'loss': 0.3451, 'grad_norm': 0.8401273488998413, 'learning_rate': 7.932234432234432e-05, 'epoch': 2.13}
********************on step end call back********************
Step 11700 finish
{'loss': 0.3294, 'grad_norm': 1.0738613605499268, 'learning_rate': 7.930402930402931e-05, 'epoch': 2.13}
{'eval_loss': 0.3288171589374542, 'eval_accuracy': 0.8645833333333334, 'eval_runtime': 130.1394, 'eval_samples_per_second': 4.902, 'eval_steps_per_second': 4.902, 'epoch': 2.13}
********************save call back********************
********************on step end call back********************
Step 11710 finish
{'loss': 0.349, 'grad_norm': 1.2330795526504517, 'learning_rate': 7.928571428571429e-05, 'epoch': 2.13}
********************on step end call back********************
Step 11720 finish
{'loss': 0.3491, 'grad_norm': 1.0429985523223877, 'learning_rate': 7.926739926739927e-05, 'epoch': 2.13}
********************on step end call back********************
Step 11730 finish
{'loss': 0.3281, 'grad_norm': 1.064768671989441, 'learning_rate': 7.924908424908424e-05, 'epoch': 2.13}
********************on step end call back********************
Step 11740 finish
{'loss': 0.3652, 'grad_norm': 1.0152091979980469, 'learning_rate': 7.923076923076924e-05, 'epoch': 2.13}
********************on step end call back********************
Step 11750 finish
{'loss': 0.3807, 'grad_norm': 0.8396379947662354, 'learning_rate': 7.921245421245422e-05, 'epoch': 2.14}
********************on step end call back********************
Step 11760 finish
{'loss': 0.343, 'grad_norm': 1.0915557146072388, 'learning_rate': 7.91941391941392e-05, 'epoch': 2.14}
********************on step end call back********************
Step 11770 finish
{'loss': 0.4362, 'grad_norm': 1.5684984922409058, 'learning_rate': 7.917582417582417e-05, 'epoch': 2.14}
********************on step end call back********************
Step 11780 finish
{'loss': 0.4013, 'grad_norm': 1.0228183269500732, 'learning_rate': 7.915750915750915e-05, 'epoch': 2.14}
********************on step end call back********************
Step 11790 finish
{'loss': 0.3869, 'grad_norm': 1.0615428686141968, 'learning_rate': 7.913919413919414e-05, 'epoch': 2.14}
********************on step end call back********************
Step 11800 finish
{'loss': 0.3672, 'grad_norm': 1.6119861602783203, 'learning_rate': 7.912087912087912e-05, 'epoch': 2.15}
{'eval_loss': 0.3195061683654785, 'eval_accuracy': 0.8645833333333334, 'eval_runtime': 130.1516, 'eval_samples_per_second': 4.902, 'eval_steps_per_second': 4.902, 'epoch': 2.15}
********************save call back********************
********************on step end call back********************
Step 11810 finish
{'loss': 0.3793, 'grad_norm': 0.7537113428115845, 'learning_rate': 7.91025641025641e-05, 'epoch': 2.15}
********************on step end call back********************
Step 11820 finish
{'loss': 0.394, 'grad_norm': 1.0828382968902588, 'learning_rate': 7.908424908424908e-05, 'epoch': 2.15}
********************on step end call back********************
Step 11830 finish
{'loss': 0.4288, 'grad_norm': 1.03457772731781, 'learning_rate': 7.906593406593407e-05, 'epoch': 2.15}
********************on step end call back********************
Step 11840 finish
{'loss': 0.3725, 'grad_norm': 1.3775912523269653, 'learning_rate': 7.904761904761905e-05, 'epoch': 2.15}
********************on step end call back********************
Step 11850 finish
{'loss': 0.4041, 'grad_norm': 1.4597680568695068, 'learning_rate': 7.902930402930403e-05, 'epoch': 2.15}
********************on step end call back********************
Step 11860 finish
{'loss': 0.3816, 'grad_norm': 1.0771318674087524, 'learning_rate': 7.901098901098901e-05, 'epoch': 2.16}
********************on step end call back********************
Step 11870 finish
{'loss': 0.3943, 'grad_norm': 1.219201683998108, 'learning_rate': 7.899267399267399e-05, 'epoch': 2.16}
********************on step end call back********************
Step 11880 finish
{'loss': 0.3858, 'grad_norm': 1.227066159248352, 'learning_rate': 7.897435897435898e-05, 'epoch': 2.16}
********************on step end call back********************
[INFO|trainer.py:3376] 2024-03-22 18:10:44,279 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-22 18:10:44,279 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-22 18:10:44,279 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-22 18:12:54,502 >> Saving model checkpoint to ./output/tmp-checkpoint-11900
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-22 18:12:54,667 >> tokenizer config file saved in ./output/tmp-checkpoint-11900/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-22 18:12:54,667 >> Special tokens file saved in ./output/tmp-checkpoint-11900/special_tokens_map.json
[INFO|trainer.py:3159] 2024-03-22 18:12:54,867 >> Deleting older checkpoint [output/checkpoint-1900] due to args.save_total_limit
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-22 18:21:36,458 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-22 18:21:36,458 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-22 18:21:36,458 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-22 18:23:46,583 >> Saving model checkpoint to ./output/tmp-checkpoint-12000
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-22 18:23:46,849 >> tokenizer config file saved in ./output/tmp-checkpoint-12000/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-22 18:23:46,849 >> Special tokens file saved in ./output/tmp-checkpoint-12000/special_tokens_map.json
[INFO|trainer.py:3159] 2024-03-22 18:23:47,052 >> Deleting older checkpoint [output/checkpoint-2000] due to args.save_total_limit
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-22 18:32:27,444 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-22 18:32:27,445 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-22 18:32:27,445 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-22 18:34:37,578 >> Saving model checkpoint to ./output/tmp-checkpoint-12100
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-22 18:34:37,784 >> tokenizer config file saved in ./output/tmp-checkpoint-12100/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-22 18:34:37,784 >> Special tokens file saved in ./output/tmp-checkpoint-12100/special_tokens_map.json
[INFO|trainer.py:3159] 2024-03-22 18:34:37,996 >> Deleting older checkpoint [output/checkpoint-2100] due to args.save_total_limit
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-22 18:43:18,865 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-22 18:43:18,866 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-22 18:43:18,866 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-22 18:45:28,937 >> Saving model checkpoint to ./output/tmp-checkpoint-12200
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-22 18:45:29,083 >> tokenizer config file saved in ./output/tmp-checkpoint-12200/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-22 18:45:29,083 >> Special tokens file saved in ./output/tmp-checkpoint-12200/special_tokens_map.json
[INFO|trainer.py:3159] 2024-03-22 18:45:29,284 >> Deleting older checkpoint [output/checkpoint-2200] due to args.save_total_limit
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
Step 11890 finish
{'loss': 0.3679, 'grad_norm': 1.343364953994751, 'learning_rate': 7.895604395604396e-05, 'epoch': 2.16}
********************on step end call back********************
Step 11900 finish
{'loss': 0.3766, 'grad_norm': 0.9935319423675537, 'learning_rate': 7.893772893772894e-05, 'epoch': 2.16}
{'eval_loss': 0.32423704862594604, 'eval_accuracy': 0.8854166666666666, 'eval_runtime': 130.2225, 'eval_samples_per_second': 4.899, 'eval_steps_per_second': 4.899, 'epoch': 2.16}
********************save call back********************
********************on step end call back********************
Step 11910 finish
{'loss': 0.4016, 'grad_norm': 1.196931004524231, 'learning_rate': 7.891941391941392e-05, 'epoch': 2.17}
********************on step end call back********************
Step 11920 finish
{'loss': 0.3856, 'grad_norm': 1.271091341972351, 'learning_rate': 7.890109890109891e-05, 'epoch': 2.17}
********************on step end call back********************
Step 11930 finish
{'loss': 0.3661, 'grad_norm': 1.080342411994934, 'learning_rate': 7.888278388278389e-05, 'epoch': 2.17}
********************on step end call back********************
Step 11940 finish
{'loss': 0.3541, 'grad_norm': 1.1350815296173096, 'learning_rate': 7.886446886446887e-05, 'epoch': 2.17}
********************on step end call back********************
Step 11950 finish
{'loss': 0.4227, 'grad_norm': 1.2908202409744263, 'learning_rate': 7.884615384615384e-05, 'epoch': 2.17}
********************on step end call back********************
Step 11960 finish
{'loss': 0.3289, 'grad_norm': 1.1189370155334473, 'learning_rate': 7.882783882783882e-05, 'epoch': 2.17}
********************on step end call back********************
Step 11970 finish
{'loss': 0.3834, 'grad_norm': 1.196317195892334, 'learning_rate': 7.880952380952382e-05, 'epoch': 2.18}
********************on step end call back********************
Step 11980 finish
{'loss': 0.4191, 'grad_norm': 1.0658811330795288, 'learning_rate': 7.87912087912088e-05, 'epoch': 2.18}
********************on step end call back********************
Step 11990 finish
{'loss': 0.3827, 'grad_norm': 0.9922154545783997, 'learning_rate': 7.877289377289377e-05, 'epoch': 2.18}
********************on step end call back********************
Step 12000 finish
{'loss': 0.3287, 'grad_norm': 1.0320309400558472, 'learning_rate': 7.875457875457875e-05, 'epoch': 2.18}
{'eval_loss': 0.3277451694011688, 'eval_accuracy': 0.8958333333333334, 'eval_runtime': 130.1245, 'eval_samples_per_second': 4.903, 'eval_steps_per_second': 4.903, 'epoch': 2.18}
********************save call back********************
********************on step end call back********************
Step 12010 finish
{'loss': 0.3915, 'grad_norm': 1.0179108381271362, 'learning_rate': 7.873626373626374e-05, 'epoch': 2.18}
********************on step end call back********************
Step 12020 finish
{'loss': 0.3636, 'grad_norm': 1.1963298320770264, 'learning_rate': 7.871794871794872e-05, 'epoch': 2.19}
********************on step end call back********************
Step 12030 finish
{'loss': 0.381, 'grad_norm': 1.1583837270736694, 'learning_rate': 7.86996336996337e-05, 'epoch': 2.19}
********************on step end call back********************
Step 12040 finish
{'loss': 0.3722, 'grad_norm': 1.0410268306732178, 'learning_rate': 7.868131868131868e-05, 'epoch': 2.19}
********************on step end call back********************
Step 12050 finish
{'loss': 0.3828, 'grad_norm': 1.2193121910095215, 'learning_rate': 7.866300366300366e-05, 'epoch': 2.19}
********************on step end call back********************
Step 12060 finish
{'loss': 0.417, 'grad_norm': 1.1101315021514893, 'learning_rate': 7.864468864468865e-05, 'epoch': 2.19}
********************on step end call back********************
Step 12070 finish
{'loss': 0.3361, 'grad_norm': 1.1690925359725952, 'learning_rate': 7.862637362637363e-05, 'epoch': 2.19}
********************on step end call back********************
Step 12080 finish
{'loss': 0.4063, 'grad_norm': 1.183940052986145, 'learning_rate': 7.860805860805861e-05, 'epoch': 2.2}
********************on step end call back********************
Step 12090 finish
{'loss': 0.3669, 'grad_norm': 1.4180059432983398, 'learning_rate': 7.858974358974359e-05, 'epoch': 2.2}
********************on step end call back********************
Step 12100 finish
{'loss': 0.4014, 'grad_norm': 0.9041258096694946, 'learning_rate': 7.857142857142858e-05, 'epoch': 2.2}
{'eval_loss': 0.32343965768814087, 'eval_accuracy': 0.875, 'eval_runtime': 130.1328, 'eval_samples_per_second': 4.903, 'eval_steps_per_second': 4.903, 'epoch': 2.2}
********************save call back********************
********************on step end call back********************
Step 12110 finish
{'loss': 0.3859, 'grad_norm': 1.0445780754089355, 'learning_rate': 7.855311355311356e-05, 'epoch': 2.2}
********************on step end call back********************
Step 12120 finish
{'loss': 0.4055, 'grad_norm': 1.3036428689956665, 'learning_rate': 7.853479853479854e-05, 'epoch': 2.2}
********************on step end call back********************
Step 12130 finish
{'loss': 0.3751, 'grad_norm': 0.8374007940292358, 'learning_rate': 7.851648351648352e-05, 'epoch': 2.21}
********************on step end call back********************
Step 12140 finish
{'loss': 0.3437, 'grad_norm': 0.8057301044464111, 'learning_rate': 7.84981684981685e-05, 'epoch': 2.21}
********************on step end call back********************
Step 12150 finish
{'loss': 0.3305, 'grad_norm': 1.202134132385254, 'learning_rate': 7.847985347985349e-05, 'epoch': 2.21}
********************on step end call back********************
Step 12160 finish
{'loss': 0.3699, 'grad_norm': 1.1651800870895386, 'learning_rate': 7.846153846153847e-05, 'epoch': 2.21}
********************on step end call back********************
Step 12170 finish
{'loss': 0.3186, 'grad_norm': 0.9079284071922302, 'learning_rate': 7.844322344322344e-05, 'epoch': 2.21}
********************on step end call back********************
Step 12180 finish
{'loss': 0.4054, 'grad_norm': 1.084908366203308, 'learning_rate': 7.842490842490842e-05, 'epoch': 2.21}
********************on step end call back********************
Step 12190 finish
{'loss': 0.4134, 'grad_norm': 0.9819716215133667, 'learning_rate': 7.840659340659342e-05, 'epoch': 2.22}
********************on step end call back********************
Step 12200 finish
{'loss': 0.3939, 'grad_norm': 1.1530972719192505, 'learning_rate': 7.83882783882784e-05, 'epoch': 2.22}
{'eval_loss': 0.3202682137489319, 'eval_accuracy': 0.875, 'eval_runtime': 130.0703, 'eval_samples_per_second': 4.905, 'eval_steps_per_second': 4.905, 'epoch': 2.22}
********************save call back********************
********************on step end call back********************
Step 12210 finish
{'loss': 0.3927, 'grad_norm': 1.0286997556686401, 'learning_rate': 7.836996336996337e-05, 'epoch': 2.22}
********************on step end call back********************
Step 12220 finish
{'loss': 0.4051, 'grad_norm': 0.9536321759223938, 'learning_rate': 7.835164835164835e-05, 'epoch': 2.22}
********************on step end call back********************
Step 12230 finish
{'loss': 0.3698, 'grad_norm': 1.1850366592407227, 'learning_rate': 7.833333333333333e-05, 'epoch': 2.22}
********************on step end call back********************
Step 12240 finish
{'loss': 0.3626, 'grad_norm': 1.1259807348251343, 'learning_rate': 7.831501831501832e-05, 'epoch': 2.23}
********************on step end call back********************
Step 12250 finish
{'loss': 0.3839, 'grad_norm': 1.2069002389907837, 'learning_rate': 7.82967032967033e-05, 'epoch': 2.23}
********************on step end call back********************
Step 12260 finish
{'loss': 0.4469, 'grad_norm': 1.1562907695770264, 'learning_rate': 7.827838827838828e-05, 'epoch': 2.23}
********************on step end call back********************
Step 12270 finish
{'loss': 0.3673, 'grad_norm': 1.0428225994110107, 'learning_rate': 7.826007326007326e-05, 'epoch': 2.23}
********************on step end call back********************
Step 12280 finish
[INFO|trainer.py:3376] 2024-03-22 18:54:06,845 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-22 18:54:06,846 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-22 18:54:06,846 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-22 18:56:16,878 >> Saving model checkpoint to ./output/tmp-checkpoint-12300
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-22 18:56:17,022 >> tokenizer config file saved in ./output/tmp-checkpoint-12300/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-22 18:56:17,022 >> Special tokens file saved in ./output/tmp-checkpoint-12300/special_tokens_map.json
[INFO|trainer.py:3159] 2024-03-22 18:56:17,478 >> Deleting older checkpoint [output/checkpoint-2300] due to args.save_total_limit
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-22 19:05:02,139 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-22 19:05:02,139 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-22 19:05:02,139 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-22 19:07:12,005 >> Saving model checkpoint to ./output/tmp-checkpoint-12400
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-22 19:07:12,148 >> tokenizer config file saved in ./output/tmp-checkpoint-12400/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-22 19:07:12,148 >> Special tokens file saved in ./output/tmp-checkpoint-12400/special_tokens_map.json
[INFO|trainer.py:3159] 2024-03-22 19:07:12,346 >> Deleting older checkpoint [output/checkpoint-2400] due to args.save_total_limit
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-22 19:15:51,793 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-22 19:15:51,794 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-22 19:15:51,794 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-22 19:18:02,308 >> Saving model checkpoint to ./output/tmp-checkpoint-12500
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-22 19:18:02,455 >> tokenizer config file saved in ./output/tmp-checkpoint-12500/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-22 19:18:02,455 >> Special tokens file saved in ./output/tmp-checkpoint-12500/special_tokens_map.json
[INFO|trainer.py:3159] 2024-03-22 19:18:02,651 >> Deleting older checkpoint [output/checkpoint-2500] due to args.save_total_limit
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-22 19:26:33,305 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-22 19:26:33,305 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-22 19:26:33,305 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-22 19:28:43,431 >> Saving model checkpoint to ./output/tmp-checkpoint-12600
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-22 19:28:43,575 >> tokenizer config file saved in ./output/tmp-checkpoint-12600/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-22 19:28:43,575 >> Special tokens file saved in ./output/tmp-checkpoint-12600/special_tokens_map.json
[INFO|trainer.py:3159] 2024-03-22 19:28:43,771 >> Deleting older checkpoint [output/checkpoint-2600] due to args.save_total_limit
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
{'loss': 0.3548, 'grad_norm': 1.2219717502593994, 'learning_rate': 7.824175824175824e-05, 'epoch': 2.23}
********************on step end call back********************
Step 12290 finish
{'loss': 0.4078, 'grad_norm': 1.4436339139938354, 'learning_rate': 7.822344322344323e-05, 'epoch': 2.23}
********************on step end call back********************
Step 12300 finish
{'loss': 0.3655, 'grad_norm': 1.1941295862197876, 'learning_rate': 7.820512820512821e-05, 'epoch': 2.24}
{'eval_loss': 0.3140907883644104, 'eval_accuracy': 0.875, 'eval_runtime': 130.0314, 'eval_samples_per_second': 4.907, 'eval_steps_per_second': 4.907, 'epoch': 2.24}
********************save call back********************
********************on step end call back********************
Step 12310 finish
{'loss': 0.3877, 'grad_norm': 1.5937576293945312, 'learning_rate': 7.818681318681319e-05, 'epoch': 2.24}
********************on step end call back********************
Step 12320 finish
{'loss': 0.3547, 'grad_norm': 1.4005602598190308, 'learning_rate': 7.816849816849817e-05, 'epoch': 2.24}
********************on step end call back********************
Step 12330 finish
{'loss': 0.3515, 'grad_norm': 1.3968909978866577, 'learning_rate': 7.815018315018316e-05, 'epoch': 2.24}
********************on step end call back********************
Step 12340 finish
{'loss': 0.4052, 'grad_norm': 1.0535587072372437, 'learning_rate': 7.813186813186814e-05, 'epoch': 2.24}
********************on step end call back********************
Step 12350 finish
{'loss': 0.4015, 'grad_norm': 1.137783169746399, 'learning_rate': 7.811355311355312e-05, 'epoch': 2.25}
********************on step end call back********************
Step 12360 finish
{'loss': 0.3751, 'grad_norm': 1.183652400970459, 'learning_rate': 7.80952380952381e-05, 'epoch': 2.25}
********************on step end call back********************
Step 12370 finish
{'loss': 0.3256, 'grad_norm': 0.8506356477737427, 'learning_rate': 7.807692307692307e-05, 'epoch': 2.25}
********************on step end call back********************
Step 12380 finish
{'loss': 0.3899, 'grad_norm': 1.001110553741455, 'learning_rate': 7.805860805860807e-05, 'epoch': 2.25}
********************on step end call back********************
Step 12390 finish
{'loss': 0.3692, 'grad_norm': 0.936587393283844, 'learning_rate': 7.804029304029305e-05, 'epoch': 2.25}
********************on step end call back********************
Step 12400 finish
{'loss': 0.3761, 'grad_norm': 1.2885228395462036, 'learning_rate': 7.802197802197802e-05, 'epoch': 2.25}
{'eval_loss': 0.3169540464878082, 'eval_accuracy': 0.875, 'eval_runtime': 129.8639, 'eval_samples_per_second': 4.913, 'eval_steps_per_second': 4.913, 'epoch': 2.25}
********************save call back********************
********************on step end call back********************
Step 12410 finish
{'loss': 0.393, 'grad_norm': 1.5809317827224731, 'learning_rate': 7.8003663003663e-05, 'epoch': 2.26}
********************on step end call back********************
Step 12420 finish
{'loss': 0.353, 'grad_norm': 0.9502698183059692, 'learning_rate': 7.7985347985348e-05, 'epoch': 2.26}
********************on step end call back********************
Step 12430 finish
{'loss': 0.3361, 'grad_norm': 1.276045799255371, 'learning_rate': 7.796703296703297e-05, 'epoch': 2.26}
********************on step end call back********************
Step 12440 finish
{'loss': 0.3419, 'grad_norm': 1.2303634881973267, 'learning_rate': 7.794871794871795e-05, 'epoch': 2.26}
********************on step end call back********************
Step 12450 finish
{'loss': 0.3516, 'grad_norm': 1.2427934408187866, 'learning_rate': 7.793040293040293e-05, 'epoch': 2.26}
********************on step end call back********************
Step 12460 finish
{'loss': 0.3233, 'grad_norm': 1.2238608598709106, 'learning_rate': 7.791208791208791e-05, 'epoch': 2.27}
********************on step end call back********************
Step 12470 finish
{'loss': 0.3861, 'grad_norm': 1.6034982204437256, 'learning_rate': 7.78937728937729e-05, 'epoch': 2.27}
********************on step end call back********************
Step 12480 finish
{'loss': 0.3455, 'grad_norm': 1.1497548818588257, 'learning_rate': 7.787545787545788e-05, 'epoch': 2.27}
********************on step end call back********************
Step 12490 finish
{'loss': 0.3625, 'grad_norm': 1.0543209314346313, 'learning_rate': 7.785714285714286e-05, 'epoch': 2.27}
********************on step end call back********************
Step 12500 finish
{'loss': 0.4166, 'grad_norm': 1.3852206468582153, 'learning_rate': 7.783882783882784e-05, 'epoch': 2.27}
{'eval_loss': 0.3203296363353729, 'eval_accuracy': 0.8645833333333334, 'eval_runtime': 130.5138, 'eval_samples_per_second': 4.888, 'eval_steps_per_second': 4.888, 'epoch': 2.27}
********************save call back********************
********************on step end call back********************
Step 12510 finish
{'loss': 0.3889, 'grad_norm': 1.0253918170928955, 'learning_rate': 7.782051282051283e-05, 'epoch': 2.27}
********************on step end call back********************
Step 12520 finish
{'loss': 0.3158, 'grad_norm': 1.0241247415542603, 'learning_rate': 7.780219780219781e-05, 'epoch': 2.28}
********************on step end call back********************
Step 12530 finish
{'loss': 0.3447, 'grad_norm': 1.3519481420516968, 'learning_rate': 7.778388278388279e-05, 'epoch': 2.28}
********************on step end call back********************
Step 12540 finish
{'loss': 0.4025, 'grad_norm': 1.019455075263977, 'learning_rate': 7.776556776556777e-05, 'epoch': 2.28}
********************on step end call back********************
Step 12550 finish
{'loss': 0.4069, 'grad_norm': 1.180403470993042, 'learning_rate': 7.774725274725275e-05, 'epoch': 2.28}
********************on step end call back********************
Step 12560 finish
{'loss': 0.3282, 'grad_norm': 1.1209691762924194, 'learning_rate': 7.772893772893774e-05, 'epoch': 2.28}
********************on step end call back********************
Step 12570 finish
{'loss': 0.3525, 'grad_norm': 1.009779453277588, 'learning_rate': 7.771062271062272e-05, 'epoch': 2.29}
********************on step end call back********************
Step 12580 finish
{'loss': 0.4461, 'grad_norm': 1.3031737804412842, 'learning_rate': 7.76923076923077e-05, 'epoch': 2.29}
********************on step end call back********************
Step 12590 finish
{'loss': 0.3477, 'grad_norm': 1.2322602272033691, 'learning_rate': 7.767399267399267e-05, 'epoch': 2.29}
********************on step end call back********************
Step 12600 finish
{'loss': 0.3789, 'grad_norm': 1.168892741203308, 'learning_rate': 7.765567765567767e-05, 'epoch': 2.29}
{'eval_loss': 0.3206081986427307, 'eval_accuracy': 0.8645833333333334, 'eval_runtime': 130.1249, 'eval_samples_per_second': 4.903, 'eval_steps_per_second': 4.903, 'epoch': 2.29}
********************save call back********************
********************on step end call back********************
Step 12610 finish
{'loss': 0.3641, 'grad_norm': 1.2842965126037598, 'learning_rate': 7.763736263736265e-05, 'epoch': 2.29}
********************on step end call back********************
Step 12620 finish
{'loss': 0.4175, 'grad_norm': 1.1264511346817017, 'learning_rate': 7.761904761904762e-05, 'epoch': 2.29}
********************on step end call back********************
Step 12630 finish
{'loss': 0.3957, 'grad_norm': 0.9304280281066895, 'learning_rate': 7.76007326007326e-05, 'epoch': 2.3}
********************on step end call back********************
Step 12640 finish
{'loss': 0.3543, 'grad_norm': 1.206040620803833, 'learning_rate': 7.758241758241758e-05, 'epoch': 2.3}
********************on step end call back********************
Step 12650 finish
{'loss': 0.4278, 'grad_norm': 1.0960469245910645, 'learning_rate': 7.756410256410257e-05, 'epoch': 2.3}
********************on step end call back********************
Step 12660 finish
{'loss': 0.3711, 'grad_norm': 1.0919530391693115, 'learning_rate': 7.754578754578755e-05, 'epoch': 2.3}
********************on step end call back********************
Step 12670 finish
[INFO|trainer.py:3376] 2024-03-22 19:37:30,671 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-22 19:37:30,671 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-22 19:37:30,671 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-22 19:39:40,539 >> Saving model checkpoint to ./output/tmp-checkpoint-12700
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-22 19:39:40,681 >> tokenizer config file saved in ./output/tmp-checkpoint-12700/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-22 19:39:40,681 >> Special tokens file saved in ./output/tmp-checkpoint-12700/special_tokens_map.json
[INFO|trainer.py:3159] 2024-03-22 19:39:40,873 >> Deleting older checkpoint [output/checkpoint-2700] due to args.save_total_limit
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-22 19:48:21,690 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-22 19:48:21,690 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-22 19:48:21,690 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-22 19:50:31,506 >> Saving model checkpoint to ./output/tmp-checkpoint-12800
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-22 19:50:31,646 >> tokenizer config file saved in ./output/tmp-checkpoint-12800/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-22 19:50:31,646 >> Special tokens file saved in ./output/tmp-checkpoint-12800/special_tokens_map.json
[INFO|trainer.py:3159] 2024-03-22 19:50:31,843 >> Deleting older checkpoint [output/checkpoint-2800] due to args.save_total_limit
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-22 19:59:09,608 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-22 19:59:09,608 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-22 19:59:09,608 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-22 20:01:19,211 >> Saving model checkpoint to ./output/tmp-checkpoint-12900
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-22 20:01:19,353 >> tokenizer config file saved in ./output/tmp-checkpoint-12900/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-22 20:01:19,353 >> Special tokens file saved in ./output/tmp-checkpoint-12900/special_tokens_map.json
[INFO|trainer.py:3159] 2024-03-22 20:01:19,547 >> Deleting older checkpoint [output/checkpoint-2900] due to args.save_total_limit
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-22 20:09:58,945 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-22 20:09:58,945 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-22 20:09:58,946 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-22 20:12:08,510 >> Saving model checkpoint to ./output/tmp-checkpoint-13000
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-22 20:12:08,655 >> tokenizer config file saved in ./output/tmp-checkpoint-13000/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-22 20:12:08,655 >> Special tokens file saved in ./output/tmp-checkpoint-13000/special_tokens_map.json
[INFO|trainer.py:3159] 2024-03-22 20:12:08,851 >> Deleting older checkpoint [output/checkpoint-3000] due to args.save_total_limit
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
{'loss': 0.3424, 'grad_norm': 0.9741811752319336, 'learning_rate': 7.752747252747253e-05, 'epoch': 2.3}
********************on step end call back********************
Step 12680 finish
{'loss': 0.3818, 'grad_norm': 1.3171896934509277, 'learning_rate': 7.750915750915751e-05, 'epoch': 2.31}
********************on step end call back********************
Step 12690 finish
{'loss': 0.4411, 'grad_norm': 1.095836877822876, 'learning_rate': 7.74908424908425e-05, 'epoch': 2.31}
********************on step end call back********************
Step 12700 finish
{'loss': 0.3569, 'grad_norm': 1.1683586835861206, 'learning_rate': 7.747252747252748e-05, 'epoch': 2.31}
{'eval_loss': 0.32245174050331116, 'eval_accuracy': 0.8958333333333334, 'eval_runtime': 129.8668, 'eval_samples_per_second': 4.913, 'eval_steps_per_second': 4.913, 'epoch': 2.31}
********************save call back********************
********************on step end call back********************
Step 12710 finish
{'loss': 0.3444, 'grad_norm': 1.0715312957763672, 'learning_rate': 7.745421245421246e-05, 'epoch': 2.31}
********************on step end call back********************
Step 12720 finish
{'loss': 0.3855, 'grad_norm': 1.085931420326233, 'learning_rate': 7.743589743589744e-05, 'epoch': 2.31}
********************on step end call back********************
Step 12730 finish
{'loss': 0.4072, 'grad_norm': 1.300126552581787, 'learning_rate': 7.741758241758242e-05, 'epoch': 2.31}
********************on step end call back********************
Step 12740 finish
{'loss': 0.3372, 'grad_norm': 0.9998498558998108, 'learning_rate': 7.739926739926741e-05, 'epoch': 2.32}
********************on step end call back********************
Step 12750 finish
{'loss': 0.4268, 'grad_norm': 1.232742428779602, 'learning_rate': 7.738095238095239e-05, 'epoch': 2.32}
********************on step end call back********************
Step 12760 finish
{'loss': 0.3425, 'grad_norm': 1.1438910961151123, 'learning_rate': 7.736263736263737e-05, 'epoch': 2.32}
********************on step end call back********************
Step 12770 finish
{'loss': 0.3999, 'grad_norm': 0.9656686782836914, 'learning_rate': 7.734432234432235e-05, 'epoch': 2.32}
********************on step end call back********************
Step 12780 finish
{'loss': 0.3845, 'grad_norm': 1.0624375343322754, 'learning_rate': 7.732600732600734e-05, 'epoch': 2.32}
********************on step end call back********************
Step 12790 finish
{'loss': 0.3817, 'grad_norm': 1.0710424184799194, 'learning_rate': 7.730769230769232e-05, 'epoch': 2.33}
********************on step end call back********************
Step 12800 finish
{'loss': 0.3733, 'grad_norm': 1.3631082773208618, 'learning_rate': 7.72893772893773e-05, 'epoch': 2.33}
{'eval_loss': 0.3203074038028717, 'eval_accuracy': 0.8854166666666666, 'eval_runtime': 129.8146, 'eval_samples_per_second': 4.915, 'eval_steps_per_second': 4.915, 'epoch': 2.33}
********************save call back********************
********************on step end call back********************
Step 12810 finish
{'loss': 0.3512, 'grad_norm': 0.778750479221344, 'learning_rate': 7.727106227106228e-05, 'epoch': 2.33}
********************on step end call back********************
Step 12820 finish
{'loss': 0.3672, 'grad_norm': 1.098862648010254, 'learning_rate': 7.725274725274725e-05, 'epoch': 2.33}
********************on step end call back********************
Step 12830 finish
{'loss': 0.3652, 'grad_norm': 0.9634442925453186, 'learning_rate': 7.723443223443225e-05, 'epoch': 2.33}
********************on step end call back********************
Step 12840 finish
{'loss': 0.3974, 'grad_norm': 1.096133828163147, 'learning_rate': 7.721611721611722e-05, 'epoch': 2.33}
********************on step end call back********************
Step 12850 finish
{'loss': 0.3749, 'grad_norm': 1.3117401599884033, 'learning_rate': 7.71978021978022e-05, 'epoch': 2.34}
********************on step end call back********************
Step 12860 finish
{'loss': 0.3238, 'grad_norm': 1.121973991394043, 'learning_rate': 7.717948717948718e-05, 'epoch': 2.34}
********************on step end call back********************
Step 12870 finish
{'loss': 0.353, 'grad_norm': 1.2983602285385132, 'learning_rate': 7.716117216117217e-05, 'epoch': 2.34}
********************on step end call back********************
Step 12880 finish
{'loss': 0.3838, 'grad_norm': 0.9950016736984253, 'learning_rate': 7.714285714285715e-05, 'epoch': 2.34}
********************on step end call back********************
Step 12890 finish
{'loss': 0.4042, 'grad_norm': 1.0617274045944214, 'learning_rate': 7.712454212454213e-05, 'epoch': 2.34}
********************on step end call back********************
Step 12900 finish
{'loss': 0.3556, 'grad_norm': 1.1101361513137817, 'learning_rate': 7.710622710622711e-05, 'epoch': 2.35}
{'eval_loss': 0.3182165026664734, 'eval_accuracy': 0.875, 'eval_runtime': 129.602, 'eval_samples_per_second': 4.923, 'eval_steps_per_second': 4.923, 'epoch': 2.35}
********************save call back********************
********************on step end call back********************
Step 12910 finish
{'loss': 0.3915, 'grad_norm': 1.0491986274719238, 'learning_rate': 7.708791208791209e-05, 'epoch': 2.35}
********************on step end call back********************
Step 12920 finish
{'loss': 0.3667, 'grad_norm': 0.8783679604530334, 'learning_rate': 7.706959706959708e-05, 'epoch': 2.35}
********************on step end call back********************
Step 12930 finish
{'loss': 0.4022, 'grad_norm': 1.2582110166549683, 'learning_rate': 7.705128205128206e-05, 'epoch': 2.35}
********************on step end call back********************
Step 12940 finish
{'loss': 0.3886, 'grad_norm': 1.4647897481918335, 'learning_rate': 7.703296703296704e-05, 'epoch': 2.35}
********************on step end call back********************
Step 12950 finish
{'loss': 0.3671, 'grad_norm': 0.8291851878166199, 'learning_rate': 7.701465201465202e-05, 'epoch': 2.35}
********************on step end call back********************
Step 12960 finish
{'loss': 0.3649, 'grad_norm': 1.082163691520691, 'learning_rate': 7.699633699633701e-05, 'epoch': 2.36}
********************on step end call back********************
Step 12970 finish
{'loss': 0.3615, 'grad_norm': 1.0827056169509888, 'learning_rate': 7.697802197802199e-05, 'epoch': 2.36}
********************on step end call back********************
Step 12980 finish
{'loss': 0.3984, 'grad_norm': 0.8686062097549438, 'learning_rate': 7.695970695970697e-05, 'epoch': 2.36}
********************on step end call back********************
Step 12990 finish
{'loss': 0.3649, 'grad_norm': 1.140754222869873, 'learning_rate': 7.694139194139195e-05, 'epoch': 2.36}
********************on step end call back********************
Step 13000 finish
{'loss': 0.3876, 'grad_norm': 1.1551103591918945, 'learning_rate': 7.692307692307693e-05, 'epoch': 2.36}
{'eval_loss': 0.3173050582408905, 'eval_accuracy': 0.8854166666666666, 'eval_runtime': 129.5641, 'eval_samples_per_second': 4.924, 'eval_steps_per_second': 4.924, 'epoch': 2.36}
********************save call back********************
********************on step end call back********************
Step 13010 finish
{'loss': 0.344, 'grad_norm': 0.8753036260604858, 'learning_rate': 7.690476190476192e-05, 'epoch': 2.37}
********************on step end call back********************
Step 13020 finish
{'loss': 0.3902, 'grad_norm': 1.420255184173584, 'learning_rate': 7.68864468864469e-05, 'epoch': 2.37}
********************on step end call back********************
Step 13030 finish
{'loss': 0.3741, 'grad_norm': 1.0359559059143066, 'learning_rate': 7.686813186813188e-05, 'epoch': 2.37}
********************on step end call back********************
Step 13040 finish
{'loss': 0.3489, 'grad_norm': 1.3634963035583496, 'learning_rate': 7.684981684981685e-05, 'epoch': 2.37}
********************on step end call back********************
Step 13050 finish
{'loss': 0.3354, 'grad_norm': 1.0082858800888062, 'learning_rate': 7.683150183150183e-05, 'epoch': 2.37}
********************on step end call back********************
Step 13060 finish
[INFO|trainer.py:3376] 2024-03-22 20:20:44,429 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-22 20:20:44,429 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-22 20:20:44,429 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-22 20:22:54,790 >> Saving model checkpoint to ./output/tmp-checkpoint-13100
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-22 20:22:54,934 >> tokenizer config file saved in ./output/tmp-checkpoint-13100/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-22 20:22:54,935 >> Special tokens file saved in ./output/tmp-checkpoint-13100/special_tokens_map.json
[INFO|trainer.py:3159] 2024-03-22 20:22:55,132 >> Deleting older checkpoint [output/checkpoint-3100] due to args.save_total_limit
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-22 20:31:39,477 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-22 20:31:39,477 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-22 20:31:39,477 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-22 20:33:49,879 >> Saving model checkpoint to ./output/tmp-checkpoint-13200
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-22 20:33:50,023 >> tokenizer config file saved in ./output/tmp-checkpoint-13200/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-22 20:33:50,023 >> Special tokens file saved in ./output/tmp-checkpoint-13200/special_tokens_map.json
[INFO|trainer.py:3159] 2024-03-22 20:33:50,229 >> Deleting older checkpoint [output/checkpoint-3200] due to args.save_total_limit
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-22 20:42:31,173 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-22 20:42:31,174 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-22 20:42:31,174 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-22 20:44:41,920 >> Saving model checkpoint to ./output/tmp-checkpoint-13300
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-22 20:44:42,068 >> tokenizer config file saved in ./output/tmp-checkpoint-13300/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-22 20:44:42,068 >> Special tokens file saved in ./output/tmp-checkpoint-13300/special_tokens_map.json
[INFO|trainer.py:3159] 2024-03-22 20:44:42,269 >> Deleting older checkpoint [output/checkpoint-3300] due to args.save_total_limit
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-22 20:53:19,130 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-22 20:53:19,130 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-22 20:53:19,130 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-22 20:55:29,663 >> Saving model checkpoint to ./output/tmp-checkpoint-13400
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-22 20:55:29,815 >> tokenizer config file saved in ./output/tmp-checkpoint-13400/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-22 20:55:29,815 >> Special tokens file saved in ./output/tmp-checkpoint-13400/special_tokens_map.json
[INFO|trainer.py:3159] 2024-03-22 20:55:30,014 >> Deleting older checkpoint [output/checkpoint-3400] due to args.save_total_limit
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
{'loss': 0.3739, 'grad_norm': 1.1601536273956299, 'learning_rate': 7.681318681318683e-05, 'epoch': 2.37}
********************on step end call back********************
Step 13070 finish
{'loss': 0.3892, 'grad_norm': 1.1295976638793945, 'learning_rate': 7.67948717948718e-05, 'epoch': 2.38}
********************on step end call back********************
Step 13080 finish
{'loss': 0.4112, 'grad_norm': 1.3244011402130127, 'learning_rate': 7.677655677655678e-05, 'epoch': 2.38}
********************on step end call back********************
Step 13090 finish
{'loss': 0.3884, 'grad_norm': 1.1567234992980957, 'learning_rate': 7.675824175824176e-05, 'epoch': 2.38}
********************on step end call back********************
Step 13100 finish
{'loss': 0.3474, 'grad_norm': 1.1120448112487793, 'learning_rate': 7.673992673992675e-05, 'epoch': 2.38}
{'eval_loss': 0.31712159514427185, 'eval_accuracy': 0.875, 'eval_runtime': 130.3598, 'eval_samples_per_second': 4.894, 'eval_steps_per_second': 4.894, 'epoch': 2.38}
********************save call back********************
********************on step end call back********************
Step 13110 finish
{'loss': 0.3554, 'grad_norm': 1.1366552114486694, 'learning_rate': 7.672161172161173e-05, 'epoch': 2.38}
********************on step end call back********************
Step 13120 finish
{'loss': 0.3748, 'grad_norm': 1.249605417251587, 'learning_rate': 7.670329670329671e-05, 'epoch': 2.39}
********************on step end call back********************
Step 13130 finish
{'loss': 0.3821, 'grad_norm': 1.0521873235702515, 'learning_rate': 7.668498168498169e-05, 'epoch': 2.39}
********************on step end call back********************
Step 13140 finish
{'loss': 0.4093, 'grad_norm': 1.515062689781189, 'learning_rate': 7.666666666666667e-05, 'epoch': 2.39}
********************on step end call back********************
Step 13150 finish
{'loss': 0.3971, 'grad_norm': 0.9312573075294495, 'learning_rate': 7.664835164835166e-05, 'epoch': 2.39}
********************on step end call back********************
Step 13160 finish
{'loss': 0.3808, 'grad_norm': 1.06031334400177, 'learning_rate': 7.663003663003664e-05, 'epoch': 2.39}
********************on step end call back********************
Step 13170 finish
{'loss': 0.4166, 'grad_norm': 1.1062966585159302, 'learning_rate': 7.661172161172162e-05, 'epoch': 2.39}
********************on step end call back********************
Step 13180 finish
{'loss': 0.3832, 'grad_norm': 0.9980775713920593, 'learning_rate': 7.65934065934066e-05, 'epoch': 2.4}
********************on step end call back********************
Step 13190 finish
{'loss': 0.3849, 'grad_norm': 0.983814537525177, 'learning_rate': 7.657509157509159e-05, 'epoch': 2.4}
********************on step end call back********************
Step 13200 finish
{'loss': 0.3946, 'grad_norm': 1.408677101135254, 'learning_rate': 7.655677655677656e-05, 'epoch': 2.4}
{'eval_loss': 0.32273316383361816, 'eval_accuracy': 0.875, 'eval_runtime': 130.4005, 'eval_samples_per_second': 4.893, 'eval_steps_per_second': 4.893, 'epoch': 2.4}
********************save call back********************
********************on step end call back********************
Step 13210 finish
{'loss': 0.4433, 'grad_norm': 1.101510763168335, 'learning_rate': 7.653846153846153e-05, 'epoch': 2.4}
********************on step end call back********************
Step 13220 finish
{'loss': 0.3594, 'grad_norm': 1.1250864267349243, 'learning_rate': 7.652014652014651e-05, 'epoch': 2.4}
********************on step end call back********************
Step 13230 finish
{'loss': 0.3536, 'grad_norm': 0.9401007294654846, 'learning_rate': 7.65018315018315e-05, 'epoch': 2.41}
********************on step end call back********************
Step 13240 finish
{'loss': 0.3701, 'grad_norm': 1.0377089977264404, 'learning_rate': 7.648351648351648e-05, 'epoch': 2.41}
********************on step end call back********************
Step 13250 finish
{'loss': 0.3722, 'grad_norm': 1.178485631942749, 'learning_rate': 7.646520146520146e-05, 'epoch': 2.41}
********************on step end call back********************
Step 13260 finish
{'loss': 0.4211, 'grad_norm': 1.1042174100875854, 'learning_rate': 7.644688644688644e-05, 'epoch': 2.41}
********************on step end call back********************
Step 13270 finish
{'loss': 0.3804, 'grad_norm': 0.7664372324943542, 'learning_rate': 7.642857142857143e-05, 'epoch': 2.41}
********************on step end call back********************
Step 13280 finish
{'loss': 0.3557, 'grad_norm': 1.3342636823654175, 'learning_rate': 7.641025641025641e-05, 'epoch': 2.41}
********************on step end call back********************
Step 13290 finish
{'loss': 0.3433, 'grad_norm': 0.8786104917526245, 'learning_rate': 7.639194139194139e-05, 'epoch': 2.42}
********************on step end call back********************
Step 13300 finish
{'loss': 0.3741, 'grad_norm': 1.1245726346969604, 'learning_rate': 7.637362637362637e-05, 'epoch': 2.42}
{'eval_loss': 0.31912440061569214, 'eval_accuracy': 0.8854166666666666, 'eval_runtime': 130.7458, 'eval_samples_per_second': 4.88, 'eval_steps_per_second': 4.88, 'epoch': 2.42}
********************save call back********************
********************on step end call back********************
Step 13310 finish
{'loss': 0.3907, 'grad_norm': 0.9957206845283508, 'learning_rate': 7.635531135531135e-05, 'epoch': 2.42}
********************on step end call back********************
Step 13320 finish
{'loss': 0.3541, 'grad_norm': 0.9865580797195435, 'learning_rate': 7.633699633699634e-05, 'epoch': 2.42}
********************on step end call back********************
Step 13330 finish
{'loss': 0.3935, 'grad_norm': 1.103810429573059, 'learning_rate': 7.631868131868132e-05, 'epoch': 2.42}
********************on step end call back********************
Step 13340 finish
{'loss': 0.346, 'grad_norm': 0.7905676960945129, 'learning_rate': 7.63003663003663e-05, 'epoch': 2.43}
********************on step end call back********************
Step 13350 finish
{'loss': 0.3534, 'grad_norm': 1.2765651941299438, 'learning_rate': 7.628205128205128e-05, 'epoch': 2.43}
********************on step end call back********************
Step 13360 finish
{'loss': 0.4136, 'grad_norm': 1.2062987089157104, 'learning_rate': 7.626373626373627e-05, 'epoch': 2.43}
********************on step end call back********************
Step 13370 finish
{'loss': 0.3611, 'grad_norm': 1.1635805368423462, 'learning_rate': 7.624542124542125e-05, 'epoch': 2.43}
********************on step end call back********************
Step 13380 finish
{'loss': 0.3519, 'grad_norm': 1.282073736190796, 'learning_rate': 7.622710622710623e-05, 'epoch': 2.43}
********************on step end call back********************
Step 13390 finish
{'loss': 0.3764, 'grad_norm': 1.3521757125854492, 'learning_rate': 7.62087912087912e-05, 'epoch': 2.43}
********************on step end call back********************
Step 13400 finish
{'loss': 0.3729, 'grad_norm': 1.0596134662628174, 'learning_rate': 7.619047619047618e-05, 'epoch': 2.44}
{'eval_loss': 0.31482723355293274, 'eval_accuracy': 0.90625, 'eval_runtime': 130.5322, 'eval_samples_per_second': 4.888, 'eval_steps_per_second': 4.888, 'epoch': 2.44}
********************save call back********************
********************on step end call back********************
Step 13410 finish
{'loss': 0.4009, 'grad_norm': 1.2230249643325806, 'learning_rate': 7.617216117216118e-05, 'epoch': 2.44}
********************on step end call back********************
Step 13420 finish
{'loss': 0.4168, 'grad_norm': 1.2954708337783813, 'learning_rate': 7.615384615384616e-05, 'epoch': 2.44}
********************on step end call back********************
Step 13430 finish
{'loss': 0.3875, 'grad_norm': 1.2022730112075806, 'learning_rate': 7.613553113553113e-05, 'epoch': 2.44}
********************on step end call back********************
Step 13440 finish
{'loss': 0.3907, 'grad_norm': 1.009534239768982, 'learning_rate': 7.611721611721611e-05, 'epoch': 2.44}
********************on step end call back********************
Step 13450 finish
{'loss': 0.4252, 'grad_norm': 1.0866793394088745, 'learning_rate': 7.60989010989011e-05, 'epoch': 2.45}[INFO|trainer.py:3376] 2024-03-22 21:04:22,501 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-22 21:04:22,501 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-22 21:04:22,501 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-22 21:06:32,917 >> Saving model checkpoint to ./output/tmp-checkpoint-13500
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-22 21:06:33,062 >> tokenizer config file saved in ./output/tmp-checkpoint-13500/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-22 21:06:33,062 >> Special tokens file saved in ./output/tmp-checkpoint-13500/special_tokens_map.json
[INFO|trainer.py:3159] 2024-03-22 21:06:33,259 >> Deleting older checkpoint [output/checkpoint-3500] due to args.save_total_limit
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-22 21:15:16,929 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-22 21:15:16,929 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-22 21:15:16,929 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-22 21:17:27,414 >> Saving model checkpoint to ./output/tmp-checkpoint-13600
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-22 21:17:27,560 >> tokenizer config file saved in ./output/tmp-checkpoint-13600/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-22 21:17:27,561 >> Special tokens file saved in ./output/tmp-checkpoint-13600/special_tokens_map.json
[INFO|trainer.py:3159] 2024-03-22 21:17:27,762 >> Deleting older checkpoint [output/checkpoint-3600] due to args.save_total_limit
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-22 21:26:02,496 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-22 21:26:02,496 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-22 21:26:02,496 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-22 21:28:12,727 >> Saving model checkpoint to ./output/tmp-checkpoint-13700
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-22 21:28:12,870 >> tokenizer config file saved in ./output/tmp-checkpoint-13700/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-22 21:28:12,870 >> Special tokens file saved in ./output/tmp-checkpoint-13700/special_tokens_map.json
[INFO|trainer.py:3159] 2024-03-22 21:28:13,074 >> Deleting older checkpoint [output/checkpoint-3700] due to args.save_total_limit
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-22 21:36:54,584 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-22 21:36:54,584 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-22 21:36:54,584 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-22 21:39:05,131 >> Saving model checkpoint to ./output/tmp-checkpoint-13800
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-22 21:39:05,282 >> tokenizer config file saved in ./output/tmp-checkpoint-13800/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-22 21:39:05,282 >> Special tokens file saved in ./output/tmp-checkpoint-13800/special_tokens_map.json
[INFO|trainer.py:3159] 2024-03-22 21:39:05,484 >> Deleting older checkpoint [output/checkpoint-3800] due to args.save_total_limit
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(

********************on step end call back********************
Step 13460 finish
{'loss': 0.3678, 'grad_norm': 1.0141631364822388, 'learning_rate': 7.608058608058608e-05, 'epoch': 2.45}
********************on step end call back********************
Step 13470 finish
{'loss': 0.3846, 'grad_norm': 1.2545689344406128, 'learning_rate': 7.606227106227106e-05, 'epoch': 2.45}
********************on step end call back********************
Step 13480 finish
{'loss': 0.4173, 'grad_norm': 1.1234629154205322, 'learning_rate': 7.604395604395604e-05, 'epoch': 2.45}
********************on step end call back********************
Step 13490 finish
{'loss': 0.407, 'grad_norm': 1.039543628692627, 'learning_rate': 7.602564102564102e-05, 'epoch': 2.45}
********************on step end call back********************
Step 13500 finish
{'loss': 0.3559, 'grad_norm': 1.0121549367904663, 'learning_rate': 7.600732600732601e-05, 'epoch': 2.45}
{'eval_loss': 0.31859180331230164, 'eval_accuracy': 0.8854166666666666, 'eval_runtime': 130.4156, 'eval_samples_per_second': 4.892, 'eval_steps_per_second': 4.892, 'epoch': 2.45}
********************save call back********************
********************on step end call back********************
Step 13510 finish
{'loss': 0.3677, 'grad_norm': 1.016581416130066, 'learning_rate': 7.598901098901099e-05, 'epoch': 2.46}
********************on step end call back********************
Step 13520 finish
{'loss': 0.4038, 'grad_norm': 1.049515962600708, 'learning_rate': 7.597069597069597e-05, 'epoch': 2.46}
********************on step end call back********************
Step 13530 finish
{'loss': 0.3833, 'grad_norm': 1.0494186878204346, 'learning_rate': 7.595238095238095e-05, 'epoch': 2.46}
********************on step end call back********************
Step 13540 finish
{'loss': 0.3726, 'grad_norm': 0.8055927157402039, 'learning_rate': 7.593406593406593e-05, 'epoch': 2.46}
********************on step end call back********************
Step 13550 finish
{'loss': 0.3719, 'grad_norm': 1.1132031679153442, 'learning_rate': 7.591575091575092e-05, 'epoch': 2.46}
********************on step end call back********************
Step 13560 finish
{'loss': 0.3513, 'grad_norm': 1.112328052520752, 'learning_rate': 7.58974358974359e-05, 'epoch': 2.47}
********************on step end call back********************
Step 13570 finish
{'loss': 0.387, 'grad_norm': 1.183045744895935, 'learning_rate': 7.587912087912088e-05, 'epoch': 2.47}
********************on step end call back********************
Step 13580 finish
{'loss': 0.3548, 'grad_norm': 0.9624986052513123, 'learning_rate': 7.586080586080586e-05, 'epoch': 2.47}
********************on step end call back********************
Step 13590 finish
{'loss': 0.3992, 'grad_norm': 1.235048532485962, 'learning_rate': 7.584249084249085e-05, 'epoch': 2.47}
********************on step end call back********************
Step 13600 finish
{'loss': 0.3824, 'grad_norm': 1.3546348810195923, 'learning_rate': 7.582417582417583e-05, 'epoch': 2.47}
{'eval_loss': 0.32154005765914917, 'eval_accuracy': 0.8854166666666666, 'eval_runtime': 130.484, 'eval_samples_per_second': 4.889, 'eval_steps_per_second': 4.889, 'epoch': 2.47}
********************save call back********************
********************on step end call back********************
Step 13610 finish
{'loss': 0.3787, 'grad_norm': 1.305773377418518, 'learning_rate': 7.58058608058608e-05, 'epoch': 2.47}
********************on step end call back********************
Step 13620 finish
{'loss': 0.3876, 'grad_norm': 1.5213418006896973, 'learning_rate': 7.578754578754578e-05, 'epoch': 2.48}
********************on step end call back********************
Step 13630 finish
{'loss': 0.4036, 'grad_norm': 1.1344908475875854, 'learning_rate': 7.576923076923076e-05, 'epoch': 2.48}
********************on step end call back********************
Step 13640 finish
{'loss': 0.4172, 'grad_norm': 1.2725712060928345, 'learning_rate': 7.575091575091576e-05, 'epoch': 2.48}
********************on step end call back********************
Step 13650 finish
{'loss': 0.3752, 'grad_norm': 1.0561821460723877, 'learning_rate': 7.573260073260073e-05, 'epoch': 2.48}
********************on step end call back********************
Step 13660 finish
{'loss': 0.3647, 'grad_norm': 1.0294655561447144, 'learning_rate': 7.571428571428571e-05, 'epoch': 2.48}
********************on step end call back********************
Step 13670 finish
{'loss': 0.3874, 'grad_norm': 1.2456327676773071, 'learning_rate': 7.569597069597069e-05, 'epoch': 2.49}
********************on step end call back********************
Step 13680 finish
{'loss': 0.3438, 'grad_norm': 0.9487637281417847, 'learning_rate': 7.567765567765568e-05, 'epoch': 2.49}
********************on step end call back********************
Step 13690 finish
{'loss': 0.3903, 'grad_norm': 1.3421376943588257, 'learning_rate': 7.565934065934066e-05, 'epoch': 2.49}
********************on step end call back********************
Step 13700 finish
{'loss': 0.413, 'grad_norm': 1.4778599739074707, 'learning_rate': 7.564102564102564e-05, 'epoch': 2.49}
{'eval_loss': 0.3224118649959564, 'eval_accuracy': 0.875, 'eval_runtime': 130.2307, 'eval_samples_per_second': 4.899, 'eval_steps_per_second': 4.899, 'epoch': 2.49}
********************save call back********************
********************on step end call back********************
Step 13710 finish
{'loss': 0.38, 'grad_norm': 1.1517176628112793, 'learning_rate': 7.562271062271062e-05, 'epoch': 2.49}
********************on step end call back********************
Step 13720 finish
{'loss': 0.3918, 'grad_norm': 1.2037979364395142, 'learning_rate': 7.56043956043956e-05, 'epoch': 2.49}
********************on step end call back********************
Step 13730 finish
{'loss': 0.3576, 'grad_norm': 1.2456035614013672, 'learning_rate': 7.558608058608059e-05, 'epoch': 2.5}
********************on step end call back********************
Step 13740 finish
{'loss': 0.3934, 'grad_norm': 1.2611335515975952, 'learning_rate': 7.556776556776557e-05, 'epoch': 2.5}
********************on step end call back********************
Step 13750 finish
{'loss': 0.3782, 'grad_norm': 1.1604894399642944, 'learning_rate': 7.554945054945055e-05, 'epoch': 2.5}
********************on step end call back********************
Step 13760 finish
{'loss': 0.4255, 'grad_norm': 1.1907377243041992, 'learning_rate': 7.553113553113553e-05, 'epoch': 2.5}
********************on step end call back********************
Step 13770 finish
{'loss': 0.3878, 'grad_norm': 1.2606226205825806, 'learning_rate': 7.551282051282052e-05, 'epoch': 2.5}
********************on step end call back********************
Step 13780 finish
{'loss': 0.3221, 'grad_norm': 1.4326131343841553, 'learning_rate': 7.54945054945055e-05, 'epoch': 2.51}
********************on step end call back********************
Step 13790 finish
{'loss': 0.3544, 'grad_norm': 1.0468032360076904, 'learning_rate': 7.547619047619048e-05, 'epoch': 2.51}
********************on step end call back********************
Step 13800 finish
{'loss': 0.3694, 'grad_norm': 1.288383960723877, 'learning_rate': 7.545787545787546e-05, 'epoch': 2.51}
{'eval_loss': 0.31904903054237366, 'eval_accuracy': 0.8958333333333334, 'eval_runtime': 130.5458, 'eval_samples_per_second': 4.887, 'eval_steps_per_second': 4.887, 'epoch': 2.51}
********************save call back********************
********************on step end call back********************
Step 13810 finish
{'loss': 0.3138, 'grad_norm': 1.0748871564865112, 'learning_rate': 7.543956043956044e-05, 'epoch': 2.51}
********************on step end call back********************
Step 13820 finish
{'loss': 0.3587, 'grad_norm': 1.2141660451889038, 'learning_rate': 7.542124542124543e-05, 'epoch': 2.51}
********************on step end call back********************
Step 13830 finish
{'loss': 0.3674, 'grad_norm': 1.0604634284973145, 'learning_rate': 7.54029304029304e-05, 'epoch': 2.51}
********************on step end call back********************
Step 13840 finish
{'loss': 0.4105, 'grad_norm': 1.2506104707717896, 'learning_rate': 7.538461538461539e-05, 'epoch': 2.52}
********************on step end call back********************
[INFO|trainer.py:3376] 2024-03-22 21:47:50,053 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-22 21:47:50,053 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-22 21:47:50,053 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-22 21:50:00,580 >> Saving model checkpoint to ./output/tmp-checkpoint-13900
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-22 21:50:00,724 >> tokenizer config file saved in ./output/tmp-checkpoint-13900/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-22 21:50:00,724 >> Special tokens file saved in ./output/tmp-checkpoint-13900/special_tokens_map.json
[INFO|trainer.py:3159] 2024-03-22 21:50:00,920 >> Deleting older checkpoint [output/checkpoint-3900] due to args.save_total_limit
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-22 21:58:32,187 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-22 21:58:32,187 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-22 21:58:32,187 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-22 22:00:42,803 >> Saving model checkpoint to ./output/tmp-checkpoint-14000
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-22 22:00:42,948 >> tokenizer config file saved in ./output/tmp-checkpoint-14000/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-22 22:00:42,948 >> Special tokens file saved in ./output/tmp-checkpoint-14000/special_tokens_map.json
[INFO|trainer.py:3159] 2024-03-22 22:00:43,150 >> Deleting older checkpoint [output/checkpoint-4000] due to args.save_total_limit
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-22 22:09:20,601 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-22 22:09:20,601 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-22 22:09:20,601 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-22 22:11:31,011 >> Saving model checkpoint to ./output/tmp-checkpoint-14100
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-22 22:11:31,162 >> tokenizer config file saved in ./output/tmp-checkpoint-14100/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-22 22:11:31,162 >> Special tokens file saved in ./output/tmp-checkpoint-14100/special_tokens_map.json
[INFO|trainer.py:3159] 2024-03-22 22:11:31,367 >> Deleting older checkpoint [output/checkpoint-4100] due to args.save_total_limit
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-22 22:20:13,287 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-22 22:20:13,287 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-22 22:20:13,287 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-22 22:22:23,621 >> Saving model checkpoint to ./output/tmp-checkpoint-14200
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-22 22:22:23,766 >> tokenizer config file saved in ./output/tmp-checkpoint-14200/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-22 22:22:23,766 >> Special tokens file saved in ./output/tmp-checkpoint-14200/special_tokens_map.json
[INFO|trainer.py:3159] 2024-03-22 22:22:23,968 >> Deleting older checkpoint [output/checkpoint-4200] due to args.save_total_limit
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
Step 13850 finish
{'loss': 0.3983, 'grad_norm': 0.9757698774337769, 'learning_rate': 7.536630036630036e-05, 'epoch': 2.52}
********************on step end call back********************
Step 13860 finish
{'loss': 0.3748, 'grad_norm': 1.0640901327133179, 'learning_rate': 7.534798534798536e-05, 'epoch': 2.52}
********************on step end call back********************
Step 13870 finish
{'loss': 0.3538, 'grad_norm': 0.931876540184021, 'learning_rate': 7.532967032967034e-05, 'epoch': 2.52}
********************on step end call back********************
Step 13880 finish
{'loss': 0.3794, 'grad_norm': 1.1224617958068848, 'learning_rate': 7.531135531135531e-05, 'epoch': 2.52}
********************on step end call back********************
Step 13890 finish
{'loss': 0.3671, 'grad_norm': 0.9967312216758728, 'learning_rate': 7.529304029304029e-05, 'epoch': 2.53}
********************on step end call back********************
Step 13900 finish
{'loss': 0.4121, 'grad_norm': 1.0159273147583008, 'learning_rate': 7.527472527472527e-05, 'epoch': 2.53}
{'eval_loss': 0.31791388988494873, 'eval_accuracy': 0.875, 'eval_runtime': 130.5255, 'eval_samples_per_second': 4.888, 'eval_steps_per_second': 4.888, 'epoch': 2.53}
********************save call back********************
********************on step end call back********************
Step 13910 finish
{'loss': 0.3969, 'grad_norm': 1.240760087966919, 'learning_rate': 7.525641025641026e-05, 'epoch': 2.53}
********************on step end call back********************
Step 13920 finish
{'loss': 0.4152, 'grad_norm': 1.017082691192627, 'learning_rate': 7.523809523809524e-05, 'epoch': 2.53}
********************on step end call back********************
Step 13930 finish
{'loss': 0.32, 'grad_norm': 1.0722557306289673, 'learning_rate': 7.521978021978022e-05, 'epoch': 2.53}
********************on step end call back********************
Step 13940 finish
{'loss': 0.3998, 'grad_norm': 1.4749515056610107, 'learning_rate': 7.52014652014652e-05, 'epoch': 2.53}
********************on step end call back********************
Step 13950 finish
{'loss': 0.4067, 'grad_norm': 1.0284233093261719, 'learning_rate': 7.518315018315019e-05, 'epoch': 2.54}
********************on step end call back********************
Step 13960 finish
{'loss': 0.3074, 'grad_norm': 1.100595474243164, 'learning_rate': 7.516483516483517e-05, 'epoch': 2.54}
********************on step end call back********************
Step 13970 finish
{'loss': 0.3555, 'grad_norm': 1.0818816423416138, 'learning_rate': 7.514652014652015e-05, 'epoch': 2.54}
********************on step end call back********************
Step 13980 finish
{'loss': 0.3777, 'grad_norm': 1.233115553855896, 'learning_rate': 7.512820512820513e-05, 'epoch': 2.54}
********************on step end call back********************
Step 13990 finish
{'loss': 0.3863, 'grad_norm': 0.9935904145240784, 'learning_rate': 7.510989010989011e-05, 'epoch': 2.54}
********************on step end call back********************
Step 14000 finish
{'loss': 0.3576, 'grad_norm': 1.0163921117782593, 'learning_rate': 7.50915750915751e-05, 'epoch': 2.55}
{'eval_loss': 0.3206492066383362, 'eval_accuracy': 0.8958333333333334, 'eval_runtime': 130.6149, 'eval_samples_per_second': 4.885, 'eval_steps_per_second': 4.885, 'epoch': 2.55}
********************save call back********************
********************on step end call back********************
Step 14010 finish
{'loss': 0.3744, 'grad_norm': 1.5220881700515747, 'learning_rate': 7.507326007326008e-05, 'epoch': 2.55}
********************on step end call back********************
Step 14020 finish
{'loss': 0.3775, 'grad_norm': 1.1362807750701904, 'learning_rate': 7.505494505494506e-05, 'epoch': 2.55}
********************on step end call back********************
Step 14030 finish
{'loss': 0.3615, 'grad_norm': 1.0734632015228271, 'learning_rate': 7.503663003663004e-05, 'epoch': 2.55}
********************on step end call back********************
Step 14040 finish
{'loss': 0.3677, 'grad_norm': 1.3721898794174194, 'learning_rate': 7.501831501831503e-05, 'epoch': 2.55}
********************on step end call back********************
Step 14050 finish
{'loss': 0.3803, 'grad_norm': 0.9853439331054688, 'learning_rate': 7.500000000000001e-05, 'epoch': 2.55}
********************on step end call back********************
Step 14060 finish
{'loss': 0.3622, 'grad_norm': 0.9384209513664246, 'learning_rate': 7.498168498168499e-05, 'epoch': 2.56}
********************on step end call back********************
Step 14070 finish
{'loss': 0.3634, 'grad_norm': 1.0784212350845337, 'learning_rate': 7.496336996336996e-05, 'epoch': 2.56}
********************on step end call back********************
Step 14080 finish
{'loss': 0.3993, 'grad_norm': 1.245498776435852, 'learning_rate': 7.494505494505494e-05, 'epoch': 2.56}
********************on step end call back********************
Step 14090 finish
{'loss': 0.3732, 'grad_norm': 1.1596940755844116, 'learning_rate': 7.492673992673994e-05, 'epoch': 2.56}
********************on step end call back********************
Step 14100 finish
{'loss': 0.4055, 'grad_norm': 1.1047277450561523, 'learning_rate': 7.490842490842491e-05, 'epoch': 2.56}
{'eval_loss': 0.3226468861103058, 'eval_accuracy': 0.8958333333333334, 'eval_runtime': 130.4092, 'eval_samples_per_second': 4.892, 'eval_steps_per_second': 4.892, 'epoch': 2.56}
********************save call back********************
********************on step end call back********************
Step 14110 finish
{'loss': 0.3864, 'grad_norm': 0.960013747215271, 'learning_rate': 7.489010989010989e-05, 'epoch': 2.57}
********************on step end call back********************
Step 14120 finish
{'loss': 0.3822, 'grad_norm': 0.9905713796615601, 'learning_rate': 7.487179487179487e-05, 'epoch': 2.57}
********************on step end call back********************
Step 14130 finish
{'loss': 0.4017, 'grad_norm': 1.3833811283111572, 'learning_rate': 7.485347985347986e-05, 'epoch': 2.57}
********************on step end call back********************
Step 14140 finish
{'loss': 0.3571, 'grad_norm': 1.056433081626892, 'learning_rate': 7.483516483516484e-05, 'epoch': 2.57}
********************on step end call back********************
Step 14150 finish
{'loss': 0.3916, 'grad_norm': 1.018429160118103, 'learning_rate': 7.481684981684982e-05, 'epoch': 2.57}
********************on step end call back********************
Step 14160 finish
{'loss': 0.3982, 'grad_norm': 1.1306862831115723, 'learning_rate': 7.47985347985348e-05, 'epoch': 2.57}
********************on step end call back********************
Step 14170 finish
{'loss': 0.3648, 'grad_norm': 0.9374749064445496, 'learning_rate': 7.478021978021978e-05, 'epoch': 2.58}
********************on step end call back********************
Step 14180 finish
{'loss': 0.4284, 'grad_norm': 1.4299405813217163, 'learning_rate': 7.476190476190477e-05, 'epoch': 2.58}
********************on step end call back********************
Step 14190 finish
{'loss': 0.3817, 'grad_norm': 1.0305854082107544, 'learning_rate': 7.474358974358975e-05, 'epoch': 2.58}
********************on step end call back********************
Step 14200 finish
{'loss': 0.3945, 'grad_norm': 1.1675004959106445, 'learning_rate': 7.472527472527473e-05, 'epoch': 2.58}
{'eval_loss': 0.32209518551826477, 'eval_accuracy': 0.8854166666666666, 'eval_runtime': 130.3323, 'eval_samples_per_second': 4.895, 'eval_steps_per_second': 4.895, 'epoch': 2.58}
********************save call back********************
********************on step end call back********************
Step 14210 finish
{'loss': 0.3864, 'grad_norm': 1.1376397609710693, 'learning_rate': 7.470695970695971e-05, 'epoch': 2.58}
********************on step end call back********************
Step 14220 finish
{'loss': 0.3511, 'grad_norm': 1.3882701396942139, 'learning_rate': 7.46886446886447e-05, 'epoch': 2.59}
********************on step end call back********************
Step 14230 finish
{'loss': 0.3918, 'grad_norm': 1.4758946895599365, 'learning_rate': 7.467032967032968e-05, 'epoch': 2.59}
********************on step end call back********************
Step 14240 finish
[INFO|trainer.py:3376] 2024-03-22 22:31:02,904 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-22 22:31:02,904 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-22 22:31:02,904 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-22 22:33:13,206 >> Saving model checkpoint to ./output/tmp-checkpoint-14300
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-22 22:33:13,354 >> tokenizer config file saved in ./output/tmp-checkpoint-14300/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-22 22:33:13,354 >> Special tokens file saved in ./output/tmp-checkpoint-14300/special_tokens_map.json
[INFO|trainer.py:3159] 2024-03-22 22:33:13,555 >> Deleting older checkpoint [output/checkpoint-4300] due to args.save_total_limit
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-22 22:41:51,579 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-22 22:41:51,579 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-22 22:41:51,579 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-22 22:44:01,723 >> Saving model checkpoint to ./output/tmp-checkpoint-14400
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-22 22:44:01,869 >> tokenizer config file saved in ./output/tmp-checkpoint-14400/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-22 22:44:01,869 >> Special tokens file saved in ./output/tmp-checkpoint-14400/special_tokens_map.json
[INFO|trainer.py:3159] 2024-03-22 22:44:02,065 >> Deleting older checkpoint [output/checkpoint-4400] due to args.save_total_limit
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-22 22:52:38,420 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-22 22:52:38,420 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-22 22:52:38,420 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-22 22:54:48,628 >> Saving model checkpoint to ./output/tmp-checkpoint-14500
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-22 22:54:48,773 >> tokenizer config file saved in ./output/tmp-checkpoint-14500/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-22 22:54:48,773 >> Special tokens file saved in ./output/tmp-checkpoint-14500/special_tokens_map.json
[INFO|trainer.py:3159] 2024-03-22 22:54:48,971 >> Deleting older checkpoint [output/checkpoint-4500] due to args.save_total_limit
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-22 23:03:30,114 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-22 23:03:30,115 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-22 23:03:30,115 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-22 23:05:40,372 >> Saving model checkpoint to ./output/tmp-checkpoint-14600
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-22 23:05:40,516 >> tokenizer config file saved in ./output/tmp-checkpoint-14600/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-22 23:05:40,517 >> Special tokens file saved in ./output/tmp-checkpoint-14600/special_tokens_map.json
[INFO|trainer.py:3159] 2024-03-22 23:05:40,712 >> Deleting older checkpoint [output/checkpoint-4600] due to args.save_total_limit
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
{'loss': 0.3956, 'grad_norm': 1.2959189414978027, 'learning_rate': 7.465201465201466e-05, 'epoch': 2.59}
********************on step end call back********************
Step 14250 finish
{'loss': 0.3148, 'grad_norm': 1.1551116704940796, 'learning_rate': 7.463369963369964e-05, 'epoch': 2.59}
********************on step end call back********************
Step 14260 finish
{'loss': 0.375, 'grad_norm': 0.9614524841308594, 'learning_rate': 7.461538461538462e-05, 'epoch': 2.59}
********************on step end call back********************
Step 14270 finish
{'loss': 0.4046, 'grad_norm': 1.1823198795318604, 'learning_rate': 7.459706959706961e-05, 'epoch': 2.59}
********************on step end call back********************
Step 14280 finish
{'loss': 0.3952, 'grad_norm': 1.1246994733810425, 'learning_rate': 7.457875457875459e-05, 'epoch': 2.6}
********************on step end call back********************
Step 14290 finish
{'loss': 0.3447, 'grad_norm': 1.1868462562561035, 'learning_rate': 7.456043956043956e-05, 'epoch': 2.6}
********************on step end call back********************
Step 14300 finish
{'loss': 0.4394, 'grad_norm': 1.2316761016845703, 'learning_rate': 7.454212454212454e-05, 'epoch': 2.6}
{'eval_loss': 0.3212916851043701, 'eval_accuracy': 0.8854166666666666, 'eval_runtime': 130.3009, 'eval_samples_per_second': 4.896, 'eval_steps_per_second': 4.896, 'epoch': 2.6}
********************save call back********************
********************on step end call back********************
Step 14310 finish
{'loss': 0.369, 'grad_norm': 1.0434374809265137, 'learning_rate': 7.452380952380952e-05, 'epoch': 2.6}
********************on step end call back********************
Step 14320 finish
{'loss': 0.3661, 'grad_norm': 1.0483386516571045, 'learning_rate': 7.450549450549451e-05, 'epoch': 2.6}
********************on step end call back********************
Step 14330 finish
{'loss': 0.3799, 'grad_norm': 1.38521146774292, 'learning_rate': 7.44871794871795e-05, 'epoch': 2.61}
********************on step end call back********************
Step 14340 finish
{'loss': 0.4516, 'grad_norm': 1.200498342514038, 'learning_rate': 7.446886446886447e-05, 'epoch': 2.61}
********************on step end call back********************
Step 14350 finish
{'loss': 0.3381, 'grad_norm': 0.9640762805938721, 'learning_rate': 7.445054945054945e-05, 'epoch': 2.61}
********************on step end call back********************
Step 14360 finish
{'loss': 0.3968, 'grad_norm': 1.1182466745376587, 'learning_rate': 7.443223443223444e-05, 'epoch': 2.61}
********************on step end call back********************
Step 14370 finish
{'loss': 0.4258, 'grad_norm': 1.2428326606750488, 'learning_rate': 7.441391941391942e-05, 'epoch': 2.61}
********************on step end call back********************
Step 14380 finish
{'loss': 0.3498, 'grad_norm': 1.274773120880127, 'learning_rate': 7.43956043956044e-05, 'epoch': 2.61}
********************on step end call back********************
Step 14390 finish
{'loss': 0.3944, 'grad_norm': 1.2345471382141113, 'learning_rate': 7.437728937728938e-05, 'epoch': 2.62}
********************on step end call back********************
Step 14400 finish
{'loss': 0.3652, 'grad_norm': 1.1013761758804321, 'learning_rate': 7.435897435897436e-05, 'epoch': 2.62}
{'eval_loss': 0.3216361999511719, 'eval_accuracy': 0.9166666666666666, 'eval_runtime': 130.1434, 'eval_samples_per_second': 4.902, 'eval_steps_per_second': 4.902, 'epoch': 2.62}
********************save call back********************
********************on step end call back********************
Step 14410 finish
{'loss': 0.4031, 'grad_norm': 1.1793508529663086, 'learning_rate': 7.434065934065935e-05, 'epoch': 2.62}
********************on step end call back********************
Step 14420 finish
{'loss': 0.4018, 'grad_norm': 1.4220205545425415, 'learning_rate': 7.432234432234433e-05, 'epoch': 2.62}
********************on step end call back********************
Step 14430 finish
{'loss': 0.3764, 'grad_norm': 0.9981856346130371, 'learning_rate': 7.430402930402931e-05, 'epoch': 2.62}
********************on step end call back********************
Step 14440 finish
{'loss': 0.3547, 'grad_norm': 1.099240779876709, 'learning_rate': 7.428571428571429e-05, 'epoch': 2.63}
********************on step end call back********************
Step 14450 finish
{'loss': 0.4215, 'grad_norm': 1.1141105890274048, 'learning_rate': 7.426739926739928e-05, 'epoch': 2.63}
********************on step end call back********************
Step 14460 finish
{'loss': 0.3461, 'grad_norm': 0.8167633414268494, 'learning_rate': 7.424908424908426e-05, 'epoch': 2.63}
********************on step end call back********************
Step 14470 finish
{'loss': 0.3274, 'grad_norm': 1.071294903755188, 'learning_rate': 7.423076923076924e-05, 'epoch': 2.63}
********************on step end call back********************
Step 14480 finish
{'loss': 0.4097, 'grad_norm': 0.9764321446418762, 'learning_rate': 7.421245421245422e-05, 'epoch': 2.63}
********************on step end call back********************
Step 14490 finish
{'loss': 0.3656, 'grad_norm': 1.0298734903335571, 'learning_rate': 7.41941391941392e-05, 'epoch': 2.63}
********************on step end call back********************
Step 14500 finish
{'loss': 0.3618, 'grad_norm': 1.485269546508789, 'learning_rate': 7.417582417582419e-05, 'epoch': 2.64}
{'eval_loss': 0.3192995488643646, 'eval_accuracy': 0.8854166666666666, 'eval_runtime': 130.2071, 'eval_samples_per_second': 4.9, 'eval_steps_per_second': 4.9, 'epoch': 2.64}
********************save call back********************
********************on step end call back********************
Step 14510 finish
{'loss': 0.4067, 'grad_norm': 0.8999108076095581, 'learning_rate': 7.415750915750917e-05, 'epoch': 2.64}
********************on step end call back********************
Step 14520 finish
{'loss': 0.3766, 'grad_norm': 1.2873331308364868, 'learning_rate': 7.413919413919414e-05, 'epoch': 2.64}
********************on step end call back********************
Step 14530 finish
{'loss': 0.3649, 'grad_norm': 1.2871521711349487, 'learning_rate': 7.412087912087912e-05, 'epoch': 2.64}
********************on step end call back********************
Step 14540 finish
{'loss': 0.3546, 'grad_norm': 1.4114766120910645, 'learning_rate': 7.410256410256412e-05, 'epoch': 2.64}
********************on step end call back********************
Step 14550 finish
{'loss': 0.4036, 'grad_norm': 1.1715421676635742, 'learning_rate': 7.40842490842491e-05, 'epoch': 2.65}
********************on step end call back********************
Step 14560 finish
{'loss': 0.3768, 'grad_norm': 1.1102747917175293, 'learning_rate': 7.406593406593407e-05, 'epoch': 2.65}
********************on step end call back********************
Step 14570 finish
{'loss': 0.4015, 'grad_norm': 1.051694393157959, 'learning_rate': 7.404761904761905e-05, 'epoch': 2.65}
********************on step end call back********************
Step 14580 finish
{'loss': 0.3817, 'grad_norm': 0.9535881876945496, 'learning_rate': 7.402930402930403e-05, 'epoch': 2.65}
********************on step end call back********************
Step 14590 finish
{'loss': 0.4186, 'grad_norm': 0.8442373871803284, 'learning_rate': 7.401098901098902e-05, 'epoch': 2.65}
********************on step end call back********************
Step 14600 finish
{'loss': 0.3728, 'grad_norm': 1.0390888452529907, 'learning_rate': 7.3992673992674e-05, 'epoch': 2.65}
{'eval_loss': 0.3185049891471863, 'eval_accuracy': 0.8958333333333334, 'eval_runtime': 130.2566, 'eval_samples_per_second': 4.898, 'eval_steps_per_second': 4.898, 'epoch': 2.65}
********************save call back********************
********************on step end call back********************
Step 14610 finish
{'loss': 0.3977, 'grad_norm': 1.1459028720855713, 'learning_rate': 7.397435897435898e-05, 'epoch': 2.66}
********************on step end call back********************
Step 14620 finish
{'loss': 0.383, 'grad_norm': 1.1009505987167358, 'learning_rate': 7.395604395604396e-05, 'epoch': 2.66}
********************on step end call back********************
Step 14630 finish
[INFO|trainer.py:3376] 2024-03-22 23:14:22,907 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-22 23:14:22,907 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-22 23:14:22,907 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-22 23:16:33,234 >> Saving model checkpoint to ./output/tmp-checkpoint-14700
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-22 23:16:33,379 >> tokenizer config file saved in ./output/tmp-checkpoint-14700/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-22 23:16:33,379 >> Special tokens file saved in ./output/tmp-checkpoint-14700/special_tokens_map.json
[INFO|trainer.py:3159] 2024-03-22 23:16:33,580 >> Deleting older checkpoint [output/checkpoint-4700] due to args.save_total_limit
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-22 23:25:23,267 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-22 23:25:23,267 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-22 23:25:23,267 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-22 23:27:33,569 >> Saving model checkpoint to ./output/tmp-checkpoint-14800
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-22 23:27:33,713 >> tokenizer config file saved in ./output/tmp-checkpoint-14800/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-22 23:27:33,714 >> Special tokens file saved in ./output/tmp-checkpoint-14800/special_tokens_map.json
[INFO|trainer.py:3159] 2024-03-22 23:27:33,909 >> Deleting older checkpoint [output/checkpoint-4800] due to args.save_total_limit
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-22 23:36:20,492 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-22 23:36:20,492 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-22 23:36:20,492 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-22 23:38:30,808 >> Saving model checkpoint to ./output/tmp-checkpoint-14900
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-22 23:38:30,955 >> tokenizer config file saved in ./output/tmp-checkpoint-14900/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-22 23:38:30,955 >> Special tokens file saved in ./output/tmp-checkpoint-14900/special_tokens_map.json
[INFO|trainer.py:3159] 2024-03-22 23:38:31,154 >> Deleting older checkpoint [output/checkpoint-4900] due to args.save_total_limit
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-22 23:47:14,373 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-22 23:47:14,373 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-22 23:47:14,373 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-22 23:49:24,631 >> Saving model checkpoint to ./output/tmp-checkpoint-15000
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-22 23:49:24,776 >> tokenizer config file saved in ./output/tmp-checkpoint-15000/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-22 23:49:24,777 >> Special tokens file saved in ./output/tmp-checkpoint-15000/special_tokens_map.json
[INFO|trainer.py:3159] 2024-03-22 23:49:24,977 >> Deleting older checkpoint [output/checkpoint-5000] due to args.save_total_limit
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
{'loss': 0.3221, 'grad_norm': 1.0904489755630493, 'learning_rate': 7.393772893772895e-05, 'epoch': 2.66}
********************on step end call back********************
Step 14640 finish
{'loss': 0.385, 'grad_norm': 1.0248432159423828, 'learning_rate': 7.391941391941393e-05, 'epoch': 2.66}
********************on step end call back********************
Step 14650 finish
{'loss': 0.3914, 'grad_norm': 0.9745180010795593, 'learning_rate': 7.390109890109891e-05, 'epoch': 2.66}
********************on step end call back********************
Step 14660 finish
{'loss': 0.3763, 'grad_norm': 1.1775810718536377, 'learning_rate': 7.388278388278389e-05, 'epoch': 2.67}
********************on step end call back********************
Step 14670 finish
{'loss': 0.3709, 'grad_norm': 1.5354061126708984, 'learning_rate': 7.386446886446887e-05, 'epoch': 2.67}
********************on step end call back********************
Step 14680 finish
{'loss': 0.3629, 'grad_norm': 1.0340684652328491, 'learning_rate': 7.384615384615386e-05, 'epoch': 2.67}
********************on step end call back********************
Step 14690 finish
{'loss': 0.4055, 'grad_norm': 1.0559465885162354, 'learning_rate': 7.382783882783884e-05, 'epoch': 2.67}
********************on step end call back********************
Step 14700 finish
{'loss': 0.3986, 'grad_norm': 1.4039922952651978, 'learning_rate': 7.380952380952382e-05, 'epoch': 2.67}
{'eval_loss': 0.3191298842430115, 'eval_accuracy': 0.8958333333333334, 'eval_runtime': 130.3255, 'eval_samples_per_second': 4.895, 'eval_steps_per_second': 4.895, 'epoch': 2.67}
********************save call back********************
********************on step end call back********************
Step 14710 finish
{'loss': 0.3825, 'grad_norm': 1.2836861610412598, 'learning_rate': 7.37912087912088e-05, 'epoch': 2.67}
********************on step end call back********************
Step 14720 finish
{'loss': 0.3408, 'grad_norm': 0.95176100730896, 'learning_rate': 7.377289377289379e-05, 'epoch': 2.68}
********************on step end call back********************
Step 14730 finish
{'loss': 0.35, 'grad_norm': 1.1503452062606812, 'learning_rate': 7.375457875457877e-05, 'epoch': 2.68}
********************on step end call back********************
Step 14740 finish
{'loss': 0.3526, 'grad_norm': 1.1947134733200073, 'learning_rate': 7.373626373626374e-05, 'epoch': 2.68}
********************on step end call back********************
Step 14750 finish
{'loss': 0.3658, 'grad_norm': 1.029953122138977, 'learning_rate': 7.371794871794872e-05, 'epoch': 2.68}
********************on step end call back********************
Step 14760 finish
{'loss': 0.3762, 'grad_norm': 0.9898970723152161, 'learning_rate': 7.36996336996337e-05, 'epoch': 2.68}
********************on step end call back********************
Step 14770 finish
{'loss': 0.3912, 'grad_norm': 1.2312474250793457, 'learning_rate': 7.36813186813187e-05, 'epoch': 2.69}
********************on step end call back********************
Step 14780 finish
{'loss': 0.3793, 'grad_norm': 0.9359196424484253, 'learning_rate': 7.366300366300367e-05, 'epoch': 2.69}
********************on step end call back********************
Step 14790 finish
{'loss': 0.363, 'grad_norm': 1.2353805303573608, 'learning_rate': 7.364468864468865e-05, 'epoch': 2.69}
********************on step end call back********************
Step 14800 finish
{'loss': 0.3836, 'grad_norm': 1.2487794160842896, 'learning_rate': 7.362637362637363e-05, 'epoch': 2.69}
{'eval_loss': 0.3235739469528198, 'eval_accuracy': 0.8854166666666666, 'eval_runtime': 130.3013, 'eval_samples_per_second': 4.896, 'eval_steps_per_second': 4.896, 'epoch': 2.69}
********************save call back********************
********************on step end call back********************
Step 14810 finish
{'loss': 0.3472, 'grad_norm': 1.1687078475952148, 'learning_rate': 7.360805860805862e-05, 'epoch': 2.69}
********************on step end call back********************
Step 14820 finish
{'loss': 0.3619, 'grad_norm': 1.3243595361709595, 'learning_rate': 7.35897435897436e-05, 'epoch': 2.69}
********************on step end call back********************
Step 14830 finish
{'loss': 0.4141, 'grad_norm': 1.0886015892028809, 'learning_rate': 7.357142857142858e-05, 'epoch': 2.7}
********************on step end call back********************
Step 14840 finish
{'loss': 0.401, 'grad_norm': 0.9314165711402893, 'learning_rate': 7.355311355311356e-05, 'epoch': 2.7}
********************on step end call back********************
Step 14850 finish
{'loss': 0.3858, 'grad_norm': 1.3004910945892334, 'learning_rate': 7.353479853479854e-05, 'epoch': 2.7}
********************on step end call back********************
Step 14860 finish
{'loss': 0.3815, 'grad_norm': 1.1045513153076172, 'learning_rate': 7.351648351648353e-05, 'epoch': 2.7}
********************on step end call back********************
Step 14870 finish
{'loss': 0.358, 'grad_norm': 1.074429988861084, 'learning_rate': 7.349816849816851e-05, 'epoch': 2.7}
********************on step end call back********************
Step 14880 finish
{'loss': 0.3428, 'grad_norm': 0.9169090390205383, 'learning_rate': 7.347985347985349e-05, 'epoch': 2.71}
********************on step end call back********************
Step 14890 finish
{'loss': 0.407, 'grad_norm': 1.236341953277588, 'learning_rate': 7.346153846153847e-05, 'epoch': 2.71}
********************on step end call back********************
Step 14900 finish
{'loss': 0.4233, 'grad_norm': 0.912883460521698, 'learning_rate': 7.344322344322346e-05, 'epoch': 2.71}
{'eval_loss': 0.322681188583374, 'eval_accuracy': 0.8958333333333334, 'eval_runtime': 130.3142, 'eval_samples_per_second': 4.896, 'eval_steps_per_second': 4.896, 'epoch': 2.71}
********************save call back********************
********************on step end call back********************
Step 14910 finish
{'loss': 0.3751, 'grad_norm': 1.1023170948028564, 'learning_rate': 7.342490842490842e-05, 'epoch': 2.71}
********************on step end call back********************
Step 14920 finish
{'loss': 0.3749, 'grad_norm': 0.87830650806427, 'learning_rate': 7.34065934065934e-05, 'epoch': 2.71}
********************on step end call back********************
Step 14930 finish
{'loss': 0.3774, 'grad_norm': 1.4678072929382324, 'learning_rate': 7.338827838827838e-05, 'epoch': 2.71}
********************on step end call back********************
Step 14940 finish
{'loss': 0.4177, 'grad_norm': 0.9772841930389404, 'learning_rate': 7.336996336996337e-05, 'epoch': 2.72}
********************on step end call back********************
Step 14950 finish
{'loss': 0.3749, 'grad_norm': 1.0214873552322388, 'learning_rate': 7.335164835164835e-05, 'epoch': 2.72}
********************on step end call back********************
Step 14960 finish
{'loss': 0.3556, 'grad_norm': 0.9934154152870178, 'learning_rate': 7.333333333333333e-05, 'epoch': 2.72}
********************on step end call back********************
Step 14970 finish
{'loss': 0.38, 'grad_norm': 0.929600179195404, 'learning_rate': 7.331501831501831e-05, 'epoch': 2.72}
********************on step end call back********************
Step 14980 finish
{'loss': 0.375, 'grad_norm': 0.641916811466217, 'learning_rate': 7.329670329670329e-05, 'epoch': 2.72}
********************on step end call back********************
Step 14990 finish
{'loss': 0.3266, 'grad_norm': 1.0186481475830078, 'learning_rate': 7.327838827838828e-05, 'epoch': 2.73}
********************on step end call back********************
Step 15000 finish
{'loss': 0.3226, 'grad_norm': 1.0397586822509766, 'learning_rate': 7.326007326007326e-05, 'epoch': 2.73}
{'eval_loss': 0.31948915123939514, 'eval_accuracy': 0.8958333333333334, 'eval_runtime': 130.257, 'eval_samples_per_second': 4.898, 'eval_steps_per_second': 4.898, 'epoch': 2.73}
********************save call back********************
********************on step end call back********************
Step 15010 finish
{'loss': 0.394, 'grad_norm': 1.0364936590194702, 'learning_rate': 7.324175824175824e-05, 'epoch': 2.73}
********************on step end call back********************
Step 15020 finish
[INFO|trainer.py:3376] 2024-03-22 23:58:08,063 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-22 23:58:08,063 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-22 23:58:08,063 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-23 00:00:18,219 >> Saving model checkpoint to ./output/tmp-checkpoint-15100
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-23 00:00:18,363 >> tokenizer config file saved in ./output/tmp-checkpoint-15100/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-23 00:00:18,364 >> Special tokens file saved in ./output/tmp-checkpoint-15100/special_tokens_map.json
[INFO|trainer.py:3159] 2024-03-23 00:00:18,563 >> Deleting older checkpoint [output/checkpoint-5100] due to args.save_total_limit
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-23 00:08:55,980 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-23 00:08:55,980 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-23 00:08:55,980 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-23 00:11:06,259 >> Saving model checkpoint to ./output/tmp-checkpoint-15200
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-23 00:11:06,404 >> tokenizer config file saved in ./output/tmp-checkpoint-15200/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-23 00:11:06,405 >> Special tokens file saved in ./output/tmp-checkpoint-15200/special_tokens_map.json
[INFO|trainer.py:3159] 2024-03-23 00:11:06,611 >> Deleting older checkpoint [output/checkpoint-5200] due to args.save_total_limit
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-23 00:19:44,212 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-23 00:19:44,212 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-23 00:19:44,212 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-23 00:21:53,804 >> Saving model checkpoint to ./output/tmp-checkpoint-15300
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-23 00:21:53,947 >> tokenizer config file saved in ./output/tmp-checkpoint-15300/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-23 00:21:53,947 >> Special tokens file saved in ./output/tmp-checkpoint-15300/special_tokens_map.json
[INFO|trainer.py:3159] 2024-03-23 00:21:54,173 >> Deleting older checkpoint [output/checkpoint-5300] due to args.save_total_limit
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-23 00:30:35,498 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-23 00:30:35,499 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-23 00:30:35,499 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-23 00:32:44,598 >> Saving model checkpoint to ./output/tmp-checkpoint-15400
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-23 00:32:44,738 >> tokenizer config file saved in ./output/tmp-checkpoint-15400/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-23 00:32:44,738 >> Special tokens file saved in ./output/tmp-checkpoint-15400/special_tokens_map.json
[INFO|trainer.py:3159] 2024-03-23 00:32:44,932 >> Deleting older checkpoint [output/checkpoint-5400] due to args.save_total_limit
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
{'loss': 0.4241, 'grad_norm': 1.0154370069503784, 'learning_rate': 7.322344322344322e-05, 'epoch': 2.73}
********************on step end call back********************
Step 15030 finish
{'loss': 0.409, 'grad_norm': 0.9088729023933411, 'learning_rate': 7.320512820512821e-05, 'epoch': 2.73}
********************on step end call back********************
Step 15040 finish
{'loss': 0.3879, 'grad_norm': 1.3026702404022217, 'learning_rate': 7.318681318681319e-05, 'epoch': 2.73}
********************on step end call back********************
Step 15050 finish
{'loss': 0.3791, 'grad_norm': 1.2890876531600952, 'learning_rate': 7.316849816849817e-05, 'epoch': 2.74}
********************on step end call back********************
Step 15060 finish
{'loss': 0.3376, 'grad_norm': 0.7450641989707947, 'learning_rate': 7.315018315018315e-05, 'epoch': 2.74}
********************on step end call back********************
Step 15070 finish
{'loss': 0.3653, 'grad_norm': 1.2973394393920898, 'learning_rate': 7.313186813186812e-05, 'epoch': 2.74}
********************on step end call back********************
Step 15080 finish
{'loss': 0.4056, 'grad_norm': 1.1352806091308594, 'learning_rate': 7.311355311355312e-05, 'epoch': 2.74}
********************on step end call back********************
Step 15090 finish
{'loss': 0.3722, 'grad_norm': 1.2727224826812744, 'learning_rate': 7.30952380952381e-05, 'epoch': 2.74}
********************on step end call back********************
Step 15100 finish
{'loss': 0.3513, 'grad_norm': 0.9977547526359558, 'learning_rate': 7.307692307692307e-05, 'epoch': 2.75}
{'eval_loss': 0.3126123249530792, 'eval_accuracy': 0.8854166666666666, 'eval_runtime': 130.1544, 'eval_samples_per_second': 4.902, 'eval_steps_per_second': 4.902, 'epoch': 2.75}
********************save call back********************
********************on step end call back********************
Step 15110 finish
{'loss': 0.3873, 'grad_norm': 0.9617816805839539, 'learning_rate': 7.305860805860805e-05, 'epoch': 2.75}
********************on step end call back********************
Step 15120 finish
{'loss': 0.3262, 'grad_norm': 1.0930064916610718, 'learning_rate': 7.304029304029305e-05, 'epoch': 2.75}
********************on step end call back********************
Step 15130 finish
{'loss': 0.4247, 'grad_norm': 1.0333040952682495, 'learning_rate': 7.302197802197802e-05, 'epoch': 2.75}
********************on step end call back********************
Step 15140 finish
{'loss': 0.3974, 'grad_norm': 1.0349844694137573, 'learning_rate': 7.3003663003663e-05, 'epoch': 2.75}
********************on step end call back********************
Step 15150 finish
{'loss': 0.3539, 'grad_norm': 1.095536708831787, 'learning_rate': 7.298534798534798e-05, 'epoch': 2.75}
********************on step end call back********************
Step 15160 finish
{'loss': 0.4132, 'grad_norm': 0.8959251642227173, 'learning_rate': 7.296703296703296e-05, 'epoch': 2.76}
********************on step end call back********************
Step 15170 finish
{'loss': 0.349, 'grad_norm': 0.769686758518219, 'learning_rate': 7.294871794871795e-05, 'epoch': 2.76}
********************on step end call back********************
Step 15180 finish
{'loss': 0.3493, 'grad_norm': 1.2069553136825562, 'learning_rate': 7.293040293040293e-05, 'epoch': 2.76}
********************on step end call back********************
Step 15190 finish
{'loss': 0.3406, 'grad_norm': 1.057942509651184, 'learning_rate': 7.291208791208791e-05, 'epoch': 2.76}
********************on step end call back********************
Step 15200 finish
{'loss': 0.3651, 'grad_norm': 1.075671911239624, 'learning_rate': 7.289377289377289e-05, 'epoch': 2.76}
{'eval_loss': 0.3189013600349426, 'eval_accuracy': 0.8958333333333334, 'eval_runtime': 130.2787, 'eval_samples_per_second': 4.897, 'eval_steps_per_second': 4.897, 'epoch': 2.76}
********************save call back********************
********************on step end call back********************
Step 15210 finish
{'loss': 0.3377, 'grad_norm': 1.0273959636688232, 'learning_rate': 7.287545787545788e-05, 'epoch': 2.77}
********************on step end call back********************
Step 15220 finish
{'loss': 0.3867, 'grad_norm': 1.2491294145584106, 'learning_rate': 7.285714285714286e-05, 'epoch': 2.77}
********************on step end call back********************
Step 15230 finish
{'loss': 0.4405, 'grad_norm': 1.4692188501358032, 'learning_rate': 7.283882783882784e-05, 'epoch': 2.77}
********************on step end call back********************
Step 15240 finish
{'loss': 0.3798, 'grad_norm': 1.1217427253723145, 'learning_rate': 7.282051282051282e-05, 'epoch': 2.77}
********************on step end call back********************
Step 15250 finish
{'loss': 0.3076, 'grad_norm': 0.8723516464233398, 'learning_rate': 7.28021978021978e-05, 'epoch': 2.77}
********************on step end call back********************
Step 15260 finish
{'loss': 0.3626, 'grad_norm': 0.9371201992034912, 'learning_rate': 7.278388278388279e-05, 'epoch': 2.77}
********************on step end call back********************
Step 15270 finish
{'loss': 0.354, 'grad_norm': 0.7952837347984314, 'learning_rate': 7.276556776556777e-05, 'epoch': 2.78}
********************on step end call back********************
Step 15280 finish
{'loss': 0.3516, 'grad_norm': 1.1958519220352173, 'learning_rate': 7.274725274725275e-05, 'epoch': 2.78}
********************on step end call back********************
Step 15290 finish
{'loss': 0.3568, 'grad_norm': 0.9438504576683044, 'learning_rate': 7.272893772893773e-05, 'epoch': 2.78}
********************on step end call back********************
Step 15300 finish
{'loss': 0.4163, 'grad_norm': 1.173101782798767, 'learning_rate': 7.271062271062272e-05, 'epoch': 2.78}
{'eval_loss': 0.3270002007484436, 'eval_accuracy': 0.8854166666666666, 'eval_runtime': 129.5907, 'eval_samples_per_second': 4.923, 'eval_steps_per_second': 4.923, 'epoch': 2.78}
********************save call back********************
********************on step end call back********************
Step 15310 finish
{'loss': 0.3636, 'grad_norm': 1.0028948783874512, 'learning_rate': 7.26923076923077e-05, 'epoch': 2.78}
********************on step end call back********************
Step 15320 finish
{'loss': 0.4033, 'grad_norm': 1.0126588344573975, 'learning_rate': 7.267399267399268e-05, 'epoch': 2.79}
********************on step end call back********************
Step 15330 finish
{'loss': 0.4147, 'grad_norm': 1.0966789722442627, 'learning_rate': 7.265567765567765e-05, 'epoch': 2.79}
********************on step end call back********************
Step 15340 finish
{'loss': 0.3898, 'grad_norm': 1.1258444786071777, 'learning_rate': 7.263736263736263e-05, 'epoch': 2.79}
********************on step end call back********************
Step 15350 finish
{'loss': 0.372, 'grad_norm': 1.1554954051971436, 'learning_rate': 7.261904761904762e-05, 'epoch': 2.79}
********************on step end call back********************
Step 15360 finish
{'loss': 0.3883, 'grad_norm': 1.3799229860305786, 'learning_rate': 7.26007326007326e-05, 'epoch': 2.79}
********************on step end call back********************
Step 15370 finish
{'loss': 0.4048, 'grad_norm': 1.4203901290893555, 'learning_rate': 7.258241758241758e-05, 'epoch': 2.79}
********************on step end call back********************
Step 15380 finish
{'loss': 0.366, 'grad_norm': 1.0255383253097534, 'learning_rate': 7.256410256410256e-05, 'epoch': 2.8}
********************on step end call back********************
Step 15390 finish
{'loss': 0.3827, 'grad_norm': 1.1645104885101318, 'learning_rate': 7.254578754578755e-05, 'epoch': 2.8}
********************on step end call back********************
Step 15400 finish
{'loss': 0.3538, 'grad_norm': 0.8067223429679871, 'learning_rate': 7.252747252747253e-05, 'epoch': 2.8}
{'eval_loss': 0.32185864448547363, 'eval_accuracy': 0.90625, 'eval_runtime': 129.0984, 'eval_samples_per_second': 4.942, 'eval_steps_per_second': 4.942, 'epoch': 2.8}
********************save call back********************
********************on step end call back********************
Step 15410 finish
[INFO|trainer.py:3376] 2024-03-23 00:41:29,837 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-23 00:41:29,837 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-23 00:41:29,837 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-23 00:43:39,377 >> Saving model checkpoint to ./output/tmp-checkpoint-15500
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-23 00:43:39,520 >> tokenizer config file saved in ./output/tmp-checkpoint-15500/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-23 00:43:39,520 >> Special tokens file saved in ./output/tmp-checkpoint-15500/special_tokens_map.json
[INFO|trainer.py:3159] 2024-03-23 00:43:39,717 >> Deleting older checkpoint [output/checkpoint-5500] due to args.save_total_limit
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-23 00:52:19,326 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-23 00:52:19,327 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-23 00:52:19,327 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-23 00:54:28,810 >> Saving model checkpoint to ./output/tmp-checkpoint-15600
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-23 00:54:28,954 >> tokenizer config file saved in ./output/tmp-checkpoint-15600/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-23 00:54:28,954 >> Special tokens file saved in ./output/tmp-checkpoint-15600/special_tokens_map.json
[INFO|trainer.py:3159] 2024-03-23 00:54:29,150 >> Deleting older checkpoint [output/checkpoint-5600] due to args.save_total_limit
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-23 01:03:08,472 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-23 01:03:08,472 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-23 01:03:08,472 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-23 01:05:18,254 >> Saving model checkpoint to ./output/tmp-checkpoint-15700
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-23 01:05:18,397 >> tokenizer config file saved in ./output/tmp-checkpoint-15700/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-23 01:05:18,397 >> Special tokens file saved in ./output/tmp-checkpoint-15700/special_tokens_map.json
[INFO|trainer.py:3159] 2024-03-23 01:05:18,596 >> Deleting older checkpoint [output/checkpoint-5700] due to args.save_total_limit
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-23 01:13:56,301 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-23 01:13:56,301 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-23 01:13:56,301 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-23 01:16:06,148 >> Saving model checkpoint to ./output/tmp-checkpoint-15800
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-23 01:16:06,297 >> tokenizer config file saved in ./output/tmp-checkpoint-15800/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-23 01:16:06,298 >> Special tokens file saved in ./output/tmp-checkpoint-15800/special_tokens_map.json
[INFO|trainer.py:3159] 2024-03-23 01:16:06,502 >> Deleting older checkpoint [output/checkpoint-5800] due to args.save_total_limit
{'loss': 0.4082, 'grad_norm': 1.0264368057250977, 'learning_rate': 7.250915750915751e-05, 'epoch': 2.8}
********************on step end call back********************
Step 15420 finish
{'loss': 0.3493, 'grad_norm': 0.8540542721748352, 'learning_rate': 7.249084249084249e-05, 'epoch': 2.8}
********************on step end call back********************
Step 15430 finish
{'loss': 0.3492, 'grad_norm': 1.2404866218566895, 'learning_rate': 7.247252747252747e-05, 'epoch': 2.81}
********************on step end call back********************
Step 15440 finish
{'loss': 0.4121, 'grad_norm': 1.3726905584335327, 'learning_rate': 7.245421245421246e-05, 'epoch': 2.81}
********************on step end call back********************
Step 15450 finish
{'loss': 0.3726, 'grad_norm': 0.9995725750923157, 'learning_rate': 7.243589743589744e-05, 'epoch': 2.81}
********************on step end call back********************
Step 15460 finish
{'loss': 0.355, 'grad_norm': 1.1822110414505005, 'learning_rate': 7.241758241758242e-05, 'epoch': 2.81}
********************on step end call back********************
Step 15470 finish
{'loss': 0.3512, 'grad_norm': 1.4064595699310303, 'learning_rate': 7.23992673992674e-05, 'epoch': 2.81}
********************on step end call back********************
Step 15480 finish
{'loss': 0.3574, 'grad_norm': 0.9191980957984924, 'learning_rate': 7.238095238095238e-05, 'epoch': 2.81}
********************on step end call back********************
Step 15490 finish
{'loss': 0.3517, 'grad_norm': 1.0646600723266602, 'learning_rate': 7.236263736263737e-05, 'epoch': 2.82}
********************on step end call back********************
Step 15500 finish
{'loss': 0.4037, 'grad_norm': 1.115222692489624, 'learning_rate': 7.234432234432235e-05, 'epoch': 2.82}
{'eval_loss': 0.324270099401474, 'eval_accuracy': 0.8958333333333334, 'eval_runtime': 129.5385, 'eval_samples_per_second': 4.925, 'eval_steps_per_second': 4.925, 'epoch': 2.82}
********************save call back********************
********************on step end call back********************
Step 15510 finish
{'loss': 0.3667, 'grad_norm': 1.096593976020813, 'learning_rate': 7.232600732600733e-05, 'epoch': 2.82}
********************on step end call back********************
Step 15520 finish
{'loss': 0.3808, 'grad_norm': 0.8375728726387024, 'learning_rate': 7.23076923076923e-05, 'epoch': 2.82}
********************on step end call back********************
Step 15530 finish
{'loss': 0.3814, 'grad_norm': 1.0304187536239624, 'learning_rate': 7.22893772893773e-05, 'epoch': 2.82}
********************on step end call back********************
Step 15540 finish
{'loss': 0.4184, 'grad_norm': 1.1398742198944092, 'learning_rate': 7.227106227106228e-05, 'epoch': 2.83}
********************on step end call back********************
Step 15550 finish
{'loss': 0.3741, 'grad_norm': 1.2932896614074707, 'learning_rate': 7.225274725274725e-05, 'epoch': 2.83}
********************on step end call back********************
Step 15560 finish
{'loss': 0.3637, 'grad_norm': 1.1880450248718262, 'learning_rate': 7.223443223443223e-05, 'epoch': 2.83}
********************on step end call back********************
Step 15570 finish
{'loss': 0.3912, 'grad_norm': 1.0027728080749512, 'learning_rate': 7.221611721611721e-05, 'epoch': 2.83}
********************on step end call back********************
Step 15580 finish
{'loss': 0.3617, 'grad_norm': 0.9074977040290833, 'learning_rate': 7.21978021978022e-05, 'epoch': 2.83}
********************on step end call back********************
Step 15590 finish
{'loss': 0.423, 'grad_norm': 1.0253409147262573, 'learning_rate': 7.217948717948718e-05, 'epoch': 2.83}
********************on step end call back********************
Step 15600 finish
{'loss': 0.3874, 'grad_norm': 1.033370852470398, 'learning_rate': 7.216117216117216e-05, 'epoch': 2.84}
{'eval_loss': 0.318251371383667, 'eval_accuracy': 0.90625, 'eval_runtime': 129.4826, 'eval_samples_per_second': 4.927, 'eval_steps_per_second': 4.927, 'epoch': 2.84}
********************save call back********************
********************on step end call back********************
Step 15610 finish
{'loss': 0.36, 'grad_norm': 1.1239042282104492, 'learning_rate': 7.214285714285714e-05, 'epoch': 2.84}
********************on step end call back********************
Step 15620 finish
{'loss': 0.3892, 'grad_norm': 1.1889897584915161, 'learning_rate': 7.212454212454213e-05, 'epoch': 2.84}
********************on step end call back********************
Step 15630 finish
{'loss': 0.4095, 'grad_norm': 1.0013870000839233, 'learning_rate': 7.210622710622711e-05, 'epoch': 2.84}
********************on step end call back********************
Step 15640 finish
{'loss': 0.3569, 'grad_norm': 0.9366075992584229, 'learning_rate': 7.208791208791209e-05, 'epoch': 2.84}
********************on step end call back********************
Step 15650 finish
{'loss': 0.3676, 'grad_norm': 0.8284813165664673, 'learning_rate': 7.206959706959707e-05, 'epoch': 2.85}
********************on step end call back********************
Step 15660 finish
{'loss': 0.3855, 'grad_norm': 1.1956937313079834, 'learning_rate': 7.205128205128205e-05, 'epoch': 2.85}
********************on step end call back********************
Step 15670 finish
{'loss': 0.3579, 'grad_norm': 1.0585054159164429, 'learning_rate': 7.203296703296704e-05, 'epoch': 2.85}
********************on step end call back********************
Step 15680 finish
{'loss': 0.3476, 'grad_norm': 1.103998064994812, 'learning_rate': 7.201465201465202e-05, 'epoch': 2.85}
********************on step end call back********************
Step 15690 finish
{'loss': 0.3933, 'grad_norm': 1.1551852226257324, 'learning_rate': 7.1996336996337e-05, 'epoch': 2.85}
********************on step end call back********************
Step 15700 finish
{'loss': 0.3663, 'grad_norm': 0.6976704597473145, 'learning_rate': 7.197802197802198e-05, 'epoch': 2.85}
{'eval_loss': 0.3138103187084198, 'eval_accuracy': 0.8958333333333334, 'eval_runtime': 129.7812, 'eval_samples_per_second': 4.916, 'eval_steps_per_second': 4.916, 'epoch': 2.85}
********************save call back********************
********************on step end call back********************
Step 15710 finish
{'loss': 0.3889, 'grad_norm': 1.1805956363677979, 'learning_rate': 7.195970695970697e-05, 'epoch': 2.86}
********************on step end call back********************
Step 15720 finish
{'loss': 0.3885, 'grad_norm': 1.2204973697662354, 'learning_rate': 7.194139194139195e-05, 'epoch': 2.86}
********************on step end call back********************
Step 15730 finish
{'loss': 0.3431, 'grad_norm': 1.325819492340088, 'learning_rate': 7.192307692307693e-05, 'epoch': 2.86}
********************on step end call back********************
Step 15740 finish
{'loss': 0.4129, 'grad_norm': 1.3382997512817383, 'learning_rate': 7.19047619047619e-05, 'epoch': 2.86}
********************on step end call back********************
Step 15750 finish
{'loss': 0.3767, 'grad_norm': 0.9642620086669922, 'learning_rate': 7.188644688644688e-05, 'epoch': 2.86}
********************on step end call back********************
Step 15760 finish
{'loss': 0.3994, 'grad_norm': 1.2673300504684448, 'learning_rate': 7.186813186813188e-05, 'epoch': 2.87}
********************on step end call back********************
Step 15770 finish
{'loss': 0.3893, 'grad_norm': 1.1654287576675415, 'learning_rate': 7.184981684981685e-05, 'epoch': 2.87}
********************on step end call back********************
Step 15780 finish
{'loss': 0.4322, 'grad_norm': 1.215185523033142, 'learning_rate': 7.183150183150183e-05, 'epoch': 2.87}
********************on step end call back********************
Step 15790 finish
{'loss': 0.3767, 'grad_norm': 1.4612948894500732, 'learning_rate': 7.181318681318681e-05, 'epoch': 2.87}
********************on step end call back********************
Step 15800 finish
{'loss': 0.4029, 'grad_norm': 1.0266437530517578, 'learning_rate': 7.17948717948718e-05, 'epoch': 2.87}
{'eval_loss': 0.3191734552383423, 'eval_accuracy': 0.8645833333333334, 'eval_runtime': 129.8454, 'eval_samples_per_second': 4.914, 'eval_steps_per_second': 4.914, 'epoch': 2.87}
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-23 01:24:50,638 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-23 01:24:50,638 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-23 01:24:50,638 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-23 01:27:00,391 >> Saving model checkpoint to ./output/tmp-checkpoint-15900
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-23 01:27:00,537 >> tokenizer config file saved in ./output/tmp-checkpoint-15900/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-23 01:27:00,537 >> Special tokens file saved in ./output/tmp-checkpoint-15900/special_tokens_map.json
[INFO|trainer.py:3159] 2024-03-23 01:27:00,741 >> Deleting older checkpoint [output/checkpoint-5900] due to args.save_total_limit
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-23 01:35:34,565 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-23 01:35:34,565 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-23 01:35:34,565 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-23 01:37:44,376 >> Saving model checkpoint to ./output/tmp-checkpoint-16000
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-23 01:37:44,521 >> tokenizer config file saved in ./output/tmp-checkpoint-16000/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-23 01:37:44,521 >> Special tokens file saved in ./output/tmp-checkpoint-16000/special_tokens_map.json
[INFO|trainer.py:3159] 2024-03-23 01:37:44,723 >> Deleting older checkpoint [output/checkpoint-6000] due to args.save_total_limit
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-23 01:46:22,220 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-23 01:46:22,220 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-23 01:46:22,220 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-23 01:48:31,885 >> Saving model checkpoint to ./output/tmp-checkpoint-16100
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-23 01:48:32,025 >> tokenizer config file saved in ./output/tmp-checkpoint-16100/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-23 01:48:32,025 >> Special tokens file saved in ./output/tmp-checkpoint-16100/special_tokens_map.json
[INFO|trainer.py:3159] 2024-03-23 01:48:32,223 >> Deleting older checkpoint [output/checkpoint-6100] due to args.save_total_limit
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-23 01:57:08,026 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-23 01:57:08,026 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-23 01:57:08,026 >>   Batch size = 1
********************save call back********************
********************on step end call back********************
Step 15810 finish
{'loss': 0.3766, 'grad_norm': 1.4326021671295166, 'learning_rate': 7.177655677655678e-05, 'epoch': 2.87}
********************on step end call back********************
Step 15820 finish
{'loss': 0.4197, 'grad_norm': 0.8887884616851807, 'learning_rate': 7.175824175824176e-05, 'epoch': 2.88}
********************on step end call back********************
Step 15830 finish
{'loss': 0.3924, 'grad_norm': 0.9723523259162903, 'learning_rate': 7.173992673992674e-05, 'epoch': 2.88}
********************on step end call back********************
Step 15840 finish
{'loss': 0.3906, 'grad_norm': 1.0234438180923462, 'learning_rate': 7.172161172161172e-05, 'epoch': 2.88}
********************on step end call back********************
Step 15850 finish
{'loss': 0.404, 'grad_norm': 0.9168581962585449, 'learning_rate': 7.170329670329671e-05, 'epoch': 2.88}
********************on step end call back********************
Step 15860 finish
{'loss': 0.3595, 'grad_norm': 1.0397950410842896, 'learning_rate': 7.168498168498169e-05, 'epoch': 2.88}
********************on step end call back********************
Step 15870 finish
{'loss': 0.4036, 'grad_norm': 1.164746880531311, 'learning_rate': 7.166666666666667e-05, 'epoch': 2.89}
********************on step end call back********************
Step 15880 finish
{'loss': 0.3875, 'grad_norm': 0.746127724647522, 'learning_rate': 7.164835164835165e-05, 'epoch': 2.89}
********************on step end call back********************
Step 15890 finish
{'loss': 0.4308, 'grad_norm': 1.1564900875091553, 'learning_rate': 7.163003663003664e-05, 'epoch': 2.89}
********************on step end call back********************
Step 15900 finish
{'loss': 0.4127, 'grad_norm': 0.8305511474609375, 'learning_rate': 7.161172161172162e-05, 'epoch': 2.89}
{'eval_loss': 0.32204821705818176, 'eval_accuracy': 0.8645833333333334, 'eval_runtime': 129.7523, 'eval_samples_per_second': 4.917, 'eval_steps_per_second': 4.917, 'epoch': 2.89}
********************save call back********************
********************on step end call back********************
Step 15910 finish
{'loss': 0.3595, 'grad_norm': 1.0824962854385376, 'learning_rate': 7.15934065934066e-05, 'epoch': 2.89}
********************on step end call back********************
Step 15920 finish
{'loss': 0.3789, 'grad_norm': 1.0795154571533203, 'learning_rate': 7.157509157509158e-05, 'epoch': 2.89}
********************on step end call back********************
Step 15930 finish
{'loss': 0.3865, 'grad_norm': 1.504976749420166, 'learning_rate': 7.155677655677656e-05, 'epoch': 2.9}
********************on step end call back********************
Step 15940 finish
{'loss': 0.3761, 'grad_norm': 1.0799272060394287, 'learning_rate': 7.153846153846155e-05, 'epoch': 2.9}
********************on step end call back********************
Step 15950 finish
{'loss': 0.425, 'grad_norm': 1.2008450031280518, 'learning_rate': 7.152014652014653e-05, 'epoch': 2.9}
********************on step end call back********************
Step 15960 finish
{'loss': 0.3713, 'grad_norm': 1.1273771524429321, 'learning_rate': 7.15018315018315e-05, 'epoch': 2.9}
********************on step end call back********************
Step 15970 finish
{'loss': 0.3263, 'grad_norm': 1.0559440851211548, 'learning_rate': 7.148351648351648e-05, 'epoch': 2.9}
********************on step end call back********************
Step 15980 finish
{'loss': 0.328, 'grad_norm': 1.0136970281600952, 'learning_rate': 7.146520146520148e-05, 'epoch': 2.91}
********************on step end call back********************
Step 15990 finish
{'loss': 0.3749, 'grad_norm': 1.105294108390808, 'learning_rate': 7.144688644688646e-05, 'epoch': 2.91}
********************on step end call back********************
Step 16000 finish
{'loss': 0.3432, 'grad_norm': 1.651337742805481, 'learning_rate': 7.142857142857143e-05, 'epoch': 2.91}
{'eval_loss': 0.3215745985507965, 'eval_accuracy': 0.8854166666666666, 'eval_runtime': 129.81, 'eval_samples_per_second': 4.915, 'eval_steps_per_second': 4.915, 'epoch': 2.91}
********************save call back********************
********************on step end call back********************
Step 16010 finish
{'loss': 0.356, 'grad_norm': 0.8418837785720825, 'learning_rate': 7.141025641025641e-05, 'epoch': 2.91}
********************on step end call back********************
Step 16020 finish
{'loss': 0.366, 'grad_norm': 1.527396559715271, 'learning_rate': 7.139194139194139e-05, 'epoch': 2.91}
********************on step end call back********************
Step 16030 finish
{'loss': 0.3986, 'grad_norm': 1.1505993604660034, 'learning_rate': 7.137362637362638e-05, 'epoch': 2.91}
********************on step end call back********************
Step 16040 finish
{'loss': 0.4046, 'grad_norm': 1.0414727926254272, 'learning_rate': 7.135531135531136e-05, 'epoch': 2.92}
********************on step end call back********************
Step 16050 finish
{'loss': 0.3615, 'grad_norm': 1.002436876296997, 'learning_rate': 7.133699633699634e-05, 'epoch': 2.92}
********************on step end call back********************
Step 16060 finish
{'loss': 0.4249, 'grad_norm': 1.3567556142807007, 'learning_rate': 7.131868131868132e-05, 'epoch': 2.92}
********************on step end call back********************
Step 16070 finish
{'loss': 0.3463, 'grad_norm': 1.2335450649261475, 'learning_rate': 7.130036630036631e-05, 'epoch': 2.92}
********************on step end call back********************
Step 16080 finish
{'loss': 0.3858, 'grad_norm': 1.2541594505310059, 'learning_rate': 7.128205128205129e-05, 'epoch': 2.92}
********************on step end call back********************
Step 16090 finish
{'loss': 0.3634, 'grad_norm': 1.2141220569610596, 'learning_rate': 7.126373626373627e-05, 'epoch': 2.93}
********************on step end call back********************
Step 16100 finish
{'loss': 0.3685, 'grad_norm': 1.2364410161972046, 'learning_rate': 7.124542124542125e-05, 'epoch': 2.93}
{'eval_loss': 0.3216748535633087, 'eval_accuracy': 0.875, 'eval_runtime': 129.6643, 'eval_samples_per_second': 4.92, 'eval_steps_per_second': 4.92, 'epoch': 2.93}
********************save call back********************
********************on step end call back********************
Step 16110 finish
{'loss': 0.3907, 'grad_norm': 1.3599380254745483, 'learning_rate': 7.122710622710623e-05, 'epoch': 2.93}
********************on step end call back********************
Step 16120 finish
{'loss': 0.3587, 'grad_norm': 1.3466159105300903, 'learning_rate': 7.120879120879122e-05, 'epoch': 2.93}
********************on step end call back********************
Step 16130 finish
{'loss': 0.3805, 'grad_norm': 1.4265680313110352, 'learning_rate': 7.11904761904762e-05, 'epoch': 2.93}
********************on step end call back********************
Step 16140 finish
{'loss': 0.4597, 'grad_norm': 0.9840874075889587, 'learning_rate': 7.117216117216118e-05, 'epoch': 2.93}
********************on step end call back********************
Step 16150 finish
{'loss': 0.3325, 'grad_norm': 1.1197277307510376, 'learning_rate': 7.115384615384616e-05, 'epoch': 2.94}
********************on step end call back********************
Step 16160 finish
{'loss': 0.3225, 'grad_norm': 0.7882065773010254, 'learning_rate': 7.113553113553115e-05, 'epoch': 2.94}
********************on step end call back********************
Step 16170 finish
{'loss': 0.3699, 'grad_norm': 1.4005032777786255, 'learning_rate': 7.111721611721613e-05, 'epoch': 2.94}
********************on step end call back********************
Step 16180 finish
{'loss': 0.371, 'grad_norm': 1.0324493646621704, 'learning_rate': 7.10989010989011e-05, 'epoch': 2.94}
********************on step end call back********************
Step 16190 finish
{'loss': 0.4441, 'grad_norm': 1.505336046218872, 'learning_rate': 7.108058608058608e-05, 'epoch': 2.94}
********************on step end call back********************
Step 16200 finish
{'loss': 0.3683, 'grad_norm': 0.9148471355438232, 'learning_rate': 7.106227106227106e-05, 'epoch': 2.95}
[INFO|trainer.py:3067] 2024-03-23 01:59:17,675 >> Saving model checkpoint to ./output/tmp-checkpoint-16200
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-23 01:59:17,823 >> tokenizer config file saved in ./output/tmp-checkpoint-16200/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-23 01:59:17,823 >> Special tokens file saved in ./output/tmp-checkpoint-16200/special_tokens_map.json
[INFO|trainer.py:3159] 2024-03-23 01:59:18,029 >> Deleting older checkpoint [output/checkpoint-6200] due to args.save_total_limit
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-23 02:08:01,879 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-23 02:08:01,879 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-23 02:08:01,879 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-23 02:10:11,427 >> Saving model checkpoint to ./output/tmp-checkpoint-16300
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-23 02:10:11,587 >> tokenizer config file saved in ./output/tmp-checkpoint-16300/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-23 02:10:11,587 >> Special tokens file saved in ./output/tmp-checkpoint-16300/special_tokens_map.json
[INFO|trainer.py:3159] 2024-03-23 02:10:11,790 >> Deleting older checkpoint [output/checkpoint-6300] due to args.save_total_limit
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-23 02:18:51,959 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-23 02:18:51,959 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-23 02:18:51,959 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-23 02:21:01,434 >> Saving model checkpoint to ./output/tmp-checkpoint-16400
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-23 02:21:01,578 >> tokenizer config file saved in ./output/tmp-checkpoint-16400/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-23 02:21:01,578 >> Special tokens file saved in ./output/tmp-checkpoint-16400/special_tokens_map.json
[INFO|trainer.py:3159] 2024-03-23 02:21:01,776 >> Deleting older checkpoint [output/checkpoint-6400] due to args.save_total_limit
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-23 02:29:33,805 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-23 02:29:33,806 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-23 02:29:33,806 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-23 02:31:43,169 >> Saving model checkpoint to ./output/tmp-checkpoint-16500
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-23 02:31:43,314 >> tokenizer config file saved in ./output/tmp-checkpoint-16500/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-23 02:31:43,315 >> Special tokens file saved in ./output/tmp-checkpoint-16500/special_tokens_map.json
[INFO|trainer.py:3159] 2024-03-23 02:31:43,513 >> Deleting older checkpoint [output/checkpoint-6500] due to args.save_total_limit
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
{'eval_loss': 0.32067951560020447, 'eval_accuracy': 0.8854166666666666, 'eval_runtime': 129.6483, 'eval_samples_per_second': 4.921, 'eval_steps_per_second': 4.921, 'epoch': 2.95}
********************save call back********************
********************on step end call back********************
Step 16210 finish
{'loss': 0.3925, 'grad_norm': 1.0071001052856445, 'learning_rate': 7.104395604395606e-05, 'epoch': 2.95}
********************on step end call back********************
Step 16220 finish
{'loss': 0.348, 'grad_norm': 0.8293119072914124, 'learning_rate': 7.102564102564103e-05, 'epoch': 2.95}
********************on step end call back********************
Step 16230 finish
{'loss': 0.4009, 'grad_norm': 0.9952884316444397, 'learning_rate': 7.100732600732601e-05, 'epoch': 2.95}
********************on step end call back********************
Step 16240 finish
{'loss': 0.401, 'grad_norm': 1.3947659730911255, 'learning_rate': 7.098901098901099e-05, 'epoch': 2.95}
********************on step end call back********************
Step 16250 finish
{'loss': 0.3897, 'grad_norm': 1.1881563663482666, 'learning_rate': 7.097069597069597e-05, 'epoch': 2.95}
********************on step end call back********************
Step 16260 finish
{'loss': 0.4101, 'grad_norm': 1.1777734756469727, 'learning_rate': 7.095238095238096e-05, 'epoch': 2.96}
********************on step end call back********************
Step 16270 finish
{'loss': 0.3687, 'grad_norm': 1.2692859172821045, 'learning_rate': 7.093406593406594e-05, 'epoch': 2.96}
********************on step end call back********************
Step 16280 finish
{'loss': 0.4051, 'grad_norm': 1.0301568508148193, 'learning_rate': 7.091575091575092e-05, 'epoch': 2.96}
********************on step end call back********************
Step 16290 finish
{'loss': 0.3847, 'grad_norm': 1.0510382652282715, 'learning_rate': 7.08974358974359e-05, 'epoch': 2.96}
********************on step end call back********************
Step 16300 finish
{'loss': 0.3657, 'grad_norm': 0.9280140399932861, 'learning_rate': 7.087912087912089e-05, 'epoch': 2.96}
{'eval_loss': 0.31528475880622864, 'eval_accuracy': 0.8958333333333334, 'eval_runtime': 129.5471, 'eval_samples_per_second': 4.925, 'eval_steps_per_second': 4.925, 'epoch': 2.96}
********************save call back********************
********************on step end call back********************
Step 16310 finish
{'loss': 0.3488, 'grad_norm': 0.944987952709198, 'learning_rate': 7.086080586080587e-05, 'epoch': 2.97}
********************on step end call back********************
Step 16320 finish
{'loss': 0.4145, 'grad_norm': 1.2035913467407227, 'learning_rate': 7.084249084249085e-05, 'epoch': 2.97}
********************on step end call back********************
Step 16330 finish
{'loss': 0.3546, 'grad_norm': 0.8923887014389038, 'learning_rate': 7.082417582417583e-05, 'epoch': 2.97}
********************on step end call back********************
Step 16340 finish
{'loss': 0.3552, 'grad_norm': 0.8980329632759094, 'learning_rate': 7.08058608058608e-05, 'epoch': 2.97}
********************on step end call back********************
Step 16350 finish
{'loss': 0.3718, 'grad_norm': 1.2960960865020752, 'learning_rate': 7.07875457875458e-05, 'epoch': 2.97}
********************on step end call back********************
Step 16360 finish
{'loss': 0.3572, 'grad_norm': 0.899729311466217, 'learning_rate': 7.076923076923078e-05, 'epoch': 2.97}
********************on step end call back********************
Step 16370 finish
{'loss': 0.393, 'grad_norm': 1.049955129623413, 'learning_rate': 7.075091575091576e-05, 'epoch': 2.98}
********************on step end call back********************
Step 16380 finish
{'loss': 0.3493, 'grad_norm': 0.8733271956443787, 'learning_rate': 7.073260073260074e-05, 'epoch': 2.98}
********************on step end call back********************
Step 16390 finish
{'loss': 0.3523, 'grad_norm': 1.011261224746704, 'learning_rate': 7.071428571428573e-05, 'epoch': 2.98}
********************on step end call back********************
Step 16400 finish
{'loss': 0.3398, 'grad_norm': 1.0165849924087524, 'learning_rate': 7.06959706959707e-05, 'epoch': 2.98}
{'eval_loss': 0.3179212808609009, 'eval_accuracy': 0.8958333333333334, 'eval_runtime': 129.4743, 'eval_samples_per_second': 4.928, 'eval_steps_per_second': 4.928, 'epoch': 2.98}
********************save call back********************
********************on step end call back********************
Step 16410 finish
{'loss': 0.4307, 'grad_norm': 1.1302392482757568, 'learning_rate': 7.067765567765568e-05, 'epoch': 2.98}
********************on step end call back********************
Step 16420 finish
{'loss': 0.3922, 'grad_norm': 1.047014832496643, 'learning_rate': 7.065934065934066e-05, 'epoch': 2.99}
********************on step end call back********************
Step 16430 finish
{'loss': 0.3642, 'grad_norm': 0.9471808075904846, 'learning_rate': 7.064102564102564e-05, 'epoch': 2.99}
********************on step end call back********************
Step 16440 finish
{'loss': 0.3613, 'grad_norm': 1.324211835861206, 'learning_rate': 7.062271062271063e-05, 'epoch': 2.99}
********************on step end call back********************
Step 16450 finish
{'loss': 0.3523, 'grad_norm': 1.317677617073059, 'learning_rate': 7.060439560439561e-05, 'epoch': 2.99}
********************on step end call back********************
Step 16460 finish
{'loss': 0.3308, 'grad_norm': 1.032686710357666, 'learning_rate': 7.058608058608059e-05, 'epoch': 2.99}
********************on step end call back********************
Step 16470 finish
{'loss': 0.338, 'grad_norm': 1.2803572416305542, 'learning_rate': 7.056776556776557e-05, 'epoch': 2.99}
********************on step end call back********************
Step 16480 finish
{'loss': 0.3653, 'grad_norm': 1.3652963638305664, 'learning_rate': 7.054945054945056e-05, 'epoch': 3.0}
********************on step end call back********************
Step 16490 finish
{'loss': 0.3895, 'grad_norm': 1.3847930431365967, 'learning_rate': 7.053113553113554e-05, 'epoch': 3.0}
********************on step end call back********************
Step 16500 finish
{'loss': 0.3932, 'grad_norm': 1.1031346321105957, 'learning_rate': 7.051282051282052e-05, 'epoch': 3.0}
{'eval_loss': 0.3250899314880371, 'eval_accuracy': 0.8958333333333334, 'eval_runtime': 129.3628, 'eval_samples_per_second': 4.932, 'eval_steps_per_second': 4.932, 'epoch': 3.0}
********************save call back********************
********************on epoch end call back********************
Epoch 2.9998409271673676 finish
********************on step end call back********************
Step 16510 finish
{'loss': 0.2842, 'grad_norm': 0.9794267416000366, 'learning_rate': 7.04945054945055e-05, 'epoch': 3.0}
********************on step end call back********************
Step 16520 finish
{'loss': 0.3064, 'grad_norm': 1.0607696771621704, 'learning_rate': 7.047619047619048e-05, 'epoch': 3.0}
********************on step end call back********************
Step 16530 finish
{'loss': 0.2918, 'grad_norm': 1.072334885597229, 'learning_rate': 7.045787545787547e-05, 'epoch': 3.01}
********************on step end call back********************
Step 16540 finish
{'loss': 0.3346, 'grad_norm': 1.2296726703643799, 'learning_rate': 7.043956043956045e-05, 'epoch': 3.01}
********************on step end call back********************
Step 16550 finish
{'loss': 0.2889, 'grad_norm': 0.9178013205528259, 'learning_rate': 7.042124542124543e-05, 'epoch': 3.01}
********************on step end call back********************
Step 16560 finish
{'loss': 0.3103, 'grad_norm': 1.4735735654830933, 'learning_rate': 7.040293040293041e-05, 'epoch': 3.01}
********************on step end call back********************
Step 16570 finish
{'loss': 0.3138, 'grad_norm': 1.0072886943817139, 'learning_rate': 7.03846153846154e-05, 'epoch': 3.01}
********************on step end call back********************
Step 16580 finish
{'loss': 0.3522, 'grad_norm': 1.1990694999694824, 'learning_rate': 7.036630036630038e-05, 'epoch': 3.01}
********************on step end call back********************
Step 16590 finish
[INFO|trainer.py:3376] 2024-03-23 02:40:19,133 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-23 02:40:19,133 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-23 02:40:19,133 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-23 02:42:28,574 >> Saving model checkpoint to ./output/tmp-checkpoint-16600
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-23 02:42:28,721 >> tokenizer config file saved in ./output/tmp-checkpoint-16600/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-23 02:42:28,722 >> Special tokens file saved in ./output/tmp-checkpoint-16600/special_tokens_map.json
[INFO|trainer.py:3159] 2024-03-23 02:42:28,920 >> Deleting older checkpoint [output/checkpoint-6600] due to args.save_total_limit
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-23 02:51:14,896 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-23 02:51:14,896 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-23 02:51:14,896 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-23 02:53:24,334 >> Saving model checkpoint to ./output/tmp-checkpoint-16700
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-23 02:53:24,478 >> tokenizer config file saved in ./output/tmp-checkpoint-16700/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-23 02:53:24,479 >> Special tokens file saved in ./output/tmp-checkpoint-16700/special_tokens_map.json
[INFO|trainer.py:3159] 2024-03-23 02:53:24,676 >> Deleting older checkpoint [output/checkpoint-6700] due to args.save_total_limit
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-23 03:02:05,758 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-23 03:02:05,759 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-23 03:02:05,759 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-23 03:04:15,032 >> Saving model checkpoint to ./output/tmp-checkpoint-16800
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-23 03:04:15,179 >> tokenizer config file saved in ./output/tmp-checkpoint-16800/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-23 03:04:15,179 >> Special tokens file saved in ./output/tmp-checkpoint-16800/special_tokens_map.json
[INFO|trainer.py:3159] 2024-03-23 03:04:15,379 >> Deleting older checkpoint [output/checkpoint-6800] due to args.save_total_limit
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-23 03:12:52,201 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-23 03:12:52,201 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-23 03:12:52,201 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-23 03:15:01,760 >> Saving model checkpoint to ./output/tmp-checkpoint-16900
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-23 03:15:01,901 >> tokenizer config file saved in ./output/tmp-checkpoint-16900/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-23 03:15:01,901 >> Special tokens file saved in ./output/tmp-checkpoint-16900/special_tokens_map.json
[INFO|trainer.py:3159] 2024-03-23 03:15:02,097 >> Deleting older checkpoint [output/checkpoint-6900] due to args.save_total_limit
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
{'loss': 0.3516, 'grad_norm': 1.414967656135559, 'learning_rate': 7.034798534798536e-05, 'epoch': 3.02}
********************on step end call back********************
Step 16600 finish
{'loss': 0.3198, 'grad_norm': 1.0158480405807495, 'learning_rate': 7.032967032967034e-05, 'epoch': 3.02}
{'eval_loss': 0.3278777599334717, 'eval_accuracy': 0.8958333333333334, 'eval_runtime': 129.4396, 'eval_samples_per_second': 4.929, 'eval_steps_per_second': 4.929, 'epoch': 3.02}
********************save call back********************
********************on step end call back********************
Step 16610 finish
{'loss': 0.3298, 'grad_norm': 1.0675201416015625, 'learning_rate': 7.031135531135531e-05, 'epoch': 3.02}
********************on step end call back********************
Step 16620 finish
{'loss': 0.3271, 'grad_norm': 1.0689339637756348, 'learning_rate': 7.029304029304029e-05, 'epoch': 3.02}
********************on step end call back********************
Step 16630 finish
{'loss': 0.3191, 'grad_norm': 1.0374783277511597, 'learning_rate': 7.027472527472527e-05, 'epoch': 3.02}
********************on step end call back********************
Step 16640 finish
{'loss': 0.303, 'grad_norm': 1.1104174852371216, 'learning_rate': 7.025641025641025e-05, 'epoch': 3.03}
********************on step end call back********************
Step 16650 finish
{'loss': 0.3035, 'grad_norm': 1.1592572927474976, 'learning_rate': 7.023809523809524e-05, 'epoch': 3.03}
********************on step end call back********************
Step 16660 finish
{'loss': 0.2769, 'grad_norm': 1.0713833570480347, 'learning_rate': 7.021978021978022e-05, 'epoch': 3.03}
********************on step end call back********************
Step 16670 finish
{'loss': 0.3144, 'grad_norm': 0.9089366793632507, 'learning_rate': 7.02014652014652e-05, 'epoch': 3.03}
********************on step end call back********************
Step 16680 finish
{'loss': 0.2888, 'grad_norm': 1.3760740756988525, 'learning_rate': 7.018315018315018e-05, 'epoch': 3.03}
********************on step end call back********************
Step 16690 finish
{'loss': 0.3238, 'grad_norm': 1.081485629081726, 'learning_rate': 7.016483516483516e-05, 'epoch': 3.03}
********************on step end call back********************
Step 16700 finish
{'loss': 0.3079, 'grad_norm': 1.1816308498382568, 'learning_rate': 7.014652014652015e-05, 'epoch': 3.04}
{'eval_loss': 0.33315417170524597, 'eval_accuracy': 0.8958333333333334, 'eval_runtime': 129.4369, 'eval_samples_per_second': 4.929, 'eval_steps_per_second': 4.929, 'epoch': 3.04}
********************save call back********************
********************on step end call back********************
Step 16710 finish
{'loss': 0.3289, 'grad_norm': 1.2163983583450317, 'learning_rate': 7.012820512820513e-05, 'epoch': 3.04}
********************on step end call back********************
Step 16720 finish
{'loss': 0.3099, 'grad_norm': 1.1871395111083984, 'learning_rate': 7.010989010989011e-05, 'epoch': 3.04}
********************on step end call back********************
Step 16730 finish
{'loss': 0.3078, 'grad_norm': 1.114798665046692, 'learning_rate': 7.009157509157509e-05, 'epoch': 3.04}
********************on step end call back********************
Step 16740 finish
{'loss': 0.3095, 'grad_norm': 1.2089213132858276, 'learning_rate': 7.007326007326007e-05, 'epoch': 3.04}
********************on step end call back********************
Step 16750 finish
{'loss': 0.3231, 'grad_norm': 1.151937484741211, 'learning_rate': 7.005494505494506e-05, 'epoch': 3.05}
********************on step end call back********************
Step 16760 finish
{'loss': 0.3309, 'grad_norm': 1.1561397314071655, 'learning_rate': 7.003663003663004e-05, 'epoch': 3.05}
********************on step end call back********************
Step 16770 finish
{'loss': 0.2958, 'grad_norm': 1.0447673797607422, 'learning_rate': 7.001831501831502e-05, 'epoch': 3.05}
********************on step end call back********************
Step 16780 finish
{'loss': 0.2946, 'grad_norm': 1.1713180541992188, 'learning_rate': 7e-05, 'epoch': 3.05}
********************on step end call back********************
Step 16790 finish
{'loss': 0.3335, 'grad_norm': 1.0534813404083252, 'learning_rate': 6.998168498168499e-05, 'epoch': 3.05}
********************on step end call back********************
Step 16800 finish
{'loss': 0.3198, 'grad_norm': 1.1564792394638062, 'learning_rate': 6.996336996336996e-05, 'epoch': 3.05}
{'eval_loss': 0.3288862705230713, 'eval_accuracy': 0.8854166666666666, 'eval_runtime': 129.2727, 'eval_samples_per_second': 4.935, 'eval_steps_per_second': 4.935, 'epoch': 3.05}
********************save call back********************
********************on step end call back********************
Step 16810 finish
{'loss': 0.3264, 'grad_norm': 0.962709903717041, 'learning_rate': 6.994505494505494e-05, 'epoch': 3.06}
********************on step end call back********************
Step 16820 finish
{'loss': 0.3193, 'grad_norm': 1.0549473762512207, 'learning_rate': 6.992673992673992e-05, 'epoch': 3.06}
********************on step end call back********************
Step 16830 finish
{'loss': 0.2981, 'grad_norm': 1.0770667791366577, 'learning_rate': 6.99084249084249e-05, 'epoch': 3.06}
********************on step end call back********************
Step 16840 finish
{'loss': 0.3188, 'grad_norm': 1.1264723539352417, 'learning_rate': 6.98901098901099e-05, 'epoch': 3.06}
********************on step end call back********************
Step 16850 finish
{'loss': 0.2957, 'grad_norm': 1.1734808683395386, 'learning_rate': 6.987179487179487e-05, 'epoch': 3.06}
********************on step end call back********************
Step 16860 finish
{'loss': 0.3131, 'grad_norm': 0.9754955768585205, 'learning_rate': 6.985347985347985e-05, 'epoch': 3.07}
********************on step end call back********************
Step 16870 finish
{'loss': 0.3255, 'grad_norm': 1.0861743688583374, 'learning_rate': 6.983516483516483e-05, 'epoch': 3.07}
********************on step end call back********************
Step 16880 finish
{'loss': 0.2838, 'grad_norm': 1.3065325021743774, 'learning_rate': 6.981684981684982e-05, 'epoch': 3.07}
********************on step end call back********************
Step 16890 finish
{'loss': 0.3199, 'grad_norm': 0.9320018887519836, 'learning_rate': 6.97985347985348e-05, 'epoch': 3.07}
********************on step end call back********************
Step 16900 finish
{'loss': 0.3092, 'grad_norm': 1.2077372074127197, 'learning_rate': 6.978021978021978e-05, 'epoch': 3.07}
{'eval_loss': 0.3355141580104828, 'eval_accuracy': 0.8854166666666666, 'eval_runtime': 129.5583, 'eval_samples_per_second': 4.924, 'eval_steps_per_second': 4.924, 'epoch': 3.07}
********************save call back********************
********************on step end call back********************
Step 16910 finish
{'loss': 0.3145, 'grad_norm': 1.3297919034957886, 'learning_rate': 6.976190476190476e-05, 'epoch': 3.07}
********************on step end call back********************
Step 16920 finish
{'loss': 0.3201, 'grad_norm': 1.103193759918213, 'learning_rate': 6.974358974358974e-05, 'epoch': 3.08}
********************on step end call back********************
Step 16930 finish
{'loss': 0.2887, 'grad_norm': 1.4136110544204712, 'learning_rate': 6.972527472527473e-05, 'epoch': 3.08}
********************on step end call back********************
Step 16940 finish
{'loss': 0.2861, 'grad_norm': 1.3467609882354736, 'learning_rate': 6.970695970695971e-05, 'epoch': 3.08}
********************on step end call back********************
Step 16950 finish
{'loss': 0.2853, 'grad_norm': 1.1331406831741333, 'learning_rate': 6.968864468864469e-05, 'epoch': 3.08}
********************on step end call back********************
Step 16960 finish
{'loss': 0.2903, 'grad_norm': 1.3250679969787598, 'learning_rate': 6.967032967032967e-05, 'epoch': 3.08}
********************on step end call back********************
Step 16970 finish
{'loss': 0.3645, 'grad_norm': 1.0897549390792847, 'learning_rate': 6.965201465201466e-05, 'epoch': 3.09}
********************on step end call back********************
Step 16980 finish
[INFO|trainer.py:3376] 2024-03-23 03:23:34,758 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-23 03:23:34,758 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-23 03:23:34,758 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-23 03:25:44,402 >> Saving model checkpoint to ./output/tmp-checkpoint-17000
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-23 03:25:44,542 >> tokenizer config file saved in ./output/tmp-checkpoint-17000/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-23 03:25:44,542 >> Special tokens file saved in ./output/tmp-checkpoint-17000/special_tokens_map.json
[INFO|trainer.py:3159] 2024-03-23 03:25:44,740 >> Deleting older checkpoint [output/checkpoint-7000] due to args.save_total_limit
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-23 03:34:15,423 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-23 03:34:15,423 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-23 03:34:15,423 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-23 03:36:25,036 >> Saving model checkpoint to ./output/tmp-checkpoint-17100
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-23 03:36:25,178 >> tokenizer config file saved in ./output/tmp-checkpoint-17100/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-23 03:36:25,178 >> Special tokens file saved in ./output/tmp-checkpoint-17100/special_tokens_map.json
[INFO|trainer.py:3159] 2024-03-23 03:36:25,379 >> Deleting older checkpoint [output/checkpoint-7100] due to args.save_total_limit
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-23 03:45:00,158 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-23 03:45:00,158 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-23 03:45:00,158 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-23 03:47:09,781 >> Saving model checkpoint to ./output/tmp-checkpoint-17200
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-23 03:47:09,921 >> tokenizer config file saved in ./output/tmp-checkpoint-17200/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-23 03:47:09,921 >> Special tokens file saved in ./output/tmp-checkpoint-17200/special_tokens_map.json
[INFO|trainer.py:3159] 2024-03-23 03:47:10,115 >> Deleting older checkpoint [output/checkpoint-7200] due to args.save_total_limit
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-23 03:55:47,140 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-23 03:55:47,140 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-23 03:55:47,140 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-23 03:57:55,916 >> Saving model checkpoint to ./output/tmp-checkpoint-17300
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-23 03:57:56,052 >> tokenizer config file saved in ./output/tmp-checkpoint-17300/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-23 03:57:56,052 >> Special tokens file saved in ./output/tmp-checkpoint-17300/special_tokens_map.json
[INFO|trainer.py:3159] 2024-03-23 03:57:56,247 >> Deleting older checkpoint [output/checkpoint-7300] due to args.save_total_limit
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
{'loss': 0.2903, 'grad_norm': 0.9468990564346313, 'learning_rate': 6.963369963369964e-05, 'epoch': 3.09}
********************on step end call back********************
Step 16990 finish
{'loss': 0.2858, 'grad_norm': 1.2285445928573608, 'learning_rate': 6.961538461538462e-05, 'epoch': 3.09}
********************on step end call back********************
Step 17000 finish
{'loss': 0.3242, 'grad_norm': 1.5155904293060303, 'learning_rate': 6.95970695970696e-05, 'epoch': 3.09}
{'eval_loss': 0.3377244472503662, 'eval_accuracy': 0.8958333333333334, 'eval_runtime': 129.643, 'eval_samples_per_second': 4.921, 'eval_steps_per_second': 4.921, 'epoch': 3.09}
********************save call back********************
********************on step end call back********************
Step 17010 finish
{'loss': 0.2809, 'grad_norm': 0.9484976530075073, 'learning_rate': 6.957875457875457e-05, 'epoch': 3.09}
********************on step end call back********************
Step 17020 finish
{'loss': 0.2957, 'grad_norm': 0.9910023212432861, 'learning_rate': 6.956043956043957e-05, 'epoch': 3.09}
********************on step end call back********************
Step 17030 finish
{'loss': 0.315, 'grad_norm': 1.0171828269958496, 'learning_rate': 6.954212454212454e-05, 'epoch': 3.1}
********************on step end call back********************
Step 17040 finish
{'loss': 0.3182, 'grad_norm': 1.1489050388336182, 'learning_rate': 6.952380952380952e-05, 'epoch': 3.1}
********************on step end call back********************
Step 17050 finish
{'loss': 0.3304, 'grad_norm': 1.160192847251892, 'learning_rate': 6.95054945054945e-05, 'epoch': 3.1}
********************on step end call back********************
Step 17060 finish
{'loss': 0.3404, 'grad_norm': 1.057522177696228, 'learning_rate': 6.94871794871795e-05, 'epoch': 3.1}
********************on step end call back********************
Step 17070 finish
{'loss': 0.309, 'grad_norm': 1.34626305103302, 'learning_rate': 6.946886446886447e-05, 'epoch': 3.1}
********************on step end call back********************
Step 17080 finish
{'loss': 0.3497, 'grad_norm': 1.2695457935333252, 'learning_rate': 6.945054945054945e-05, 'epoch': 3.11}
********************on step end call back********************
Step 17090 finish
{'loss': 0.3207, 'grad_norm': 1.085201382637024, 'learning_rate': 6.943223443223443e-05, 'epoch': 3.11}
********************on step end call back********************
Step 17100 finish
{'loss': 0.3363, 'grad_norm': 1.2225887775421143, 'learning_rate': 6.941391941391941e-05, 'epoch': 3.11}
{'eval_loss': 0.33619871735572815, 'eval_accuracy': 0.8854166666666666, 'eval_runtime': 129.6117, 'eval_samples_per_second': 4.922, 'eval_steps_per_second': 4.922, 'epoch': 3.11}
********************save call back********************
********************on step end call back********************
Step 17110 finish
{'loss': 0.3023, 'grad_norm': 1.0663517713546753, 'learning_rate': 6.93956043956044e-05, 'epoch': 3.11}
********************on step end call back********************
Step 17120 finish
{'loss': 0.3023, 'grad_norm': 1.2727521657943726, 'learning_rate': 6.937728937728938e-05, 'epoch': 3.11}
********************on step end call back********************
Step 17130 finish
{'loss': 0.321, 'grad_norm': 0.7465043663978577, 'learning_rate': 6.935897435897436e-05, 'epoch': 3.11}
********************on step end call back********************
Step 17140 finish
{'loss': 0.335, 'grad_norm': 1.1261932849884033, 'learning_rate': 6.934065934065934e-05, 'epoch': 3.12}
********************on step end call back********************
Step 17150 finish
{'loss': 0.3268, 'grad_norm': 1.453595757484436, 'learning_rate': 6.932234432234433e-05, 'epoch': 3.12}
********************on step end call back********************
Step 17160 finish
{'loss': 0.3307, 'grad_norm': 1.2941077947616577, 'learning_rate': 6.930402930402931e-05, 'epoch': 3.12}
********************on step end call back********************
Step 17170 finish
{'loss': 0.3046, 'grad_norm': 0.953489363193512, 'learning_rate': 6.928571428571429e-05, 'epoch': 3.12}
********************on step end call back********************
Step 17180 finish
{'loss': 0.2624, 'grad_norm': 0.8755996823310852, 'learning_rate': 6.926739926739927e-05, 'epoch': 3.12}
********************on step end call back********************
Step 17190 finish
{'loss': 0.3169, 'grad_norm': 1.1973891258239746, 'learning_rate': 6.924908424908424e-05, 'epoch': 3.13}
********************on step end call back********************
Step 17200 finish
{'loss': 0.332, 'grad_norm': 1.1808339357376099, 'learning_rate': 6.923076923076924e-05, 'epoch': 3.13}
{'eval_loss': 0.3424313962459564, 'eval_accuracy': 0.875, 'eval_runtime': 129.622, 'eval_samples_per_second': 4.922, 'eval_steps_per_second': 4.922, 'epoch': 3.13}
********************save call back********************
********************on step end call back********************
Step 17210 finish
{'loss': 0.3217, 'grad_norm': 1.2409557104110718, 'learning_rate': 6.921245421245422e-05, 'epoch': 3.13}
********************on step end call back********************
Step 17220 finish
{'loss': 0.312, 'grad_norm': 1.2070224285125732, 'learning_rate': 6.91941391941392e-05, 'epoch': 3.13}
********************on step end call back********************
Step 17230 finish
{'loss': 0.2987, 'grad_norm': 1.418392539024353, 'learning_rate': 6.917582417582417e-05, 'epoch': 3.13}
********************on step end call back********************
Step 17240 finish
{'loss': 0.3084, 'grad_norm': 1.1650272607803345, 'learning_rate': 6.915750915750917e-05, 'epoch': 3.13}
********************on step end call back********************
Step 17250 finish
{'loss': 0.3066, 'grad_norm': 0.951508641242981, 'learning_rate': 6.913919413919414e-05, 'epoch': 3.14}
********************on step end call back********************
Step 17260 finish
{'loss': 0.356, 'grad_norm': 1.1392594575881958, 'learning_rate': 6.912087912087912e-05, 'epoch': 3.14}
********************on step end call back********************
Step 17270 finish
{'loss': 0.3336, 'grad_norm': 0.8392098546028137, 'learning_rate': 6.91025641025641e-05, 'epoch': 3.14}
********************on step end call back********************
Step 17280 finish
{'loss': 0.3066, 'grad_norm': 0.9824003577232361, 'learning_rate': 6.908424908424908e-05, 'epoch': 3.14}
********************on step end call back********************
Step 17290 finish
{'loss': 0.354, 'grad_norm': 1.4301695823669434, 'learning_rate': 6.906593406593407e-05, 'epoch': 3.14}
********************on step end call back********************
Step 17300 finish
{'loss': 0.3226, 'grad_norm': 1.434968113899231, 'learning_rate': 6.904761904761905e-05, 'epoch': 3.15}
{'eval_loss': 0.34120386838912964, 'eval_accuracy': 0.875, 'eval_runtime': 128.7748, 'eval_samples_per_second': 4.954, 'eval_steps_per_second': 4.954, 'epoch': 3.15}
********************save call back********************
********************on step end call back********************
Step 17310 finish
{'loss': 0.3236, 'grad_norm': 1.1649836301803589, 'learning_rate': 6.902930402930403e-05, 'epoch': 3.15}
********************on step end call back********************
Step 17320 finish
{'loss': 0.2892, 'grad_norm': 1.2304209470748901, 'learning_rate': 6.901098901098901e-05, 'epoch': 3.15}
********************on step end call back********************
Step 17330 finish
{'loss': 0.3522, 'grad_norm': 0.891278088092804, 'learning_rate': 6.8992673992674e-05, 'epoch': 3.15}
********************on step end call back********************
Step 17340 finish
{'loss': 0.3131, 'grad_norm': 1.3567373752593994, 'learning_rate': 6.897435897435898e-05, 'epoch': 3.15}
********************on step end call back********************
Step 17350 finish
{'loss': 0.3211, 'grad_norm': 0.8001877069473267, 'learning_rate': 6.895604395604396e-05, 'epoch': 3.15}
********************on step end call back********************
Step 17360 finish
{'loss': 0.3454, 'grad_norm': 1.0330324172973633, 'learning_rate': 6.893772893772894e-05, 'epoch': 3.16}
********************on step end call back********************
Step 17370 finish
[INFO|trainer.py:3376] 2024-03-23 04:06:32,936 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-23 04:06:32,936 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-23 04:06:32,936 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-23 04:08:41,600 >> Saving model checkpoint to ./output/tmp-checkpoint-17400
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-23 04:08:41,739 >> tokenizer config file saved in ./output/tmp-checkpoint-17400/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-23 04:08:41,740 >> Special tokens file saved in ./output/tmp-checkpoint-17400/special_tokens_map.json
[INFO|trainer.py:3159] 2024-03-23 04:08:41,935 >> Deleting older checkpoint [output/checkpoint-7400] due to args.save_total_limit
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-23 04:17:16,455 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-23 04:17:16,455 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-23 04:17:16,455 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-23 04:19:25,288 >> Saving model checkpoint to ./output/tmp-checkpoint-17500
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-23 04:19:25,427 >> tokenizer config file saved in ./output/tmp-checkpoint-17500/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-23 04:19:25,427 >> Special tokens file saved in ./output/tmp-checkpoint-17500/special_tokens_map.json
[INFO|trainer.py:3159] 2024-03-23 04:19:25,622 >> Deleting older checkpoint [output/checkpoint-7500] due to args.save_total_limit
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-23 04:28:12,722 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-23 04:28:12,722 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-23 04:28:12,722 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-23 04:30:21,556 >> Saving model checkpoint to ./output/tmp-checkpoint-17600
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-23 04:30:21,698 >> tokenizer config file saved in ./output/tmp-checkpoint-17600/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-23 04:30:21,698 >> Special tokens file saved in ./output/tmp-checkpoint-17600/special_tokens_map.json
[INFO|trainer.py:3159] 2024-03-23 04:30:21,913 >> Deleting older checkpoint [output/checkpoint-7600] due to args.save_total_limit
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-23 04:39:01,701 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-23 04:39:01,701 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-23 04:39:01,701 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-23 04:41:10,598 >> Saving model checkpoint to ./output/tmp-checkpoint-17700
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-23 04:41:10,740 >> tokenizer config file saved in ./output/tmp-checkpoint-17700/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-23 04:41:10,741 >> Special tokens file saved in ./output/tmp-checkpoint-17700/special_tokens_map.json
[INFO|trainer.py:3159] 2024-03-23 04:41:10,938 >> Deleting older checkpoint [output/checkpoint-7700] due to args.save_total_limit
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
{'loss': 0.3334, 'grad_norm': 1.3918678760528564, 'learning_rate': 6.891941391941392e-05, 'epoch': 3.16}
********************on step end call back********************
Step 17380 finish
{'loss': 0.3086, 'grad_norm': 1.1604372262954712, 'learning_rate': 6.890109890109891e-05, 'epoch': 3.16}
********************on step end call back********************
Step 17390 finish
{'loss': 0.3127, 'grad_norm': 1.29550039768219, 'learning_rate': 6.888278388278389e-05, 'epoch': 3.16}
********************on step end call back********************
Step 17400 finish
{'loss': 0.3201, 'grad_norm': 1.3346292972564697, 'learning_rate': 6.886446886446887e-05, 'epoch': 3.16}
{'eval_loss': 0.33375200629234314, 'eval_accuracy': 0.8854166666666666, 'eval_runtime': 128.663, 'eval_samples_per_second': 4.959, 'eval_steps_per_second': 4.959, 'epoch': 3.16}
********************save call back********************
********************on step end call back********************
Step 17410 finish
{'loss': 0.2958, 'grad_norm': 1.0667895078659058, 'learning_rate': 6.884615384615385e-05, 'epoch': 3.17}
********************on step end call back********************
Step 17420 finish
{'loss': 0.2724, 'grad_norm': 1.0270779132843018, 'learning_rate': 6.882783882783882e-05, 'epoch': 3.17}
********************on step end call back********************
Step 17430 finish
{'loss': 0.3058, 'grad_norm': 0.8437949419021606, 'learning_rate': 6.880952380952382e-05, 'epoch': 3.17}
********************on step end call back********************
Step 17440 finish
{'loss': 0.3168, 'grad_norm': 1.0526682138442993, 'learning_rate': 6.87912087912088e-05, 'epoch': 3.17}
********************on step end call back********************
Step 17450 finish
{'loss': 0.3106, 'grad_norm': 1.2911913394927979, 'learning_rate': 6.877289377289377e-05, 'epoch': 3.17}
********************on step end call back********************
Step 17460 finish
{'loss': 0.3057, 'grad_norm': 1.0200886726379395, 'learning_rate': 6.875457875457875e-05, 'epoch': 3.17}
********************on step end call back********************
Step 17470 finish
{'loss': 0.2987, 'grad_norm': 1.4702537059783936, 'learning_rate': 6.873626373626374e-05, 'epoch': 3.18}
********************on step end call back********************
Step 17480 finish
{'loss': 0.3101, 'grad_norm': 1.0940808057785034, 'learning_rate': 6.871794871794872e-05, 'epoch': 3.18}
********************on step end call back********************
Step 17490 finish
{'loss': 0.3314, 'grad_norm': 0.9475674629211426, 'learning_rate': 6.86996336996337e-05, 'epoch': 3.18}
********************on step end call back********************
Step 17500 finish
{'loss': 0.3141, 'grad_norm': 1.0359175205230713, 'learning_rate': 6.868131868131868e-05, 'epoch': 3.18}
{'eval_loss': 0.3329828083515167, 'eval_accuracy': 0.8854166666666666, 'eval_runtime': 128.8316, 'eval_samples_per_second': 4.952, 'eval_steps_per_second': 4.952, 'epoch': 3.18}
********************save call back********************
********************on step end call back********************
Step 17510 finish
{'loss': 0.3405, 'grad_norm': 1.2960453033447266, 'learning_rate': 6.866300366300366e-05, 'epoch': 3.18}
********************on step end call back********************
Step 17520 finish
{'loss': 0.3163, 'grad_norm': 1.0545185804367065, 'learning_rate': 6.864468864468865e-05, 'epoch': 3.19}
********************on step end call back********************
Step 17530 finish
{'loss': 0.3089, 'grad_norm': 1.2334773540496826, 'learning_rate': 6.862637362637363e-05, 'epoch': 3.19}
********************on step end call back********************
Step 17540 finish
{'loss': 0.3334, 'grad_norm': 1.0758540630340576, 'learning_rate': 6.860805860805861e-05, 'epoch': 3.19}
********************on step end call back********************
Step 17550 finish
{'loss': 0.3594, 'grad_norm': 1.2164024114608765, 'learning_rate': 6.858974358974359e-05, 'epoch': 3.19}
********************on step end call back********************
Step 17560 finish
{'loss': 0.3134, 'grad_norm': 0.8965830206871033, 'learning_rate': 6.857142857142858e-05, 'epoch': 3.19}
********************on step end call back********************
Step 17570 finish
{'loss': 0.3155, 'grad_norm': 1.274503469467163, 'learning_rate': 6.855311355311356e-05, 'epoch': 3.19}
********************on step end call back********************
Step 17580 finish
{'loss': 0.3303, 'grad_norm': 0.926603376865387, 'learning_rate': 6.853479853479854e-05, 'epoch': 3.2}
********************on step end call back********************
Step 17590 finish
{'loss': 0.2822, 'grad_norm': 0.9803441166877747, 'learning_rate': 6.851648351648352e-05, 'epoch': 3.2}
********************on step end call back********************
Step 17600 finish
{'loss': 0.3158, 'grad_norm': 1.2130801677703857, 'learning_rate': 6.84981684981685e-05, 'epoch': 3.2}
{'eval_loss': 0.33618882298469543, 'eval_accuracy': 0.8854166666666666, 'eval_runtime': 128.8337, 'eval_samples_per_second': 4.952, 'eval_steps_per_second': 4.952, 'epoch': 3.2}
********************save call back********************
********************on step end call back********************
Step 17610 finish
{'loss': 0.3247, 'grad_norm': 1.294111967086792, 'learning_rate': 6.847985347985349e-05, 'epoch': 3.2}
********************on step end call back********************
Step 17620 finish
{'loss': 0.3076, 'grad_norm': 1.0276364088058472, 'learning_rate': 6.846153846153847e-05, 'epoch': 3.2}
********************on step end call back********************
Step 17630 finish
{'loss': 0.2664, 'grad_norm': 0.9310181736946106, 'learning_rate': 6.844322344322345e-05, 'epoch': 3.21}
********************on step end call back********************
Step 17640 finish
{'loss': 0.3448, 'grad_norm': 0.9437875151634216, 'learning_rate': 6.842490842490842e-05, 'epoch': 3.21}
********************on step end call back********************
Step 17650 finish
{'loss': 0.3034, 'grad_norm': 1.0838444232940674, 'learning_rate': 6.840659340659342e-05, 'epoch': 3.21}
********************on step end call back********************
Step 17660 finish
{'loss': 0.2811, 'grad_norm': 1.5940965414047241, 'learning_rate': 6.83882783882784e-05, 'epoch': 3.21}
********************on step end call back********************
Step 17670 finish
{'loss': 0.3507, 'grad_norm': 0.9867628216743469, 'learning_rate': 6.836996336996337e-05, 'epoch': 3.21}
********************on step end call back********************
Step 17680 finish
{'loss': 0.3041, 'grad_norm': 1.2826935052871704, 'learning_rate': 6.835164835164835e-05, 'epoch': 3.21}
********************on step end call back********************
Step 17690 finish
{'loss': 0.333, 'grad_norm': 1.1258972883224487, 'learning_rate': 6.833333333333333e-05, 'epoch': 3.22}
********************on step end call back********************
Step 17700 finish
{'loss': 0.3254, 'grad_norm': 1.128771424293518, 'learning_rate': 6.831501831501832e-05, 'epoch': 3.22}
{'eval_loss': 0.3391495645046234, 'eval_accuracy': 0.8854166666666666, 'eval_runtime': 128.8959, 'eval_samples_per_second': 4.95, 'eval_steps_per_second': 4.95, 'epoch': 3.22}
********************save call back********************
********************on step end call back********************
Step 17710 finish
{'loss': 0.3625, 'grad_norm': 1.3511022329330444, 'learning_rate': 6.82967032967033e-05, 'epoch': 3.22}
********************on step end call back********************
Step 17720 finish
{'loss': 0.3299, 'grad_norm': 1.3274056911468506, 'learning_rate': 6.827838827838828e-05, 'epoch': 3.22}
********************on step end call back********************
Step 17730 finish
{'loss': 0.3237, 'grad_norm': 1.030405879020691, 'learning_rate': 6.826007326007326e-05, 'epoch': 3.22}
********************on step end call back********************
Step 17740 finish
{'loss': 0.3252, 'grad_norm': 1.4786431789398193, 'learning_rate': 6.824175824175825e-05, 'epoch': 3.23}
********************on step end call back********************
Step 17750 finish
{'loss': 0.3491, 'grad_norm': 1.5000461339950562, 'learning_rate': 6.822344322344323e-05, 'epoch': 3.23}
********************on step end call back********************
Step 17760 finish
[INFO|trainer.py:3376] 2024-03-23 04:49:45,704 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-23 04:49:45,704 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-23 04:49:45,704 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-23 04:51:54,546 >> Saving model checkpoint to ./output/tmp-checkpoint-17800
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-23 04:51:54,688 >> tokenizer config file saved in ./output/tmp-checkpoint-17800/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-23 04:51:54,688 >> Special tokens file saved in ./output/tmp-checkpoint-17800/special_tokens_map.json
[INFO|trainer.py:3159] 2024-03-23 04:51:54,887 >> Deleting older checkpoint [output/checkpoint-7800] due to args.save_total_limit
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-23 05:00:33,460 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-23 05:00:33,460 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-23 05:00:33,460 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-23 05:02:42,318 >> Saving model checkpoint to ./output/tmp-checkpoint-17900
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-23 05:02:42,460 >> tokenizer config file saved in ./output/tmp-checkpoint-17900/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-23 05:02:42,460 >> Special tokens file saved in ./output/tmp-checkpoint-17900/special_tokens_map.json
[INFO|trainer.py:3159] 2024-03-23 05:02:42,657 >> Deleting older checkpoint [output/checkpoint-7900] due to args.save_total_limit
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-23 05:11:20,468 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-23 05:11:20,468 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-23 05:11:20,468 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-23 05:13:29,301 >> Saving model checkpoint to ./output/tmp-checkpoint-18000
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-23 05:13:29,442 >> tokenizer config file saved in ./output/tmp-checkpoint-18000/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-23 05:13:29,442 >> Special tokens file saved in ./output/tmp-checkpoint-18000/special_tokens_map.json
[INFO|trainer.py:3159] 2024-03-23 05:13:29,641 >> Deleting older checkpoint [output/checkpoint-8000] due to args.save_total_limit
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-23 05:21:55,550 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-23 05:21:55,550 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-23 05:21:55,550 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-23 05:24:04,355 >> Saving model checkpoint to ./output/tmp-checkpoint-18100
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-23 05:24:04,497 >> tokenizer config file saved in ./output/tmp-checkpoint-18100/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-23 05:24:04,497 >> Special tokens file saved in ./output/tmp-checkpoint-18100/special_tokens_map.json
[INFO|trainer.py:3159] 2024-03-23 05:24:04,694 >> Deleting older checkpoint [output/checkpoint-8100] due to args.save_total_limit
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
{'loss': 0.3162, 'grad_norm': 1.283197045326233, 'learning_rate': 6.820512820512821e-05, 'epoch': 3.23}
********************on step end call back********************
Step 17770 finish
{'loss': 0.2625, 'grad_norm': 1.1530057191848755, 'learning_rate': 6.818681318681319e-05, 'epoch': 3.23}
********************on step end call back********************
Step 17780 finish
{'loss': 0.2864, 'grad_norm': 1.0047392845153809, 'learning_rate': 6.816849816849817e-05, 'epoch': 3.23}
********************on step end call back********************
Step 17790 finish
{'loss': 0.3288, 'grad_norm': 1.1731314659118652, 'learning_rate': 6.815018315018316e-05, 'epoch': 3.23}
********************on step end call back********************
Step 17800 finish
{'loss': 0.3665, 'grad_norm': 0.8782110810279846, 'learning_rate': 6.813186813186814e-05, 'epoch': 3.24}
{'eval_loss': 0.32723620533943176, 'eval_accuracy': 0.875, 'eval_runtime': 128.8407, 'eval_samples_per_second': 4.952, 'eval_steps_per_second': 4.952, 'epoch': 3.24}
********************save call back********************
********************on step end call back********************
Step 17810 finish
{'loss': 0.3387, 'grad_norm': 1.198667287826538, 'learning_rate': 6.811355311355312e-05, 'epoch': 3.24}
********************on step end call back********************
Step 17820 finish
{'loss': 0.294, 'grad_norm': 1.5430845022201538, 'learning_rate': 6.80952380952381e-05, 'epoch': 3.24}
********************on step end call back********************
Step 17830 finish
{'loss': 0.3212, 'grad_norm': 1.2446436882019043, 'learning_rate': 6.807692307692309e-05, 'epoch': 3.24}
********************on step end call back********************
Step 17840 finish
{'loss': 0.3297, 'grad_norm': 1.4140779972076416, 'learning_rate': 6.805860805860807e-05, 'epoch': 3.24}
********************on step end call back********************
Step 17850 finish
{'loss': 0.319, 'grad_norm': 1.188118577003479, 'learning_rate': 6.804029304029305e-05, 'epoch': 3.25}
********************on step end call back********************
Step 17860 finish
{'loss': 0.3059, 'grad_norm': 1.2345683574676514, 'learning_rate': 6.802197802197802e-05, 'epoch': 3.25}
********************on step end call back********************
Step 17870 finish
{'loss': 0.3387, 'grad_norm': 1.3971482515335083, 'learning_rate': 6.8003663003663e-05, 'epoch': 3.25}
********************on step end call back********************
Step 17880 finish
{'loss': 0.3189, 'grad_norm': 1.4972795248031616, 'learning_rate': 6.7985347985348e-05, 'epoch': 3.25}
********************on step end call back********************
Step 17890 finish
{'loss': 0.3061, 'grad_norm': 1.273559808731079, 'learning_rate': 6.796703296703297e-05, 'epoch': 3.25}
********************on step end call back********************
Step 17900 finish
{'loss': 0.362, 'grad_norm': 1.0810080766677856, 'learning_rate': 6.794871794871795e-05, 'epoch': 3.25}
{'eval_loss': 0.33748379349708557, 'eval_accuracy': 0.8854166666666666, 'eval_runtime': 128.8573, 'eval_samples_per_second': 4.951, 'eval_steps_per_second': 4.951, 'epoch': 3.25}
********************save call back********************
********************on step end call back********************
Step 17910 finish
{'loss': 0.3385, 'grad_norm': 1.3890653848648071, 'learning_rate': 6.793040293040293e-05, 'epoch': 3.26}
********************on step end call back********************
Step 17920 finish
{'loss': 0.3112, 'grad_norm': 1.1512237787246704, 'learning_rate': 6.791208791208792e-05, 'epoch': 3.26}
********************on step end call back********************
Step 17930 finish
{'loss': 0.2981, 'grad_norm': 1.2918899059295654, 'learning_rate': 6.78937728937729e-05, 'epoch': 3.26}
********************on step end call back********************
Step 17940 finish
{'loss': 0.3131, 'grad_norm': 1.0761685371398926, 'learning_rate': 6.787545787545788e-05, 'epoch': 3.26}
********************on step end call back********************
Step 17950 finish
{'loss': 0.302, 'grad_norm': 1.239318609237671, 'learning_rate': 6.785714285714286e-05, 'epoch': 3.26}
********************on step end call back********************
Step 17960 finish
{'loss': 0.3095, 'grad_norm': 1.111274242401123, 'learning_rate': 6.783882783882784e-05, 'epoch': 3.27}
********************on step end call back********************
Step 17970 finish
{'loss': 0.3114, 'grad_norm': 1.3732990026474, 'learning_rate': 6.782051282051283e-05, 'epoch': 3.27}
********************on step end call back********************
Step 17980 finish
{'loss': 0.3565, 'grad_norm': 1.0346070528030396, 'learning_rate': 6.780219780219781e-05, 'epoch': 3.27}
********************on step end call back********************
Step 17990 finish
{'loss': 0.2906, 'grad_norm': 1.2985559701919556, 'learning_rate': 6.778388278388279e-05, 'epoch': 3.27}
********************on step end call back********************
Step 18000 finish
{'loss': 0.3573, 'grad_norm': 1.2294330596923828, 'learning_rate': 6.776556776556777e-05, 'epoch': 3.27}
{'eval_loss': 0.3312947154045105, 'eval_accuracy': 0.875, 'eval_runtime': 128.8324, 'eval_samples_per_second': 4.952, 'eval_steps_per_second': 4.952, 'epoch': 3.27}
********************save call back********************
********************on step end call back********************
Step 18010 finish
{'loss': 0.3118, 'grad_norm': 1.2833991050720215, 'learning_rate': 6.774725274725276e-05, 'epoch': 3.27}
********************on step end call back********************
Step 18020 finish
{'loss': 0.35, 'grad_norm': 1.0822257995605469, 'learning_rate': 6.772893772893774e-05, 'epoch': 3.28}
********************on step end call back********************
Step 18030 finish
{'loss': 0.3626, 'grad_norm': 1.3721064329147339, 'learning_rate': 6.771062271062272e-05, 'epoch': 3.28}
********************on step end call back********************
Step 18040 finish
{'loss': 0.2952, 'grad_norm': 1.0081325769424438, 'learning_rate': 6.76923076923077e-05, 'epoch': 3.28}
********************on step end call back********************
Step 18050 finish
{'loss': 0.317, 'grad_norm': 0.9411416053771973, 'learning_rate': 6.767399267399268e-05, 'epoch': 3.28}
********************on step end call back********************
Step 18060 finish
{'loss': 0.3255, 'grad_norm': 1.1645854711532593, 'learning_rate': 6.765567765567767e-05, 'epoch': 3.28}
********************on step end call back********************
Step 18070 finish
{'loss': 0.3273, 'grad_norm': 1.1979111433029175, 'learning_rate': 6.763736263736265e-05, 'epoch': 3.29}
********************on step end call back********************
Step 18080 finish
{'loss': 0.2753, 'grad_norm': 0.9949355125427246, 'learning_rate': 6.761904761904763e-05, 'epoch': 3.29}
********************on step end call back********************
Step 18090 finish
{'loss': 0.3236, 'grad_norm': 1.14084792137146, 'learning_rate': 6.76007326007326e-05, 'epoch': 3.29}
********************on step end call back********************
Step 18100 finish
{'loss': 0.2967, 'grad_norm': 1.0716488361358643, 'learning_rate': 6.75824175824176e-05, 'epoch': 3.29}
{'eval_loss': 0.3331959843635559, 'eval_accuracy': 0.875, 'eval_runtime': 128.8046, 'eval_samples_per_second': 4.953, 'eval_steps_per_second': 4.953, 'epoch': 3.29}
********************save call back********************
********************on step end call back********************
Step 18110 finish
{'loss': 0.3199, 'grad_norm': 1.7221041917800903, 'learning_rate': 6.756410256410258e-05, 'epoch': 3.29}
********************on step end call back********************
Step 18120 finish
{'loss': 0.3354, 'grad_norm': 1.2575154304504395, 'learning_rate': 6.754578754578755e-05, 'epoch': 3.29}
********************on step end call back********************
Step 18130 finish
{'loss': 0.3049, 'grad_norm': 1.0058386325836182, 'learning_rate': 6.752747252747253e-05, 'epoch': 3.3}
********************on step end call back********************
Step 18140 finish
{'loss': 0.363, 'grad_norm': 1.1669538021087646, 'learning_rate': 6.750915750915751e-05, 'epoch': 3.3}
********************on step end call back********************
Step 18150 finish
{'loss': 0.3084, 'grad_norm': 1.1371071338653564, 'learning_rate': 6.74908424908425e-05, 'epoch': 3.3}
[INFO|trainer.py:3376] 2024-03-23 05:32:33,231 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-23 05:32:33,232 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-23 05:32:33,232 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-23 05:34:42,412 >> Saving model checkpoint to ./output/tmp-checkpoint-18200
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-23 05:34:42,556 >> tokenizer config file saved in ./output/tmp-checkpoint-18200/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-23 05:34:42,556 >> Special tokens file saved in ./output/tmp-checkpoint-18200/special_tokens_map.json
[INFO|trainer.py:3159] 2024-03-23 05:34:42,751 >> Deleting older checkpoint [output/checkpoint-8200] due to args.save_total_limit
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-23 05:43:21,439 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-23 05:43:21,439 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-23 05:43:21,439 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-23 05:45:30,179 >> Saving model checkpoint to ./output/tmp-checkpoint-18300
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-23 05:45:30,322 >> tokenizer config file saved in ./output/tmp-checkpoint-18300/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-23 05:45:30,322 >> Special tokens file saved in ./output/tmp-checkpoint-18300/special_tokens_map.json
[INFO|trainer.py:3159] 2024-03-23 05:45:30,519 >> Deleting older checkpoint [output/checkpoint-8300] due to args.save_total_limit
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-23 05:54:01,342 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-23 05:54:01,342 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-23 05:54:01,342 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-23 05:56:10,048 >> Saving model checkpoint to ./output/tmp-checkpoint-18400
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-23 05:56:10,189 >> tokenizer config file saved in ./output/tmp-checkpoint-18400/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-23 05:56:10,189 >> Special tokens file saved in ./output/tmp-checkpoint-18400/special_tokens_map.json
[INFO|trainer.py:3159] 2024-03-23 05:56:10,386 >> Deleting older checkpoint [output/checkpoint-8400] due to args.save_total_limit
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-23 06:04:34,775 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-23 06:04:34,776 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-23 06:04:34,776 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-23 06:06:43,309 >> Saving model checkpoint to ./output/tmp-checkpoint-18500
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-23 06:06:43,466 >> tokenizer config file saved in ./output/tmp-checkpoint-18500/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-23 06:06:43,466 >> Special tokens file saved in ./output/tmp-checkpoint-18500/special_tokens_map.json
[INFO|trainer.py:3159] 2024-03-23 06:06:43,658 >> Deleting older checkpoint [output/checkpoint-8500] due to args.save_total_limit
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
********************on step end call back********************
Step 18160 finish
{'loss': 0.3328, 'grad_norm': 1.3899110555648804, 'learning_rate': 6.747252747252748e-05, 'epoch': 3.3}
********************on step end call back********************
Step 18170 finish
{'loss': 0.2893, 'grad_norm': 1.3075883388519287, 'learning_rate': 6.745421245421246e-05, 'epoch': 3.3}
********************on step end call back********************
Step 18180 finish
{'loss': 0.3155, 'grad_norm': 0.8579227924346924, 'learning_rate': 6.743589743589744e-05, 'epoch': 3.31}
********************on step end call back********************
Step 18190 finish
{'loss': 0.319, 'grad_norm': 1.303871989250183, 'learning_rate': 6.741758241758242e-05, 'epoch': 3.31}
********************on step end call back********************
Step 18200 finish
{'loss': 0.3176, 'grad_norm': 1.197603464126587, 'learning_rate': 6.739926739926741e-05, 'epoch': 3.31}
{'eval_loss': 0.324623167514801, 'eval_accuracy': 0.8958333333333334, 'eval_runtime': 129.1791, 'eval_samples_per_second': 4.939, 'eval_steps_per_second': 4.939, 'epoch': 3.31}
********************save call back********************
********************on step end call back********************
Step 18210 finish
{'loss': 0.3132, 'grad_norm': 1.1188852787017822, 'learning_rate': 6.738095238095239e-05, 'epoch': 3.31}
********************on step end call back********************
Step 18220 finish
{'loss': 0.3226, 'grad_norm': 1.2790913581848145, 'learning_rate': 6.736263736263737e-05, 'epoch': 3.31}
********************on step end call back********************
Step 18230 finish
{'loss': 0.3236, 'grad_norm': 1.3140103816986084, 'learning_rate': 6.734432234432235e-05, 'epoch': 3.31}
********************on step end call back********************
Step 18240 finish
{'loss': 0.3017, 'grad_norm': 0.9443607330322266, 'learning_rate': 6.732600732600734e-05, 'epoch': 3.32}
********************on step end call back********************
Step 18250 finish
{'loss': 0.3245, 'grad_norm': 1.590216875076294, 'learning_rate': 6.730769230769232e-05, 'epoch': 3.32}
********************on step end call back********************
Step 18260 finish
{'loss': 0.3705, 'grad_norm': 1.1148055791854858, 'learning_rate': 6.72893772893773e-05, 'epoch': 3.32}
********************on step end call back********************
Step 18270 finish
{'loss': 0.3243, 'grad_norm': 1.2567014694213867, 'learning_rate': 6.727106227106228e-05, 'epoch': 3.32}
********************on step end call back********************
Step 18280 finish
{'loss': 0.3046, 'grad_norm': 1.131927251815796, 'learning_rate': 6.725274725274725e-05, 'epoch': 3.32}
********************on step end call back********************
Step 18290 finish
{'loss': 0.3329, 'grad_norm': 1.249845027923584, 'learning_rate': 6.723443223443225e-05, 'epoch': 3.33}
********************on step end call back********************
Step 18300 finish
{'loss': 0.2883, 'grad_norm': 1.0844476222991943, 'learning_rate': 6.721611721611723e-05, 'epoch': 3.33}
{'eval_loss': 0.3349827826023102, 'eval_accuracy': 0.8958333333333334, 'eval_runtime': 128.7394, 'eval_samples_per_second': 4.956, 'eval_steps_per_second': 4.956, 'epoch': 3.33}
********************save call back********************
********************on step end call back********************
Step 18310 finish
{'loss': 0.3271, 'grad_norm': 0.9981684684753418, 'learning_rate': 6.71978021978022e-05, 'epoch': 3.33}
********************on step end call back********************
Step 18320 finish
{'loss': 0.3333, 'grad_norm': 1.4099351167678833, 'learning_rate': 6.717948717948718e-05, 'epoch': 3.33}
********************on step end call back********************
Step 18330 finish
{'loss': 0.3377, 'grad_norm': 1.6606473922729492, 'learning_rate': 6.716117216117216e-05, 'epoch': 3.33}
********************on step end call back********************
Step 18340 finish
{'loss': 0.3256, 'grad_norm': 1.1903051137924194, 'learning_rate': 6.714285714285714e-05, 'epoch': 3.33}
********************on step end call back********************
Step 18350 finish
{'loss': 0.3099, 'grad_norm': 1.2186086177825928, 'learning_rate': 6.712454212454212e-05, 'epoch': 3.34}
********************on step end call back********************
Step 18360 finish
{'loss': 0.3342, 'grad_norm': 1.3257535696029663, 'learning_rate': 6.71062271062271e-05, 'epoch': 3.34}
********************on step end call back********************
Step 18370 finish
{'loss': 0.3456, 'grad_norm': 1.0314151048660278, 'learning_rate': 6.708791208791209e-05, 'epoch': 3.34}
********************on step end call back********************
Step 18380 finish
{'loss': 0.3259, 'grad_norm': 1.2545125484466553, 'learning_rate': 6.706959706959707e-05, 'epoch': 3.34}
********************on step end call back********************
Step 18390 finish
{'loss': 0.3055, 'grad_norm': 1.0431114435195923, 'learning_rate': 6.705128205128205e-05, 'epoch': 3.34}
********************on step end call back********************
Step 18400 finish
{'loss': 0.3084, 'grad_norm': 1.0489287376403809, 'learning_rate': 6.703296703296703e-05, 'epoch': 3.35}
{'eval_loss': 0.3316156268119812, 'eval_accuracy': 0.8645833333333334, 'eval_runtime': 128.7057, 'eval_samples_per_second': 4.957, 'eval_steps_per_second': 4.957, 'epoch': 3.35}
********************save call back********************
********************on step end call back********************
Step 18410 finish
{'loss': 0.3299, 'grad_norm': 1.681190013885498, 'learning_rate': 6.701465201465202e-05, 'epoch': 3.35}
********************on step end call back********************
Step 18420 finish
{'loss': 0.2939, 'grad_norm': 1.3824849128723145, 'learning_rate': 6.6996336996337e-05, 'epoch': 3.35}
********************on step end call back********************
Step 18430 finish
{'loss': 0.3434, 'grad_norm': 1.2990182638168335, 'learning_rate': 6.697802197802198e-05, 'epoch': 3.35}
********************on step end call back********************
Step 18440 finish
{'loss': 0.3131, 'grad_norm': 1.692672848701477, 'learning_rate': 6.695970695970696e-05, 'epoch': 3.35}
********************on step end call back********************
Step 18450 finish
{'loss': 0.3143, 'grad_norm': 1.119924545288086, 'learning_rate': 6.694139194139193e-05, 'epoch': 3.35}
********************on step end call back********************
Step 18460 finish
{'loss': 0.3513, 'grad_norm': 1.2327580451965332, 'learning_rate': 6.692307692307693e-05, 'epoch': 3.36}
********************on step end call back********************
Step 18470 finish
{'loss': 0.3315, 'grad_norm': 1.3440247774124146, 'learning_rate': 6.69047619047619e-05, 'epoch': 3.36}
********************on step end call back********************
Step 18480 finish
{'loss': 0.3038, 'grad_norm': 1.1470223665237427, 'learning_rate': 6.688644688644688e-05, 'epoch': 3.36}
********************on step end call back********************
Step 18490 finish
{'loss': 0.3508, 'grad_norm': 1.0985236167907715, 'learning_rate': 6.686813186813186e-05, 'epoch': 3.36}
********************on step end call back********************
Step 18500 finish
{'loss': 0.3548, 'grad_norm': 1.2990115880966187, 'learning_rate': 6.684981684981686e-05, 'epoch': 3.36}
{'eval_loss': 0.3304442763328552, 'eval_accuracy': 0.8854166666666666, 'eval_runtime': 128.5329, 'eval_samples_per_second': 4.964, 'eval_steps_per_second': 4.964, 'epoch': 3.36}
********************save call back********************
********************on step end call back********************
Step 18510 finish
{'loss': 0.3344, 'grad_norm': 1.575528621673584, 'learning_rate': 6.683150183150183e-05, 'epoch': 3.37}
********************on step end call back********************
Step 18520 finish
{'loss': 0.3412, 'grad_norm': 1.102967619895935, 'learning_rate': 6.681318681318681e-05, 'epoch': 3.37}
********************on step end call back********************
Step 18530 finish
{'loss': 0.2959, 'grad_norm': 1.258316159248352, 'learning_rate': 6.679487179487179e-05, 'epoch': 3.37}
********************on step end call back********************
Step 18540 finish
{'loss': 0.3089, 'grad_norm': 1.2889217138290405, 'learning_rate': 6.677655677655677e-05, 'epoch': 3.37}
********************on step end call back********************
[INFO|trainer.py:3376] 2024-03-23 06:15:19,589 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-23 06:15:19,589 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-23 06:15:19,589 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-23 06:17:28,120 >> Saving model checkpoint to ./output/tmp-checkpoint-18600
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-23 06:17:28,287 >> tokenizer config file saved in ./output/tmp-checkpoint-18600/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-23 06:17:28,287 >> Special tokens file saved in ./output/tmp-checkpoint-18600/special_tokens_map.json
[INFO|trainer.py:3159] 2024-03-23 06:17:28,479 >> Deleting older checkpoint [output/checkpoint-8600] due to args.save_total_limit
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-23 06:26:03,982 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-23 06:26:03,982 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-23 06:26:03,982 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-23 06:28:12,448 >> Saving model checkpoint to ./output/tmp-checkpoint-18700
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-23 06:28:12,588 >> tokenizer config file saved in ./output/tmp-checkpoint-18700/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-23 06:28:12,588 >> Special tokens file saved in ./output/tmp-checkpoint-18700/special_tokens_map.json
[INFO|trainer.py:3159] 2024-03-23 06:28:12,784 >> Deleting older checkpoint [output/checkpoint-8700] due to args.save_total_limit
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-23 06:36:43,184 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-23 06:36:43,184 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-23 06:36:43,184 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-23 06:38:51,622 >> Saving model checkpoint to ./output/tmp-checkpoint-18800
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-23 06:38:51,761 >> tokenizer config file saved in ./output/tmp-checkpoint-18800/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-23 06:38:51,761 >> Special tokens file saved in ./output/tmp-checkpoint-18800/special_tokens_map.json
[INFO|trainer.py:3159] 2024-03-23 06:38:51,956 >> Deleting older checkpoint [output/checkpoint-8800] due to args.save_total_limit
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-23 06:47:35,572 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-23 06:47:35,572 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-23 06:47:35,572 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-23 06:49:44,157 >> Saving model checkpoint to ./output/tmp-checkpoint-18900
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-23 06:49:44,295 >> tokenizer config file saved in ./output/tmp-checkpoint-18900/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-23 06:49:44,296 >> Special tokens file saved in ./output/tmp-checkpoint-18900/special_tokens_map.json
[INFO|trainer.py:3159] 2024-03-23 06:49:44,498 >> Deleting older checkpoint [output/checkpoint-8900] due to args.save_total_limit
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
Step 18550 finish
{'loss': 0.2927, 'grad_norm': 1.4095046520233154, 'learning_rate': 6.675824175824176e-05, 'epoch': 3.37}
********************on step end call back********************
Step 18560 finish
{'loss': 0.3271, 'grad_norm': 0.8636687994003296, 'learning_rate': 6.673992673992674e-05, 'epoch': 3.37}
********************on step end call back********************
Step 18570 finish
{'loss': 0.3079, 'grad_norm': 1.0507115125656128, 'learning_rate': 6.672161172161172e-05, 'epoch': 3.38}
********************on step end call back********************
Step 18580 finish
{'loss': 0.3487, 'grad_norm': 1.4097113609313965, 'learning_rate': 6.67032967032967e-05, 'epoch': 3.38}
********************on step end call back********************
Step 18590 finish
{'loss': 0.3127, 'grad_norm': 1.1482703685760498, 'learning_rate': 6.668498168498169e-05, 'epoch': 3.38}
********************on step end call back********************
Step 18600 finish
{'loss': 0.356, 'grad_norm': 1.3101500272750854, 'learning_rate': 6.666666666666667e-05, 'epoch': 3.38}
{'eval_loss': 0.3265586197376251, 'eval_accuracy': 0.8854166666666666, 'eval_runtime': 128.5304, 'eval_samples_per_second': 4.964, 'eval_steps_per_second': 4.964, 'epoch': 3.38}
********************save call back********************
********************on step end call back********************
Step 18610 finish
{'loss': 0.3813, 'grad_norm': 0.7757560610771179, 'learning_rate': 6.664835164835165e-05, 'epoch': 3.38}
********************on step end call back********************
Step 18620 finish
{'loss': 0.3201, 'grad_norm': 1.0689030885696411, 'learning_rate': 6.663003663003663e-05, 'epoch': 3.39}
********************on step end call back********************
Step 18630 finish
{'loss': 0.2939, 'grad_norm': 1.3015625476837158, 'learning_rate': 6.66117216117216e-05, 'epoch': 3.39}
********************on step end call back********************
Step 18640 finish
{'loss': 0.3242, 'grad_norm': 1.010597825050354, 'learning_rate': 6.65934065934066e-05, 'epoch': 3.39}
********************on step end call back********************
Step 18650 finish
{'loss': 0.3309, 'grad_norm': 1.164679765701294, 'learning_rate': 6.657509157509158e-05, 'epoch': 3.39}
********************on step end call back********************
Step 18660 finish
{'loss': 0.3409, 'grad_norm': 1.2620844841003418, 'learning_rate': 6.655677655677656e-05, 'epoch': 3.39}
********************on step end call back********************
Step 18670 finish
{'loss': 0.3075, 'grad_norm': 0.9144389629364014, 'learning_rate': 6.653846153846153e-05, 'epoch': 3.39}
********************on step end call back********************
Step 18680 finish
{'loss': 0.3147, 'grad_norm': 1.2929627895355225, 'learning_rate': 6.652014652014651e-05, 'epoch': 3.4}
********************on step end call back********************
Step 18690 finish
{'loss': 0.3206, 'grad_norm': 1.011825442314148, 'learning_rate': 6.65018315018315e-05, 'epoch': 3.4}
********************on step end call back********************
Step 18700 finish
{'loss': 0.3014, 'grad_norm': 1.2697663307189941, 'learning_rate': 6.648351648351648e-05, 'epoch': 3.4}
{'eval_loss': 0.33233335614204407, 'eval_accuracy': 0.8854166666666666, 'eval_runtime': 128.4656, 'eval_samples_per_second': 4.966, 'eval_steps_per_second': 4.966, 'epoch': 3.4}
********************save call back********************
********************on step end call back********************
Step 18710 finish
{'loss': 0.3273, 'grad_norm': 1.0196536779403687, 'learning_rate': 6.646520146520146e-05, 'epoch': 3.4}
********************on step end call back********************
Step 18720 finish
{'loss': 0.3204, 'grad_norm': 0.7148890495300293, 'learning_rate': 6.644688644688644e-05, 'epoch': 3.4}
********************on step end call back********************
Step 18730 finish
{'loss': 0.3291, 'grad_norm': 1.1811389923095703, 'learning_rate': 6.642857142857143e-05, 'epoch': 3.41}
********************on step end call back********************
Step 18740 finish
{'loss': 0.3307, 'grad_norm': 1.2453126907348633, 'learning_rate': 6.641025641025641e-05, 'epoch': 3.41}
********************on step end call back********************
Step 18750 finish
{'loss': 0.3287, 'grad_norm': 1.8154679536819458, 'learning_rate': 6.639194139194139e-05, 'epoch': 3.41}
********************on step end call back********************
Step 18760 finish
{'loss': 0.3115, 'grad_norm': 0.9031868577003479, 'learning_rate': 6.637362637362637e-05, 'epoch': 3.41}
********************on step end call back********************
Step 18770 finish
{'loss': 0.2806, 'grad_norm': 1.190617322921753, 'learning_rate': 6.635531135531135e-05, 'epoch': 3.41}
********************on step end call back********************
Step 18780 finish
{'loss': 0.3128, 'grad_norm': 1.294577956199646, 'learning_rate': 6.633699633699634e-05, 'epoch': 3.41}
********************on step end call back********************
Step 18790 finish
{'loss': 0.3357, 'grad_norm': 1.492011547088623, 'learning_rate': 6.631868131868132e-05, 'epoch': 3.42}
********************on step end call back********************
Step 18800 finish
{'loss': 0.3498, 'grad_norm': 1.284412145614624, 'learning_rate': 6.63003663003663e-05, 'epoch': 3.42}
{'eval_loss': 0.32669320702552795, 'eval_accuracy': 0.875, 'eval_runtime': 128.4374, 'eval_samples_per_second': 4.967, 'eval_steps_per_second': 4.967, 'epoch': 3.42}
********************save call back********************
********************on step end call back********************
Step 18810 finish
{'loss': 0.3089, 'grad_norm': 1.365007758140564, 'learning_rate': 6.628205128205128e-05, 'epoch': 3.42}
********************on step end call back********************
Step 18820 finish
{'loss': 0.3445, 'grad_norm': 1.1754876375198364, 'learning_rate': 6.626373626373627e-05, 'epoch': 3.42}
********************on step end call back********************
Step 18830 finish
{'loss': 0.3255, 'grad_norm': 1.3341307640075684, 'learning_rate': 6.624542124542125e-05, 'epoch': 3.42}
********************on step end call back********************
Step 18840 finish
{'loss': 0.3623, 'grad_norm': 0.9760066866874695, 'learning_rate': 6.622710622710623e-05, 'epoch': 3.43}
********************on step end call back********************
Step 18850 finish
{'loss': 0.3299, 'grad_norm': 1.3447906970977783, 'learning_rate': 6.62087912087912e-05, 'epoch': 3.43}
********************on step end call back********************
Step 18860 finish
{'loss': 0.3547, 'grad_norm': 1.22422456741333, 'learning_rate': 6.619047619047619e-05, 'epoch': 3.43}
********************on step end call back********************
Step 18870 finish
{'loss': 0.3267, 'grad_norm': 1.2417771816253662, 'learning_rate': 6.617216117216118e-05, 'epoch': 3.43}
********************on step end call back********************
Step 18880 finish
{'loss': 0.3194, 'grad_norm': 1.2604866027832031, 'learning_rate': 6.615384615384616e-05, 'epoch': 3.43}
********************on step end call back********************
Step 18890 finish
{'loss': 0.3504, 'grad_norm': 1.2765132188796997, 'learning_rate': 6.613553113553114e-05, 'epoch': 3.43}
********************on step end call back********************
Step 18900 finish
{'loss': 0.303, 'grad_norm': 1.304762601852417, 'learning_rate': 6.611721611721611e-05, 'epoch': 3.44}
{'eval_loss': 0.3294922709465027, 'eval_accuracy': 0.8958333333333334, 'eval_runtime': 128.5843, 'eval_samples_per_second': 4.962, 'eval_steps_per_second': 4.962, 'epoch': 3.44}
********************save call back********************
********************on step end call back********************
Step 18910 finish
{'loss': 0.3401, 'grad_norm': 1.0875906944274902, 'learning_rate': 6.60989010989011e-05, 'epoch': 3.44}
********************on step end call back********************
Step 18920 finish
{'loss': 0.3352, 'grad_norm': 0.8958565592765808, 'learning_rate': 6.608058608058608e-05, 'epoch': 3.44}
********************on step end call back********************
Step 18930 finish
{'loss': 0.2995, 'grad_norm': 1.2298572063446045, 'learning_rate': 6.606227106227106e-05, 'epoch': 3.44}
********************on step end call back********************
Step 18940 finish
[INFO|trainer.py:3376] 2024-03-23 06:58:23,562 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-23 06:58:23,562 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-23 06:58:23,562 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-23 07:00:32,152 >> Saving model checkpoint to ./output/tmp-checkpoint-19000
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-23 07:00:32,294 >> tokenizer config file saved in ./output/tmp-checkpoint-19000/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-23 07:00:32,294 >> Special tokens file saved in ./output/tmp-checkpoint-19000/special_tokens_map.json
[INFO|trainer.py:3159] 2024-03-23 07:00:32,491 >> Deleting older checkpoint [output/checkpoint-9000] due to args.save_total_limit
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-23 07:09:07,461 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-23 07:09:07,461 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-23 07:09:07,461 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-23 07:11:16,026 >> Saving model checkpoint to ./output/tmp-checkpoint-19100
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-23 07:11:16,165 >> tokenizer config file saved in ./output/tmp-checkpoint-19100/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-23 07:11:16,165 >> Special tokens file saved in ./output/tmp-checkpoint-19100/special_tokens_map.json
[INFO|trainer.py:3159] 2024-03-23 07:11:16,363 >> Deleting older checkpoint [output/checkpoint-9100] due to args.save_total_limit
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-23 07:20:01,986 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-23 07:20:01,986 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-23 07:20:01,986 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-23 07:22:11,045 >> Saving model checkpoint to ./output/tmp-checkpoint-19200
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-23 07:22:11,184 >> tokenizer config file saved in ./output/tmp-checkpoint-19200/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-23 07:22:11,184 >> Special tokens file saved in ./output/tmp-checkpoint-19200/special_tokens_map.json
[INFO|trainer.py:3159] 2024-03-23 07:22:11,380 >> Deleting older checkpoint [output/checkpoint-9200] due to args.save_total_limit
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-23 07:30:48,274 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-23 07:30:48,274 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-23 07:30:48,274 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-23 07:32:56,834 >> Saving model checkpoint to ./output/tmp-checkpoint-19300
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-23 07:32:56,973 >> tokenizer config file saved in ./output/tmp-checkpoint-19300/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-23 07:32:56,973 >> Special tokens file saved in ./output/tmp-checkpoint-19300/special_tokens_map.json
[INFO|trainer.py:3159] 2024-03-23 07:32:57,170 >> Deleting older checkpoint [output/checkpoint-9300] due to args.save_total_limit
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
{'loss': 0.3364, 'grad_norm': 1.4070730209350586, 'learning_rate': 6.604395604395604e-05, 'epoch': 3.44}
********************on step end call back********************
Step 18950 finish
{'loss': 0.3443, 'grad_norm': 1.0130834579467773, 'learning_rate': 6.602564102564102e-05, 'epoch': 3.45}
********************on step end call back********************
Step 18960 finish
{'loss': 0.3173, 'grad_norm': 1.0808621644973755, 'learning_rate': 6.600732600732601e-05, 'epoch': 3.45}
********************on step end call back********************
Step 18970 finish
{'loss': 0.309, 'grad_norm': 1.6449635028839111, 'learning_rate': 6.598901098901099e-05, 'epoch': 3.45}
********************on step end call back********************
Step 18980 finish
{'loss': 0.289, 'grad_norm': 1.4592926502227783, 'learning_rate': 6.597069597069597e-05, 'epoch': 3.45}
********************on step end call back********************
Step 18990 finish
{'loss': 0.3118, 'grad_norm': 1.354176640510559, 'learning_rate': 6.595238095238095e-05, 'epoch': 3.45}
********************on step end call back********************
Step 19000 finish
{'loss': 0.346, 'grad_norm': 1.0278241634368896, 'learning_rate': 6.593406593406594e-05, 'epoch': 3.45}
{'eval_loss': 0.3386381268501282, 'eval_accuracy': 0.8854166666666666, 'eval_runtime': 128.5898, 'eval_samples_per_second': 4.962, 'eval_steps_per_second': 4.962, 'epoch': 3.45}
********************save call back********************
********************on step end call back********************
Step 19010 finish
{'loss': 0.3346, 'grad_norm': 1.0330004692077637, 'learning_rate': 6.591575091575092e-05, 'epoch': 3.46}
********************on step end call back********************
Step 19020 finish
{'loss': 0.3484, 'grad_norm': 1.0146740674972534, 'learning_rate': 6.58974358974359e-05, 'epoch': 3.46}
********************on step end call back********************
Step 19030 finish
{'loss': 0.3211, 'grad_norm': 0.9925948977470398, 'learning_rate': 6.587912087912088e-05, 'epoch': 3.46}
********************on step end call back********************
Step 19040 finish
{'loss': 0.2905, 'grad_norm': 1.1603975296020508, 'learning_rate': 6.586080586080586e-05, 'epoch': 3.46}
********************on step end call back********************
Step 19050 finish
{'loss': 0.3514, 'grad_norm': 1.6132436990737915, 'learning_rate': 6.584249084249085e-05, 'epoch': 3.46}
********************on step end call back********************
Step 19060 finish
{'loss': 0.3368, 'grad_norm': 1.4240844249725342, 'learning_rate': 6.582417582417583e-05, 'epoch': 3.47}
********************on step end call back********************
Step 19070 finish
{'loss': 0.3305, 'grad_norm': 0.9925509095191956, 'learning_rate': 6.580586080586081e-05, 'epoch': 3.47}
********************on step end call back********************
Step 19080 finish
{'loss': 0.3191, 'grad_norm': 0.9381992816925049, 'learning_rate': 6.578754578754579e-05, 'epoch': 3.47}
********************on step end call back********************
Step 19090 finish
{'loss': 0.3112, 'grad_norm': 1.2041727304458618, 'learning_rate': 6.576923076923078e-05, 'epoch': 3.47}
********************on step end call back********************
Step 19100 finish
{'loss': 0.334, 'grad_norm': 1.2729408740997314, 'learning_rate': 6.575091575091576e-05, 'epoch': 3.47}
{'eval_loss': 0.33324921131134033, 'eval_accuracy': 0.8958333333333334, 'eval_runtime': 128.5639, 'eval_samples_per_second': 4.963, 'eval_steps_per_second': 4.963, 'epoch': 3.47}
********************save call back********************
********************on step end call back********************
Step 19110 finish
{'loss': 0.348, 'grad_norm': 1.0454171895980835, 'learning_rate': 6.573260073260074e-05, 'epoch': 3.47}
********************on step end call back********************
Step 19120 finish
{'loss': 0.336, 'grad_norm': 1.3084790706634521, 'learning_rate': 6.571428571428571e-05, 'epoch': 3.48}
********************on step end call back********************
Step 19130 finish
{'loss': 0.3368, 'grad_norm': 1.0287617444992065, 'learning_rate': 6.569597069597069e-05, 'epoch': 3.48}
********************on step end call back********************
Step 19140 finish
{'loss': 0.3408, 'grad_norm': 1.382624864578247, 'learning_rate': 6.567765567765569e-05, 'epoch': 3.48}
********************on step end call back********************
Step 19150 finish
{'loss': 0.3764, 'grad_norm': 1.0465404987335205, 'learning_rate': 6.565934065934066e-05, 'epoch': 3.48}
********************on step end call back********************
Step 19160 finish
{'loss': 0.2918, 'grad_norm': 1.441550850868225, 'learning_rate': 6.564102564102564e-05, 'epoch': 3.48}
********************on step end call back********************
Step 19170 finish
{'loss': 0.3126, 'grad_norm': 1.1625730991363525, 'learning_rate': 6.562271062271062e-05, 'epoch': 3.49}
********************on step end call back********************
Step 19180 finish
{'loss': 0.3638, 'grad_norm': 1.3863554000854492, 'learning_rate': 6.560439560439561e-05, 'epoch': 3.49}
********************on step end call back********************
Step 19190 finish
{'loss': 0.3015, 'grad_norm': 0.9992620944976807, 'learning_rate': 6.558608058608059e-05, 'epoch': 3.49}
********************on step end call back********************
Step 19200 finish
{'loss': 0.3617, 'grad_norm': 1.2419513463974, 'learning_rate': 6.556776556776557e-05, 'epoch': 3.49}
{'eval_loss': 0.32663998007774353, 'eval_accuracy': 0.8958333333333334, 'eval_runtime': 129.058, 'eval_samples_per_second': 4.944, 'eval_steps_per_second': 4.944, 'epoch': 3.49}
********************save call back********************
********************on step end call back********************
Step 19210 finish
{'loss': 0.3372, 'grad_norm': 1.2264530658721924, 'learning_rate': 6.554945054945055e-05, 'epoch': 3.49}
********************on step end call back********************
Step 19220 finish
{'loss': 0.2931, 'grad_norm': 1.188358187675476, 'learning_rate': 6.553113553113553e-05, 'epoch': 3.49}
********************on step end call back********************
Step 19230 finish
{'loss': 0.344, 'grad_norm': 1.2471855878829956, 'learning_rate': 6.551282051282052e-05, 'epoch': 3.5}
********************on step end call back********************
Step 19240 finish
{'loss': 0.2974, 'grad_norm': 1.1912891864776611, 'learning_rate': 6.54945054945055e-05, 'epoch': 3.5}
********************on step end call back********************
Step 19250 finish
{'loss': 0.3274, 'grad_norm': 1.1053135395050049, 'learning_rate': 6.547619047619048e-05, 'epoch': 3.5}
********************on step end call back********************
Step 19260 finish
{'loss': 0.3314, 'grad_norm': 1.2072025537490845, 'learning_rate': 6.545787545787546e-05, 'epoch': 3.5}
********************on step end call back********************
Step 19270 finish
{'loss': 0.3625, 'grad_norm': 1.1357190608978271, 'learning_rate': 6.543956043956045e-05, 'epoch': 3.5}
********************on step end call back********************
Step 19280 finish
{'loss': 0.305, 'grad_norm': 1.0862358808517456, 'learning_rate': 6.542124542124543e-05, 'epoch': 3.51}
********************on step end call back********************
Step 19290 finish
{'loss': 0.3252, 'grad_norm': 1.2393851280212402, 'learning_rate': 6.540293040293041e-05, 'epoch': 3.51}
********************on step end call back********************
Step 19300 finish
{'loss': 0.3373, 'grad_norm': 1.2002569437026978, 'learning_rate': 6.538461538461539e-05, 'epoch': 3.51}
{'eval_loss': 0.33509907126426697, 'eval_accuracy': 0.8854166666666666, 'eval_runtime': 128.5591, 'eval_samples_per_second': 4.963, 'eval_steps_per_second': 4.963, 'epoch': 3.51}
********************save call back********************
********************on step end call back********************
Step 19310 finish
{'loss': 0.2978, 'grad_norm': 2.2503764629364014, 'learning_rate': 6.536630036630036e-05, 'epoch': 3.51}
********************on step end call back********************
Step 19320 finish
{'loss': 0.2756, 'grad_norm': 1.1336669921875, 'learning_rate': 6.534798534798536e-05, 'epoch': 3.51}
********************on step end call back********************
Step 19330 finish
[INFO|trainer.py:3376] 2024-03-23 07:41:28,562 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-23 07:41:28,563 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-23 07:41:28,563 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-23 07:43:37,281 >> Saving model checkpoint to ./output/tmp-checkpoint-19400
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-23 07:43:37,439 >> tokenizer config file saved in ./output/tmp-checkpoint-19400/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-23 07:43:37,439 >> Special tokens file saved in ./output/tmp-checkpoint-19400/special_tokens_map.json
[INFO|trainer.py:3159] 2024-03-23 07:43:37,633 >> Deleting older checkpoint [output/checkpoint-9400] due to args.save_total_limit
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-23 07:52:09,410 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-23 07:52:09,410 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-23 07:52:09,410 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-23 07:54:18,021 >> Saving model checkpoint to ./output/tmp-checkpoint-19500
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-23 07:54:18,177 >> tokenizer config file saved in ./output/tmp-checkpoint-19500/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-23 07:54:18,178 >> Special tokens file saved in ./output/tmp-checkpoint-19500/special_tokens_map.json
[INFO|trainer.py:3159] 2024-03-23 07:54:18,370 >> Deleting older checkpoint [output/checkpoint-9500] due to args.save_total_limit
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-23 08:02:54,210 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-23 08:02:54,210 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-23 08:02:54,210 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-23 08:05:02,881 >> Saving model checkpoint to ./output/tmp-checkpoint-19600
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-23 08:05:03,020 >> tokenizer config file saved in ./output/tmp-checkpoint-19600/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-23 08:05:03,020 >> Special tokens file saved in ./output/tmp-checkpoint-19600/special_tokens_map.json
[INFO|trainer.py:3159] 2024-03-23 08:05:03,218 >> Deleting older checkpoint [output/checkpoint-9600] due to args.save_total_limit
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-23 08:13:34,561 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-23 08:13:34,561 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-23 08:13:34,561 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-23 08:15:43,139 >> Saving model checkpoint to ./output/tmp-checkpoint-19700
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-23 08:15:43,276 >> tokenizer config file saved in ./output/tmp-checkpoint-19700/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-23 08:15:43,276 >> Special tokens file saved in ./output/tmp-checkpoint-19700/special_tokens_map.json
[INFO|trainer.py:3159] 2024-03-23 08:15:43,478 >> Deleting older checkpoint [output/checkpoint-9700] due to args.save_total_limit
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
{'loss': 0.3202, 'grad_norm': 1.4466211795806885, 'learning_rate': 6.532967032967034e-05, 'epoch': 3.51}
********************on step end call back********************
Step 19340 finish
{'loss': 0.3271, 'grad_norm': 1.2231535911560059, 'learning_rate': 6.531135531135531e-05, 'epoch': 3.52}
********************on step end call back********************
Step 19350 finish
{'loss': 0.3493, 'grad_norm': 1.2895272970199585, 'learning_rate': 6.52930402930403e-05, 'epoch': 3.52}
********************on step end call back********************
Step 19360 finish
{'loss': 0.3297, 'grad_norm': 1.1656608581542969, 'learning_rate': 6.527472527472529e-05, 'epoch': 3.52}
********************on step end call back********************
Step 19370 finish
{'loss': 0.3207, 'grad_norm': 1.3211631774902344, 'learning_rate': 6.525641025641026e-05, 'epoch': 3.52}
********************on step end call back********************
Step 19380 finish
{'loss': 0.296, 'grad_norm': 0.9269040822982788, 'learning_rate': 6.523809523809524e-05, 'epoch': 3.52}
********************on step end call back********************
Step 19390 finish
{'loss': 0.3419, 'grad_norm': 1.5691505670547485, 'learning_rate': 6.521978021978022e-05, 'epoch': 3.53}
********************on step end call back********************
Step 19400 finish
{'loss': 0.358, 'grad_norm': 1.4411808252334595, 'learning_rate': 6.52014652014652e-05, 'epoch': 3.53}
{'eval_loss': 0.3384748697280884, 'eval_accuracy': 0.8958333333333334, 'eval_runtime': 128.7175, 'eval_samples_per_second': 4.957, 'eval_steps_per_second': 4.957, 'epoch': 3.53}
********************save call back********************
********************on step end call back********************
Step 19410 finish
{'loss': 0.316, 'grad_norm': 0.9002452492713928, 'learning_rate': 6.518315018315019e-05, 'epoch': 3.53}
********************on step end call back********************
Step 19420 finish
{'loss': 0.3197, 'grad_norm': 1.208980917930603, 'learning_rate': 6.516483516483517e-05, 'epoch': 3.53}
********************on step end call back********************
Step 19430 finish
{'loss': 0.338, 'grad_norm': 0.9908721446990967, 'learning_rate': 6.514652014652015e-05, 'epoch': 3.53}
********************on step end call back********************
Step 19440 finish
{'loss': 0.3104, 'grad_norm': 1.3354319334030151, 'learning_rate': 6.512820512820513e-05, 'epoch': 3.53}
********************on step end call back********************
Step 19450 finish
{'loss': 0.3513, 'grad_norm': 1.1223384141921997, 'learning_rate': 6.510989010989011e-05, 'epoch': 3.54}
********************on step end call back********************
Step 19460 finish
{'loss': 0.321, 'grad_norm': 1.0971291065216064, 'learning_rate': 6.50915750915751e-05, 'epoch': 3.54}
********************on step end call back********************
Step 19470 finish
{'loss': 0.3333, 'grad_norm': 1.3024615049362183, 'learning_rate': 6.507326007326008e-05, 'epoch': 3.54}
********************on step end call back********************
Step 19480 finish
{'loss': 0.3317, 'grad_norm': 0.9294833540916443, 'learning_rate': 6.505494505494506e-05, 'epoch': 3.54}
********************on step end call back********************
Step 19490 finish
{'loss': 0.3544, 'grad_norm': 0.9281341433525085, 'learning_rate': 6.503663003663004e-05, 'epoch': 3.54}
********************on step end call back********************
Step 19500 finish
{'loss': 0.327, 'grad_norm': 1.2886208295822144, 'learning_rate': 6.501831501831503e-05, 'epoch': 3.55}
{'eval_loss': 0.33517134189605713, 'eval_accuracy': 0.875, 'eval_runtime': 128.6107, 'eval_samples_per_second': 4.961, 'eval_steps_per_second': 4.961, 'epoch': 3.55}
********************save call back********************
********************on step end call back********************
Step 19510 finish
{'loss': 0.2972, 'grad_norm': 1.3164032697677612, 'learning_rate': 6.500000000000001e-05, 'epoch': 3.55}
********************on step end call back********************
Step 19520 finish
{'loss': 0.357, 'grad_norm': 0.9882730841636658, 'learning_rate': 6.498168498168499e-05, 'epoch': 3.55}
********************on step end call back********************
Step 19530 finish
{'loss': 0.3168, 'grad_norm': 0.9674710631370544, 'learning_rate': 6.496336996336997e-05, 'epoch': 3.55}
********************on step end call back********************
Step 19540 finish
{'loss': 0.3103, 'grad_norm': 1.3716869354248047, 'learning_rate': 6.494505494505494e-05, 'epoch': 3.55}
********************on step end call back********************
Step 19550 finish
{'loss': 0.3028, 'grad_norm': 0.9963425397872925, 'learning_rate': 6.492673992673994e-05, 'epoch': 3.55}
********************on step end call back********************
Step 19560 finish
{'loss': 0.3089, 'grad_norm': 1.1469807624816895, 'learning_rate': 6.490842490842492e-05, 'epoch': 3.56}
********************on step end call back********************
Step 19570 finish
{'loss': 0.3051, 'grad_norm': 1.1153523921966553, 'learning_rate': 6.48901098901099e-05, 'epoch': 3.56}
********************on step end call back********************
Step 19580 finish
{'loss': 0.3094, 'grad_norm': 0.8070205450057983, 'learning_rate': 6.487179487179487e-05, 'epoch': 3.56}
********************on step end call back********************
Step 19590 finish
{'loss': 0.3356, 'grad_norm': 1.2002280950546265, 'learning_rate': 6.485347985347987e-05, 'epoch': 3.56}
********************on step end call back********************
Step 19600 finish
{'loss': 0.3463, 'grad_norm': 1.2950562238693237, 'learning_rate': 6.483516483516484e-05, 'epoch': 3.56}
{'eval_loss': 0.3348083198070526, 'eval_accuracy': 0.8958333333333334, 'eval_runtime': 128.6697, 'eval_samples_per_second': 4.958, 'eval_steps_per_second': 4.958, 'epoch': 3.56}
********************save call back********************
********************on step end call back********************
Step 19610 finish
{'loss': 0.338, 'grad_norm': 1.2218163013458252, 'learning_rate': 6.481684981684982e-05, 'epoch': 3.57}
********************on step end call back********************
Step 19620 finish
{'loss': 0.3315, 'grad_norm': 1.2690130472183228, 'learning_rate': 6.47985347985348e-05, 'epoch': 3.57}
********************on step end call back********************
Step 19630 finish
{'loss': 0.3436, 'grad_norm': 1.1095787286758423, 'learning_rate': 6.478021978021978e-05, 'epoch': 3.57}
********************on step end call back********************
Step 19640 finish
{'loss': 0.3097, 'grad_norm': 1.4214709997177124, 'learning_rate': 6.476190476190477e-05, 'epoch': 3.57}
********************on step end call back********************
Step 19650 finish
{'loss': 0.3387, 'grad_norm': 1.0968050956726074, 'learning_rate': 6.474358974358975e-05, 'epoch': 3.57}
********************on step end call back********************
Step 19660 finish
{'loss': 0.309, 'grad_norm': 1.4108102321624756, 'learning_rate': 6.472527472527473e-05, 'epoch': 3.57}
********************on step end call back********************
Step 19670 finish
{'loss': 0.3141, 'grad_norm': 1.203876256942749, 'learning_rate': 6.470695970695971e-05, 'epoch': 3.58}
********************on step end call back********************
Step 19680 finish
{'loss': 0.3508, 'grad_norm': 1.1613060235977173, 'learning_rate': 6.46886446886447e-05, 'epoch': 3.58}
********************on step end call back********************
Step 19690 finish
{'loss': 0.3027, 'grad_norm': 1.26467764377594, 'learning_rate': 6.467032967032968e-05, 'epoch': 3.58}
********************on step end call back********************
Step 19700 finish
{'loss': 0.348, 'grad_norm': 0.8661839365959167, 'learning_rate': 6.465201465201466e-05, 'epoch': 3.58}
{'eval_loss': 0.33470216393470764, 'eval_accuracy': 0.8958333333333334, 'eval_runtime': 128.5768, 'eval_samples_per_second': 4.962, 'eval_steps_per_second': 4.962, 'epoch': 3.58}
********************save call back********************
********************on step end call back********************
Step 19710 finish
{'loss': 0.3028, 'grad_norm': 0.8626697659492493, 'learning_rate': 6.463369963369964e-05, 'epoch': 3.58}
********************on step end call back********************
Step 19720 finish
[INFO|trainer.py:3376] 2024-03-23 08:24:22,584 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-23 08:24:22,584 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-23 08:24:22,584 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-23 08:26:31,366 >> Saving model checkpoint to ./output/tmp-checkpoint-19800
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-23 08:26:31,506 >> tokenizer config file saved in ./output/tmp-checkpoint-19800/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-23 08:26:31,506 >> Special tokens file saved in ./output/tmp-checkpoint-19800/special_tokens_map.json
[INFO|trainer.py:3159] 2024-03-23 08:26:31,710 >> Deleting older checkpoint [output/checkpoint-9800] due to args.save_total_limit
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-23 08:35:07,952 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-23 08:35:07,952 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-23 08:35:07,952 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-23 08:37:16,684 >> Saving model checkpoint to ./output/tmp-checkpoint-19900
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-23 08:37:16,822 >> tokenizer config file saved in ./output/tmp-checkpoint-19900/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-23 08:37:16,822 >> Special tokens file saved in ./output/tmp-checkpoint-19900/special_tokens_map.json
[INFO|trainer.py:3159] 2024-03-23 08:37:17,020 >> Deleting older checkpoint [output/checkpoint-9900] due to args.save_total_limit
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-23 08:45:44,682 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-23 08:45:44,682 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-23 08:45:44,682 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-23 08:47:53,234 >> Saving model checkpoint to ./output/tmp-checkpoint-20000
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-23 08:47:53,375 >> tokenizer config file saved in ./output/tmp-checkpoint-20000/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-23 08:47:53,375 >> Special tokens file saved in ./output/tmp-checkpoint-20000/special_tokens_map.json
[INFO|trainer.py:3159] 2024-03-23 08:47:53,579 >> Deleting older checkpoint [output/checkpoint-10000] due to args.save_total_limit
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-23 08:56:22,466 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-23 08:56:22,466 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-23 08:56:22,466 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-23 08:58:32,374 >> Saving model checkpoint to ./output/tmp-checkpoint-20100
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-23 08:58:32,530 >> tokenizer config file saved in ./output/tmp-checkpoint-20100/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-23 08:58:32,530 >> Special tokens file saved in ./output/tmp-checkpoint-20100/special_tokens_map.json
[INFO|trainer.py:3159] 2024-03-23 08:58:32,745 >> Deleting older checkpoint [output/checkpoint-10100] due to args.save_total_limit
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
{'loss': 0.35, 'grad_norm': 1.3386149406433105, 'learning_rate': 6.461538461538462e-05, 'epoch': 3.59}
********************on step end call back********************
Step 19730 finish
{'loss': 0.3244, 'grad_norm': 1.1783453226089478, 'learning_rate': 6.459706959706961e-05, 'epoch': 3.59}
********************on step end call back********************
Step 19740 finish
{'loss': 0.3599, 'grad_norm': 1.4573607444763184, 'learning_rate': 6.457875457875459e-05, 'epoch': 3.59}
********************on step end call back********************
Step 19750 finish
{'loss': 0.306, 'grad_norm': 1.0981172323226929, 'learning_rate': 6.456043956043957e-05, 'epoch': 3.59}
********************on step end call back********************
Step 19760 finish
{'loss': 0.3444, 'grad_norm': 1.443839430809021, 'learning_rate': 6.454212454212454e-05, 'epoch': 3.59}
********************on step end call back********************
Step 19770 finish
{'loss': 0.2951, 'grad_norm': 0.7986695766448975, 'learning_rate': 6.452380952380954e-05, 'epoch': 3.59}
********************on step end call back********************
Step 19780 finish
{'loss': 0.3217, 'grad_norm': 1.3681384325027466, 'learning_rate': 6.450549450549452e-05, 'epoch': 3.6}
********************on step end call back********************
Step 19790 finish
{'loss': 0.3407, 'grad_norm': 1.0469753742218018, 'learning_rate': 6.44871794871795e-05, 'epoch': 3.6}
********************on step end call back********************
Step 19800 finish
{'loss': 0.3271, 'grad_norm': 1.048406720161438, 'learning_rate': 6.446886446886447e-05, 'epoch': 3.6}
{'eval_loss': 0.32979562878608704, 'eval_accuracy': 0.8958333333333334, 'eval_runtime': 128.7806, 'eval_samples_per_second': 4.954, 'eval_steps_per_second': 4.954, 'epoch': 3.6}
********************save call back********************
********************on step end call back********************
Step 19810 finish
{'loss': 0.3523, 'grad_norm': 1.03429114818573, 'learning_rate': 6.445054945054945e-05, 'epoch': 3.6}
********************on step end call back********************
Step 19820 finish
{'loss': 0.3389, 'grad_norm': 1.266223430633545, 'learning_rate': 6.443223443223444e-05, 'epoch': 3.6}
********************on step end call back********************
Step 19830 finish
{'loss': 0.348, 'grad_norm': 1.4060758352279663, 'learning_rate': 6.441391941391942e-05, 'epoch': 3.61}
********************on step end call back********************
Step 19840 finish
{'loss': 0.3285, 'grad_norm': 1.2084715366363525, 'learning_rate': 6.43956043956044e-05, 'epoch': 3.61}
********************on step end call back********************
Step 19850 finish
{'loss': 0.311, 'grad_norm': 1.1879065036773682, 'learning_rate': 6.437728937728938e-05, 'epoch': 3.61}
********************on step end call back********************
Step 19860 finish
{'loss': 0.3404, 'grad_norm': 1.3173801898956299, 'learning_rate': 6.435897435897437e-05, 'epoch': 3.61}
********************on step end call back********************
Step 19870 finish
{'loss': 0.2925, 'grad_norm': 1.0567412376403809, 'learning_rate': 6.434065934065935e-05, 'epoch': 3.61}
********************on step end call back********************
Step 19880 finish
{'loss': 0.2891, 'grad_norm': 0.8215053677558899, 'learning_rate': 6.432234432234433e-05, 'epoch': 3.61}
********************on step end call back********************
Step 19890 finish
{'loss': 0.3408, 'grad_norm': 1.3065180778503418, 'learning_rate': 6.430402930402931e-05, 'epoch': 3.62}
********************on step end call back********************
Step 19900 finish
{'loss': 0.3564, 'grad_norm': 1.0370739698410034, 'learning_rate': 6.428571428571429e-05, 'epoch': 3.62}
{'eval_loss': 0.32669171690940857, 'eval_accuracy': 0.8958333333333334, 'eval_runtime': 128.7308, 'eval_samples_per_second': 4.956, 'eval_steps_per_second': 4.956, 'epoch': 3.62}
********************save call back********************
********************on step end call back********************
Step 19910 finish
{'loss': 0.3385, 'grad_norm': 1.173139214515686, 'learning_rate': 6.426739926739928e-05, 'epoch': 3.62}
********************on step end call back********************
Step 19920 finish
{'loss': 0.3218, 'grad_norm': 1.1750398874282837, 'learning_rate': 6.424908424908426e-05, 'epoch': 3.62}
********************on step end call back********************
Step 19930 finish
{'loss': 0.3101, 'grad_norm': 1.3367611169815063, 'learning_rate': 6.423076923076924e-05, 'epoch': 3.62}
********************on step end call back********************
Step 19940 finish
{'loss': 0.3063, 'grad_norm': 0.9317274689674377, 'learning_rate': 6.421245421245422e-05, 'epoch': 3.63}
********************on step end call back********************
Step 19950 finish
{'loss': 0.3414, 'grad_norm': 1.2946115732192993, 'learning_rate': 6.419413919413921e-05, 'epoch': 3.63}
********************on step end call back********************
Step 19960 finish
{'loss': 0.3412, 'grad_norm': 1.154159426689148, 'learning_rate': 6.417582417582419e-05, 'epoch': 3.63}
********************on step end call back********************
Step 19970 finish
{'loss': 0.3083, 'grad_norm': 1.2721576690673828, 'learning_rate': 6.415750915750917e-05, 'epoch': 3.63}
********************on step end call back********************
Step 19980 finish
{'loss': 0.3021, 'grad_norm': 1.9453848600387573, 'learning_rate': 6.413919413919414e-05, 'epoch': 3.63}
********************on step end call back********************
Step 19990 finish
{'loss': 0.3175, 'grad_norm': 1.2766337394714355, 'learning_rate': 6.412087912087912e-05, 'epoch': 3.63}
********************on step end call back********************
Step 20000 finish
{'loss': 0.3125, 'grad_norm': 1.3328309059143066, 'learning_rate': 6.410256410256412e-05, 'epoch': 3.64}
{'eval_loss': 0.33345863223075867, 'eval_accuracy': 0.8854166666666666, 'eval_runtime': 128.5506, 'eval_samples_per_second': 4.963, 'eval_steps_per_second': 4.963, 'epoch': 3.64}
********************save call back********************
********************on step end call back********************
Step 20010 finish
{'loss': 0.3376, 'grad_norm': 1.3594427108764648, 'learning_rate': 6.40842490842491e-05, 'epoch': 3.64}
********************on step end call back********************
Step 20020 finish
{'loss': 0.3063, 'grad_norm': 0.8467184901237488, 'learning_rate': 6.406593406593407e-05, 'epoch': 3.64}
********************on step end call back********************
Step 20030 finish
{'loss': 0.288, 'grad_norm': 1.121785283088684, 'learning_rate': 6.404761904761904e-05, 'epoch': 3.64}
********************on step end call back********************
Step 20040 finish
{'loss': 0.283, 'grad_norm': 1.3159520626068115, 'learning_rate': 6.402930402930403e-05, 'epoch': 3.64}
********************on step end call back********************
Step 20050 finish
{'loss': 0.3027, 'grad_norm': 1.0984206199645996, 'learning_rate': 6.401098901098901e-05, 'epoch': 3.65}
********************on step end call back********************
Step 20060 finish
{'loss': 0.3331, 'grad_norm': 1.204742431640625, 'learning_rate': 6.399267399267399e-05, 'epoch': 3.65}
********************on step end call back********************
Step 20070 finish
{'loss': 0.3189, 'grad_norm': 1.2617801427841187, 'learning_rate': 6.397435897435897e-05, 'epoch': 3.65}
********************on step end call back********************
Step 20080 finish
{'loss': 0.2907, 'grad_norm': 0.728773295879364, 'learning_rate': 6.395604395604396e-05, 'epoch': 3.65}
********************on step end call back********************
Step 20090 finish
{'loss': 0.3403, 'grad_norm': 1.1785238981246948, 'learning_rate': 6.393772893772894e-05, 'epoch': 3.65}
********************on step end call back********************
Step 20100 finish
{'loss': 0.3379, 'grad_norm': 1.3960893154144287, 'learning_rate': 6.391941391941392e-05, 'epoch': 3.65}
{'eval_loss': 0.3321477770805359, 'eval_accuracy': 0.8854166666666666, 'eval_runtime': 129.9069, 'eval_samples_per_second': 4.911, 'eval_steps_per_second': 4.911, 'epoch': 3.65}
********************save call back********************
********************on step end call back********************
Step 20110 finish
[INFO|trainer.py:3376] 2024-03-23 09:07:17,218 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-23 09:07:17,218 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-23 09:07:17,218 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-23 09:09:27,023 >> Saving model checkpoint to ./output/tmp-checkpoint-20200
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-23 09:09:27,199 >> tokenizer config file saved in ./output/tmp-checkpoint-20200/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-23 09:09:27,199 >> Special tokens file saved in ./output/tmp-checkpoint-20200/special_tokens_map.json
[INFO|trainer.py:3159] 2024-03-23 09:09:27,412 >> Deleting older checkpoint [output/checkpoint-10200] due to args.save_total_limit
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-23 09:18:04,969 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-23 09:18:04,970 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-23 09:18:04,970 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-23 09:20:14,196 >> Saving model checkpoint to ./output/tmp-checkpoint-20300
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-23 09:20:14,349 >> tokenizer config file saved in ./output/tmp-checkpoint-20300/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-23 09:20:14,349 >> Special tokens file saved in ./output/tmp-checkpoint-20300/special_tokens_map.json
[INFO|trainer.py:3159] 2024-03-23 09:20:14,563 >> Deleting older checkpoint [output/checkpoint-10300] due to args.save_total_limit
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-23 09:28:59,290 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-23 09:28:59,291 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-23 09:28:59,291 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-23 09:31:08,329 >> Saving model checkpoint to ./output/tmp-checkpoint-20400
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-23 09:31:08,571 >> tokenizer config file saved in ./output/tmp-checkpoint-20400/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-23 09:31:08,572 >> Special tokens file saved in ./output/tmp-checkpoint-20400/special_tokens_map.json
[INFO|trainer.py:3159] 2024-03-23 09:31:08,776 >> Deleting older checkpoint [output/checkpoint-10400] due to args.save_total_limit
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-23 09:39:46,518 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-23 09:39:46,518 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-23 09:39:46,518 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-23 09:41:55,671 >> Saving model checkpoint to ./output/tmp-checkpoint-20500
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-23 09:41:55,832 >> tokenizer config file saved in ./output/tmp-checkpoint-20500/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-23 09:41:55,833 >> Special tokens file saved in ./output/tmp-checkpoint-20500/special_tokens_map.json
[INFO|trainer.py:3159] 2024-03-23 09:41:56,031 >> Deleting older checkpoint [output/checkpoint-10500] due to args.save_total_limit
{'loss': 0.323, 'grad_norm': 0.9827672839164734, 'learning_rate': 6.39010989010989e-05, 'epoch': 3.66}
********************on step end call back********************
Step 20120 finish
{'loss': 0.3503, 'grad_norm': 1.1418157815933228, 'learning_rate': 6.388278388278387e-05, 'epoch': 3.66}
********************on step end call back********************
Step 20130 finish
{'loss': 0.3094, 'grad_norm': 1.0275235176086426, 'learning_rate': 6.386446886446887e-05, 'epoch': 3.66}
********************on step end call back********************
Step 20140 finish
{'loss': 0.3159, 'grad_norm': 1.0598974227905273, 'learning_rate': 6.384615384615385e-05, 'epoch': 3.66}
********************on step end call back********************
Step 20150 finish
{'loss': 0.3172, 'grad_norm': 1.2169764041900635, 'learning_rate': 6.382783882783882e-05, 'epoch': 3.66}
********************on step end call back********************
Step 20160 finish
{'loss': 0.3566, 'grad_norm': 1.0258963108062744, 'learning_rate': 6.38095238095238e-05, 'epoch': 3.67}
********************on step end call back********************
Step 20170 finish
{'loss': 0.3333, 'grad_norm': 1.2357546091079712, 'learning_rate': 6.37912087912088e-05, 'epoch': 3.67}
********************on step end call back********************
Step 20180 finish
{'loss': 0.3285, 'grad_norm': 1.2413324117660522, 'learning_rate': 6.377289377289377e-05, 'epoch': 3.67}
********************on step end call back********************
Step 20190 finish
{'loss': 0.3613, 'grad_norm': 1.2659072875976562, 'learning_rate': 6.375457875457875e-05, 'epoch': 3.67}
********************on step end call back********************
Step 20200 finish
{'loss': 0.2972, 'grad_norm': 1.3540979623794556, 'learning_rate': 6.373626373626373e-05, 'epoch': 3.67}
{'eval_loss': 0.3290233910083771, 'eval_accuracy': 0.8645833333333334, 'eval_runtime': 129.8035, 'eval_samples_per_second': 4.915, 'eval_steps_per_second': 4.915, 'epoch': 3.67}
********************save call back********************
********************on step end call back********************
Step 20210 finish
{'loss': 0.3019, 'grad_norm': 1.0207222700119019, 'learning_rate': 6.371794871794871e-05, 'epoch': 3.67}
********************on step end call back********************
Step 20220 finish
{'loss': 0.3276, 'grad_norm': 0.9904522895812988, 'learning_rate': 6.36996336996337e-05, 'epoch': 3.68}
********************on step end call back********************
Step 20230 finish
{'loss': 0.3191, 'grad_norm': 0.9389744997024536, 'learning_rate': 6.368131868131868e-05, 'epoch': 3.68}
********************on step end call back********************
Step 20240 finish
{'loss': 0.3214, 'grad_norm': 1.4020432233810425, 'learning_rate': 6.366300366300366e-05, 'epoch': 3.68}
********************on step end call back********************
Step 20250 finish
{'loss': 0.3449, 'grad_norm': 1.336022973060608, 'learning_rate': 6.364468864468864e-05, 'epoch': 3.68}
********************on step end call back********************
Step 20260 finish
{'loss': 0.3502, 'grad_norm': 1.2876348495483398, 'learning_rate': 6.362637362637363e-05, 'epoch': 3.68}
********************on step end call back********************
Step 20270 finish
{'loss': 0.354, 'grad_norm': 1.2331206798553467, 'learning_rate': 6.360805860805861e-05, 'epoch': 3.69}
********************on step end call back********************
Step 20280 finish
{'loss': 0.3236, 'grad_norm': 1.209884762763977, 'learning_rate': 6.358974358974359e-05, 'epoch': 3.69}
********************on step end call back********************
Step 20290 finish
{'loss': 0.2828, 'grad_norm': 1.2780656814575195, 'learning_rate': 6.357142857142857e-05, 'epoch': 3.69}
********************on step end call back********************
Step 20300 finish
{'loss': 0.342, 'grad_norm': 1.080557107925415, 'learning_rate': 6.355311355311355e-05, 'epoch': 3.69}
{'eval_loss': 0.3301606774330139, 'eval_accuracy': 0.8541666666666666, 'eval_runtime': 129.2254, 'eval_samples_per_second': 4.937, 'eval_steps_per_second': 4.937, 'epoch': 3.69}
********************save call back********************
********************on step end call back********************
Step 20310 finish
{'loss': 0.3181, 'grad_norm': 0.954565167427063, 'learning_rate': 6.353479853479854e-05, 'epoch': 3.69}
********************on step end call back********************
Step 20320 finish
{'loss': 0.3477, 'grad_norm': 1.351815938949585, 'learning_rate': 6.351648351648352e-05, 'epoch': 3.69}
********************on step end call back********************
Step 20330 finish
{'loss': 0.343, 'grad_norm': 1.5360589027404785, 'learning_rate': 6.34981684981685e-05, 'epoch': 3.7}
********************on step end call back********************
Step 20340 finish
{'loss': 0.3252, 'grad_norm': 1.329696536064148, 'learning_rate': 6.347985347985348e-05, 'epoch': 3.7}
********************on step end call back********************
Step 20350 finish
{'loss': 0.2831, 'grad_norm': 1.4100062847137451, 'learning_rate': 6.346153846153847e-05, 'epoch': 3.7}
********************on step end call back********************
Step 20360 finish
{'loss': 0.3232, 'grad_norm': 1.1009917259216309, 'learning_rate': 6.344322344322345e-05, 'epoch': 3.7}
********************on step end call back********************
Step 20370 finish
{'loss': 0.2724, 'grad_norm': 1.1519352197647095, 'learning_rate': 6.342490842490842e-05, 'epoch': 3.7}
********************on step end call back********************
Step 20380 finish
{'loss': 0.3748, 'grad_norm': 2.137515068054199, 'learning_rate': 6.34065934065934e-05, 'epoch': 3.71}
********************on step end call back********************
Step 20390 finish
{'loss': 0.3372, 'grad_norm': 1.5234975814819336, 'learning_rate': 6.338827838827838e-05, 'epoch': 3.71}
********************on step end call back********************
Step 20400 finish
{'loss': 0.3573, 'grad_norm': 1.4721869230270386, 'learning_rate': 6.336996336996337e-05, 'epoch': 3.71}
{'eval_loss': 0.32296812534332275, 'eval_accuracy': 0.8645833333333334, 'eval_runtime': 129.0375, 'eval_samples_per_second': 4.944, 'eval_steps_per_second': 4.944, 'epoch': 3.71}
********************save call back********************
********************on step end call back********************
Step 20410 finish
{'loss': 0.287, 'grad_norm': 0.9919646978378296, 'learning_rate': 6.335164835164835e-05, 'epoch': 3.71}
********************on step end call back********************
Step 20420 finish
{'loss': 0.3033, 'grad_norm': 0.8096774220466614, 'learning_rate': 6.333333333333333e-05, 'epoch': 3.71}
********************on step end call back********************
Step 20430 finish
{'loss': 0.3654, 'grad_norm': 1.6326497793197632, 'learning_rate': 6.331501831501831e-05, 'epoch': 3.71}
********************on step end call back********************
Step 20440 finish
{'loss': 0.3513, 'grad_norm': 1.3088940382003784, 'learning_rate': 6.32967032967033e-05, 'epoch': 3.72}
********************on step end call back********************
Step 20450 finish
{'loss': 0.3448, 'grad_norm': 1.4673233032226562, 'learning_rate': 6.327838827838828e-05, 'epoch': 3.72}
********************on step end call back********************
Step 20460 finish
{'loss': 0.3659, 'grad_norm': 1.384169340133667, 'learning_rate': 6.326007326007326e-05, 'epoch': 3.72}
********************on step end call back********************
Step 20470 finish
{'loss': 0.334, 'grad_norm': 1.0724879503250122, 'learning_rate': 6.324175824175824e-05, 'epoch': 3.72}
********************on step end call back********************
Step 20480 finish
{'loss': 0.3011, 'grad_norm': 1.0420303344726562, 'learning_rate': 6.322344322344322e-05, 'epoch': 3.72}
********************on step end call back********************
Step 20490 finish
{'loss': 0.305, 'grad_norm': 1.3753916025161743, 'learning_rate': 6.320512820512821e-05, 'epoch': 3.73}
********************on step end call back********************
Step 20500 finish
{'loss': 0.3636, 'grad_norm': 1.1399500370025635, 'learning_rate': 6.318681318681319e-05, 'epoch': 3.73}
{'eval_loss': 0.3353254795074463, 'eval_accuracy': 0.8958333333333334, 'eval_runtime': 129.1521, 'eval_samples_per_second': 4.94, 'eval_steps_per_second': 4.94, 'epoch': 3.73}
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-23 09:50:29,019 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-23 09:50:29,019 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-23 09:50:29,019 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-23 09:52:37,995 >> Saving model checkpoint to ./output/tmp-checkpoint-20600
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-23 09:52:38,161 >> tokenizer config file saved in ./output/tmp-checkpoint-20600/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-23 09:52:38,161 >> Special tokens file saved in ./output/tmp-checkpoint-20600/special_tokens_map.json
[INFO|trainer.py:3159] 2024-03-23 09:52:38,370 >> Deleting older checkpoint [output/checkpoint-10600] due to args.save_total_limit
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-23 10:01:16,748 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-23 10:01:16,748 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-23 10:01:16,748 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-23 10:03:25,788 >> Saving model checkpoint to ./output/tmp-checkpoint-20700
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-23 10:03:25,945 >> tokenizer config file saved in ./output/tmp-checkpoint-20700/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-23 10:03:25,945 >> Special tokens file saved in ./output/tmp-checkpoint-20700/special_tokens_map.json
[INFO|trainer.py:3159] 2024-03-23 10:03:26,155 >> Deleting older checkpoint [output/checkpoint-10700] due to args.save_total_limit
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-23 10:12:03,810 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-23 10:12:03,810 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-23 10:12:03,811 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-23 10:14:13,009 >> Saving model checkpoint to ./output/tmp-checkpoint-20800
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-23 10:14:13,219 >> tokenizer config file saved in ./output/tmp-checkpoint-20800/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-23 10:14:13,220 >> Special tokens file saved in ./output/tmp-checkpoint-20800/special_tokens_map.json
[INFO|trainer.py:3159] 2024-03-23 10:14:13,432 >> Deleting older checkpoint [output/checkpoint-10800] due to args.save_total_limit
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-23 10:22:48,053 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-23 10:22:48,053 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-23 10:22:48,053 >>   Batch size = 1
********************save call back********************
********************on step end call back********************
Step 20510 finish
{'loss': 0.3167, 'grad_norm': 1.3326804637908936, 'learning_rate': 6.316849816849817e-05, 'epoch': 3.73}
********************on step end call back********************
Step 20520 finish
{'loss': 0.3254, 'grad_norm': 1.8619400262832642, 'learning_rate': 6.315018315018315e-05, 'epoch': 3.73}
********************on step end call back********************
Step 20530 finish
{'loss': 0.2934, 'grad_norm': 1.1886367797851562, 'learning_rate': 6.313186813186814e-05, 'epoch': 3.73}
********************on step end call back********************
Step 20540 finish
{'loss': 0.3314, 'grad_norm': 1.1286121606826782, 'learning_rate': 6.311355311355312e-05, 'epoch': 3.73}
********************on step end call back********************
Step 20550 finish
{'loss': 0.3113, 'grad_norm': 1.4941760301589966, 'learning_rate': 6.30952380952381e-05, 'epoch': 3.74}
********************on step end call back********************
Step 20560 finish
{'loss': 0.3044, 'grad_norm': 1.2660186290740967, 'learning_rate': 6.307692307692308e-05, 'epoch': 3.74}
********************on step end call back********************
Step 20570 finish
{'loss': 0.3342, 'grad_norm': 1.7402064800262451, 'learning_rate': 6.305860805860805e-05, 'epoch': 3.74}
********************on step end call back********************
Step 20580 finish
{'loss': 0.3239, 'grad_norm': 1.230907678604126, 'learning_rate': 6.304029304029305e-05, 'epoch': 3.74}
********************on step end call back********************
Step 20590 finish
{'loss': 0.3367, 'grad_norm': 1.2382534742355347, 'learning_rate': 6.302197802197803e-05, 'epoch': 3.74}
********************on step end call back********************
Step 20600 finish
{'loss': 0.318, 'grad_norm': 1.074265718460083, 'learning_rate': 6.3003663003663e-05, 'epoch': 3.75}
{'eval_loss': 0.3258562684059143, 'eval_accuracy': 0.8854166666666666, 'eval_runtime': 128.9758, 'eval_samples_per_second': 4.947, 'eval_steps_per_second': 4.947, 'epoch': 3.75}
********************save call back********************
********************on step end call back********************
Step 20610 finish
{'loss': 0.3029, 'grad_norm': 0.9613075256347656, 'learning_rate': 6.298534798534798e-05, 'epoch': 3.75}
********************on step end call back********************
Step 20620 finish
{'loss': 0.3623, 'grad_norm': 1.0201035737991333, 'learning_rate': 6.296703296703296e-05, 'epoch': 3.75}
********************on step end call back********************
Step 20630 finish
{'loss': 0.3323, 'grad_norm': 1.3546879291534424, 'learning_rate': 6.294871794871795e-05, 'epoch': 3.75}
********************on step end call back********************
Step 20640 finish
{'loss': 0.3195, 'grad_norm': 1.1047159433364868, 'learning_rate': 6.293040293040293e-05, 'epoch': 3.75}
********************on step end call back********************
Step 20650 finish
{'loss': 0.356, 'grad_norm': 1.1470412015914917, 'learning_rate': 6.291208791208791e-05, 'epoch': 3.75}
********************on step end call back********************
Step 20660 finish
{'loss': 0.2881, 'grad_norm': 1.1650493144989014, 'learning_rate': 6.289377289377289e-05, 'epoch': 3.76}
********************on step end call back********************
Step 20670 finish
{'loss': 0.3227, 'grad_norm': 1.1471666097640991, 'learning_rate': 6.287545787545788e-05, 'epoch': 3.76}
********************on step end call back********************
Step 20680 finish
{'loss': 0.3128, 'grad_norm': 0.9929109215736389, 'learning_rate': 6.285714285714286e-05, 'epoch': 3.76}
********************on step end call back********************
Step 20690 finish
{'loss': 0.3266, 'grad_norm': 1.0426979064941406, 'learning_rate': 6.283882783882784e-05, 'epoch': 3.76}
********************on step end call back********************
Step 20700 finish
{'loss': 0.3365, 'grad_norm': 1.5270997285842896, 'learning_rate': 6.282051282051282e-05, 'epoch': 3.76}
{'eval_loss': 0.3277686834335327, 'eval_accuracy': 0.875, 'eval_runtime': 129.0386, 'eval_samples_per_second': 4.944, 'eval_steps_per_second': 4.944, 'epoch': 3.76}
********************save call back********************
********************on step end call back********************
Step 20710 finish
{'loss': 0.3207, 'grad_norm': 1.1507130861282349, 'learning_rate': 6.28021978021978e-05, 'epoch': 3.77}
********************on step end call back********************
Step 20720 finish
{'loss': 0.3324, 'grad_norm': 1.1533082723617554, 'learning_rate': 6.278388278388279e-05, 'epoch': 3.77}
********************on step end call back********************
Step 20730 finish
{'loss': 0.3383, 'grad_norm': 1.2140636444091797, 'learning_rate': 6.276556776556777e-05, 'epoch': 3.77}
********************on step end call back********************
Step 20740 finish
{'loss': 0.328, 'grad_norm': 1.3645535707473755, 'learning_rate': 6.274725274725275e-05, 'epoch': 3.77}
********************on step end call back********************
Step 20750 finish
{'loss': 0.3347, 'grad_norm': 1.1216118335723877, 'learning_rate': 6.272893772893773e-05, 'epoch': 3.77}
********************on step end call back********************
Step 20760 finish
{'loss': 0.313, 'grad_norm': 1.234715223312378, 'learning_rate': 6.271062271062272e-05, 'epoch': 3.77}
********************on step end call back********************
Step 20770 finish
{'loss': 0.3112, 'grad_norm': 1.0310178995132446, 'learning_rate': 6.26923076923077e-05, 'epoch': 3.78}
********************on step end call back********************
Step 20780 finish
{'loss': 0.3255, 'grad_norm': 0.9847497344017029, 'learning_rate': 6.267399267399268e-05, 'epoch': 3.78}
********************on step end call back********************
Step 20790 finish
{'loss': 0.3805, 'grad_norm': 1.0936390161514282, 'learning_rate': 6.265567765567765e-05, 'epoch': 3.78}
********************on step end call back********************
Step 20800 finish
{'loss': 0.3505, 'grad_norm': 0.9522353410720825, 'learning_rate': 6.263736263736263e-05, 'epoch': 3.78}
{'eval_loss': 0.33296841382980347, 'eval_accuracy': 0.8854166666666666, 'eval_runtime': 129.1974, 'eval_samples_per_second': 4.938, 'eval_steps_per_second': 4.938, 'epoch': 3.78}
********************save call back********************
********************on step end call back********************
Step 20810 finish
{'loss': 0.3185, 'grad_norm': 1.0484727621078491, 'learning_rate': 6.261904761904763e-05, 'epoch': 3.78}
********************on step end call back********************
Step 20820 finish
{'loss': 0.3461, 'grad_norm': 1.3749655485153198, 'learning_rate': 6.26007326007326e-05, 'epoch': 3.79}
********************on step end call back********************
Step 20830 finish
{'loss': 0.3201, 'grad_norm': 1.3877861499786377, 'learning_rate': 6.258241758241758e-05, 'epoch': 3.79}
********************on step end call back********************
Step 20840 finish
{'loss': 0.2779, 'grad_norm': 1.2606772184371948, 'learning_rate': 6.256410256410256e-05, 'epoch': 3.79}
********************on step end call back********************
Step 20850 finish
{'loss': 0.2917, 'grad_norm': 1.2533442974090576, 'learning_rate': 6.254578754578755e-05, 'epoch': 3.79}
********************on step end call back********************
Step 20860 finish
{'loss': 0.3628, 'grad_norm': 1.7407597303390503, 'learning_rate': 6.252747252747253e-05, 'epoch': 3.79}
********************on step end call back********************
Step 20870 finish
{'loss': 0.3514, 'grad_norm': 1.161627173423767, 'learning_rate': 6.250915750915751e-05, 'epoch': 3.79}
********************on step end call back********************
Step 20880 finish
{'loss': 0.3291, 'grad_norm': 1.3080381155014038, 'learning_rate': 6.249084249084249e-05, 'epoch': 3.8}
********************on step end call back********************
Step 20890 finish
{'loss': 0.3261, 'grad_norm': 1.2830679416656494, 'learning_rate': 6.247252747252747e-05, 'epoch': 3.8}
********************on step end call back********************
Step 20900 finish
{'loss': 0.3261, 'grad_norm': 1.0624116659164429, 'learning_rate': 6.245421245421246e-05, 'epoch': 3.8}
[INFO|trainer.py:3067] 2024-03-23 10:24:57,371 >> Saving model checkpoint to ./output/tmp-checkpoint-20900
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-23 10:24:57,533 >> tokenizer config file saved in ./output/tmp-checkpoint-20900/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-23 10:24:57,534 >> Special tokens file saved in ./output/tmp-checkpoint-20900/special_tokens_map.json
[INFO|trainer.py:3159] 2024-03-23 10:24:57,744 >> Deleting older checkpoint [output/checkpoint-10900] due to args.save_total_limit
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-23 10:33:31,258 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-23 10:33:31,258 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-23 10:33:31,258 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-23 10:35:40,555 >> Saving model checkpoint to ./output/tmp-checkpoint-21000
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-23 10:35:40,696 >> tokenizer config file saved in ./output/tmp-checkpoint-21000/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-23 10:35:40,696 >> Special tokens file saved in ./output/tmp-checkpoint-21000/special_tokens_map.json
[INFO|trainer.py:3159] 2024-03-23 10:35:40,901 >> Deleting older checkpoint [output/checkpoint-11000] due to args.save_total_limit
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-23 10:44:18,011 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-23 10:44:18,011 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-23 10:44:18,011 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-23 10:46:27,348 >> Saving model checkpoint to ./output/tmp-checkpoint-21100
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-23 10:46:27,492 >> tokenizer config file saved in ./output/tmp-checkpoint-21100/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-23 10:46:27,492 >> Special tokens file saved in ./output/tmp-checkpoint-21100/special_tokens_map.json
[INFO|trainer.py:3159] 2024-03-23 10:46:27,696 >> Deleting older checkpoint [output/checkpoint-11100] due to args.save_total_limit
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-23 10:55:04,906 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-23 10:55:04,906 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-23 10:55:04,906 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-23 10:57:14,271 >> Saving model checkpoint to ./output/tmp-checkpoint-21200
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-23 10:57:14,413 >> tokenizer config file saved in ./output/tmp-checkpoint-21200/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-23 10:57:14,414 >> Special tokens file saved in ./output/tmp-checkpoint-21200/special_tokens_map.json
[INFO|trainer.py:3159] 2024-03-23 10:57:14,620 >> Deleting older checkpoint [output/checkpoint-11200] due to args.save_total_limit
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
{'eval_loss': 0.33652669191360474, 'eval_accuracy': 0.90625, 'eval_runtime': 129.3166, 'eval_samples_per_second': 4.934, 'eval_steps_per_second': 4.934, 'epoch': 3.8}
********************save call back********************
********************on step end call back********************
Step 20910 finish
{'loss': 0.3414, 'grad_norm': 1.3541734218597412, 'learning_rate': 6.243589743589744e-05, 'epoch': 3.8}
********************on step end call back********************
Step 20920 finish
{'loss': 0.3195, 'grad_norm': 1.1855212450027466, 'learning_rate': 6.241758241758242e-05, 'epoch': 3.8}
********************on step end call back********************
Step 20930 finish
{'loss': 0.3032, 'grad_norm': 1.2718697786331177, 'learning_rate': 6.23992673992674e-05, 'epoch': 3.81}
********************on step end call back********************
Step 20940 finish
{'loss': 0.3329, 'grad_norm': 1.0109117031097412, 'learning_rate': 6.238095238095239e-05, 'epoch': 3.81}
********************on step end call back********************
Step 20950 finish
{'loss': 0.3113, 'grad_norm': 0.9214931726455688, 'learning_rate': 6.236263736263737e-05, 'epoch': 3.81}
********************on step end call back********************
Step 20960 finish
{'loss': 0.3607, 'grad_norm': 1.3797495365142822, 'learning_rate': 6.234432234432235e-05, 'epoch': 3.81}
********************on step end call back********************
Step 20970 finish
{'loss': 0.3049, 'grad_norm': 1.0233687162399292, 'learning_rate': 6.232600732600733e-05, 'epoch': 3.81}
********************on step end call back********************
Step 20980 finish
{'loss': 0.3307, 'grad_norm': 1.1759650707244873, 'learning_rate': 6.23076923076923e-05, 'epoch': 3.81}
********************on step end call back********************
Step 20990 finish
{'loss': 0.2938, 'grad_norm': 0.8952001929283142, 'learning_rate': 6.22893772893773e-05, 'epoch': 3.82}
********************on step end call back********************
Step 21000 finish
{'loss': 0.323, 'grad_norm': 1.170756459236145, 'learning_rate': 6.227106227106228e-05, 'epoch': 3.82}
{'eval_loss': 0.3321230709552765, 'eval_accuracy': 0.8958333333333334, 'eval_runtime': 129.2964, 'eval_samples_per_second': 4.934, 'eval_steps_per_second': 4.934, 'epoch': 3.82}
********************save call back********************
********************on step end call back********************
Step 21010 finish
{'loss': 0.3093, 'grad_norm': 1.2370877265930176, 'learning_rate': 6.225274725274726e-05, 'epoch': 3.82}
********************on step end call back********************
Step 21020 finish
{'loss': 0.3405, 'grad_norm': 1.5999687910079956, 'learning_rate': 6.223443223443223e-05, 'epoch': 3.82}
********************on step end call back********************
Step 21030 finish
{'loss': 0.2977, 'grad_norm': 1.279615044593811, 'learning_rate': 6.221611721611723e-05, 'epoch': 3.82}
********************on step end call back********************
Step 21040 finish
{'loss': 0.3412, 'grad_norm': 0.8143678903579712, 'learning_rate': 6.21978021978022e-05, 'epoch': 3.83}
********************on step end call back********************
Step 21050 finish
{'loss': 0.3048, 'grad_norm': 0.9600524306297302, 'learning_rate': 6.217948717948718e-05, 'epoch': 3.83}
********************on step end call back********************
Step 21060 finish
{'loss': 0.3392, 'grad_norm': 1.3310129642486572, 'learning_rate': 6.216117216117216e-05, 'epoch': 3.83}
********************on step end call back********************
Step 21070 finish
{'loss': 0.3128, 'grad_norm': 1.231142282485962, 'learning_rate': 6.214285714285714e-05, 'epoch': 3.83}
********************on step end call back********************
Step 21080 finish
{'loss': 0.3122, 'grad_norm': 1.1984302997589111, 'learning_rate': 6.212454212454213e-05, 'epoch': 3.83}
********************on step end call back********************
Step 21090 finish
{'loss': 0.3124, 'grad_norm': 1.0267043113708496, 'learning_rate': 6.210622710622711e-05, 'epoch': 3.83}
********************on step end call back********************
Step 21100 finish
{'loss': 0.337, 'grad_norm': 1.41006600856781, 'learning_rate': 6.208791208791209e-05, 'epoch': 3.84}
{'eval_loss': 0.3268190622329712, 'eval_accuracy': 0.8958333333333334, 'eval_runtime': 129.3362, 'eval_samples_per_second': 4.933, 'eval_steps_per_second': 4.933, 'epoch': 3.84}
********************save call back********************
********************on step end call back********************
Step 21110 finish
{'loss': 0.3441, 'grad_norm': 1.3534586429595947, 'learning_rate': 6.206959706959707e-05, 'epoch': 3.84}
********************on step end call back********************
Step 21120 finish
{'loss': 0.3274, 'grad_norm': 1.1048048734664917, 'learning_rate': 6.205128205128206e-05, 'epoch': 3.84}
********************on step end call back********************
Step 21130 finish
{'loss': 0.3554, 'grad_norm': 1.4313247203826904, 'learning_rate': 6.203296703296704e-05, 'epoch': 3.84}
********************on step end call back********************
Step 21140 finish
{'loss': 0.3047, 'grad_norm': 1.3522511720657349, 'learning_rate': 6.201465201465202e-05, 'epoch': 3.84}
********************on step end call back********************
Step 21150 finish
{'loss': 0.3285, 'grad_norm': 1.2344597578048706, 'learning_rate': 6.1996336996337e-05, 'epoch': 3.85}
********************on step end call back********************
Step 21160 finish
{'loss': 0.3467, 'grad_norm': 1.0684987306594849, 'learning_rate': 6.197802197802198e-05, 'epoch': 3.85}
********************on step end call back********************
Step 21170 finish
{'loss': 0.3469, 'grad_norm': 1.2120680809020996, 'learning_rate': 6.195970695970697e-05, 'epoch': 3.85}
********************on step end call back********************
Step 21180 finish
{'loss': 0.3092, 'grad_norm': 0.9667508006095886, 'learning_rate': 6.194139194139195e-05, 'epoch': 3.85}
********************on step end call back********************
Step 21190 finish
{'loss': 0.3195, 'grad_norm': 1.4208827018737793, 'learning_rate': 6.192307692307693e-05, 'epoch': 3.85}
********************on step end call back********************
Step 21200 finish
{'loss': 0.2947, 'grad_norm': 1.1102489233016968, 'learning_rate': 6.19047619047619e-05, 'epoch': 3.85}
{'eval_loss': 0.3268197178840637, 'eval_accuracy': 0.8958333333333334, 'eval_runtime': 129.364, 'eval_samples_per_second': 4.932, 'eval_steps_per_second': 4.932, 'epoch': 3.85}
********************save call back********************
********************on step end call back********************
Step 21210 finish
{'loss': 0.3505, 'grad_norm': 1.1596879959106445, 'learning_rate': 6.18864468864469e-05, 'epoch': 3.86}
********************on step end call back********************
Step 21220 finish
{'loss': 0.3544, 'grad_norm': 1.0138187408447266, 'learning_rate': 6.186813186813188e-05, 'epoch': 3.86}
********************on step end call back********************
Step 21230 finish
{'loss': 0.3164, 'grad_norm': 1.306776523590088, 'learning_rate': 6.184981684981686e-05, 'epoch': 3.86}
********************on step end call back********************
Step 21240 finish
{'loss': 0.3299, 'grad_norm': 1.2601920366287231, 'learning_rate': 6.183150183150183e-05, 'epoch': 3.86}
********************on step end call back********************
Step 21250 finish
{'loss': 0.3307, 'grad_norm': 1.2049133777618408, 'learning_rate': 6.181318681318681e-05, 'epoch': 3.86}
********************on step end call back********************
Step 21260 finish
{'loss': 0.3203, 'grad_norm': 1.0657457113265991, 'learning_rate': 6.17948717948718e-05, 'epoch': 3.87}
********************on step end call back********************
Step 21270 finish
{'loss': 0.2809, 'grad_norm': 1.0578614473342896, 'learning_rate': 6.177655677655678e-05, 'epoch': 3.87}
********************on step end call back********************
Step 21280 finish
{'loss': 0.3049, 'grad_norm': 1.4161386489868164, 'learning_rate': 6.175824175824176e-05, 'epoch': 3.87}
********************on step end call back********************
Step 21290 finish
{'loss': 0.3523, 'grad_norm': 1.167049765586853, 'learning_rate': 6.173992673992674e-05, 'epoch': 3.87}
********************on step end call back********************
[INFO|trainer.py:3376] 2024-03-23 11:05:50,490 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-23 11:05:50,490 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-23 11:05:50,490 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-23 11:07:59,844 >> Saving model checkpoint to ./output/tmp-checkpoint-21300
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-23 11:07:59,984 >> tokenizer config file saved in ./output/tmp-checkpoint-21300/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-23 11:07:59,985 >> Special tokens file saved in ./output/tmp-checkpoint-21300/special_tokens_map.json
[INFO|trainer.py:3159] 2024-03-23 11:08:00,193 >> Deleting older checkpoint [output/checkpoint-11300] due to args.save_total_limit
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-23 11:16:44,020 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-23 11:16:44,020 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-23 11:16:44,020 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-23 11:18:53,357 >> Saving model checkpoint to ./output/tmp-checkpoint-21400
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-23 11:18:53,498 >> tokenizer config file saved in ./output/tmp-checkpoint-21400/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-23 11:18:53,498 >> Special tokens file saved in ./output/tmp-checkpoint-21400/special_tokens_map.json
[INFO|trainer.py:3159] 2024-03-23 11:18:53,702 >> Deleting older checkpoint [output/checkpoint-11400] due to args.save_total_limit
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-23 11:27:31,686 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-23 11:27:31,686 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-23 11:27:31,686 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-23 11:29:41,581 >> Saving model checkpoint to ./output/tmp-checkpoint-21500
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-23 11:29:41,728 >> tokenizer config file saved in ./output/tmp-checkpoint-21500/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-23 11:29:41,728 >> Special tokens file saved in ./output/tmp-checkpoint-21500/special_tokens_map.json
[INFO|trainer.py:3159] 2024-03-23 11:29:41,942 >> Deleting older checkpoint [output/checkpoint-11500] due to args.save_total_limit
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-23 11:38:14,719 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-23 11:38:14,719 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-23 11:38:14,719 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-23 11:40:24,311 >> Saving model checkpoint to ./output/tmp-checkpoint-21600
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-23 11:40:24,450 >> tokenizer config file saved in ./output/tmp-checkpoint-21600/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-23 11:40:24,450 >> Special tokens file saved in ./output/tmp-checkpoint-21600/special_tokens_map.json
[INFO|trainer.py:3159] 2024-03-23 11:40:24,659 >> Deleting older checkpoint [output/checkpoint-11600] due to args.save_total_limit
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
Step 21300 finish
{'loss': 0.3278, 'grad_norm': 1.4802957773208618, 'learning_rate': 6.172161172161173e-05, 'epoch': 3.87}
{'eval_loss': 0.3269160985946655, 'eval_accuracy': 0.8854166666666666, 'eval_runtime': 129.3526, 'eval_samples_per_second': 4.932, 'eval_steps_per_second': 4.932, 'epoch': 3.87}
********************save call back********************
********************on step end call back********************
Step 21310 finish
{'loss': 0.322, 'grad_norm': 1.1055161952972412, 'learning_rate': 6.170329670329671e-05, 'epoch': 3.87}
********************on step end call back********************
Step 21320 finish
{'loss': 0.3227, 'grad_norm': 1.4008923768997192, 'learning_rate': 6.168498168498169e-05, 'epoch': 3.88}
********************on step end call back********************
Step 21330 finish
{'loss': 0.3485, 'grad_norm': 1.233424186706543, 'learning_rate': 6.166666666666667e-05, 'epoch': 3.88}
********************on step end call back********************
Step 21340 finish
{'loss': 0.3066, 'grad_norm': 1.1430584192276, 'learning_rate': 6.164835164835165e-05, 'epoch': 3.88}
********************on step end call back********************
Step 21350 finish
{'loss': 0.3162, 'grad_norm': 1.1490094661712646, 'learning_rate': 6.163003663003664e-05, 'epoch': 3.88}
********************on step end call back********************
Step 21360 finish
{'loss': 0.3422, 'grad_norm': 1.0669885873794556, 'learning_rate': 6.161172161172162e-05, 'epoch': 3.88}
********************on step end call back********************
Step 21370 finish
{'loss': 0.2834, 'grad_norm': 1.075345754623413, 'learning_rate': 6.15934065934066e-05, 'epoch': 3.89}
********************on step end call back********************
Step 21380 finish
{'loss': 0.3463, 'grad_norm': 1.1900280714035034, 'learning_rate': 6.157509157509158e-05, 'epoch': 3.89}
********************on step end call back********************
Step 21390 finish
{'loss': 0.3233, 'grad_norm': 1.3187603950500488, 'learning_rate': 6.155677655677656e-05, 'epoch': 3.89}
********************on step end call back********************
Step 21400 finish
{'loss': 0.3217, 'grad_norm': 0.8301804661750793, 'learning_rate': 6.153846153846155e-05, 'epoch': 3.89}
{'eval_loss': 0.32638847827911377, 'eval_accuracy': 0.8854166666666666, 'eval_runtime': 129.3355, 'eval_samples_per_second': 4.933, 'eval_steps_per_second': 4.933, 'epoch': 3.89}
********************save call back********************
********************on step end call back********************
Step 21410 finish
{'loss': 0.3422, 'grad_norm': 1.134164810180664, 'learning_rate': 6.152014652014653e-05, 'epoch': 3.89}
********************on step end call back********************
Step 21420 finish
{'loss': 0.3257, 'grad_norm': 1.3243463039398193, 'learning_rate': 6.15018315018315e-05, 'epoch': 3.89}
********************on step end call back********************
Step 21430 finish
{'loss': 0.3166, 'grad_norm': 0.8520512580871582, 'learning_rate': 6.148351648351648e-05, 'epoch': 3.9}
********************on step end call back********************
Step 21440 finish
{'loss': 0.302, 'grad_norm': 1.1022521257400513, 'learning_rate': 6.146520146520148e-05, 'epoch': 3.9}
********************on step end call back********************
Step 21450 finish
{'loss': 0.2839, 'grad_norm': 1.0905405282974243, 'learning_rate': 6.144688644688646e-05, 'epoch': 3.9}
********************on step end call back********************
Step 21460 finish
{'loss': 0.303, 'grad_norm': 1.5598920583724976, 'learning_rate': 6.142857142857143e-05, 'epoch': 3.9}
********************on step end call back********************
Step 21470 finish
{'loss': 0.3277, 'grad_norm': 0.982802152633667, 'learning_rate': 6.141025641025641e-05, 'epoch': 3.9}
********************on step end call back********************
Step 21480 finish
{'loss': 0.3128, 'grad_norm': 1.4776536226272583, 'learning_rate': 6.139194139194139e-05, 'epoch': 3.91}
********************on step end call back********************
Step 21490 finish
{'loss': 0.3646, 'grad_norm': 1.3201444149017334, 'learning_rate': 6.137362637362638e-05, 'epoch': 3.91}
********************on step end call back********************
Step 21500 finish
{'loss': 0.3482, 'grad_norm': 1.3021665811538696, 'learning_rate': 6.135531135531136e-05, 'epoch': 3.91}
{'eval_loss': 0.32733240723609924, 'eval_accuracy': 0.8854166666666666, 'eval_runtime': 129.894, 'eval_samples_per_second': 4.912, 'eval_steps_per_second': 4.912, 'epoch': 3.91}
********************save call back********************
********************on step end call back********************
Step 21510 finish
{'loss': 0.2851, 'grad_norm': 1.0095537900924683, 'learning_rate': 6.133699633699634e-05, 'epoch': 3.91}
********************on step end call back********************
Step 21520 finish
{'loss': 0.3581, 'grad_norm': 1.1073520183563232, 'learning_rate': 6.131868131868132e-05, 'epoch': 3.91}
********************on step end call back********************
Step 21530 finish
{'loss': 0.3001, 'grad_norm': 1.1835596561431885, 'learning_rate': 6.130036630036631e-05, 'epoch': 3.91}
********************on step end call back********************
Step 21540 finish
{'loss': 0.3497, 'grad_norm': 1.2156636714935303, 'learning_rate': 6.128205128205129e-05, 'epoch': 3.92}
********************on step end call back********************
Step 21550 finish
{'loss': 0.3302, 'grad_norm': 1.482547640800476, 'learning_rate': 6.126373626373627e-05, 'epoch': 3.92}
********************on step end call back********************
Step 21560 finish
{'loss': 0.3229, 'grad_norm': 1.1756994724273682, 'learning_rate': 6.124542124542125e-05, 'epoch': 3.92}
********************on step end call back********************
Step 21570 finish
{'loss': 0.3343, 'grad_norm': 1.0883764028549194, 'learning_rate': 6.122710622710623e-05, 'epoch': 3.92}
********************on step end call back********************
Step 21580 finish
{'loss': 0.3206, 'grad_norm': 1.1333417892456055, 'learning_rate': 6.120879120879122e-05, 'epoch': 3.92}
********************on step end call back********************
Step 21590 finish
{'loss': 0.3258, 'grad_norm': 1.7989283800125122, 'learning_rate': 6.11904761904762e-05, 'epoch': 3.93}
********************on step end call back********************
Step 21600 finish
{'loss': 0.3049, 'grad_norm': 1.0365090370178223, 'learning_rate': 6.117216117216118e-05, 'epoch': 3.93}
{'eval_loss': 0.335803359746933, 'eval_accuracy': 0.8854166666666666, 'eval_runtime': 129.5908, 'eval_samples_per_second': 4.923, 'eval_steps_per_second': 4.923, 'epoch': 3.93}
********************save call back********************
********************on step end call back********************
Step 21610 finish
{'loss': 0.3278, 'grad_norm': 0.8794721364974976, 'learning_rate': 6.115384615384616e-05, 'epoch': 3.93}
********************on step end call back********************
Step 21620 finish
{'loss': 0.3414, 'grad_norm': 1.2277449369430542, 'learning_rate': 6.113553113553115e-05, 'epoch': 3.93}
********************on step end call back********************
Step 21630 finish
{'loss': 0.3641, 'grad_norm': 1.1179673671722412, 'learning_rate': 6.111721611721613e-05, 'epoch': 3.93}
********************on step end call back********************
Step 21640 finish
{'loss': 0.3137, 'grad_norm': 1.2539095878601074, 'learning_rate': 6.10989010989011e-05, 'epoch': 3.93}
********************on step end call back********************
Step 21650 finish
{'loss': 0.3062, 'grad_norm': 0.7051047682762146, 'learning_rate': 6.108058608058609e-05, 'epoch': 3.94}
********************on step end call back********************
Step 21660 finish
{'loss': 0.3297, 'grad_norm': 1.4975858926773071, 'learning_rate': 6.106227106227106e-05, 'epoch': 3.94}
********************on step end call back********************
Step 21670 finish
{'loss': 0.3016, 'grad_norm': 1.0832163095474243, 'learning_rate': 6.104395604395606e-05, 'epoch': 3.94}
********************on step end call back********************
Step 21680 finish
{'loss': 0.313, 'grad_norm': 1.0882816314697266, 'learning_rate': 6.1025641025641035e-05, 'epoch': 3.94}
********************on step end call back********************
Step 21690 finish
[INFO|trainer.py:3376] 2024-03-23 11:48:56,089 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-23 11:48:56,089 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-23 11:48:56,089 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-23 11:51:05,733 >> Saving model checkpoint to ./output/tmp-checkpoint-21700
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-23 11:51:05,902 >> tokenizer config file saved in ./output/tmp-checkpoint-21700/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-23 11:51:05,902 >> Special tokens file saved in ./output/tmp-checkpoint-21700/special_tokens_map.json
[INFO|trainer.py:3159] 2024-03-23 11:51:06,109 >> Deleting older checkpoint [output/checkpoint-11700] due to args.save_total_limit
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-23 11:59:46,397 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-23 11:59:46,397 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-23 11:59:46,397 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-23 12:01:56,132 >> Saving model checkpoint to ./output/tmp-checkpoint-21800
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-23 12:01:56,280 >> tokenizer config file saved in ./output/tmp-checkpoint-21800/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-23 12:01:56,281 >> Special tokens file saved in ./output/tmp-checkpoint-21800/special_tokens_map.json
[INFO|trainer.py:3159] 2024-03-23 12:01:56,489 >> Deleting older checkpoint [output/checkpoint-11800] due to args.save_total_limit
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-23 12:10:42,023 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-23 12:10:42,023 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-23 12:10:42,023 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-23 12:12:51,609 >> Saving model checkpoint to ./output/tmp-checkpoint-21900
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-23 12:12:51,751 >> tokenizer config file saved in ./output/tmp-checkpoint-21900/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-23 12:12:51,751 >> Special tokens file saved in ./output/tmp-checkpoint-21900/special_tokens_map.json
[INFO|trainer.py:3159] 2024-03-23 12:12:51,959 >> Deleting older checkpoint [output/checkpoint-11900] due to args.save_total_limit
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-23 12:21:24,994 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-23 12:21:24,994 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-23 12:21:24,994 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-23 12:23:34,572 >> Saving model checkpoint to ./output/tmp-checkpoint-22000
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-23 12:23:34,747 >> tokenizer config file saved in ./output/tmp-checkpoint-22000/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-23 12:23:34,747 >> Special tokens file saved in ./output/tmp-checkpoint-22000/special_tokens_map.json
[INFO|trainer.py:3159] 2024-03-23 12:23:34,954 >> Deleting older checkpoint [output/checkpoint-12000] due to args.save_total_limit
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
{'loss': 0.31, 'grad_norm': 0.8417353630065918, 'learning_rate': 6.1007326007326014e-05, 'epoch': 3.94}
********************on step end call back********************
Step 21700 finish
{'loss': 0.3221, 'grad_norm': 1.0633559226989746, 'learning_rate': 6.0989010989011e-05, 'epoch': 3.95}
{'eval_loss': 0.3314574062824249, 'eval_accuracy': 0.8854166666666666, 'eval_runtime': 129.6429, 'eval_samples_per_second': 4.921, 'eval_steps_per_second': 4.921, 'epoch': 3.95}
********************save call back********************
********************on step end call back********************
Step 21710 finish
{'loss': 0.3316, 'grad_norm': 1.3940643072128296, 'learning_rate': 6.097069597069598e-05, 'epoch': 3.95}
********************on step end call back********************
Step 21720 finish
{'loss': 0.2775, 'grad_norm': 1.2360743284225464, 'learning_rate': 6.0952380952380964e-05, 'epoch': 3.95}
********************on step end call back********************
Step 21730 finish
{'loss': 0.3583, 'grad_norm': 1.4861516952514648, 'learning_rate': 6.093406593406593e-05, 'epoch': 3.95}
********************on step end call back********************
Step 21740 finish
{'loss': 0.3087, 'grad_norm': 1.0181747674942017, 'learning_rate': 6.0915750915750915e-05, 'epoch': 3.95}
********************on step end call back********************
Step 21750 finish
{'loss': 0.3191, 'grad_norm': 1.0757924318313599, 'learning_rate': 6.089743589743589e-05, 'epoch': 3.95}
********************on step end call back********************
Step 21760 finish
{'loss': 0.3347, 'grad_norm': 1.4674965143203735, 'learning_rate': 6.087912087912088e-05, 'epoch': 3.96}
********************on step end call back********************
Step 21770 finish
{'loss': 0.3354, 'grad_norm': 1.024503469467163, 'learning_rate': 6.086080586080586e-05, 'epoch': 3.96}
********************on step end call back********************
Step 21780 finish
{'loss': 0.3382, 'grad_norm': 1.1011927127838135, 'learning_rate': 6.084249084249084e-05, 'epoch': 3.96}
********************on step end call back********************
Step 21790 finish
{'loss': 0.3043, 'grad_norm': 0.7870799899101257, 'learning_rate': 6.082417582417582e-05, 'epoch': 3.96}
********************on step end call back********************
Step 21800 finish
{'loss': 0.3414, 'grad_norm': 1.4494364261627197, 'learning_rate': 6.08058608058608e-05, 'epoch': 3.96}
{'eval_loss': 0.3326261043548584, 'eval_accuracy': 0.8854166666666666, 'eval_runtime': 129.7338, 'eval_samples_per_second': 4.918, 'eval_steps_per_second': 4.918, 'epoch': 3.96}
********************save call back********************
********************on step end call back********************
Step 21810 finish
{'loss': 0.3224, 'grad_norm': 1.0792579650878906, 'learning_rate': 6.0787545787545786e-05, 'epoch': 3.97}
********************on step end call back********************
Step 21820 finish
{'loss': 0.3257, 'grad_norm': 1.0742182731628418, 'learning_rate': 6.0769230769230765e-05, 'epoch': 3.97}
********************on step end call back********************
Step 21830 finish
{'loss': 0.3125, 'grad_norm': 1.1891461610794067, 'learning_rate': 6.075091575091575e-05, 'epoch': 3.97}
********************on step end call back********************
Step 21840 finish
{'loss': 0.332, 'grad_norm': 1.3403123617172241, 'learning_rate': 6.073260073260073e-05, 'epoch': 3.97}
********************on step end call back********************
Step 21850 finish
{'loss': 0.3172, 'grad_norm': 0.8845241069793701, 'learning_rate': 6.0714285714285715e-05, 'epoch': 3.97}
********************on step end call back********************
Step 21860 finish
{'loss': 0.2831, 'grad_norm': 1.3128697872161865, 'learning_rate': 6.0695970695970694e-05, 'epoch': 3.97}
********************on step end call back********************
Step 21870 finish
{'loss': 0.358, 'grad_norm': 1.3700085878372192, 'learning_rate': 6.067765567765568e-05, 'epoch': 3.98}
********************on step end call back********************
Step 21880 finish
{'loss': 0.3484, 'grad_norm': 1.3024992942810059, 'learning_rate': 6.065934065934066e-05, 'epoch': 3.98}
********************on step end call back********************
Step 21890 finish
{'loss': 0.3147, 'grad_norm': 1.0092562437057495, 'learning_rate': 6.0641025641025637e-05, 'epoch': 3.98}
********************on step end call back********************
Step 21900 finish
{'loss': 0.393, 'grad_norm': 1.410373330116272, 'learning_rate': 6.062271062271062e-05, 'epoch': 3.98}
{'eval_loss': 0.3282680809497833, 'eval_accuracy': 0.8958333333333334, 'eval_runtime': 129.5852, 'eval_samples_per_second': 4.923, 'eval_steps_per_second': 4.923, 'epoch': 3.98}
********************save call back********************
********************on step end call back********************
Step 21910 finish
{'loss': 0.3595, 'grad_norm': 1.221476674079895, 'learning_rate': 6.06043956043956e-05, 'epoch': 3.98}
********************on step end call back********************
Step 21920 finish
{'loss': 0.3497, 'grad_norm': 0.9896041750907898, 'learning_rate': 6.0586080586080586e-05, 'epoch': 3.99}
********************on step end call back********************
Step 21930 finish
{'loss': 0.335, 'grad_norm': 1.081938624382019, 'learning_rate': 6.0567765567765565e-05, 'epoch': 3.99}
********************on step end call back********************
Step 21940 finish
{'loss': 0.3009, 'grad_norm': 1.1547660827636719, 'learning_rate': 6.054945054945055e-05, 'epoch': 3.99}
********************on step end call back********************
Step 21950 finish
{'loss': 0.3284, 'grad_norm': 0.9484009742736816, 'learning_rate': 6.053113553113553e-05, 'epoch': 3.99}
********************on step end call back********************
Step 21960 finish
{'loss': 0.3544, 'grad_norm': 1.2418653964996338, 'learning_rate': 6.0512820512820515e-05, 'epoch': 3.99}
********************on step end call back********************
Step 21970 finish
{'loss': 0.3206, 'grad_norm': 1.2279915809631348, 'learning_rate': 6.0494505494505494e-05, 'epoch': 3.99}
********************on step end call back********************
Step 21980 finish
{'loss': 0.3446, 'grad_norm': 0.9120499491691589, 'learning_rate': 6.047619047619047e-05, 'epoch': 4.0}
********************on step end call back********************
Step 21990 finish
{'loss': 0.3145, 'grad_norm': 1.5285056829452515, 'learning_rate': 6.045787545787546e-05, 'epoch': 4.0}
********************on step end call back********************
Step 22000 finish
{'loss': 0.3038, 'grad_norm': 1.2730106115341187, 'learning_rate': 6.043956043956044e-05, 'epoch': 4.0}
{'eval_loss': 0.33788856863975525, 'eval_accuracy': 0.8854166666666666, 'eval_runtime': 129.5765, 'eval_samples_per_second': 4.924, 'eval_steps_per_second': 4.924, 'epoch': 4.0}
********************save call back********************
********************on epoch end call back********************
Epoch 3.9999091012384955 finish
********************on step end call back********************
Step 22010 finish
{'loss': 0.2491, 'grad_norm': 0.9173345565795898, 'learning_rate': 6.042124542124542e-05, 'epoch': 4.0}
********************on step end call back********************
Step 22020 finish
{'loss': 0.2499, 'grad_norm': 1.0646634101867676, 'learning_rate': 6.04029304029304e-05, 'epoch': 4.0}
********************on step end call back********************
Step 22030 finish
{'loss': 0.2616, 'grad_norm': 0.9516208171844482, 'learning_rate': 6.038461538461539e-05, 'epoch': 4.0}
********************on step end call back********************
Step 22040 finish
{'loss': 0.2537, 'grad_norm': 1.0579065084457397, 'learning_rate': 6.0366300366300365e-05, 'epoch': 4.01}
********************on step end call back********************
Step 22050 finish
{'loss': 0.2493, 'grad_norm': 1.229498028755188, 'learning_rate': 6.0347985347985344e-05, 'epoch': 4.01}
********************on step end call back********************
Step 22060 finish
{'loss': 0.2874, 'grad_norm': 1.2855923175811768, 'learning_rate': 6.032967032967033e-05, 'epoch': 4.01}
********************on step end call back********************
Step 22070 finish
{'loss': 0.2628, 'grad_norm': 1.2355293035507202, 'learning_rate': 6.031135531135531e-05, 'epoch': 4.01}
[INFO|trainer.py:3376] 2024-03-23 12:32:13,536 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-23 12:32:13,537 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-23 12:32:13,537 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-23 12:34:23,247 >> Saving model checkpoint to ./output/tmp-checkpoint-22100
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-23 12:34:23,422 >> tokenizer config file saved in ./output/tmp-checkpoint-22100/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-23 12:34:23,422 >> Special tokens file saved in ./output/tmp-checkpoint-22100/special_tokens_map.json
[INFO|trainer.py:3159] 2024-03-23 12:34:23,641 >> Deleting older checkpoint [output/checkpoint-12100] due to args.save_total_limit
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-23 12:42:59,845 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-23 12:42:59,845 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-23 12:42:59,845 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-23 12:45:08,916 >> Saving model checkpoint to ./output/tmp-checkpoint-22200
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-23 12:45:09,077 >> tokenizer config file saved in ./output/tmp-checkpoint-22200/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-23 12:45:09,077 >> Special tokens file saved in ./output/tmp-checkpoint-22200/special_tokens_map.json
[INFO|trainer.py:3159] 2024-03-23 12:45:09,284 >> Deleting older checkpoint [output/checkpoint-12200] due to args.save_total_limit
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-23 12:53:52,561 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-23 12:53:52,561 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-23 12:53:52,561 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-23 12:56:01,650 >> Saving model checkpoint to ./output/tmp-checkpoint-22300
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-23 12:56:01,797 >> tokenizer config file saved in ./output/tmp-checkpoint-22300/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-23 12:56:01,797 >> Special tokens file saved in ./output/tmp-checkpoint-22300/special_tokens_map.json
[INFO|trainer.py:3159] 2024-03-23 12:56:02,001 >> Deleting older checkpoint [output/checkpoint-12300] due to args.save_total_limit
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-23 13:04:40,924 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-23 13:04:40,924 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-23 13:04:40,924 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-23 13:06:50,033 >> Saving model checkpoint to ./output/tmp-checkpoint-22400
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-23 13:06:50,179 >> tokenizer config file saved in ./output/tmp-checkpoint-22400/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-23 13:06:50,179 >> Special tokens file saved in ./output/tmp-checkpoint-22400/special_tokens_map.json
[INFO|trainer.py:3159] 2024-03-23 13:06:50,399 >> Deleting older checkpoint [output/checkpoint-12400] due to args.save_total_limit
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
********************on step end call back********************
Step 22080 finish
{'loss': 0.2611, 'grad_norm': 0.930507481098175, 'learning_rate': 6.0293040293040294e-05, 'epoch': 4.01}
********************on step end call back********************
Step 22090 finish
{'loss': 0.2848, 'grad_norm': 1.2207632064819336, 'learning_rate': 6.027472527472527e-05, 'epoch': 4.02}
********************on step end call back********************
Step 22100 finish
{'loss': 0.3053, 'grad_norm': 1.1782301664352417, 'learning_rate': 6.025641025641026e-05, 'epoch': 4.02}
{'eval_loss': 0.345211923122406, 'eval_accuracy': 0.8958333333333334, 'eval_runtime': 129.71, 'eval_samples_per_second': 4.919, 'eval_steps_per_second': 4.919, 'epoch': 4.02}
********************save call back********************
********************on step end call back********************
Step 22110 finish
{'loss': 0.2589, 'grad_norm': 1.1780521869659424, 'learning_rate': 6.023809523809524e-05, 'epoch': 4.02}
********************on step end call back********************
Step 22120 finish
{'loss': 0.2834, 'grad_norm': 1.1965826749801636, 'learning_rate': 6.021978021978022e-05, 'epoch': 4.02}
********************on step end call back********************
Step 22130 finish
{'loss': 0.2701, 'grad_norm': 1.1652495861053467, 'learning_rate': 6.02014652014652e-05, 'epoch': 4.02}
********************on step end call back********************
Step 22140 finish
{'loss': 0.2777, 'grad_norm': 1.1764434576034546, 'learning_rate': 6.018315018315018e-05, 'epoch': 4.02}
********************on step end call back********************
Step 22150 finish
{'loss': 0.2404, 'grad_norm': 1.2230007648468018, 'learning_rate': 6.0164835164835166e-05, 'epoch': 4.03}
********************on step end call back********************
Step 22160 finish
{'loss': 0.291, 'grad_norm': 1.8839647769927979, 'learning_rate': 6.0146520146520144e-05, 'epoch': 4.03}
********************on step end call back********************
Step 22170 finish
{'loss': 0.2398, 'grad_norm': 1.2704851627349854, 'learning_rate': 6.012820512820513e-05, 'epoch': 4.03}
********************on step end call back********************
Step 22180 finish
{'loss': 0.2509, 'grad_norm': 1.7010022401809692, 'learning_rate': 6.010989010989011e-05, 'epoch': 4.03}
********************on step end call back********************
Step 22190 finish
{'loss': 0.2856, 'grad_norm': 1.0089964866638184, 'learning_rate': 6.0091575091575094e-05, 'epoch': 4.03}
********************on step end call back********************
Step 22200 finish
{'loss': 0.2608, 'grad_norm': 1.4501841068267822, 'learning_rate': 6.007326007326007e-05, 'epoch': 4.04}
{'eval_loss': 0.34538283944129944, 'eval_accuracy': 0.8958333333333334, 'eval_runtime': 129.0696, 'eval_samples_per_second': 4.943, 'eval_steps_per_second': 4.943, 'epoch': 4.04}
********************save call back********************
********************on step end call back********************
Step 22210 finish
{'loss': 0.2518, 'grad_norm': 1.2001909017562866, 'learning_rate': 6.005494505494506e-05, 'epoch': 4.04}
********************on step end call back********************
Step 22220 finish
{'loss': 0.2745, 'grad_norm': 1.1947922706604004, 'learning_rate': 6.003663003663004e-05, 'epoch': 4.04}
********************on step end call back********************
Step 22230 finish
{'loss': 0.271, 'grad_norm': 1.0248230695724487, 'learning_rate': 6.0018315018315016e-05, 'epoch': 4.04}
********************on step end call back********************
Step 22240 finish
{'loss': 0.264, 'grad_norm': 1.1056431531906128, 'learning_rate': 6e-05, 'epoch': 4.04}
********************on step end call back********************
Step 22250 finish
{'loss': 0.2687, 'grad_norm': 1.1790186166763306, 'learning_rate': 5.998168498168498e-05, 'epoch': 4.04}
********************on step end call back********************
Step 22260 finish
{'loss': 0.2728, 'grad_norm': 1.2090402841567993, 'learning_rate': 5.9963369963369966e-05, 'epoch': 4.05}
********************on step end call back********************
Step 22270 finish
{'loss': 0.2959, 'grad_norm': 1.4586637020111084, 'learning_rate': 5.9945054945054945e-05, 'epoch': 4.05}
********************on step end call back********************
Step 22280 finish
{'loss': 0.2509, 'grad_norm': 0.9858034253120422, 'learning_rate': 5.992673992673993e-05, 'epoch': 4.05}
********************on step end call back********************
Step 22290 finish
{'loss': 0.2836, 'grad_norm': 1.1426331996917725, 'learning_rate': 5.990842490842491e-05, 'epoch': 4.05}
********************on step end call back********************
Step 22300 finish
{'loss': 0.269, 'grad_norm': 0.8995447754859924, 'learning_rate': 5.9890109890109894e-05, 'epoch': 4.05}
{'eval_loss': 0.34564462304115295, 'eval_accuracy': 0.8854166666666666, 'eval_runtime': 129.0887, 'eval_samples_per_second': 4.942, 'eval_steps_per_second': 4.942, 'epoch': 4.05}
********************save call back********************
********************on step end call back********************
Step 22310 finish
{'loss': 0.2599, 'grad_norm': 1.1356403827667236, 'learning_rate': 5.987179487179487e-05, 'epoch': 4.06}
********************on step end call back********************
Step 22320 finish
{'loss': 0.2759, 'grad_norm': 1.4430279731750488, 'learning_rate': 5.985347985347985e-05, 'epoch': 4.06}
********************on step end call back********************
Step 22330 finish
{'loss': 0.2894, 'grad_norm': 0.9504699110984802, 'learning_rate': 5.983516483516484e-05, 'epoch': 4.06}
********************on step end call back********************
Step 22340 finish
{'loss': 0.2949, 'grad_norm': 0.9289103150367737, 'learning_rate': 5.9816849816849816e-05, 'epoch': 4.06}
********************on step end call back********************
Step 22350 finish
{'loss': 0.2956, 'grad_norm': 1.6488020420074463, 'learning_rate': 5.97985347985348e-05, 'epoch': 4.06}
********************on step end call back********************
Step 22360 finish
{'loss': 0.2504, 'grad_norm': 1.213647484779358, 'learning_rate': 5.978021978021978e-05, 'epoch': 4.06}
********************on step end call back********************
Step 22370 finish
{'loss': 0.2676, 'grad_norm': 0.8970281481742859, 'learning_rate': 5.9761904761904766e-05, 'epoch': 4.07}
********************on step end call back********************
Step 22380 finish
{'loss': 0.2658, 'grad_norm': 1.0842748880386353, 'learning_rate': 5.9743589743589745e-05, 'epoch': 4.07}
********************on step end call back********************
Step 22390 finish
{'loss': 0.257, 'grad_norm': 1.365709900856018, 'learning_rate': 5.9725274725274724e-05, 'epoch': 4.07}
********************on step end call back********************
Step 22400 finish
{'loss': 0.3084, 'grad_norm': 1.183632731437683, 'learning_rate': 5.970695970695971e-05, 'epoch': 4.07}
{'eval_loss': 0.3534548878669739, 'eval_accuracy': 0.8854166666666666, 'eval_runtime': 129.1085, 'eval_samples_per_second': 4.942, 'eval_steps_per_second': 4.942, 'epoch': 4.07}
********************save call back********************
********************on step end call back********************
Step 22410 finish
{'loss': 0.29, 'grad_norm': 1.1277785301208496, 'learning_rate': 5.968864468864469e-05, 'epoch': 4.07}
********************on step end call back********************
Step 22420 finish
{'loss': 0.2701, 'grad_norm': 1.3097939491271973, 'learning_rate': 5.967032967032967e-05, 'epoch': 4.08}
********************on step end call back********************
Step 22430 finish
{'loss': 0.2697, 'grad_norm': 1.0462918281555176, 'learning_rate': 5.965201465201465e-05, 'epoch': 4.08}
********************on step end call back********************
Step 22440 finish
{'loss': 0.2837, 'grad_norm': 1.1119505167007446, 'learning_rate': 5.963369963369964e-05, 'epoch': 4.08}
********************on step end call back********************
Step 22450 finish
{'loss': 0.3026, 'grad_norm': 1.110385537147522, 'learning_rate': 5.9615384615384616e-05, 'epoch': 4.08}
********************on step end call back********************
Step 22460 finish
{'loss': 0.2839, 'grad_norm': 1.1153836250305176, 'learning_rate': 5.95970695970696e-05, 'epoch': 4.08}
********************on step end call back********************
[INFO|trainer.py:3376] 2024-03-23 13:15:31,366 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-23 13:15:31,367 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-23 13:15:31,367 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-23 13:17:40,711 >> Saving model checkpoint to ./output/tmp-checkpoint-22500
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-23 13:17:40,857 >> tokenizer config file saved in ./output/tmp-checkpoint-22500/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-23 13:17:40,857 >> Special tokens file saved in ./output/tmp-checkpoint-22500/special_tokens_map.json
[INFO|trainer.py:3159] 2024-03-23 13:17:41,086 >> Deleting older checkpoint [output/checkpoint-12500] due to args.save_total_limit
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-23 13:26:15,071 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-23 13:26:15,071 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-23 13:26:15,071 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-23 13:28:24,257 >> Saving model checkpoint to ./output/tmp-checkpoint-22600
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-23 13:28:24,418 >> tokenizer config file saved in ./output/tmp-checkpoint-22600/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-23 13:28:24,418 >> Special tokens file saved in ./output/tmp-checkpoint-22600/special_tokens_map.json
[INFO|trainer.py:3159] 2024-03-23 13:28:24,653 >> Deleting older checkpoint [output/checkpoint-12600] due to args.save_total_limit
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-23 13:37:02,133 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-23 13:37:02,133 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-23 13:37:02,133 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-23 13:39:11,268 >> Saving model checkpoint to ./output/tmp-checkpoint-22700
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-23 13:39:11,409 >> tokenizer config file saved in ./output/tmp-checkpoint-22700/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-23 13:39:11,409 >> Special tokens file saved in ./output/tmp-checkpoint-22700/special_tokens_map.json
[INFO|trainer.py:3159] 2024-03-23 13:39:11,616 >> Deleting older checkpoint [output/checkpoint-12700] due to args.save_total_limit
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-23 13:47:46,585 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-23 13:47:46,585 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-23 13:47:46,585 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-23 13:49:55,805 >> Saving model checkpoint to ./output/tmp-checkpoint-22800
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-23 13:49:55,948 >> tokenizer config file saved in ./output/tmp-checkpoint-22800/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-23 13:49:55,948 >> Special tokens file saved in ./output/tmp-checkpoint-22800/special_tokens_map.json
[INFO|trainer.py:3159] 2024-03-23 13:49:56,157 >> Deleting older checkpoint [output/checkpoint-12800] due to args.save_total_limit
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
Step 22470 finish
{'loss': 0.2531, 'grad_norm': 1.1738618612289429, 'learning_rate': 5.957875457875458e-05, 'epoch': 4.08}
********************on step end call back********************
Step 22480 finish
{'loss': 0.2772, 'grad_norm': 1.4627548456192017, 'learning_rate': 5.956043956043956e-05, 'epoch': 4.09}
********************on step end call back********************
Step 22490 finish
{'loss': 0.2437, 'grad_norm': 1.200109601020813, 'learning_rate': 5.9542124542124545e-05, 'epoch': 4.09}
********************on step end call back********************
Step 22500 finish
{'loss': 0.2732, 'grad_norm': 1.0683091878890991, 'learning_rate': 5.9523809523809524e-05, 'epoch': 4.09}
{'eval_loss': 0.34252631664276123, 'eval_accuracy': 0.8854166666666666, 'eval_runtime': 129.3428, 'eval_samples_per_second': 4.933, 'eval_steps_per_second': 4.933, 'epoch': 4.09}
********************save call back********************
********************on step end call back********************
Step 22510 finish
{'loss': 0.2921, 'grad_norm': 1.7224910259246826, 'learning_rate': 5.950549450549451e-05, 'epoch': 4.09}
********************on step end call back********************
Step 22520 finish
{'loss': 0.2607, 'grad_norm': 1.5250678062438965, 'learning_rate': 5.948717948717949e-05, 'epoch': 4.09}
********************on step end call back********************
Step 22530 finish
{'loss': 0.2859, 'grad_norm': 1.5230485200881958, 'learning_rate': 5.9468864468864474e-05, 'epoch': 4.1}
********************on step end call back********************
Step 22540 finish
{'loss': 0.2735, 'grad_norm': 1.3313578367233276, 'learning_rate': 5.945054945054945e-05, 'epoch': 4.1}
********************on step end call back********************
Step 22550 finish
{'loss': 0.2598, 'grad_norm': 1.158303141593933, 'learning_rate': 5.943223443223444e-05, 'epoch': 4.1}
********************on step end call back********************
Step 22560 finish
{'loss': 0.2473, 'grad_norm': 1.2332972288131714, 'learning_rate': 5.941391941391942e-05, 'epoch': 4.1}
********************on step end call back********************
Step 22570 finish
{'loss': 0.2824, 'grad_norm': 1.365831971168518, 'learning_rate': 5.9395604395604395e-05, 'epoch': 4.1}
********************on step end call back********************
Step 22580 finish
{'loss': 0.2346, 'grad_norm': 1.0205224752426147, 'learning_rate': 5.937728937728938e-05, 'epoch': 4.1}
********************on step end call back********************
Step 22590 finish
{'loss': 0.2711, 'grad_norm': 1.081709384918213, 'learning_rate': 5.935897435897436e-05, 'epoch': 4.11}
********************on step end call back********************
Step 22600 finish
{'loss': 0.2815, 'grad_norm': 1.4096837043762207, 'learning_rate': 5.9340659340659345e-05, 'epoch': 4.11}
{'eval_loss': 0.3545917272567749, 'eval_accuracy': 0.8958333333333334, 'eval_runtime': 129.1849, 'eval_samples_per_second': 4.939, 'eval_steps_per_second': 4.939, 'epoch': 4.11}
********************save call back********************
********************on step end call back********************
Step 22610 finish
{'loss': 0.2982, 'grad_norm': 1.4407668113708496, 'learning_rate': 5.9322344322344324e-05, 'epoch': 4.11}
********************on step end call back********************
Step 22620 finish
{'loss': 0.2552, 'grad_norm': 1.013307809829712, 'learning_rate': 5.930402930402931e-05, 'epoch': 4.11}
********************on step end call back********************
Step 22630 finish
{'loss': 0.2822, 'grad_norm': 1.3037705421447754, 'learning_rate': 5.928571428571429e-05, 'epoch': 4.11}
********************on step end call back********************
Step 22640 finish
{'loss': 0.2625, 'grad_norm': 1.1498847007751465, 'learning_rate': 5.9267399267399274e-05, 'epoch': 4.12}
********************on step end call back********************
Step 22650 finish
{'loss': 0.2598, 'grad_norm': 1.0569047927856445, 'learning_rate': 5.924908424908425e-05, 'epoch': 4.12}
********************on step end call back********************
Step 22660 finish
{'loss': 0.2745, 'grad_norm': 1.0915236473083496, 'learning_rate': 5.923076923076923e-05, 'epoch': 4.12}
********************on step end call back********************
Step 22670 finish
{'loss': 0.294, 'grad_norm': 1.026362657546997, 'learning_rate': 5.921245421245422e-05, 'epoch': 4.12}
********************on step end call back********************
Step 22680 finish
{'loss': 0.2519, 'grad_norm': 1.2380878925323486, 'learning_rate': 5.9194139194139196e-05, 'epoch': 4.12}
********************on step end call back********************
Step 22690 finish
{'loss': 0.2703, 'grad_norm': 1.1194767951965332, 'learning_rate': 5.917582417582418e-05, 'epoch': 4.12}
********************on step end call back********************
Step 22700 finish
{'loss': 0.2849, 'grad_norm': 1.0900206565856934, 'learning_rate': 5.915750915750916e-05, 'epoch': 4.13}
{'eval_loss': 0.34407082200050354, 'eval_accuracy': 0.8854166666666666, 'eval_runtime': 129.1337, 'eval_samples_per_second': 4.941, 'eval_steps_per_second': 4.941, 'epoch': 4.13}
********************save call back********************
********************on step end call back********************
Step 22710 finish
{'loss': 0.2672, 'grad_norm': 1.0215364694595337, 'learning_rate': 5.9139194139194145e-05, 'epoch': 4.13}
********************on step end call back********************
Step 22720 finish
{'loss': 0.2786, 'grad_norm': 1.3641581535339355, 'learning_rate': 5.9120879120879124e-05, 'epoch': 4.13}
********************on step end call back********************
Step 22730 finish
{'loss': 0.2748, 'grad_norm': 1.0175162553787231, 'learning_rate': 5.910256410256411e-05, 'epoch': 4.13}
********************on step end call back********************
Step 22740 finish
{'loss': 0.2559, 'grad_norm': 1.3161667585372925, 'learning_rate': 5.908424908424909e-05, 'epoch': 4.13}
********************on step end call back********************
Step 22750 finish
{'loss': 0.2571, 'grad_norm': 0.7760206460952759, 'learning_rate': 5.906593406593407e-05, 'epoch': 4.14}
********************on step end call back********************
Step 22760 finish
{'loss': 0.2771, 'grad_norm': 1.4029074907302856, 'learning_rate': 5.904761904761905e-05, 'epoch': 4.14}
********************on step end call back********************
Step 22770 finish
{'loss': 0.307, 'grad_norm': 1.0654274225234985, 'learning_rate': 5.902930402930403e-05, 'epoch': 4.14}
********************on step end call back********************
Step 22780 finish
{'loss': 0.28, 'grad_norm': 1.3471894264221191, 'learning_rate': 5.901098901098902e-05, 'epoch': 4.14}
********************on step end call back********************
Step 22790 finish
{'loss': 0.2818, 'grad_norm': 1.0631827116012573, 'learning_rate': 5.8992673992673996e-05, 'epoch': 4.14}
********************on step end call back********************
Step 22800 finish
{'loss': 0.2848, 'grad_norm': 1.2070282697677612, 'learning_rate': 5.897435897435898e-05, 'epoch': 4.14}
{'eval_loss': 0.35108253359794617, 'eval_accuracy': 0.8854166666666666, 'eval_runtime': 129.2194, 'eval_samples_per_second': 4.937, 'eval_steps_per_second': 4.937, 'epoch': 4.14}
********************save call back********************
********************on step end call back********************
Step 22810 finish
{'loss': 0.2539, 'grad_norm': 1.192032814025879, 'learning_rate': 5.895604395604396e-05, 'epoch': 4.15}
********************on step end call back********************
Step 22820 finish
{'loss': 0.2807, 'grad_norm': 1.2338234186172485, 'learning_rate': 5.893772893772894e-05, 'epoch': 4.15}
********************on step end call back********************
Step 22830 finish
{'loss': 0.3019, 'grad_norm': 1.133981466293335, 'learning_rate': 5.8919413919413924e-05, 'epoch': 4.15}
********************on step end call back********************
Step 22840 finish
{'loss': 0.2559, 'grad_norm': 1.002377986907959, 'learning_rate': 5.89010989010989e-05, 'epoch': 4.15}
********************on step end call back********************
Step 22850 finish
{'loss': 0.2667, 'grad_norm': 1.100285291671753, 'learning_rate': 5.888278388278389e-05, 'epoch': 4.15}
********************on step end call back********************
Step 22860 finish
[INFO|trainer.py:3376] 2024-03-23 13:58:30,247 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-23 13:58:30,247 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-23 13:58:30,247 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-23 14:00:39,353 >> Saving model checkpoint to ./output/tmp-checkpoint-22900
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-23 14:00:39,514 >> tokenizer config file saved in ./output/tmp-checkpoint-22900/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-23 14:00:39,514 >> Special tokens file saved in ./output/tmp-checkpoint-22900/special_tokens_map.json
[INFO|trainer.py:3159] 2024-03-23 14:00:39,717 >> Deleting older checkpoint [output/checkpoint-12900] due to args.save_total_limit
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-23 14:09:09,533 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-23 14:09:09,533 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-23 14:09:09,533 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-23 14:11:19,027 >> Saving model checkpoint to ./output/tmp-checkpoint-23000
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-23 14:11:19,173 >> tokenizer config file saved in ./output/tmp-checkpoint-23000/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-23 14:11:19,173 >> Special tokens file saved in ./output/tmp-checkpoint-23000/special_tokens_map.json
[INFO|trainer.py:3159] 2024-03-23 14:11:19,384 >> Deleting older checkpoint [output/checkpoint-13000] due to args.save_total_limit
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-23 14:19:55,497 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-23 14:19:55,498 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-23 14:19:55,498 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-23 14:22:05,257 >> Saving model checkpoint to ./output/tmp-checkpoint-23100
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-23 14:22:05,397 >> tokenizer config file saved in ./output/tmp-checkpoint-23100/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-23 14:22:05,398 >> Special tokens file saved in ./output/tmp-checkpoint-23100/special_tokens_map.json
[INFO|trainer.py:3159] 2024-03-23 14:22:05,609 >> Deleting older checkpoint [output/checkpoint-13100] due to args.save_total_limit
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-23 14:30:34,169 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-23 14:30:34,169 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-23 14:30:34,169 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-23 14:32:43,516 >> Saving model checkpoint to ./output/tmp-checkpoint-23200
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-23 14:32:43,658 >> tokenizer config file saved in ./output/tmp-checkpoint-23200/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-23 14:32:43,658 >> Special tokens file saved in ./output/tmp-checkpoint-23200/special_tokens_map.json
[INFO|trainer.py:3159] 2024-03-23 14:32:43,869 >> Deleting older checkpoint [output/checkpoint-13200] due to args.save_total_limit
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
{'loss': 0.2645, 'grad_norm': 1.017867088317871, 'learning_rate': 5.886446886446887e-05, 'epoch': 4.16}
********************on step end call back********************
Step 22870 finish
{'loss': 0.2558, 'grad_norm': 1.1225214004516602, 'learning_rate': 5.884615384615385e-05, 'epoch': 4.16}
********************on step end call back********************
Step 22880 finish
{'loss': 0.2568, 'grad_norm': 0.9414896965026855, 'learning_rate': 5.882783882783883e-05, 'epoch': 4.16}
********************on step end call back********************
Step 22890 finish
{'loss': 0.2508, 'grad_norm': 1.2729802131652832, 'learning_rate': 5.880952380952382e-05, 'epoch': 4.16}
********************on step end call back********************
Step 22900 finish
{'loss': 0.256, 'grad_norm': 1.3011407852172852, 'learning_rate': 5.8791208791208796e-05, 'epoch': 4.16}
{'eval_loss': 0.3458033502101898, 'eval_accuracy': 0.8854166666666666, 'eval_runtime': 129.1052, 'eval_samples_per_second': 4.942, 'eval_steps_per_second': 4.942, 'epoch': 4.16}
********************save call back********************
********************on step end call back********************
Step 22910 finish
{'loss': 0.2476, 'grad_norm': 1.2018321752548218, 'learning_rate': 5.8772893772893775e-05, 'epoch': 4.16}
********************on step end call back********************
Step 22920 finish
{'loss': 0.2665, 'grad_norm': 1.316679835319519, 'learning_rate': 5.875457875457876e-05, 'epoch': 4.17}
********************on step end call back********************
Step 22930 finish
{'loss': 0.3062, 'grad_norm': 1.5200846195220947, 'learning_rate': 5.873626373626374e-05, 'epoch': 4.17}
********************on step end call back********************
Step 22940 finish
{'loss': 0.2519, 'grad_norm': 1.1639705896377563, 'learning_rate': 5.8717948717948725e-05, 'epoch': 4.17}
********************on step end call back********************
Step 22950 finish
{'loss': 0.2497, 'grad_norm': 1.0655758380889893, 'learning_rate': 5.86996336996337e-05, 'epoch': 4.17}
********************on step end call back********************
Step 22960 finish
{'loss': 0.2543, 'grad_norm': 1.2529922723770142, 'learning_rate': 5.868131868131869e-05, 'epoch': 4.17}
********************on step end call back********************
Step 22970 finish
{'loss': 0.2703, 'grad_norm': 1.524043083190918, 'learning_rate': 5.866300366300367e-05, 'epoch': 4.18}
********************on step end call back********************
Step 22980 finish
{'loss': 0.272, 'grad_norm': 0.8932060599327087, 'learning_rate': 5.864468864468865e-05, 'epoch': 4.18}
********************on step end call back********************
Step 22990 finish
{'loss': 0.2856, 'grad_norm': 1.166303277015686, 'learning_rate': 5.862637362637363e-05, 'epoch': 4.18}
********************on step end call back********************
Step 23000 finish
{'loss': 0.2608, 'grad_norm': 1.1668481826782227, 'learning_rate': 5.860805860805861e-05, 'epoch': 4.18}
{'eval_loss': 0.3470976650714874, 'eval_accuracy': 0.875, 'eval_runtime': 129.4927, 'eval_samples_per_second': 4.927, 'eval_steps_per_second': 4.927, 'epoch': 4.18}
********************save call back********************
********************on step end call back********************
Step 23010 finish
{'loss': 0.2883, 'grad_norm': 1.2737447023391724, 'learning_rate': 5.8589743589743596e-05, 'epoch': 4.18}
********************on step end call back********************
Step 23020 finish
{'loss': 0.262, 'grad_norm': 1.034579873085022, 'learning_rate': 5.8571428571428575e-05, 'epoch': 4.18}
********************on step end call back********************
Step 23030 finish
{'loss': 0.2502, 'grad_norm': 1.4952309131622314, 'learning_rate': 5.855311355311356e-05, 'epoch': 4.19}
********************on step end call back********************
Step 23040 finish
{'loss': 0.2735, 'grad_norm': 0.9778289794921875, 'learning_rate': 5.853479853479854e-05, 'epoch': 4.19}
********************on step end call back********************
Step 23050 finish
{'loss': 0.252, 'grad_norm': 1.068505883216858, 'learning_rate': 5.8516483516483525e-05, 'epoch': 4.19}
********************on step end call back********************
Step 23060 finish
{'loss': 0.2523, 'grad_norm': 1.153836965560913, 'learning_rate': 5.8498168498168504e-05, 'epoch': 4.19}
********************on step end call back********************
Step 23070 finish
{'loss': 0.2952, 'grad_norm': 0.9568727016448975, 'learning_rate': 5.847985347985349e-05, 'epoch': 4.19}
********************on step end call back********************
Step 23080 finish
{'loss': 0.2558, 'grad_norm': 1.3916414976119995, 'learning_rate': 5.846153846153847e-05, 'epoch': 4.2}
********************on step end call back********************
Step 23090 finish
{'loss': 0.2929, 'grad_norm': 1.2619866132736206, 'learning_rate': 5.844322344322345e-05, 'epoch': 4.2}
********************on step end call back********************
Step 23100 finish
{'loss': 0.2883, 'grad_norm': 1.0127233266830444, 'learning_rate': 5.842490842490843e-05, 'epoch': 4.2}
{'eval_loss': 0.3455565571784973, 'eval_accuracy': 0.875, 'eval_runtime': 129.7582, 'eval_samples_per_second': 4.917, 'eval_steps_per_second': 4.917, 'epoch': 4.2}
********************save call back********************
********************on step end call back********************
Step 23110 finish
{'loss': 0.267, 'grad_norm': 1.3968851566314697, 'learning_rate': 5.840659340659341e-05, 'epoch': 4.2}
********************on step end call back********************
Step 23120 finish
{'loss': 0.282, 'grad_norm': 1.2074700593948364, 'learning_rate': 5.8388278388278396e-05, 'epoch': 4.2}
********************on step end call back********************
Step 23130 finish
{'loss': 0.2593, 'grad_norm': 1.069037914276123, 'learning_rate': 5.8369963369963375e-05, 'epoch': 4.2}
********************on step end call back********************
Step 23140 finish
{'loss': 0.2471, 'grad_norm': 1.1372698545455933, 'learning_rate': 5.835164835164836e-05, 'epoch': 4.21}
********************on step end call back********************
Step 23150 finish
{'loss': 0.2523, 'grad_norm': 1.1161935329437256, 'learning_rate': 5.833333333333334e-05, 'epoch': 4.21}
********************on step end call back********************
Step 23160 finish
{'loss': 0.2615, 'grad_norm': 0.984485924243927, 'learning_rate': 5.831501831501832e-05, 'epoch': 4.21}
********************on step end call back********************
Step 23170 finish
{'loss': 0.2718, 'grad_norm': 1.267495036125183, 'learning_rate': 5.8296703296703304e-05, 'epoch': 4.21}
********************on step end call back********************
Step 23180 finish
{'loss': 0.2702, 'grad_norm': 1.219680666923523, 'learning_rate': 5.827838827838828e-05, 'epoch': 4.21}
********************on step end call back********************
Step 23190 finish
{'loss': 0.2692, 'grad_norm': 1.2302391529083252, 'learning_rate': 5.826007326007327e-05, 'epoch': 4.22}
********************on step end call back********************
Step 23200 finish
{'loss': 0.2835, 'grad_norm': 1.1227256059646606, 'learning_rate': 5.824175824175825e-05, 'epoch': 4.22}
{'eval_loss': 0.34599149227142334, 'eval_accuracy': 0.875, 'eval_runtime': 129.3467, 'eval_samples_per_second': 4.932, 'eval_steps_per_second': 4.932, 'epoch': 4.22}
********************save call back********************
********************on step end call back********************
Step 23210 finish
{'loss': 0.2911, 'grad_norm': 1.1211819648742676, 'learning_rate': 5.822344322344323e-05, 'epoch': 4.22}
********************on step end call back********************
Step 23220 finish
{'loss': 0.2637, 'grad_norm': 1.102796196937561, 'learning_rate': 5.820512820512821e-05, 'epoch': 4.22}
********************on step end call back********************
Step 23230 finish
{'loss': 0.2755, 'grad_norm': 1.017216682434082, 'learning_rate': 5.81868131868132e-05, 'epoch': 4.22}
********************on step end call back********************
Step 23240 finish
{'loss': 0.2976, 'grad_norm': 0.8900154829025269, 'learning_rate': 5.8168498168498175e-05, 'epoch': 4.22}
********************on step end call back********************
Step 23250 finish
[INFO|trainer.py:3376] 2024-03-23 14:41:16,364 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-23 14:41:16,364 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-23 14:41:16,364 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-23 14:43:25,638 >> Saving model checkpoint to ./output/tmp-checkpoint-23300
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-23 14:43:25,779 >> tokenizer config file saved in ./output/tmp-checkpoint-23300/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-23 14:43:25,779 >> Special tokens file saved in ./output/tmp-checkpoint-23300/special_tokens_map.json
[INFO|trainer.py:3159] 2024-03-23 14:43:25,989 >> Deleting older checkpoint [output/checkpoint-13300] due to args.save_total_limit
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-23 14:52:04,950 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-23 14:52:04,950 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-23 14:52:04,950 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-23 14:54:14,005 >> Saving model checkpoint to ./output/tmp-checkpoint-23400
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-23 14:54:14,143 >> tokenizer config file saved in ./output/tmp-checkpoint-23400/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-23 14:54:14,143 >> Special tokens file saved in ./output/tmp-checkpoint-23400/special_tokens_map.json
[INFO|trainer.py:3159] 2024-03-23 14:54:14,380 >> Deleting older checkpoint [output/checkpoint-13400] due to args.save_total_limit
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-23 15:02:51,084 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-23 15:02:51,084 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-23 15:02:51,084 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-23 15:05:00,353 >> Saving model checkpoint to ./output/tmp-checkpoint-23500
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-23 15:05:00,498 >> tokenizer config file saved in ./output/tmp-checkpoint-23500/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-23 15:05:00,499 >> Special tokens file saved in ./output/tmp-checkpoint-23500/special_tokens_map.json
[INFO|trainer.py:3159] 2024-03-23 15:05:00,731 >> Deleting older checkpoint [output/checkpoint-13500] due to args.save_total_limit
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-23 15:13:28,470 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-23 15:13:28,470 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-23 15:13:28,470 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-23 15:15:38,031 >> Saving model checkpoint to ./output/tmp-checkpoint-23600
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-23 15:15:38,175 >> tokenizer config file saved in ./output/tmp-checkpoint-23600/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-23 15:15:38,175 >> Special tokens file saved in ./output/tmp-checkpoint-23600/special_tokens_map.json
[INFO|trainer.py:3159] 2024-03-23 15:15:38,413 >> Deleting older checkpoint [output/checkpoint-13600] due to args.save_total_limit
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
{'loss': 0.2747, 'grad_norm': 1.2347092628479004, 'learning_rate': 5.8150183150183154e-05, 'epoch': 4.23}
********************on step end call back********************
Step 23260 finish
{'loss': 0.2767, 'grad_norm': 1.2720805406570435, 'learning_rate': 5.813186813186814e-05, 'epoch': 4.23}
********************on step end call back********************
Step 23270 finish
{'loss': 0.2983, 'grad_norm': 0.9438452124595642, 'learning_rate': 5.811355311355312e-05, 'epoch': 4.23}
********************on step end call back********************
Step 23280 finish
{'loss': 0.2424, 'grad_norm': 1.5370575189590454, 'learning_rate': 5.8095238095238104e-05, 'epoch': 4.23}
********************on step end call back********************
Step 23290 finish
{'loss': 0.2256, 'grad_norm': 0.9278869032859802, 'learning_rate': 5.807692307692308e-05, 'epoch': 4.23}
********************on step end call back********************
Step 23300 finish
{'loss': 0.2901, 'grad_norm': 1.3328282833099365, 'learning_rate': 5.805860805860807e-05, 'epoch': 4.24}
{'eval_loss': 0.3464806079864502, 'eval_accuracy': 0.875, 'eval_runtime': 129.2725, 'eval_samples_per_second': 4.935, 'eval_steps_per_second': 4.935, 'epoch': 4.24}
********************save call back********************
********************on step end call back********************
Step 23310 finish
{'loss': 0.2555, 'grad_norm': 1.5040863752365112, 'learning_rate': 5.804029304029305e-05, 'epoch': 4.24}
********************on step end call back********************
Step 23320 finish
{'loss': 0.2533, 'grad_norm': 1.1414459943771362, 'learning_rate': 5.802197802197803e-05, 'epoch': 4.24}
********************on step end call back********************
Step 23330 finish
{'loss': 0.2844, 'grad_norm': 1.4169113636016846, 'learning_rate': 5.800366300366301e-05, 'epoch': 4.24}
********************on step end call back********************
Step 23340 finish
{'loss': 0.2912, 'grad_norm': 1.201570987701416, 'learning_rate': 5.798534798534799e-05, 'epoch': 4.24}
********************on step end call back********************
Step 23350 finish
{'loss': 0.2745, 'grad_norm': 1.0157077312469482, 'learning_rate': 5.7967032967032976e-05, 'epoch': 4.24}
********************on step end call back********************
Step 23360 finish
{'loss': 0.2975, 'grad_norm': 1.323532223701477, 'learning_rate': 5.7948717948717954e-05, 'epoch': 4.25}
********************on step end call back********************
Step 23370 finish
{'loss': 0.2808, 'grad_norm': 1.3667100667953491, 'learning_rate': 5.793040293040294e-05, 'epoch': 4.25}
********************on step end call back********************
Step 23380 finish
{'loss': 0.3107, 'grad_norm': 1.2706716060638428, 'learning_rate': 5.791208791208792e-05, 'epoch': 4.25}
********************on step end call back********************
Step 23390 finish
{'loss': 0.2327, 'grad_norm': 1.1395305395126343, 'learning_rate': 5.7893772893772904e-05, 'epoch': 4.25}
********************on step end call back********************
Step 23400 finish
{'loss': 0.228, 'grad_norm': 1.1644567251205444, 'learning_rate': 5.787545787545788e-05, 'epoch': 4.25}
{'eval_loss': 0.35441455245018005, 'eval_accuracy': 0.8854166666666666, 'eval_runtime': 129.0536, 'eval_samples_per_second': 4.944, 'eval_steps_per_second': 4.944, 'epoch': 4.25}
********************save call back********************
********************on step end call back********************
Step 23410 finish
{'loss': 0.2934, 'grad_norm': 1.3798551559448242, 'learning_rate': 5.785714285714287e-05, 'epoch': 4.26}
********************on step end call back********************
Step 23420 finish
{'loss': 0.2666, 'grad_norm': 1.0323445796966553, 'learning_rate': 5.783882783882785e-05, 'epoch': 4.26}
********************on step end call back********************
Step 23430 finish
{'loss': 0.2818, 'grad_norm': 1.5216962099075317, 'learning_rate': 5.7820512820512826e-05, 'epoch': 4.26}
********************on step end call back********************
Step 23440 finish
{'loss': 0.308, 'grad_norm': 1.1923922300338745, 'learning_rate': 5.78021978021978e-05, 'epoch': 4.26}
********************on step end call back********************
Step 23450 finish
{'loss': 0.2634, 'grad_norm': 1.0906429290771484, 'learning_rate': 5.7783882783882784e-05, 'epoch': 4.26}
********************on step end call back********************
Step 23460 finish
{'loss': 0.3033, 'grad_norm': 1.1943373680114746, 'learning_rate': 5.776556776556776e-05, 'epoch': 4.26}
********************on step end call back********************
Step 23470 finish
{'loss': 0.2959, 'grad_norm': 1.669825553894043, 'learning_rate': 5.774725274725275e-05, 'epoch': 4.27}
********************on step end call back********************
Step 23480 finish
{'loss': 0.2684, 'grad_norm': 1.3664034605026245, 'learning_rate': 5.7728937728937727e-05, 'epoch': 4.27}
********************on step end call back********************
Step 23490 finish
{'loss': 0.2861, 'grad_norm': 1.001561164855957, 'learning_rate': 5.7710622710622705e-05, 'epoch': 4.27}
********************on step end call back********************
Step 23500 finish
{'loss': 0.2828, 'grad_norm': 1.1248128414154053, 'learning_rate': 5.769230769230769e-05, 'epoch': 4.27}
{'eval_loss': 0.3488540053367615, 'eval_accuracy': 0.875, 'eval_runtime': 129.2674, 'eval_samples_per_second': 4.936, 'eval_steps_per_second': 4.936, 'epoch': 4.27}
********************save call back********************
********************on step end call back********************
Step 23510 finish
{'loss': 0.2424, 'grad_norm': 0.9240359663963318, 'learning_rate': 5.767399267399267e-05, 'epoch': 4.27}
********************on step end call back********************
Step 23520 finish
{'loss': 0.2577, 'grad_norm': 1.2458748817443848, 'learning_rate': 5.7655677655677655e-05, 'epoch': 4.28}
********************on step end call back********************
Step 23530 finish
{'loss': 0.2487, 'grad_norm': 1.234114646911621, 'learning_rate': 5.7637362637362634e-05, 'epoch': 4.28}
********************on step end call back********************
Step 23540 finish
{'loss': 0.2589, 'grad_norm': 1.1088765859603882, 'learning_rate': 5.761904761904762e-05, 'epoch': 4.28}
********************on step end call back********************
Step 23550 finish
{'loss': 0.2532, 'grad_norm': 1.1303752660751343, 'learning_rate': 5.76007326007326e-05, 'epoch': 4.28}
********************on step end call back********************
Step 23560 finish
{'loss': 0.2888, 'grad_norm': 1.4033597707748413, 'learning_rate': 5.7582417582417584e-05, 'epoch': 4.28}
********************on step end call back********************
Step 23570 finish
{'loss': 0.2924, 'grad_norm': 1.0265752077102661, 'learning_rate': 5.756410256410256e-05, 'epoch': 4.28}
********************on step end call back********************
Step 23580 finish
{'loss': 0.2735, 'grad_norm': 1.1579958200454712, 'learning_rate': 5.754578754578754e-05, 'epoch': 4.29}
********************on step end call back********************
Step 23590 finish
{'loss': 0.2669, 'grad_norm': 1.3662400245666504, 'learning_rate': 5.752747252747253e-05, 'epoch': 4.29}
********************on step end call back********************
Step 23600 finish
{'loss': 0.2811, 'grad_norm': 1.1536287069320679, 'learning_rate': 5.7509157509157506e-05, 'epoch': 4.29}
{'eval_loss': 0.3502885699272156, 'eval_accuracy': 0.875, 'eval_runtime': 129.5598, 'eval_samples_per_second': 4.924, 'eval_steps_per_second': 4.924, 'epoch': 4.29}
********************save call back********************
********************on step end call back********************
Step 23610 finish
{'loss': 0.2575, 'grad_norm': 0.9724352955818176, 'learning_rate': 5.749084249084249e-05, 'epoch': 4.29}
********************on step end call back********************
Step 23620 finish
{'loss': 0.2556, 'grad_norm': 1.3366384506225586, 'learning_rate': 5.747252747252747e-05, 'epoch': 4.29}
********************on step end call back********************
Step 23630 finish
{'loss': 0.2753, 'grad_norm': 1.0150574445724487, 'learning_rate': 5.7454212454212455e-05, 'epoch': 4.3}
********************on step end call back********************
Step 23640 finish
[INFO|trainer.py:3376] 2024-03-23 15:24:06,708 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-23 15:24:06,708 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-23 15:24:06,708 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-23 15:26:15,765 >> Saving model checkpoint to ./output/tmp-checkpoint-23700
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-23 15:26:15,926 >> tokenizer config file saved in ./output/tmp-checkpoint-23700/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-23 15:26:15,927 >> Special tokens file saved in ./output/tmp-checkpoint-23700/special_tokens_map.json
[INFO|trainer.py:3159] 2024-03-23 15:26:16,160 >> Deleting older checkpoint [output/checkpoint-13700] due to args.save_total_limit
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-23 15:34:59,863 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-23 15:34:59,863 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-23 15:34:59,863 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-23 15:37:08,679 >> Saving model checkpoint to ./output/tmp-checkpoint-23800
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-23 15:37:08,822 >> tokenizer config file saved in ./output/tmp-checkpoint-23800/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-23 15:37:08,822 >> Special tokens file saved in ./output/tmp-checkpoint-23800/special_tokens_map.json
[INFO|trainer.py:3159] 2024-03-23 15:37:09,057 >> Deleting older checkpoint [output/checkpoint-13800] due to args.save_total_limit
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-23 15:45:40,711 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-23 15:45:40,712 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-23 15:45:40,712 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-23 15:47:49,774 >> Saving model checkpoint to ./output/tmp-checkpoint-23900
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-23 15:47:49,915 >> tokenizer config file saved in ./output/tmp-checkpoint-23900/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-23 15:47:49,916 >> Special tokens file saved in ./output/tmp-checkpoint-23900/special_tokens_map.json
[INFO|trainer.py:3159] 2024-03-23 15:47:50,155 >> Deleting older checkpoint [output/checkpoint-13900] due to args.save_total_limit
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-23 15:56:19,916 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-23 15:56:19,916 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-23 15:56:19,916 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-23 15:58:29,340 >> Saving model checkpoint to ./output/tmp-checkpoint-24000
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-23 15:58:29,498 >> tokenizer config file saved in ./output/tmp-checkpoint-24000/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-23 15:58:29,498 >> Special tokens file saved in ./output/tmp-checkpoint-24000/special_tokens_map.json
[INFO|trainer.py:3159] 2024-03-23 15:58:29,746 >> Deleting older checkpoint [output/checkpoint-14000] due to args.save_total_limit
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
{'loss': 0.2246, 'grad_norm': 1.3338335752487183, 'learning_rate': 5.7435897435897434e-05, 'epoch': 4.3}
********************on step end call back********************
Step 23650 finish
{'loss': 0.2953, 'grad_norm': 1.029957890510559, 'learning_rate': 5.741758241758241e-05, 'epoch': 4.3}
********************on step end call back********************
Step 23660 finish
{'loss': 0.2972, 'grad_norm': 1.4355239868164062, 'learning_rate': 5.73992673992674e-05, 'epoch': 4.3}
********************on step end call back********************
Step 23670 finish
{'loss': 0.2502, 'grad_norm': 1.1852048635482788, 'learning_rate': 5.738095238095238e-05, 'epoch': 4.3}
********************on step end call back********************
Step 23680 finish
{'loss': 0.2552, 'grad_norm': 1.0370125770568848, 'learning_rate': 5.736263736263736e-05, 'epoch': 4.3}
********************on step end call back********************
Step 23690 finish
{'loss': 0.2731, 'grad_norm': 1.3839668035507202, 'learning_rate': 5.734432234432234e-05, 'epoch': 4.31}
********************on step end call back********************
Step 23700 finish
{'loss': 0.293, 'grad_norm': 1.4921319484710693, 'learning_rate': 5.732600732600733e-05, 'epoch': 4.31}
{'eval_loss': 0.34821927547454834, 'eval_accuracy': 0.875, 'eval_runtime': 129.0563, 'eval_samples_per_second': 4.944, 'eval_steps_per_second': 4.944, 'epoch': 4.31}
********************save call back********************
********************on step end call back********************
Step 23710 finish
{'loss': 0.2555, 'grad_norm': 0.8896192312240601, 'learning_rate': 5.7307692307692306e-05, 'epoch': 4.31}
********************on step end call back********************
Step 23720 finish
{'loss': 0.3043, 'grad_norm': 0.9891618490219116, 'learning_rate': 5.728937728937729e-05, 'epoch': 4.31}
********************on step end call back********************
Step 23730 finish
{'loss': 0.2885, 'grad_norm': 1.1565256118774414, 'learning_rate': 5.727106227106227e-05, 'epoch': 4.31}
********************on step end call back********************
Step 23740 finish
{'loss': 0.2831, 'grad_norm': 1.4348359107971191, 'learning_rate': 5.725274725274725e-05, 'epoch': 4.32}
********************on step end call back********************
Step 23750 finish
{'loss': 0.2666, 'grad_norm': 1.378751516342163, 'learning_rate': 5.7234432234432234e-05, 'epoch': 4.32}
********************on step end call back********************
Step 23760 finish
{'loss': 0.2752, 'grad_norm': 1.2574009895324707, 'learning_rate': 5.721611721611721e-05, 'epoch': 4.32}
********************on step end call back********************
Step 23770 finish
{'loss': 0.2462, 'grad_norm': 0.9212291240692139, 'learning_rate': 5.71978021978022e-05, 'epoch': 4.32}
********************on step end call back********************
Step 23780 finish
{'loss': 0.2846, 'grad_norm': 1.2026251554489136, 'learning_rate': 5.717948717948718e-05, 'epoch': 4.32}
********************on step end call back********************
Step 23790 finish
{'loss': 0.313, 'grad_norm': 1.4140300750732422, 'learning_rate': 5.716117216117216e-05, 'epoch': 4.32}
********************on step end call back********************
Step 23800 finish
{'loss': 0.2959, 'grad_norm': 1.2032883167266846, 'learning_rate': 5.714285714285714e-05, 'epoch': 4.33}
{'eval_loss': 0.3513390123844147, 'eval_accuracy': 0.875, 'eval_runtime': 128.8147, 'eval_samples_per_second': 4.953, 'eval_steps_per_second': 4.953, 'epoch': 4.33}
********************save call back********************
********************on step end call back********************
Step 23810 finish
{'loss': 0.2551, 'grad_norm': 1.1341311931610107, 'learning_rate': 5.712454212454213e-05, 'epoch': 4.33}
********************on step end call back********************
Step 23820 finish
{'loss': 0.2531, 'grad_norm': 1.157590389251709, 'learning_rate': 5.7106227106227106e-05, 'epoch': 4.33}
********************on step end call back********************
Step 23830 finish
{'loss': 0.2817, 'grad_norm': 1.0682101249694824, 'learning_rate': 5.7087912087912085e-05, 'epoch': 4.33}
********************on step end call back********************
Step 23840 finish
{'loss': 0.284, 'grad_norm': 1.5123809576034546, 'learning_rate': 5.706959706959707e-05, 'epoch': 4.33}
********************on step end call back********************
Step 23850 finish
{'loss': 0.3005, 'grad_norm': 1.3319015502929688, 'learning_rate': 5.705128205128205e-05, 'epoch': 4.34}
********************on step end call back********************
Step 23860 finish
{'loss': 0.2757, 'grad_norm': 1.0891433954238892, 'learning_rate': 5.7032967032967035e-05, 'epoch': 4.34}
********************on step end call back********************
Step 23870 finish
{'loss': 0.2505, 'grad_norm': 1.1919341087341309, 'learning_rate': 5.701465201465201e-05, 'epoch': 4.34}
********************on step end call back********************
Step 23880 finish
{'loss': 0.2752, 'grad_norm': 1.1662064790725708, 'learning_rate': 5.6996336996337e-05, 'epoch': 4.34}
********************on step end call back********************
Step 23890 finish
{'loss': 0.3098, 'grad_norm': 1.2502466440200806, 'learning_rate': 5.697802197802198e-05, 'epoch': 4.34}
********************on step end call back********************
Step 23900 finish
{'loss': 0.2917, 'grad_norm': 1.1894547939300537, 'learning_rate': 5.695970695970696e-05, 'epoch': 4.34}
{'eval_loss': 0.3449922204017639, 'eval_accuracy': 0.875, 'eval_runtime': 129.0608, 'eval_samples_per_second': 4.943, 'eval_steps_per_second': 4.943, 'epoch': 4.34}
********************save call back********************
********************on step end call back********************
Step 23910 finish
{'loss': 0.2999, 'grad_norm': 1.2425264120101929, 'learning_rate': 5.694139194139194e-05, 'epoch': 4.35}
********************on step end call back********************
Step 23920 finish
{'loss': 0.2955, 'grad_norm': 1.3606433868408203, 'learning_rate': 5.692307692307692e-05, 'epoch': 4.35}
********************on step end call back********************
Step 23930 finish
{'loss': 0.253, 'grad_norm': 0.915816068649292, 'learning_rate': 5.6904761904761906e-05, 'epoch': 4.35}
********************on step end call back********************
Step 23940 finish
{'loss': 0.2518, 'grad_norm': 1.130705714225769, 'learning_rate': 5.6886446886446885e-05, 'epoch': 4.35}
********************on step end call back********************
Step 23950 finish
{'loss': 0.3088, 'grad_norm': 1.2211464643478394, 'learning_rate': 5.686813186813187e-05, 'epoch': 4.35}
********************on step end call back********************
Step 23960 finish
{'loss': 0.2629, 'grad_norm': 1.3740135431289673, 'learning_rate': 5.684981684981685e-05, 'epoch': 4.36}
********************on step end call back********************
Step 23970 finish
{'loss': 0.2561, 'grad_norm': 1.3291523456573486, 'learning_rate': 5.6831501831501835e-05, 'epoch': 4.36}
********************on step end call back********************
Step 23980 finish
{'loss': 0.2863, 'grad_norm': 1.2432125806808472, 'learning_rate': 5.6813186813186814e-05, 'epoch': 4.36}
********************on step end call back********************
Step 23990 finish
{'loss': 0.2492, 'grad_norm': 1.1388678550720215, 'learning_rate': 5.679487179487179e-05, 'epoch': 4.36}
********************on step end call back********************
Step 24000 finish
{'loss': 0.2911, 'grad_norm': 0.954673707485199, 'learning_rate': 5.677655677655678e-05, 'epoch': 4.36}
{'eval_loss': 0.3464431166648865, 'eval_accuracy': 0.8854166666666666, 'eval_runtime': 129.4231, 'eval_samples_per_second': 4.93, 'eval_steps_per_second': 4.93, 'epoch': 4.36}
********************save call back********************
********************on step end call back********************
Step 24010 finish
{'loss': 0.2736, 'grad_norm': 1.4457191228866577, 'learning_rate': 5.675824175824176e-05, 'epoch': 4.36}
********************on step end call back********************
Step 24020 finish
{'loss': 0.2874, 'grad_norm': 1.1653449535369873, 'learning_rate': 5.673992673992674e-05, 'epoch': 4.37}
********************on step end call back********************
Step 24030 finish
[INFO|trainer.py:3376] 2024-03-23 16:07:03,085 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-23 16:07:03,085 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-23 16:07:03,085 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-23 16:09:11,916 >> Saving model checkpoint to ./output/tmp-checkpoint-24100
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-23 16:09:12,355 >> tokenizer config file saved in ./output/tmp-checkpoint-24100/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-23 16:09:12,356 >> Special tokens file saved in ./output/tmp-checkpoint-24100/special_tokens_map.json
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-23 16:17:43,963 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-23 16:17:43,963 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-23 16:17:43,963 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-23 16:20:01,115 >> Saving model checkpoint to ./output/tmp-checkpoint-24200
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-23 16:20:01,258 >> tokenizer config file saved in ./output/tmp-checkpoint-24200/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-23 16:20:01,258 >> Special tokens file saved in ./output/tmp-checkpoint-24200/special_tokens_map.json
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-23 16:28:27,254 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-23 16:28:27,254 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-23 16:28:27,254 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-23 16:30:36,154 >> Saving model checkpoint to ./output/tmp-checkpoint-24300
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-23 16:30:36,308 >> tokenizer config file saved in ./output/tmp-checkpoint-24300/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-23 16:30:36,308 >> Special tokens file saved in ./output/tmp-checkpoint-24300/special_tokens_map.json
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-23 16:39:26,277 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-23 16:39:26,277 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-23 16:39:26,277 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-23 16:41:35,194 >> Saving model checkpoint to ./output/tmp-checkpoint-24400
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-23 16:41:35,335 >> tokenizer config file saved in ./output/tmp-checkpoint-24400/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-23 16:41:35,335 >> Special tokens file saved in ./output/tmp-checkpoint-24400/special_tokens_map.json
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
{'loss': 0.3036, 'grad_norm': 1.0256679058074951, 'learning_rate': 5.672161172161172e-05, 'epoch': 4.37}
********************on step end call back********************
Step 24040 finish
{'loss': 0.2856, 'grad_norm': 0.9276105165481567, 'learning_rate': 5.6703296703296706e-05, 'epoch': 4.37}
********************on step end call back********************
Step 24050 finish
{'loss': 0.2858, 'grad_norm': 1.513368844985962, 'learning_rate': 5.6684981684981685e-05, 'epoch': 4.37}
********************on step end call back********************
Step 24060 finish
{'loss': 0.2842, 'grad_norm': 1.0986394882202148, 'learning_rate': 5.666666666666667e-05, 'epoch': 4.37}
********************on step end call back********************
Step 24070 finish
{'loss': 0.2638, 'grad_norm': 1.2929669618606567, 'learning_rate': 5.664835164835165e-05, 'epoch': 4.38}
********************on step end call back********************
Step 24080 finish
{'loss': 0.2758, 'grad_norm': 0.7948028445243835, 'learning_rate': 5.663003663003663e-05, 'epoch': 4.38}
********************on step end call back********************
Step 24090 finish
{'loss': 0.2954, 'grad_norm': 1.263956904411316, 'learning_rate': 5.6611721611721614e-05, 'epoch': 4.38}
********************on step end call back********************
Step 24100 finish
{'loss': 0.2734, 'grad_norm': 1.070150375366211, 'learning_rate': 5.659340659340659e-05, 'epoch': 4.38}
{'eval_loss': 0.3410071134567261, 'eval_accuracy': 0.8854166666666666, 'eval_runtime': 128.8299, 'eval_samples_per_second': 4.952, 'eval_steps_per_second': 4.952, 'epoch': 4.38}
********************save call back********************
********************on step end call back********************
Step 24110 finish
{'loss': 0.2973, 'grad_norm': 1.0582736730575562, 'learning_rate': 5.657509157509158e-05, 'epoch': 4.38}
********************on step end call back********************
Step 24120 finish
{'loss': 0.278, 'grad_norm': 1.433577299118042, 'learning_rate': 5.655677655677656e-05, 'epoch': 4.38}
********************on step end call back********************
Step 24130 finish
{'loss': 0.2617, 'grad_norm': 1.3097450733184814, 'learning_rate': 5.653846153846154e-05, 'epoch': 4.39}
********************on step end call back********************
Step 24140 finish
{'loss': 0.2626, 'grad_norm': 0.6982110142707825, 'learning_rate': 5.652014652014652e-05, 'epoch': 4.39}
********************on step end call back********************
Step 24150 finish
{'loss': 0.2905, 'grad_norm': 1.2204904556274414, 'learning_rate': 5.650183150183151e-05, 'epoch': 4.39}
********************on step end call back********************
Step 24160 finish
{'loss': 0.2771, 'grad_norm': 1.3715850114822388, 'learning_rate': 5.6483516483516485e-05, 'epoch': 4.39}
********************on step end call back********************
Step 24170 finish
{'loss': 0.2729, 'grad_norm': 1.2339394092559814, 'learning_rate': 5.6465201465201464e-05, 'epoch': 4.39}
********************on step end call back********************
Step 24180 finish
{'loss': 0.3106, 'grad_norm': 1.2850453853607178, 'learning_rate': 5.644688644688645e-05, 'epoch': 4.4}
********************on step end call back********************
Step 24190 finish
{'loss': 0.2649, 'grad_norm': 1.2057793140411377, 'learning_rate': 5.642857142857143e-05, 'epoch': 4.4}
********************on step end call back********************
Step 24200 finish
{'loss': 0.2808, 'grad_norm': 1.2929974794387817, 'learning_rate': 5.6410256410256414e-05, 'epoch': 4.4}
{'eval_loss': 0.34369876980781555, 'eval_accuracy': 0.8854166666666666, 'eval_runtime': 137.1513, 'eval_samples_per_second': 4.652, 'eval_steps_per_second': 4.652, 'epoch': 4.4}
********************save call back********************
********************on step end call back********************
Step 24210 finish
{'loss': 0.266, 'grad_norm': 1.1155122518539429, 'learning_rate': 5.639194139194139e-05, 'epoch': 4.4}
********************on step end call back********************
Step 24220 finish
{'loss': 0.3059, 'grad_norm': 1.1574625968933105, 'learning_rate': 5.637362637362638e-05, 'epoch': 4.4}
********************on step end call back********************
Step 24230 finish
{'loss': 0.3021, 'grad_norm': 1.1891698837280273, 'learning_rate': 5.635531135531136e-05, 'epoch': 4.4}
********************on step end call back********************
Step 24240 finish
{'loss': 0.262, 'grad_norm': 1.3463128805160522, 'learning_rate': 5.633699633699634e-05, 'epoch': 4.41}
********************on step end call back********************
Step 24250 finish
{'loss': 0.2864, 'grad_norm': 1.2946281433105469, 'learning_rate': 5.631868131868132e-05, 'epoch': 4.41}
********************on step end call back********************
Step 24260 finish
{'loss': 0.2903, 'grad_norm': 1.4196852445602417, 'learning_rate': 5.63003663003663e-05, 'epoch': 4.41}
********************on step end call back********************
Step 24270 finish
{'loss': 0.3066, 'grad_norm': 1.2817429304122925, 'learning_rate': 5.6282051282051286e-05, 'epoch': 4.41}
********************on step end call back********************
Step 24280 finish
{'loss': 0.2836, 'grad_norm': 1.468271017074585, 'learning_rate': 5.6263736263736264e-05, 'epoch': 4.41}
********************on step end call back********************
Step 24290 finish
{'loss': 0.2557, 'grad_norm': 0.9708901047706604, 'learning_rate': 5.624542124542125e-05, 'epoch': 4.42}
********************on step end call back********************
Step 24300 finish
{'loss': 0.2525, 'grad_norm': 1.2462531328201294, 'learning_rate': 5.622710622710623e-05, 'epoch': 4.42}
{'eval_loss': 0.34177035093307495, 'eval_accuracy': 0.8854166666666666, 'eval_runtime': 128.8989, 'eval_samples_per_second': 4.95, 'eval_steps_per_second': 4.95, 'epoch': 4.42}
********************save call back********************
********************on step end call back********************
Step 24310 finish
{'loss': 0.2792, 'grad_norm': 1.9233171939849854, 'learning_rate': 5.6208791208791214e-05, 'epoch': 4.42}
********************on step end call back********************
Step 24320 finish
{'loss': 0.2759, 'grad_norm': 1.0513548851013184, 'learning_rate': 5.619047619047619e-05, 'epoch': 4.42}
********************on step end call back********************
Step 24330 finish
{'loss': 0.2979, 'grad_norm': 0.9957106113433838, 'learning_rate': 5.617216117216118e-05, 'epoch': 4.42}
********************on step end call back********************
Step 24340 finish
{'loss': 0.2957, 'grad_norm': 1.4229212999343872, 'learning_rate': 5.615384615384616e-05, 'epoch': 4.42}
********************on step end call back********************
Step 24350 finish
{'loss': 0.3118, 'grad_norm': 1.1271798610687256, 'learning_rate': 5.6135531135531136e-05, 'epoch': 4.43}
********************on step end call back********************
Step 24360 finish
{'loss': 0.2994, 'grad_norm': 1.0018069744110107, 'learning_rate': 5.611721611721612e-05, 'epoch': 4.43}
********************on step end call back********************
Step 24370 finish
{'loss': 0.2917, 'grad_norm': 1.3932567834854126, 'learning_rate': 5.60989010989011e-05, 'epoch': 4.43}
********************on step end call back********************
Step 24380 finish
{'loss': 0.27, 'grad_norm': 1.546800136566162, 'learning_rate': 5.6080586080586086e-05, 'epoch': 4.43}
********************on step end call back********************
Step 24390 finish
{'loss': 0.3002, 'grad_norm': 1.069345474243164, 'learning_rate': 5.6062271062271065e-05, 'epoch': 4.43}
********************on step end call back********************
Step 24400 finish
{'loss': 0.3188, 'grad_norm': 1.1955394744873047, 'learning_rate': 5.604395604395605e-05, 'epoch': 4.44}
{'eval_loss': 0.3384568989276886, 'eval_accuracy': 0.8958333333333334, 'eval_runtime': 128.9152, 'eval_samples_per_second': 4.949, 'eval_steps_per_second': 4.949, 'epoch': 4.44}
********************save call back********************
********************on step end call back********************
Step 24410 finish
{'loss': 0.2958, 'grad_norm': 1.7239596843719482, 'learning_rate': 5.602564102564103e-05, 'epoch': 4.44}
********************on step end call back********************
Step 24420 finish
[INFO|trainer.py:3376] 2024-03-23 16:50:16,867 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-23 16:50:16,867 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-23 16:50:16,867 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-23 16:52:26,007 >> Saving model checkpoint to ./output/tmp-checkpoint-24500
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-23 16:52:26,147 >> tokenizer config file saved in ./output/tmp-checkpoint-24500/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-23 16:52:26,147 >> Special tokens file saved in ./output/tmp-checkpoint-24500/special_tokens_map.json
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-23 17:01:06,435 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-23 17:01:06,435 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-23 17:01:06,435 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-23 17:03:15,251 >> Saving model checkpoint to ./output/tmp-checkpoint-24600
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-23 17:03:15,389 >> tokenizer config file saved in ./output/tmp-checkpoint-24600/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-23 17:03:15,389 >> Special tokens file saved in ./output/tmp-checkpoint-24600/special_tokens_map.json
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-23 17:11:47,292 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-23 17:11:47,292 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-23 17:11:47,292 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-23 17:13:56,243 >> Saving model checkpoint to ./output/tmp-checkpoint-24700
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-23 17:13:56,397 >> tokenizer config file saved in ./output/tmp-checkpoint-24700/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-23 17:13:56,397 >> Special tokens file saved in ./output/tmp-checkpoint-24700/special_tokens_map.json
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-23 17:22:31,670 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-23 17:22:31,670 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-23 17:22:31,670 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-23 17:24:40,530 >> Saving model checkpoint to ./output/tmp-checkpoint-24800
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-23 17:24:40,666 >> tokenizer config file saved in ./output/tmp-checkpoint-24800/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-23 17:24:40,667 >> Special tokens file saved in ./output/tmp-checkpoint-24800/special_tokens_map.json
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
{'loss': 0.3195, 'grad_norm': 1.3221060037612915, 'learning_rate': 5.600732600732601e-05, 'epoch': 4.44}
********************on step end call back********************
Step 24430 finish
{'loss': 0.2824, 'grad_norm': 1.2468794584274292, 'learning_rate': 5.598901098901099e-05, 'epoch': 4.44}
********************on step end call back********************
Step 24440 finish
{'loss': 0.277, 'grad_norm': 1.3797260522842407, 'learning_rate': 5.597069597069597e-05, 'epoch': 4.44}
********************on step end call back********************
Step 24450 finish
{'loss': 0.2487, 'grad_norm': 1.3654366731643677, 'learning_rate': 5.595238095238096e-05, 'epoch': 4.44}
********************on step end call back********************
Step 24460 finish
{'loss': 0.2947, 'grad_norm': 1.022274374961853, 'learning_rate': 5.5934065934065936e-05, 'epoch': 4.45}
********************on step end call back********************
Step 24470 finish
{'loss': 0.2796, 'grad_norm': 1.4406846761703491, 'learning_rate': 5.591575091575092e-05, 'epoch': 4.45}
********************on step end call back********************
Step 24480 finish
{'loss': 0.2416, 'grad_norm': 1.231963872909546, 'learning_rate': 5.58974358974359e-05, 'epoch': 4.45}
********************on step end call back********************
Step 24490 finish
{'loss': 0.2561, 'grad_norm': 1.2721920013427734, 'learning_rate': 5.5879120879120886e-05, 'epoch': 4.45}
********************on step end call back********************
Step 24500 finish
{'loss': 0.3044, 'grad_norm': 1.3007546663284302, 'learning_rate': 5.5860805860805865e-05, 'epoch': 4.45}
{'eval_loss': 0.34679093956947327, 'eval_accuracy': 0.8854166666666666, 'eval_runtime': 129.1393, 'eval_samples_per_second': 4.94, 'eval_steps_per_second': 4.94, 'epoch': 4.45}
********************save call back********************
********************on step end call back********************
Step 24510 finish
{'loss': 0.3066, 'grad_norm': 1.376577615737915, 'learning_rate': 5.5842490842490844e-05, 'epoch': 4.46}
********************on step end call back********************
Step 24520 finish
{'loss': 0.3131, 'grad_norm': 1.2790558338165283, 'learning_rate': 5.582417582417583e-05, 'epoch': 4.46}
********************on step end call back********************
Step 24530 finish
{'loss': 0.2839, 'grad_norm': 1.2432266473770142, 'learning_rate': 5.580586080586081e-05, 'epoch': 4.46}
********************on step end call back********************
Step 24540 finish
{'loss': 0.3515, 'grad_norm': 1.5884450674057007, 'learning_rate': 5.5787545787545793e-05, 'epoch': 4.46}
********************on step end call back********************
Step 24550 finish
{'loss': 0.3075, 'grad_norm': 1.2078163623809814, 'learning_rate': 5.576923076923077e-05, 'epoch': 4.46}
********************on step end call back********************
Step 24560 finish
{'loss': 0.2844, 'grad_norm': 1.188210129737854, 'learning_rate': 5.575091575091576e-05, 'epoch': 4.46}
********************on step end call back********************
Step 24570 finish
{'loss': 0.2621, 'grad_norm': 0.7811513543128967, 'learning_rate': 5.5732600732600736e-05, 'epoch': 4.47}
********************on step end call back********************
Step 24580 finish
{'loss': 0.2747, 'grad_norm': 1.0209141969680786, 'learning_rate': 5.571428571428572e-05, 'epoch': 4.47}
********************on step end call back********************
Step 24590 finish
{'loss': 0.2942, 'grad_norm': 1.5560983419418335, 'learning_rate': 5.56959706959707e-05, 'epoch': 4.47}
********************on step end call back********************
Step 24600 finish
{'loss': 0.2734, 'grad_norm': 1.0562101602554321, 'learning_rate': 5.567765567765568e-05, 'epoch': 4.47}
{'eval_loss': 0.34393224120140076, 'eval_accuracy': 0.90625, 'eval_runtime': 128.8148, 'eval_samples_per_second': 4.953, 'eval_steps_per_second': 4.953, 'epoch': 4.47}
********************save call back********************
********************on step end call back********************
Step 24610 finish
{'loss': 0.2927, 'grad_norm': 1.2539077997207642, 'learning_rate': 5.5659340659340665e-05, 'epoch': 4.47}
********************on step end call back********************
Step 24620 finish
{'loss': 0.2672, 'grad_norm': 1.7333165407180786, 'learning_rate': 5.5641025641025644e-05, 'epoch': 4.48}
********************on step end call back********************
Step 24630 finish
{'loss': 0.2943, 'grad_norm': 1.222142219543457, 'learning_rate': 5.562271062271063e-05, 'epoch': 4.48}
********************on step end call back********************
Step 24640 finish
{'loss': 0.2636, 'grad_norm': 1.2335633039474487, 'learning_rate': 5.560439560439561e-05, 'epoch': 4.48}
********************on step end call back********************
Step 24650 finish
{'loss': 0.2892, 'grad_norm': 1.4200899600982666, 'learning_rate': 5.5586080586080594e-05, 'epoch': 4.48}
********************on step end call back********************
Step 24660 finish
{'loss': 0.2948, 'grad_norm': 1.065969467163086, 'learning_rate': 5.556776556776557e-05, 'epoch': 4.48}
********************on step end call back********************
Step 24670 finish
{'loss': 0.2808, 'grad_norm': 1.424285650253296, 'learning_rate': 5.554945054945056e-05, 'epoch': 4.48}
********************on step end call back********************
Step 24680 finish
{'loss': 0.2702, 'grad_norm': 1.3447632789611816, 'learning_rate': 5.553113553113554e-05, 'epoch': 4.49}
********************on step end call back********************
Step 24690 finish
{'loss': 0.2941, 'grad_norm': 1.4290281534194946, 'learning_rate': 5.5512820512820515e-05, 'epoch': 4.49}
********************on step end call back********************
Step 24700 finish
{'loss': 0.2925, 'grad_norm': 1.177321434020996, 'learning_rate': 5.54945054945055e-05, 'epoch': 4.49}
{'eval_loss': 0.34332701563835144, 'eval_accuracy': 0.90625, 'eval_runtime': 128.9507, 'eval_samples_per_second': 4.948, 'eval_steps_per_second': 4.948, 'epoch': 4.49}
********************save call back********************
********************on step end call back********************
Step 24710 finish
{'loss': 0.2904, 'grad_norm': 1.3525601625442505, 'learning_rate': 5.547619047619048e-05, 'epoch': 4.49}
********************on step end call back********************
Step 24720 finish
{'loss': 0.2782, 'grad_norm': 1.2664902210235596, 'learning_rate': 5.5457875457875465e-05, 'epoch': 4.49}
********************on step end call back********************
Step 24730 finish
{'loss': 0.277, 'grad_norm': 1.0939102172851562, 'learning_rate': 5.5439560439560444e-05, 'epoch': 4.5}
********************on step end call back********************
Step 24740 finish
{'loss': 0.2882, 'grad_norm': 1.1385791301727295, 'learning_rate': 5.542124542124543e-05, 'epoch': 4.5}
********************on step end call back********************
Step 24750 finish
{'loss': 0.273, 'grad_norm': 1.383873462677002, 'learning_rate': 5.540293040293041e-05, 'epoch': 4.5}
********************on step end call back********************
Step 24760 finish
{'loss': 0.3018, 'grad_norm': 0.9699171185493469, 'learning_rate': 5.538461538461539e-05, 'epoch': 4.5}
********************on step end call back********************
Step 24770 finish
{'loss': 0.2888, 'grad_norm': 0.6623384356498718, 'learning_rate': 5.536630036630037e-05, 'epoch': 4.5}
********************on step end call back********************
Step 24780 finish
{'loss': 0.315, 'grad_norm': 1.7160511016845703, 'learning_rate': 5.534798534798535e-05, 'epoch': 4.5}
********************on step end call back********************
Step 24790 finish
{'loss': 0.2708, 'grad_norm': 1.011377215385437, 'learning_rate': 5.532967032967034e-05, 'epoch': 4.51}
********************on step end call back********************
Step 24800 finish
{'loss': 0.2443, 'grad_norm': 1.2861264944076538, 'learning_rate': 5.5311355311355316e-05, 'epoch': 4.51}
{'eval_loss': 0.3446590304374695, 'eval_accuracy': 0.8854166666666666, 'eval_runtime': 128.8589, 'eval_samples_per_second': 4.951, 'eval_steps_per_second': 4.951, 'epoch': 4.51}
********************save call back********************
********************on step end call back********************
Step 24810 finish
[INFO|trainer.py:3376] 2024-03-23 17:33:18,387 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-23 17:33:18,387 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-23 17:33:18,387 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-23 17:35:27,274 >> Saving model checkpoint to ./output/tmp-checkpoint-24900
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-23 17:35:27,409 >> tokenizer config file saved in ./output/tmp-checkpoint-24900/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-23 17:35:27,410 >> Special tokens file saved in ./output/tmp-checkpoint-24900/special_tokens_map.json
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-23 17:43:56,810 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-23 17:43:56,810 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-23 17:43:56,810 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-23 17:46:05,702 >> Saving model checkpoint to ./output/tmp-checkpoint-25000
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-23 17:46:05,864 >> tokenizer config file saved in ./output/tmp-checkpoint-25000/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-23 17:46:05,864 >> Special tokens file saved in ./output/tmp-checkpoint-25000/special_tokens_map.json
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-23 17:54:39,505 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-23 17:54:39,506 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-23 17:54:39,506 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-23 17:56:48,300 >> Saving model checkpoint to ./output/tmp-checkpoint-25100
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-23 17:56:48,436 >> tokenizer config file saved in ./output/tmp-checkpoint-25100/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-23 17:56:48,436 >> Special tokens file saved in ./output/tmp-checkpoint-25100/special_tokens_map.json
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-23 18:05:29,002 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-23 18:05:29,002 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-23 18:05:29,002 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-23 18:07:37,807 >> Saving model checkpoint to ./output/tmp-checkpoint-25200
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-23 18:07:37,941 >> tokenizer config file saved in ./output/tmp-checkpoint-25200/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-23 18:07:37,941 >> Special tokens file saved in ./output/tmp-checkpoint-25200/special_tokens_map.json
{'loss': 0.2472, 'grad_norm': 0.9293966293334961, 'learning_rate': 5.52930402930403e-05, 'epoch': 4.51}
********************on step end call back********************
Step 24820 finish
{'loss': 0.2798, 'grad_norm': 1.166725516319275, 'learning_rate': 5.527472527472528e-05, 'epoch': 4.51}
********************on step end call back********************
Step 24830 finish
{'loss': 0.27, 'grad_norm': 1.1310877799987793, 'learning_rate': 5.5256410256410265e-05, 'epoch': 4.51}
********************on step end call back********************
Step 24840 finish
{'loss': 0.3323, 'grad_norm': 1.1753886938095093, 'learning_rate': 5.5238095238095244e-05, 'epoch': 4.52}
********************on step end call back********************
Step 24850 finish
{'loss': 0.2648, 'grad_norm': 1.3654946088790894, 'learning_rate': 5.521978021978022e-05, 'epoch': 4.52}
********************on step end call back********************
Step 24860 finish
{'loss': 0.2976, 'grad_norm': 1.1342958211898804, 'learning_rate': 5.520146520146521e-05, 'epoch': 4.52}
********************on step end call back********************
Step 24870 finish
{'loss': 0.3188, 'grad_norm': 1.1507924795150757, 'learning_rate': 5.518315018315019e-05, 'epoch': 4.52}
********************on step end call back********************
Step 24880 finish
{'loss': 0.2939, 'grad_norm': 1.0812605619430542, 'learning_rate': 5.516483516483517e-05, 'epoch': 4.52}
********************on step end call back********************
Step 24890 finish
{'loss': 0.321, 'grad_norm': 1.3075848817825317, 'learning_rate': 5.514652014652015e-05, 'epoch': 4.52}
********************on step end call back********************
Step 24900 finish
{'loss': 0.2767, 'grad_norm': 1.2379581928253174, 'learning_rate': 5.512820512820514e-05, 'epoch': 4.53}
{'eval_loss': 0.3436059057712555, 'eval_accuracy': 0.8854166666666666, 'eval_runtime': 128.8862, 'eval_samples_per_second': 4.95, 'eval_steps_per_second': 4.95, 'epoch': 4.53}
********************save call back********************
********************on step end call back********************
Step 24910 finish
{'loss': 0.284, 'grad_norm': 1.3013997077941895, 'learning_rate': 5.5109890109890116e-05, 'epoch': 4.53}
********************on step end call back********************
Step 24920 finish
{'loss': 0.2881, 'grad_norm': 0.9239460825920105, 'learning_rate': 5.50915750915751e-05, 'epoch': 4.53}
********************on step end call back********************
Step 24930 finish
{'loss': 0.3027, 'grad_norm': 0.8357484340667725, 'learning_rate': 5.507326007326008e-05, 'epoch': 4.53}
********************on step end call back********************
Step 24940 finish
{'loss': 0.2801, 'grad_norm': 1.2489367723464966, 'learning_rate': 5.505494505494506e-05, 'epoch': 4.53}
********************on step end call back********************
Step 24950 finish
{'loss': 0.2468, 'grad_norm': 1.2646117210388184, 'learning_rate': 5.5036630036630044e-05, 'epoch': 4.54}
********************on step end call back********************
Step 24960 finish
{'loss': 0.2795, 'grad_norm': 1.1175390481948853, 'learning_rate': 5.501831501831502e-05, 'epoch': 4.54}
********************on step end call back********************
Step 24970 finish
{'loss': 0.2879, 'grad_norm': 1.1239475011825562, 'learning_rate': 5.500000000000001e-05, 'epoch': 4.54}
********************on step end call back********************
Step 24980 finish
{'loss': 0.2847, 'grad_norm': 0.6339412331581116, 'learning_rate': 5.498168498168499e-05, 'epoch': 4.54}
********************on step end call back********************
Step 24990 finish
{'loss': 0.2724, 'grad_norm': 1.19963800907135, 'learning_rate': 5.496336996336997e-05, 'epoch': 4.54}
********************on step end call back********************
Step 25000 finish
{'loss': 0.2263, 'grad_norm': 1.2940495014190674, 'learning_rate': 5.494505494505495e-05, 'epoch': 4.54}
{'eval_loss': 0.3462288975715637, 'eval_accuracy': 0.8854166666666666, 'eval_runtime': 128.8904, 'eval_samples_per_second': 4.95, 'eval_steps_per_second': 4.95, 'epoch': 4.54}
********************save call back********************
********************on step end call back********************
Step 25010 finish
{'loss': 0.2614, 'grad_norm': 1.2069354057312012, 'learning_rate': 5.492673992673994e-05, 'epoch': 4.55}
********************on step end call back********************
Step 25020 finish
{'loss': 0.2862, 'grad_norm': 1.2272515296936035, 'learning_rate': 5.4908424908424916e-05, 'epoch': 4.55}
********************on step end call back********************
Step 25030 finish
{'loss': 0.3178, 'grad_norm': 1.1272269487380981, 'learning_rate': 5.4890109890109895e-05, 'epoch': 4.55}
********************on step end call back********************
Step 25040 finish
{'loss': 0.2716, 'grad_norm': 1.306583285331726, 'learning_rate': 5.487179487179488e-05, 'epoch': 4.55}
********************on step end call back********************
Step 25050 finish
{'loss': 0.2589, 'grad_norm': 1.2958505153656006, 'learning_rate': 5.485347985347986e-05, 'epoch': 4.55}
********************on step end call back********************
Step 25060 finish
{'loss': 0.2624, 'grad_norm': 1.0906254053115845, 'learning_rate': 5.4835164835164845e-05, 'epoch': 4.56}
********************on step end call back********************
Step 25070 finish
{'loss': 0.25, 'grad_norm': 1.2455424070358276, 'learning_rate': 5.4816849816849823e-05, 'epoch': 4.56}
********************on step end call back********************
Step 25080 finish
{'loss': 0.2742, 'grad_norm': 0.875766396522522, 'learning_rate': 5.479853479853481e-05, 'epoch': 4.56}
********************on step end call back********************
Step 25090 finish
{'loss': 0.2411, 'grad_norm': 1.0419588088989258, 'learning_rate': 5.478021978021979e-05, 'epoch': 4.56}
********************on step end call back********************
Step 25100 finish
{'loss': 0.3008, 'grad_norm': 1.1194180250167847, 'learning_rate': 5.4761904761904766e-05, 'epoch': 4.56}
{'eval_loss': 0.34577876329421997, 'eval_accuracy': 0.8958333333333334, 'eval_runtime': 128.794, 'eval_samples_per_second': 4.954, 'eval_steps_per_second': 4.954, 'epoch': 4.56}
********************save call back********************
********************on step end call back********************
Step 25110 finish
{'loss': 0.294, 'grad_norm': 1.2759002447128296, 'learning_rate': 5.474358974358975e-05, 'epoch': 4.56}
********************on step end call back********************
Step 25120 finish
{'loss': 0.2869, 'grad_norm': 1.2904831171035767, 'learning_rate': 5.472527472527473e-05, 'epoch': 4.57}
********************on step end call back********************
Step 25130 finish
{'loss': 0.2879, 'grad_norm': 1.342712163925171, 'learning_rate': 5.4706959706959716e-05, 'epoch': 4.57}
********************on step end call back********************
Step 25140 finish
{'loss': 0.296, 'grad_norm': 1.4264323711395264, 'learning_rate': 5.4688644688644695e-05, 'epoch': 4.57}
********************on step end call back********************
Step 25150 finish
{'loss': 0.2513, 'grad_norm': 1.3407570123672485, 'learning_rate': 5.467032967032967e-05, 'epoch': 4.57}
********************on step end call back********************
Step 25160 finish
{'loss': 0.2617, 'grad_norm': 1.2717325687408447, 'learning_rate': 5.465201465201465e-05, 'epoch': 4.57}
********************on step end call back********************
Step 25170 finish
{'loss': 0.2811, 'grad_norm': 1.2265832424163818, 'learning_rate': 5.463369963369963e-05, 'epoch': 4.58}
********************on step end call back********************
Step 25180 finish
{'loss': 0.2779, 'grad_norm': 1.0740753412246704, 'learning_rate': 5.461538461538461e-05, 'epoch': 4.58}
********************on step end call back********************
Step 25190 finish
{'loss': 0.2814, 'grad_norm': 1.047224998474121, 'learning_rate': 5.4597069597069596e-05, 'epoch': 4.58}
********************on step end call back********************
Step 25200 finish
{'loss': 0.2847, 'grad_norm': 1.2436732053756714, 'learning_rate': 5.4578754578754574e-05, 'epoch': 4.58}
{'eval_loss': 0.34746381640434265, 'eval_accuracy': 0.8958333333333334, 'eval_runtime': 128.8035, 'eval_samples_per_second': 4.953, 'eval_steps_per_second': 4.953, 'epoch': 4.58}
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-23 18:16:11,922 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-23 18:16:11,922 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-23 18:16:11,923 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-23 18:18:20,848 >> Saving model checkpoint to ./output/tmp-checkpoint-25300
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-23 18:18:20,978 >> tokenizer config file saved in ./output/tmp-checkpoint-25300/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-23 18:18:20,979 >> Special tokens file saved in ./output/tmp-checkpoint-25300/special_tokens_map.json
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-23 18:26:55,488 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-23 18:26:55,488 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-23 18:26:55,488 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-23 18:29:04,837 >> Saving model checkpoint to ./output/tmp-checkpoint-25400
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-23 18:29:05,004 >> tokenizer config file saved in ./output/tmp-checkpoint-25400/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-23 18:29:05,004 >> Special tokens file saved in ./output/tmp-checkpoint-25400/special_tokens_map.json
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-23 18:37:45,297 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-23 18:37:45,297 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-23 18:37:45,297 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-23 18:39:54,743 >> Saving model checkpoint to ./output/tmp-checkpoint-25500
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-23 18:39:54,902 >> tokenizer config file saved in ./output/tmp-checkpoint-25500/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-23 18:39:54,902 >> Special tokens file saved in ./output/tmp-checkpoint-25500/special_tokens_map.json
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-23 18:48:24,806 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-23 18:48:24,806 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-23 18:48:24,806 >>   Batch size = 1
********************save call back********************
********************on step end call back********************
Step 25210 finish
{'loss': 0.2814, 'grad_norm': 1.4785914421081543, 'learning_rate': 5.456043956043956e-05, 'epoch': 4.58}
********************on step end call back********************
Step 25220 finish
{'loss': 0.2622, 'grad_norm': 1.414868712425232, 'learning_rate': 5.454212454212454e-05, 'epoch': 4.58}
********************on step end call back********************
Step 25230 finish
{'loss': 0.267, 'grad_norm': 1.293440818786621, 'learning_rate': 5.4523809523809524e-05, 'epoch': 4.59}
********************on step end call back********************
Step 25240 finish
{'loss': 0.3134, 'grad_norm': 0.8972959518432617, 'learning_rate': 5.45054945054945e-05, 'epoch': 4.59}
********************on step end call back********************
Step 25250 finish
{'loss': 0.2494, 'grad_norm': 1.413000226020813, 'learning_rate': 5.448717948717948e-05, 'epoch': 4.59}
********************on step end call back********************
Step 25260 finish
{'loss': 0.2857, 'grad_norm': 1.1710628271102905, 'learning_rate': 5.446886446886447e-05, 'epoch': 4.59}
********************on step end call back********************
Step 25270 finish
{'loss': 0.2449, 'grad_norm': 1.2204616069793701, 'learning_rate': 5.4450549450549446e-05, 'epoch': 4.59}
********************on step end call back********************
Step 25280 finish
{'loss': 0.3047, 'grad_norm': 1.4348642826080322, 'learning_rate': 5.443223443223443e-05, 'epoch': 4.6}
********************on step end call back********************
Step 25290 finish
{'loss': 0.2988, 'grad_norm': 0.8555904030799866, 'learning_rate': 5.441391941391941e-05, 'epoch': 4.6}
********************on step end call back********************
Step 25300 finish
{'loss': 0.3048, 'grad_norm': 1.0488201379776, 'learning_rate': 5.4395604395604396e-05, 'epoch': 4.6}
{'eval_loss': 0.3471734821796417, 'eval_accuracy': 0.875, 'eval_runtime': 128.9242, 'eval_samples_per_second': 4.949, 'eval_steps_per_second': 4.949, 'epoch': 4.6}
********************save call back********************
********************on step end call back********************
Step 25310 finish
{'loss': 0.2633, 'grad_norm': 1.3059453964233398, 'learning_rate': 5.4377289377289375e-05, 'epoch': 4.6}
********************on step end call back********************
Step 25320 finish
{'loss': 0.2576, 'grad_norm': 1.2365366220474243, 'learning_rate': 5.435897435897436e-05, 'epoch': 4.6}
********************on step end call back********************
Step 25330 finish
{'loss': 0.2902, 'grad_norm': 1.5363950729370117, 'learning_rate': 5.434065934065934e-05, 'epoch': 4.6}
********************on step end call back********************
Step 25340 finish
{'loss': 0.2825, 'grad_norm': 1.3530782461166382, 'learning_rate': 5.432234432234432e-05, 'epoch': 4.61}
********************on step end call back********************
Step 25350 finish
{'loss': 0.2698, 'grad_norm': 0.9703320264816284, 'learning_rate': 5.43040293040293e-05, 'epoch': 4.61}
********************on step end call back********************
Step 25360 finish
{'loss': 0.2757, 'grad_norm': 1.1019763946533203, 'learning_rate': 5.428571428571428e-05, 'epoch': 4.61}
********************on step end call back********************
Step 25370 finish
{'loss': 0.2953, 'grad_norm': 1.5162817239761353, 'learning_rate': 5.426739926739927e-05, 'epoch': 4.61}
********************on step end call back********************
Step 25380 finish
{'loss': 0.2871, 'grad_norm': 1.3763049840927124, 'learning_rate': 5.4249084249084246e-05, 'epoch': 4.61}
********************on step end call back********************
Step 25390 finish
{'loss': 0.261, 'grad_norm': 1.1675200462341309, 'learning_rate': 5.423076923076923e-05, 'epoch': 4.62}
********************on step end call back********************
Step 25400 finish
{'loss': 0.2794, 'grad_norm': 1.486761450767517, 'learning_rate': 5.421245421245421e-05, 'epoch': 4.62}
{'eval_loss': 0.3515111207962036, 'eval_accuracy': 0.875, 'eval_runtime': 129.3481, 'eval_samples_per_second': 4.932, 'eval_steps_per_second': 4.932, 'epoch': 4.62}
********************save call back********************
********************on step end call back********************
Step 25410 finish
{'loss': 0.2993, 'grad_norm': 1.9098321199417114, 'learning_rate': 5.4194139194139196e-05, 'epoch': 4.62}
********************on step end call back********************
Step 25420 finish
{'loss': 0.3069, 'grad_norm': 1.3169970512390137, 'learning_rate': 5.4175824175824175e-05, 'epoch': 4.62}
********************on step end call back********************
Step 25430 finish
{'loss': 0.2934, 'grad_norm': 1.1389504671096802, 'learning_rate': 5.4157509157509154e-05, 'epoch': 4.62}
********************on step end call back********************
Step 25440 finish
{'loss': 0.301, 'grad_norm': 1.0771121978759766, 'learning_rate': 5.413919413919414e-05, 'epoch': 4.62}
********************on step end call back********************
Step 25450 finish
{'loss': 0.2545, 'grad_norm': 0.9499108791351318, 'learning_rate': 5.412087912087912e-05, 'epoch': 4.63}
********************on step end call back********************
Step 25460 finish
{'loss': 0.2842, 'grad_norm': 1.438464641571045, 'learning_rate': 5.41025641025641e-05, 'epoch': 4.63}
********************on step end call back********************
Step 25470 finish
{'loss': 0.3101, 'grad_norm': 1.1503112316131592, 'learning_rate': 5.408424908424908e-05, 'epoch': 4.63}
********************on step end call back********************
Step 25480 finish
{'loss': 0.2881, 'grad_norm': 1.3139371871948242, 'learning_rate': 5.406593406593407e-05, 'epoch': 4.63}
********************on step end call back********************
Step 25490 finish
{'loss': 0.2813, 'grad_norm': 1.2722727060317993, 'learning_rate': 5.4047619047619046e-05, 'epoch': 4.63}
********************on step end call back********************
Step 25500 finish
{'loss': 0.2821, 'grad_norm': 1.637800693511963, 'learning_rate': 5.402930402930403e-05, 'epoch': 4.64}
{'eval_loss': 0.3468533754348755, 'eval_accuracy': 0.8854166666666666, 'eval_runtime': 129.4454, 'eval_samples_per_second': 4.929, 'eval_steps_per_second': 4.929, 'epoch': 4.64}
********************save call back********************
********************on step end call back********************
Step 25510 finish
{'loss': 0.2683, 'grad_norm': 1.36625075340271, 'learning_rate': 5.401098901098901e-05, 'epoch': 4.64}
********************on step end call back********************
Step 25520 finish
{'loss': 0.3051, 'grad_norm': 1.2168564796447754, 'learning_rate': 5.399267399267399e-05, 'epoch': 4.64}
********************on step end call back********************
Step 25530 finish
{'loss': 0.2808, 'grad_norm': 1.1489683389663696, 'learning_rate': 5.3974358974358975e-05, 'epoch': 4.64}
********************on step end call back********************
Step 25540 finish
{'loss': 0.2801, 'grad_norm': 1.2489614486694336, 'learning_rate': 5.3956043956043954e-05, 'epoch': 4.64}
********************on step end call back********************
Step 25550 finish
{'loss': 0.2545, 'grad_norm': 1.843785047531128, 'learning_rate': 5.393772893772894e-05, 'epoch': 4.64}
********************on step end call back********************
Step 25560 finish
{'loss': 0.2559, 'grad_norm': 1.3667970895767212, 'learning_rate': 5.391941391941392e-05, 'epoch': 4.65}
********************on step end call back********************
Step 25570 finish
{'loss': 0.2722, 'grad_norm': 0.9587650299072266, 'learning_rate': 5.3901098901098904e-05, 'epoch': 4.65}
********************on step end call back********************
Step 25580 finish
{'loss': 0.3087, 'grad_norm': 1.23494553565979, 'learning_rate': 5.388278388278388e-05, 'epoch': 4.65}
********************on step end call back********************
Step 25590 finish
{'loss': 0.291, 'grad_norm': 0.9321296811103821, 'learning_rate': 5.386446886446886e-05, 'epoch': 4.65}
********************on step end call back********************
Step 25600 finish
{'loss': 0.2738, 'grad_norm': 0.9969858527183533, 'learning_rate': 5.384615384615385e-05, 'epoch': 4.65}
[INFO|trainer.py:3067] 2024-03-23 18:50:34,605 >> Saving model checkpoint to ./output/tmp-checkpoint-25600
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-23 18:50:34,749 >> tokenizer config file saved in ./output/tmp-checkpoint-25600/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-23 18:50:34,750 >> Special tokens file saved in ./output/tmp-checkpoint-25600/special_tokens_map.json
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-23 18:59:12,865 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-23 18:59:12,865 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-23 18:59:12,865 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-23 19:01:22,015 >> Saving model checkpoint to ./output/tmp-checkpoint-25700
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-23 19:01:22,174 >> tokenizer config file saved in ./output/tmp-checkpoint-25700/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-23 19:01:22,174 >> Special tokens file saved in ./output/tmp-checkpoint-25700/special_tokens_map.json
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-23 19:09:59,562 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-23 19:09:59,563 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-23 19:09:59,563 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-23 19:12:08,681 >> Saving model checkpoint to ./output/tmp-checkpoint-25800
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-23 19:12:08,842 >> tokenizer config file saved in ./output/tmp-checkpoint-25800/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-23 19:12:08,842 >> Special tokens file saved in ./output/tmp-checkpoint-25800/special_tokens_map.json
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-23 19:20:47,653 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-23 19:20:47,654 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-23 19:20:47,654 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-23 19:22:57,205 >> Saving model checkpoint to ./output/tmp-checkpoint-25900
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-23 19:22:57,372 >> tokenizer config file saved in ./output/tmp-checkpoint-25900/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-23 19:22:57,372 >> Special tokens file saved in ./output/tmp-checkpoint-25900/special_tokens_map.json
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
{'eval_loss': 0.34208711981773376, 'eval_accuracy': 0.875, 'eval_runtime': 129.7976, 'eval_samples_per_second': 4.915, 'eval_steps_per_second': 4.915, 'epoch': 4.65}
********************save call back********************
********************on step end call back********************
Step 25610 finish
{'loss': 0.26, 'grad_norm': 1.2439758777618408, 'learning_rate': 5.3827838827838825e-05, 'epoch': 4.66}
********************on step end call back********************
Step 25620 finish
{'loss': 0.2525, 'grad_norm': 1.3075617551803589, 'learning_rate': 5.380952380952381e-05, 'epoch': 4.66}
********************on step end call back********************
Step 25630 finish
{'loss': 0.2615, 'grad_norm': 1.3924410343170166, 'learning_rate': 5.379120879120879e-05, 'epoch': 4.66}
********************on step end call back********************
Step 25640 finish
{'loss': 0.263, 'grad_norm': 1.3538762331008911, 'learning_rate': 5.3772893772893775e-05, 'epoch': 4.66}
********************on step end call back********************
Step 25650 finish
{'loss': 0.2805, 'grad_norm': 1.2878317832946777, 'learning_rate': 5.3754578754578754e-05, 'epoch': 4.66}
********************on step end call back********************
Step 25660 finish
{'loss': 0.2899, 'grad_norm': 1.3814799785614014, 'learning_rate': 5.373626373626374e-05, 'epoch': 4.66}
********************on step end call back********************
Step 25670 finish
{'loss': 0.2573, 'grad_norm': 1.1801118850708008, 'learning_rate': 5.371794871794872e-05, 'epoch': 4.67}
********************on step end call back********************
Step 25680 finish
{'loss': 0.266, 'grad_norm': 1.4016505479812622, 'learning_rate': 5.36996336996337e-05, 'epoch': 4.67}
********************on step end call back********************
Step 25690 finish
{'loss': 0.269, 'grad_norm': 1.1313998699188232, 'learning_rate': 5.368131868131868e-05, 'epoch': 4.67}
********************on step end call back********************
Step 25700 finish
{'loss': 0.2871, 'grad_norm': 0.7873522639274597, 'learning_rate': 5.366300366300366e-05, 'epoch': 4.67}
{'eval_loss': 0.3450559377670288, 'eval_accuracy': 0.8958333333333334, 'eval_runtime': 129.1491, 'eval_samples_per_second': 4.94, 'eval_steps_per_second': 4.94, 'epoch': 4.67}
********************save call back********************
********************on step end call back********************
Step 25710 finish
{'loss': 0.2804, 'grad_norm': 1.1911351680755615, 'learning_rate': 5.364468864468865e-05, 'epoch': 4.67}
********************on step end call back********************
Step 25720 finish
{'loss': 0.2743, 'grad_norm': 1.5544558763504028, 'learning_rate': 5.3626373626373626e-05, 'epoch': 4.68}
********************on step end call back********************
Step 25730 finish
{'loss': 0.2993, 'grad_norm': 1.2088391780853271, 'learning_rate': 5.360805860805861e-05, 'epoch': 4.68}
********************on step end call back********************
Step 25740 finish
{'loss': 0.3068, 'grad_norm': 1.0185086727142334, 'learning_rate': 5.358974358974359e-05, 'epoch': 4.68}
********************on step end call back********************
Step 25750 finish
{'loss': 0.2786, 'grad_norm': 1.0642503499984741, 'learning_rate': 5.3571428571428575e-05, 'epoch': 4.68}
********************on step end call back********************
Step 25760 finish
{'loss': 0.2533, 'grad_norm': 1.233096718788147, 'learning_rate': 5.3553113553113554e-05, 'epoch': 4.68}
********************on step end call back********************
Step 25770 finish
{'loss': 0.2778, 'grad_norm': 0.9331755042076111, 'learning_rate': 5.353479853479853e-05, 'epoch': 4.68}
********************on step end call back********************
Step 25780 finish
{'loss': 0.2951, 'grad_norm': 1.0719891786575317, 'learning_rate': 5.351648351648352e-05, 'epoch': 4.69}
********************on step end call back********************
Step 25790 finish
{'loss': 0.274, 'grad_norm': 1.1551016569137573, 'learning_rate': 5.34981684981685e-05, 'epoch': 4.69}
********************on step end call back********************
Step 25800 finish
{'loss': 0.3072, 'grad_norm': 1.3372523784637451, 'learning_rate': 5.347985347985348e-05, 'epoch': 4.69}
{'eval_loss': 0.3425942063331604, 'eval_accuracy': 0.8854166666666666, 'eval_runtime': 129.1175, 'eval_samples_per_second': 4.941, 'eval_steps_per_second': 4.941, 'epoch': 4.69}
********************save call back********************
********************on step end call back********************
Step 25810 finish
{'loss': 0.2854, 'grad_norm': 1.4731619358062744, 'learning_rate': 5.346153846153846e-05, 'epoch': 4.69}
********************on step end call back********************
Step 25820 finish
{'loss': 0.2972, 'grad_norm': 1.1739819049835205, 'learning_rate': 5.344322344322345e-05, 'epoch': 4.69}
********************on step end call back********************
Step 25830 finish
{'loss': 0.2608, 'grad_norm': 1.1382461786270142, 'learning_rate': 5.3424908424908426e-05, 'epoch': 4.7}
********************on step end call back********************
Step 25840 finish
{'loss': 0.2774, 'grad_norm': 1.5002739429473877, 'learning_rate': 5.340659340659341e-05, 'epoch': 4.7}
********************on step end call back********************
Step 25850 finish
{'loss': 0.2974, 'grad_norm': 1.3467340469360352, 'learning_rate': 5.338827838827839e-05, 'epoch': 4.7}
********************on step end call back********************
Step 25860 finish
{'loss': 0.2936, 'grad_norm': 1.198541283607483, 'learning_rate': 5.336996336996337e-05, 'epoch': 4.7}
********************on step end call back********************
Step 25870 finish
{'loss': 0.2615, 'grad_norm': 1.0701502561569214, 'learning_rate': 5.3351648351648354e-05, 'epoch': 4.7}
********************on step end call back********************
Step 25880 finish
{'loss': 0.298, 'grad_norm': 1.2814714908599854, 'learning_rate': 5.333333333333333e-05, 'epoch': 4.7}
********************on step end call back********************
Step 25890 finish
{'loss': 0.3025, 'grad_norm': 1.4389983415603638, 'learning_rate': 5.331501831501832e-05, 'epoch': 4.71}
********************on step end call back********************
Step 25900 finish
{'loss': 0.2861, 'grad_norm': 0.9496966004371643, 'learning_rate': 5.32967032967033e-05, 'epoch': 4.71}
{'eval_loss': 0.3414683938026428, 'eval_accuracy': 0.8854166666666666, 'eval_runtime': 129.55, 'eval_samples_per_second': 4.925, 'eval_steps_per_second': 4.925, 'epoch': 4.71}
********************save call back********************
********************on step end call back********************
Step 25910 finish
{'loss': 0.2869, 'grad_norm': 1.2612205743789673, 'learning_rate': 5.327838827838828e-05, 'epoch': 4.71}
********************on step end call back********************
Step 25920 finish
{'loss': 0.3109, 'grad_norm': 1.1025429964065552, 'learning_rate': 5.326007326007326e-05, 'epoch': 4.71}
********************on step end call back********************
Step 25930 finish
{'loss': 0.2454, 'grad_norm': 1.355794906616211, 'learning_rate': 5.324175824175824e-05, 'epoch': 4.71}
********************on step end call back********************
Step 25940 finish
{'loss': 0.275, 'grad_norm': 1.0583083629608154, 'learning_rate': 5.3223443223443226e-05, 'epoch': 4.72}
********************on step end call back********************
Step 25950 finish
{'loss': 0.2564, 'grad_norm': 1.2189464569091797, 'learning_rate': 5.3205128205128205e-05, 'epoch': 4.72}
********************on step end call back********************
Step 25960 finish
{'loss': 0.2481, 'grad_norm': 1.2808315753936768, 'learning_rate': 5.318681318681319e-05, 'epoch': 4.72}
********************on step end call back********************
Step 25970 finish
{'loss': 0.2737, 'grad_norm': 1.1766198873519897, 'learning_rate': 5.316849816849817e-05, 'epoch': 4.72}
********************on step end call back********************
Step 25980 finish
{'loss': 0.3037, 'grad_norm': 1.388166069984436, 'learning_rate': 5.3150183150183155e-05, 'epoch': 4.72}
********************on step end call back********************
Step 25990 finish
{'loss': 0.2501, 'grad_norm': 1.4742019176483154, 'learning_rate': 5.3131868131868133e-05, 'epoch': 4.72}
********************on step end call back********************
[INFO|trainer.py:3376] 2024-03-23 19:31:35,414 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-23 19:31:35,414 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-23 19:31:35,414 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-23 19:33:44,733 >> Saving model checkpoint to ./output/tmp-checkpoint-26000
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-23 19:33:44,896 >> tokenizer config file saved in ./output/tmp-checkpoint-26000/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-23 19:33:44,896 >> Special tokens file saved in ./output/tmp-checkpoint-26000/special_tokens_map.json
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-23 19:42:30,862 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-23 19:42:30,862 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-23 19:42:30,862 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-23 19:44:39,995 >> Saving model checkpoint to ./output/tmp-checkpoint-26100
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-23 19:44:40,141 >> tokenizer config file saved in ./output/tmp-checkpoint-26100/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-23 19:44:40,141 >> Special tokens file saved in ./output/tmp-checkpoint-26100/special_tokens_map.json
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-23 19:53:13,637 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-23 19:53:13,637 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-23 19:53:13,637 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-23 19:55:23,162 >> Saving model checkpoint to ./output/tmp-checkpoint-26200
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-23 19:55:23,329 >> tokenizer config file saved in ./output/tmp-checkpoint-26200/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-23 19:55:23,329 >> Special tokens file saved in ./output/tmp-checkpoint-26200/special_tokens_map.json
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-23 20:03:55,658 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-23 20:03:55,658 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-23 20:03:55,658 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-23 20:06:05,564 >> Saving model checkpoint to ./output/tmp-checkpoint-26300
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-23 20:06:05,733 >> tokenizer config file saved in ./output/tmp-checkpoint-26300/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-23 20:06:05,733 >> Special tokens file saved in ./output/tmp-checkpoint-26300/special_tokens_map.json
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
Step 26000 finish
{'loss': 0.274, 'grad_norm': 1.2673676013946533, 'learning_rate': 5.311355311355312e-05, 'epoch': 4.73}
{'eval_loss': 0.3439432978630066, 'eval_accuracy': 0.8854166666666666, 'eval_runtime': 129.3179, 'eval_samples_per_second': 4.934, 'eval_steps_per_second': 4.934, 'epoch': 4.73}
********************save call back********************
********************on step end call back********************
Step 26010 finish
{'loss': 0.2887, 'grad_norm': 1.7857749462127686, 'learning_rate': 5.30952380952381e-05, 'epoch': 4.73}
********************on step end call back********************
Step 26020 finish
{'loss': 0.2935, 'grad_norm': 1.2489956617355347, 'learning_rate': 5.3076923076923076e-05, 'epoch': 4.73}
********************on step end call back********************
Step 26030 finish
{'loss': 0.2738, 'grad_norm': 1.1538530588150024, 'learning_rate': 5.305860805860806e-05, 'epoch': 4.73}
********************on step end call back********************
Step 26040 finish
{'loss': 0.284, 'grad_norm': 1.6891286373138428, 'learning_rate': 5.304029304029304e-05, 'epoch': 4.73}
********************on step end call back********************
Step 26050 finish
{'loss': 0.2864, 'grad_norm': 1.6275936365127563, 'learning_rate': 5.3021978021978026e-05, 'epoch': 4.74}
********************on step end call back********************
Step 26060 finish
{'loss': 0.2869, 'grad_norm': 1.0858875513076782, 'learning_rate': 5.3003663003663005e-05, 'epoch': 4.74}
********************on step end call back********************
Step 26070 finish
{'loss': 0.2918, 'grad_norm': 1.1966995000839233, 'learning_rate': 5.298534798534799e-05, 'epoch': 4.74}
********************on step end call back********************
Step 26080 finish
{'loss': 0.2772, 'grad_norm': 1.0745350122451782, 'learning_rate': 5.296703296703297e-05, 'epoch': 4.74}
********************on step end call back********************
Step 26090 finish
{'loss': 0.305, 'grad_norm': 1.2249796390533447, 'learning_rate': 5.2948717948717955e-05, 'epoch': 4.74}
********************on step end call back********************
Step 26100 finish
{'loss': 0.2812, 'grad_norm': 1.4985460042953491, 'learning_rate': 5.2930402930402934e-05, 'epoch': 4.74}
{'eval_loss': 0.3466652035713196, 'eval_accuracy': 0.8854166666666666, 'eval_runtime': 129.1317, 'eval_samples_per_second': 4.941, 'eval_steps_per_second': 4.941, 'epoch': 4.74}
********************save call back********************
********************on step end call back********************
Step 26110 finish
{'loss': 0.2785, 'grad_norm': 1.3297573328018188, 'learning_rate': 5.291208791208791e-05, 'epoch': 4.75}
********************on step end call back********************
Step 26120 finish
{'loss': 0.3004, 'grad_norm': 0.9008048176765442, 'learning_rate': 5.28937728937729e-05, 'epoch': 4.75}
********************on step end call back********************
Step 26130 finish
{'loss': 0.2702, 'grad_norm': 1.4243875741958618, 'learning_rate': 5.287545787545788e-05, 'epoch': 4.75}
********************on step end call back********************
Step 26140 finish
{'loss': 0.2699, 'grad_norm': 1.193251132965088, 'learning_rate': 5.285714285714286e-05, 'epoch': 4.75}
********************on step end call back********************
Step 26150 finish
{'loss': 0.2849, 'grad_norm': 1.219569444656372, 'learning_rate': 5.283882783882784e-05, 'epoch': 4.75}
********************on step end call back********************
Step 26160 finish
{'loss': 0.2602, 'grad_norm': 1.3142458200454712, 'learning_rate': 5.2820512820512826e-05, 'epoch': 4.76}
********************on step end call back********************
Step 26170 finish
{'loss': 0.2858, 'grad_norm': 1.5063198804855347, 'learning_rate': 5.2802197802197805e-05, 'epoch': 4.76}
********************on step end call back********************
Step 26180 finish
{'loss': 0.2755, 'grad_norm': 0.9790242314338684, 'learning_rate': 5.278388278388279e-05, 'epoch': 4.76}
********************on step end call back********************
Step 26190 finish
{'loss': 0.2615, 'grad_norm': 1.2000651359558105, 'learning_rate': 5.276556776556777e-05, 'epoch': 4.76}
********************on step end call back********************
Step 26200 finish
{'loss': 0.2667, 'grad_norm': 1.2121987342834473, 'learning_rate': 5.274725274725275e-05, 'epoch': 4.76}
{'eval_loss': 0.35034802556037903, 'eval_accuracy': 0.8958333333333334, 'eval_runtime': 129.5242, 'eval_samples_per_second': 4.926, 'eval_steps_per_second': 4.926, 'epoch': 4.76}
********************save call back********************
********************on step end call back********************
Step 26210 finish
{'loss': 0.2383, 'grad_norm': 1.18059504032135, 'learning_rate': 5.2728937728937734e-05, 'epoch': 4.76}
********************on step end call back********************
Step 26220 finish
{'loss': 0.2955, 'grad_norm': 1.1837230920791626, 'learning_rate': 5.271062271062271e-05, 'epoch': 4.77}
********************on step end call back********************
Step 26230 finish
{'loss': 0.2738, 'grad_norm': 1.1736422777175903, 'learning_rate': 5.26923076923077e-05, 'epoch': 4.77}
********************on step end call back********************
Step 26240 finish
{'loss': 0.3077, 'grad_norm': 1.6062555313110352, 'learning_rate': 5.267399267399268e-05, 'epoch': 4.77}
********************on step end call back********************
Step 26250 finish
{'loss': 0.2775, 'grad_norm': 1.4595226049423218, 'learning_rate': 5.265567765567766e-05, 'epoch': 4.77}
********************on step end call back********************
Step 26260 finish
{'loss': 0.2871, 'grad_norm': 1.162233591079712, 'learning_rate': 5.263736263736264e-05, 'epoch': 4.77}
********************on step end call back********************
Step 26270 finish
{'loss': 0.2907, 'grad_norm': 0.8932403922080994, 'learning_rate': 5.261904761904763e-05, 'epoch': 4.78}
********************on step end call back********************
Step 26280 finish
{'loss': 0.283, 'grad_norm': 1.6753937005996704, 'learning_rate': 5.2600732600732605e-05, 'epoch': 4.78}
********************on step end call back********************
Step 26290 finish
{'loss': 0.2879, 'grad_norm': 0.9781455397605896, 'learning_rate': 5.2582417582417584e-05, 'epoch': 4.78}
********************on step end call back********************
Step 26300 finish
{'loss': 0.2792, 'grad_norm': 2.0485291481018066, 'learning_rate': 5.256410256410257e-05, 'epoch': 4.78}
{'eval_loss': 0.3461058735847473, 'eval_accuracy': 0.875, 'eval_runtime': 129.9043, 'eval_samples_per_second': 4.911, 'eval_steps_per_second': 4.911, 'epoch': 4.78}
********************save call back********************
********************on step end call back********************
Step 26310 finish
{'loss': 0.2861, 'grad_norm': 1.3906850814819336, 'learning_rate': 5.254578754578755e-05, 'epoch': 4.78}
********************on step end call back********************
Step 26320 finish
{'loss': 0.272, 'grad_norm': 1.55638587474823, 'learning_rate': 5.2527472527472534e-05, 'epoch': 4.78}
********************on step end call back********************
Step 26330 finish
{'loss': 0.2945, 'grad_norm': 1.0920743942260742, 'learning_rate': 5.250915750915751e-05, 'epoch': 4.79}
********************on step end call back********************
Step 26340 finish
{'loss': 0.253, 'grad_norm': 1.217858076095581, 'learning_rate': 5.24908424908425e-05, 'epoch': 4.79}
********************on step end call back********************
Step 26350 finish
{'loss': 0.2587, 'grad_norm': 0.7456172108650208, 'learning_rate': 5.247252747252748e-05, 'epoch': 4.79}
********************on step end call back********************
Step 26360 finish
{'loss': 0.2581, 'grad_norm': 1.186957836151123, 'learning_rate': 5.2454212454212456e-05, 'epoch': 4.79}
********************on step end call back********************
Step 26370 finish
{'loss': 0.3368, 'grad_norm': 1.1963191032409668, 'learning_rate': 5.243589743589744e-05, 'epoch': 4.79}
********************on step end call back********************
Step 26380 finish
{'loss': 0.2775, 'grad_norm': 1.2156058549880981, 'learning_rate': 5.241758241758242e-05, 'epoch': 4.8}
********************on step end call back********************
Step 26390 finish
[INFO|trainer.py:3376] 2024-03-23 20:14:40,424 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-23 20:14:40,425 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-23 20:14:40,425 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-23 20:16:49,863 >> Saving model checkpoint to ./output/tmp-checkpoint-26400
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-23 20:16:50,030 >> tokenizer config file saved in ./output/tmp-checkpoint-26400/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-23 20:16:50,030 >> Special tokens file saved in ./output/tmp-checkpoint-26400/special_tokens_map.json
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-23 20:25:28,905 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-23 20:25:28,905 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-23 20:25:28,905 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-23 20:27:38,148 >> Saving model checkpoint to ./output/tmp-checkpoint-26500
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-23 20:27:38,315 >> tokenizer config file saved in ./output/tmp-checkpoint-26500/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-23 20:27:38,315 >> Special tokens file saved in ./output/tmp-checkpoint-26500/special_tokens_map.json
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-23 20:36:19,023 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-23 20:36:19,023 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-23 20:36:19,023 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-23 20:38:28,278 >> Saving model checkpoint to ./output/tmp-checkpoint-26600
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-23 20:38:28,791 >> tokenizer config file saved in ./output/tmp-checkpoint-26600/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-23 20:38:28,791 >> Special tokens file saved in ./output/tmp-checkpoint-26600/special_tokens_map.json
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-23 20:47:07,070 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-23 20:47:07,070 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-23 20:47:07,070 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-23 20:49:16,356 >> Saving model checkpoint to ./output/tmp-checkpoint-26700
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-23 20:49:16,502 >> tokenizer config file saved in ./output/tmp-checkpoint-26700/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-23 20:49:16,502 >> Special tokens file saved in ./output/tmp-checkpoint-26700/special_tokens_map.json
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
{'loss': 0.2869, 'grad_norm': 1.2643824815750122, 'learning_rate': 5.2399267399267406e-05, 'epoch': 4.8}
********************on step end call back********************
Step 26400 finish
{'loss': 0.2774, 'grad_norm': 1.1643861532211304, 'learning_rate': 5.2380952380952384e-05, 'epoch': 4.8}
{'eval_loss': 0.344820499420166, 'eval_accuracy': 0.8854166666666666, 'eval_runtime': 129.4375, 'eval_samples_per_second': 4.929, 'eval_steps_per_second': 4.929, 'epoch': 4.8}
********************save call back********************
********************on step end call back********************
Step 26410 finish
{'loss': 0.2349, 'grad_norm': 1.0387380123138428, 'learning_rate': 5.236263736263737e-05, 'epoch': 4.8}
********************on step end call back********************
Step 26420 finish
{'loss': 0.2498, 'grad_norm': 1.0794461965560913, 'learning_rate': 5.234432234432235e-05, 'epoch': 4.8}
********************on step end call back********************
Step 26430 finish
{'loss': 0.3004, 'grad_norm': 1.0015013217926025, 'learning_rate': 5.2326007326007334e-05, 'epoch': 4.8}
********************on step end call back********************
Step 26440 finish
{'loss': 0.3164, 'grad_norm': 1.070457935333252, 'learning_rate': 5.230769230769231e-05, 'epoch': 4.81}
********************on step end call back********************
Step 26450 finish
{'loss': 0.2704, 'grad_norm': 1.4580323696136475, 'learning_rate': 5.228937728937729e-05, 'epoch': 4.81}
********************on step end call back********************
Step 26460 finish
{'loss': 0.3168, 'grad_norm': 1.217828392982483, 'learning_rate': 5.227106227106228e-05, 'epoch': 4.81}
********************on step end call back********************
Step 26470 finish
{'loss': 0.3295, 'grad_norm': 1.2718408107757568, 'learning_rate': 5.2252747252747256e-05, 'epoch': 4.81}
********************on step end call back********************
Step 26480 finish
{'loss': 0.2866, 'grad_norm': 1.1968721151351929, 'learning_rate': 5.223443223443224e-05, 'epoch': 4.81}
********************on step end call back********************
Step 26490 finish
{'loss': 0.2691, 'grad_norm': 1.0089986324310303, 'learning_rate': 5.221611721611722e-05, 'epoch': 4.82}
********************on step end call back********************
Step 26500 finish
{'loss': 0.2622, 'grad_norm': 1.228285789489746, 'learning_rate': 5.2197802197802206e-05, 'epoch': 4.82}
{'eval_loss': 0.3437071442604065, 'eval_accuracy': 0.875, 'eval_runtime': 129.2414, 'eval_samples_per_second': 4.936, 'eval_steps_per_second': 4.936, 'epoch': 4.82}
********************save call back********************
********************on step end call back********************
Step 26510 finish
{'loss': 0.2749, 'grad_norm': 1.489958643913269, 'learning_rate': 5.2179487179487185e-05, 'epoch': 4.82}
********************on step end call back********************
Step 26520 finish
{'loss': 0.3148, 'grad_norm': 1.1724241971969604, 'learning_rate': 5.216117216117217e-05, 'epoch': 4.82}
********************on step end call back********************
Step 26530 finish
{'loss': 0.2973, 'grad_norm': 1.253669023513794, 'learning_rate': 5.214285714285715e-05, 'epoch': 4.82}
********************on step end call back********************
Step 26540 finish
{'loss': 0.2968, 'grad_norm': 1.091474175453186, 'learning_rate': 5.212454212454213e-05, 'epoch': 4.82}
********************on step end call back********************
Step 26550 finish
{'loss': 0.2837, 'grad_norm': 1.3807119131088257, 'learning_rate': 5.210622710622711e-05, 'epoch': 4.83}
********************on step end call back********************
Step 26560 finish
{'loss': 0.2819, 'grad_norm': 1.3310173749923706, 'learning_rate': 5.208791208791209e-05, 'epoch': 4.83}
********************on step end call back********************
Step 26570 finish
{'loss': 0.2736, 'grad_norm': 0.8028648495674133, 'learning_rate': 5.206959706959708e-05, 'epoch': 4.83}
********************on step end call back********************
Step 26580 finish
{'loss': 0.2857, 'grad_norm': 1.1962451934814453, 'learning_rate': 5.2051282051282056e-05, 'epoch': 4.83}
********************on step end call back********************
Step 26590 finish
{'loss': 0.2989, 'grad_norm': 1.565028190612793, 'learning_rate': 5.203296703296704e-05, 'epoch': 4.83}
********************on step end call back********************
Step 26600 finish
{'loss': 0.2906, 'grad_norm': 1.182896614074707, 'learning_rate': 5.201465201465202e-05, 'epoch': 4.84}
{'eval_loss': 0.34853026270866394, 'eval_accuracy': 0.8854166666666666, 'eval_runtime': 129.254, 'eval_samples_per_second': 4.936, 'eval_steps_per_second': 4.936, 'epoch': 4.84}
********************save call back********************
********************on step end call back********************
Step 26610 finish
{'loss': 0.2636, 'grad_norm': 1.2020434141159058, 'learning_rate': 5.1996336996337006e-05, 'epoch': 4.84}
********************on step end call back********************
Step 26620 finish
{'loss': 0.3012, 'grad_norm': 0.9284949898719788, 'learning_rate': 5.1978021978021985e-05, 'epoch': 4.84}
********************on step end call back********************
Step 26630 finish
{'loss': 0.2885, 'grad_norm': 1.2881251573562622, 'learning_rate': 5.1959706959706964e-05, 'epoch': 4.84}
********************on step end call back********************
Step 26640 finish
{'loss': 0.2677, 'grad_norm': 0.972533643245697, 'learning_rate': 5.194139194139195e-05, 'epoch': 4.84}
********************on step end call back********************
Step 26650 finish
{'loss': 0.2676, 'grad_norm': 1.5825568437576294, 'learning_rate': 5.192307692307693e-05, 'epoch': 4.84}
********************on step end call back********************
Step 26660 finish
{'loss': 0.3256, 'grad_norm': 1.3285913467407227, 'learning_rate': 5.1904761904761913e-05, 'epoch': 4.85}
********************on step end call back********************
Step 26670 finish
{'loss': 0.2733, 'grad_norm': 1.1469773054122925, 'learning_rate': 5.188644688644689e-05, 'epoch': 4.85}
********************on step end call back********************
Step 26680 finish
{'loss': 0.2846, 'grad_norm': 0.9702718257904053, 'learning_rate': 5.186813186813188e-05, 'epoch': 4.85}
********************on step end call back********************
Step 26690 finish
{'loss': 0.2621, 'grad_norm': 0.9324048161506653, 'learning_rate': 5.1849816849816856e-05, 'epoch': 4.85}
********************on step end call back********************
Step 26700 finish
{'loss': 0.2879, 'grad_norm': 1.0616402626037598, 'learning_rate': 5.1831501831501835e-05, 'epoch': 4.85}
{'eval_loss': 0.34532418847084045, 'eval_accuracy': 0.90625, 'eval_runtime': 129.2844, 'eval_samples_per_second': 4.935, 'eval_steps_per_second': 4.935, 'epoch': 4.85}
********************save call back********************
********************on step end call back********************
Step 26710 finish
{'loss': 0.2586, 'grad_norm': 1.2560036182403564, 'learning_rate': 5.181318681318682e-05, 'epoch': 4.86}
********************on step end call back********************
Step 26720 finish
{'loss': 0.2808, 'grad_norm': 1.3345202207565308, 'learning_rate': 5.17948717948718e-05, 'epoch': 4.86}
********************on step end call back********************
Step 26730 finish
{'loss': 0.3034, 'grad_norm': 1.6282002925872803, 'learning_rate': 5.1776556776556785e-05, 'epoch': 4.86}
********************on step end call back********************
Step 26740 finish
{'loss': 0.2667, 'grad_norm': 1.3570767641067505, 'learning_rate': 5.1758241758241764e-05, 'epoch': 4.86}
********************on step end call back********************
Step 26750 finish
{'loss': 0.3143, 'grad_norm': 1.4610354900360107, 'learning_rate': 5.173992673992675e-05, 'epoch': 4.86}
********************on step end call back********************
Step 26760 finish
{'loss': 0.284, 'grad_norm': 0.9703906178474426, 'learning_rate': 5.172161172161173e-05, 'epoch': 4.86}
********************on step end call back********************
Step 26770 finish
{'loss': 0.2982, 'grad_norm': 1.2037768363952637, 'learning_rate': 5.1703296703296714e-05, 'epoch': 4.87}
********************on step end call back********************
Step 26780 finish
[INFO|trainer.py:3376] 2024-03-23 20:57:54,809 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-23 20:57:54,810 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-23 20:57:54,810 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-23 21:00:03,831 >> Saving model checkpoint to ./output/tmp-checkpoint-26800
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-23 21:00:03,972 >> tokenizer config file saved in ./output/tmp-checkpoint-26800/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-23 21:00:03,973 >> Special tokens file saved in ./output/tmp-checkpoint-26800/special_tokens_map.json
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-23 21:08:49,101 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-23 21:08:49,101 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-23 21:08:49,101 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-23 21:10:58,238 >> Saving model checkpoint to ./output/tmp-checkpoint-26900
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-23 21:10:58,378 >> tokenizer config file saved in ./output/tmp-checkpoint-26900/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-23 21:10:58,379 >> Special tokens file saved in ./output/tmp-checkpoint-26900/special_tokens_map.json
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-23 21:19:33,027 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-23 21:19:33,027 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-23 21:19:33,027 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-23 21:21:42,335 >> Saving model checkpoint to ./output/tmp-checkpoint-27000
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-23 21:21:42,480 >> tokenizer config file saved in ./output/tmp-checkpoint-27000/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-23 21:21:42,480 >> Special tokens file saved in ./output/tmp-checkpoint-27000/special_tokens_map.json
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-23 21:30:24,784 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-23 21:30:24,784 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-23 21:30:24,784 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-23 21:32:33,724 >> Saving model checkpoint to ./output/tmp-checkpoint-27100
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-23 21:32:33,869 >> tokenizer config file saved in ./output/tmp-checkpoint-27100/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-23 21:32:33,869 >> Special tokens file saved in ./output/tmp-checkpoint-27100/special_tokens_map.json
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
{'loss': 0.2806, 'grad_norm': 1.4361474514007568, 'learning_rate': 5.168498168498169e-05, 'epoch': 4.87}
********************on step end call back********************
Step 26790 finish
{'loss': 0.2501, 'grad_norm': 1.2725635766983032, 'learning_rate': 5.166666666666667e-05, 'epoch': 4.87}
********************on step end call back********************
Step 26800 finish
{'loss': 0.2781, 'grad_norm': 1.364764928817749, 'learning_rate': 5.164835164835166e-05, 'epoch': 4.87}
{'eval_loss': 0.34532418847084045, 'eval_accuracy': 0.875, 'eval_runtime': 129.0208, 'eval_samples_per_second': 4.945, 'eval_steps_per_second': 4.945, 'epoch': 4.87}
********************save call back********************
********************on step end call back********************
Step 26810 finish
{'loss': 0.2892, 'grad_norm': 0.7482876181602478, 'learning_rate': 5.1630036630036635e-05, 'epoch': 4.87}
********************on step end call back********************
Step 26820 finish
{'loss': 0.2799, 'grad_norm': 1.275895357131958, 'learning_rate': 5.161172161172162e-05, 'epoch': 4.88}
********************on step end call back********************
Step 26830 finish
{'loss': 0.2774, 'grad_norm': 1.433228611946106, 'learning_rate': 5.15934065934066e-05, 'epoch': 4.88}
********************on step end call back********************
Step 26840 finish
{'loss': 0.2872, 'grad_norm': 1.1161528825759888, 'learning_rate': 5.1575091575091585e-05, 'epoch': 4.88}
********************on step end call back********************
Step 26850 finish
{'loss': 0.2769, 'grad_norm': 1.4796562194824219, 'learning_rate': 5.155677655677655e-05, 'epoch': 4.88}
********************on step end call back********************
Step 26860 finish
{'loss': 0.2643, 'grad_norm': 1.3443920612335205, 'learning_rate': 5.1538461538461536e-05, 'epoch': 4.88}
********************on step end call back********************
Step 26870 finish
{'loss': 0.2875, 'grad_norm': 1.3017752170562744, 'learning_rate': 5.1520146520146515e-05, 'epoch': 4.88}
********************on step end call back********************
Step 26880 finish
{'loss': 0.3209, 'grad_norm': 1.1143834590911865, 'learning_rate': 5.15018315018315e-05, 'epoch': 4.89}
********************on step end call back********************
Step 26890 finish
{'loss': 0.2955, 'grad_norm': 0.9483789205551147, 'learning_rate': 5.148351648351648e-05, 'epoch': 4.89}
********************on step end call back********************
Step 26900 finish
{'loss': 0.2804, 'grad_norm': 1.1464987993240356, 'learning_rate': 5.1465201465201465e-05, 'epoch': 4.89}
{'eval_loss': 0.3464035391807556, 'eval_accuracy': 0.8854166666666666, 'eval_runtime': 129.1362, 'eval_samples_per_second': 4.941, 'eval_steps_per_second': 4.941, 'epoch': 4.89}
********************save call back********************
********************on step end call back********************
Step 26910 finish
{'loss': 0.2849, 'grad_norm': 1.201849102973938, 'learning_rate': 5.144688644688644e-05, 'epoch': 4.89}
********************on step end call back********************
Step 26920 finish
{'loss': 0.2669, 'grad_norm': 1.4067445993423462, 'learning_rate': 5.142857142857143e-05, 'epoch': 4.89}
********************on step end call back********************
Step 26930 finish
{'loss': 0.2454, 'grad_norm': 1.183409333229065, 'learning_rate': 5.141025641025641e-05, 'epoch': 4.9}
********************on step end call back********************
Step 26940 finish
{'loss': 0.251, 'grad_norm': 1.1581214666366577, 'learning_rate': 5.1391941391941386e-05, 'epoch': 4.9}
********************on step end call back********************
Step 26950 finish
{'loss': 0.3024, 'grad_norm': 1.2215116024017334, 'learning_rate': 5.137362637362637e-05, 'epoch': 4.9}
********************on step end call back********************
Step 26960 finish
{'loss': 0.2965, 'grad_norm': 1.1130198240280151, 'learning_rate': 5.135531135531135e-05, 'epoch': 4.9}
********************on step end call back********************
Step 26970 finish
{'loss': 0.3059, 'grad_norm': 1.1513760089874268, 'learning_rate': 5.1336996336996336e-05, 'epoch': 4.9}
********************on step end call back********************
Step 26980 finish
{'loss': 0.2769, 'grad_norm': 1.056799292564392, 'learning_rate': 5.1318681318681315e-05, 'epoch': 4.9}
********************on step end call back********************
Step 26990 finish
{'loss': 0.3056, 'grad_norm': 1.073358416557312, 'learning_rate': 5.13003663003663e-05, 'epoch': 4.91}
********************on step end call back********************
Step 27000 finish
{'loss': 0.3137, 'grad_norm': 1.272546410560608, 'learning_rate': 5.128205128205128e-05, 'epoch': 4.91}
{'eval_loss': 0.34654179215431213, 'eval_accuracy': 0.875, 'eval_runtime': 129.3069, 'eval_samples_per_second': 4.934, 'eval_steps_per_second': 4.934, 'epoch': 4.91}
********************save call back********************
********************on step end call back********************
Step 27010 finish
{'loss': 0.2804, 'grad_norm': 1.2928762435913086, 'learning_rate': 5.1263736263736265e-05, 'epoch': 4.91}
********************on step end call back********************
Step 27020 finish
{'loss': 0.2887, 'grad_norm': 1.3371747732162476, 'learning_rate': 5.1245421245421244e-05, 'epoch': 4.91}
********************on step end call back********************
Step 27030 finish
{'loss': 0.2562, 'grad_norm': 1.0636470317840576, 'learning_rate': 5.122710622710622e-05, 'epoch': 4.91}
********************on step end call back********************
Step 27040 finish
{'loss': 0.2961, 'grad_norm': 1.1573766469955444, 'learning_rate': 5.120879120879121e-05, 'epoch': 4.92}
********************on step end call back********************
Step 27050 finish
{'loss': 0.3116, 'grad_norm': 1.1894006729125977, 'learning_rate': 5.119047619047619e-05, 'epoch': 4.92}
********************on step end call back********************
Step 27060 finish
{'loss': 0.2662, 'grad_norm': 1.416553020477295, 'learning_rate': 5.117216117216117e-05, 'epoch': 4.92}
********************on step end call back********************
Step 27070 finish
{'loss': 0.3216, 'grad_norm': 1.2483223676681519, 'learning_rate': 5.115384615384615e-05, 'epoch': 4.92}
********************on step end call back********************
Step 27080 finish
{'loss': 0.2654, 'grad_norm': 1.448269009590149, 'learning_rate': 5.1135531135531136e-05, 'epoch': 4.92}
********************on step end call back********************
Step 27090 finish
{'loss': 0.2917, 'grad_norm': 1.4238595962524414, 'learning_rate': 5.1117216117216115e-05, 'epoch': 4.92}
********************on step end call back********************
Step 27100 finish
{'loss': 0.2873, 'grad_norm': 0.9699898958206177, 'learning_rate': 5.10989010989011e-05, 'epoch': 4.93}
{'eval_loss': 0.3392203748226166, 'eval_accuracy': 0.8958333333333334, 'eval_runtime': 128.9393, 'eval_samples_per_second': 4.948, 'eval_steps_per_second': 4.948, 'epoch': 4.93}
********************save call back********************
********************on step end call back********************
Step 27110 finish
{'loss': 0.2993, 'grad_norm': 1.0930132865905762, 'learning_rate': 5.108058608058608e-05, 'epoch': 4.93}
********************on step end call back********************
Step 27120 finish
{'loss': 0.3056, 'grad_norm': 1.0686012506484985, 'learning_rate': 5.106227106227106e-05, 'epoch': 4.93}
********************on step end call back********************
Step 27130 finish
{'loss': 0.2632, 'grad_norm': 0.9798325300216675, 'learning_rate': 5.1043956043956044e-05, 'epoch': 4.93}
********************on step end call back********************
Step 27140 finish
{'loss': 0.2908, 'grad_norm': 1.2929943799972534, 'learning_rate': 5.102564102564102e-05, 'epoch': 4.93}
********************on step end call back********************
Step 27150 finish
{'loss': 0.2881, 'grad_norm': 1.308536410331726, 'learning_rate': 5.100732600732601e-05, 'epoch': 4.94}
********************on step end call back********************
Step 27160 finish
{'loss': 0.2717, 'grad_norm': 1.1440728902816772, 'learning_rate': 5.098901098901099e-05, 'epoch': 4.94}
********************on step end call back********************
Step 27170 finish
[INFO|trainer.py:3376] 2024-03-23 21:41:16,802 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-23 21:41:16,802 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-23 21:41:16,802 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-23 21:43:25,756 >> Saving model checkpoint to ./output/tmp-checkpoint-27200
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-23 21:43:25,900 >> tokenizer config file saved in ./output/tmp-checkpoint-27200/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-23 21:43:25,900 >> Special tokens file saved in ./output/tmp-checkpoint-27200/special_tokens_map.json
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-23 21:51:56,700 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-23 21:51:56,700 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-23 21:51:56,700 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-23 21:54:05,700 >> Saving model checkpoint to ./output/tmp-checkpoint-27300
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-23 21:54:05,847 >> tokenizer config file saved in ./output/tmp-checkpoint-27300/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-23 21:54:05,847 >> Special tokens file saved in ./output/tmp-checkpoint-27300/special_tokens_map.json
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-23 22:02:47,289 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-23 22:02:47,289 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-23 22:02:47,289 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-23 22:04:56,433 >> Saving model checkpoint to ./output/tmp-checkpoint-27400
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-23 22:04:56,575 >> tokenizer config file saved in ./output/tmp-checkpoint-27400/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-23 22:04:56,576 >> Special tokens file saved in ./output/tmp-checkpoint-27400/special_tokens_map.json
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-23 22:13:39,938 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-23 22:13:39,938 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-23 22:13:39,938 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-23 22:15:49,261 >> Saving model checkpoint to ./output/tmp-checkpoint-27500
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-23 22:15:49,413 >> tokenizer config file saved in ./output/tmp-checkpoint-27500/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-23 22:15:49,413 >> Special tokens file saved in ./output/tmp-checkpoint-27500/special_tokens_map.json
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
{'loss': 0.2913, 'grad_norm': 1.0106663703918457, 'learning_rate': 5.097069597069597e-05, 'epoch': 4.94}
********************on step end call back********************
Step 27180 finish
{'loss': 0.3076, 'grad_norm': 1.4926800727844238, 'learning_rate': 5.095238095238095e-05, 'epoch': 4.94}
********************on step end call back********************
Step 27190 finish
{'loss': 0.2567, 'grad_norm': 1.3678256273269653, 'learning_rate': 5.093406593406593e-05, 'epoch': 4.94}
********************on step end call back********************
Step 27200 finish
{'loss': 0.2732, 'grad_norm': 1.0774102210998535, 'learning_rate': 5.0915750915750915e-05, 'epoch': 4.94}
{'eval_loss': 0.3353695273399353, 'eval_accuracy': 0.8854166666666666, 'eval_runtime': 128.953, 'eval_samples_per_second': 4.948, 'eval_steps_per_second': 4.948, 'epoch': 4.94}
********************save call back********************
********************on step end call back********************
Step 27210 finish
{'loss': 0.2688, 'grad_norm': 1.2086524963378906, 'learning_rate': 5.0897435897435894e-05, 'epoch': 4.95}
********************on step end call back********************
Step 27220 finish
{'loss': 0.2941, 'grad_norm': 1.3529609441757202, 'learning_rate': 5.087912087912088e-05, 'epoch': 4.95}
********************on step end call back********************
Step 27230 finish
{'loss': 0.3095, 'grad_norm': 1.6447750329971313, 'learning_rate': 5.086080586080586e-05, 'epoch': 4.95}
********************on step end call back********************
Step 27240 finish
{'loss': 0.2975, 'grad_norm': 1.6350135803222656, 'learning_rate': 5.0842490842490844e-05, 'epoch': 4.95}
********************on step end call back********************
Step 27250 finish
{'loss': 0.2996, 'grad_norm': 1.0130494832992554, 'learning_rate': 5.082417582417582e-05, 'epoch': 4.95}
********************on step end call back********************
Step 27260 finish
{'loss': 0.2813, 'grad_norm': 1.1469388008117676, 'learning_rate': 5.080586080586081e-05, 'epoch': 4.96}
********************on step end call back********************
Step 27270 finish
{'loss': 0.2877, 'grad_norm': 1.1239700317382812, 'learning_rate': 5.078754578754579e-05, 'epoch': 4.96}
********************on step end call back********************
Step 27280 finish
{'loss': 0.2633, 'grad_norm': 0.8568774461746216, 'learning_rate': 5.0769230769230766e-05, 'epoch': 4.96}
********************on step end call back********************
Step 27290 finish
{'loss': 0.264, 'grad_norm': 1.1714177131652832, 'learning_rate': 5.075091575091575e-05, 'epoch': 4.96}
********************on step end call back********************
Step 27300 finish
{'loss': 0.2936, 'grad_norm': 1.0447466373443604, 'learning_rate': 5.073260073260073e-05, 'epoch': 4.96}
{'eval_loss': 0.3407335579395294, 'eval_accuracy': 0.8854166666666666, 'eval_runtime': 128.999, 'eval_samples_per_second': 4.946, 'eval_steps_per_second': 4.946, 'epoch': 4.96}
********************save call back********************
********************on step end call back********************
Step 27310 finish
{'loss': 0.2928, 'grad_norm': 1.3956290483474731, 'learning_rate': 5.0714285714285716e-05, 'epoch': 4.96}
********************on step end call back********************
Step 27320 finish
{'loss': 0.2737, 'grad_norm': 1.227781057357788, 'learning_rate': 5.0695970695970694e-05, 'epoch': 4.97}
********************on step end call back********************
Step 27330 finish
{'loss': 0.3062, 'grad_norm': 1.1507211923599243, 'learning_rate': 5.067765567765568e-05, 'epoch': 4.97}
********************on step end call back********************
Step 27340 finish
{'loss': 0.2976, 'grad_norm': 0.9905893206596375, 'learning_rate': 5.065934065934066e-05, 'epoch': 4.97}
********************on step end call back********************
Step 27350 finish
{'loss': 0.3112, 'grad_norm': 1.219832420349121, 'learning_rate': 5.0641025641025644e-05, 'epoch': 4.97}
********************on step end call back********************
Step 27360 finish
{'loss': 0.2576, 'grad_norm': 1.2051544189453125, 'learning_rate': 5.062271062271062e-05, 'epoch': 4.97}
********************on step end call back********************
Step 27370 finish
{'loss': 0.2787, 'grad_norm': 1.0033130645751953, 'learning_rate': 5.06043956043956e-05, 'epoch': 4.98}
********************on step end call back********************
Step 27380 finish
{'loss': 0.2842, 'grad_norm': 1.171929955482483, 'learning_rate': 5.058608058608059e-05, 'epoch': 4.98}
********************on step end call back********************
Step 27390 finish
{'loss': 0.2756, 'grad_norm': 1.3415957689285278, 'learning_rate': 5.0567765567765566e-05, 'epoch': 4.98}
********************on step end call back********************
Step 27400 finish
{'loss': 0.302, 'grad_norm': 1.3824061155319214, 'learning_rate': 5.054945054945055e-05, 'epoch': 4.98}
{'eval_loss': 0.3512428402900696, 'eval_accuracy': 0.875, 'eval_runtime': 129.1435, 'eval_samples_per_second': 4.94, 'eval_steps_per_second': 4.94, 'epoch': 4.98}
********************save call back********************
********************on step end call back********************
Step 27410 finish
{'loss': 0.2568, 'grad_norm': 1.174495816230774, 'learning_rate': 5.053113553113553e-05, 'epoch': 4.98}
********************on step end call back********************
Step 27420 finish
{'loss': 0.3069, 'grad_norm': 1.360876202583313, 'learning_rate': 5.0512820512820516e-05, 'epoch': 4.98}
********************on step end call back********************
Step 27430 finish
{'loss': 0.2869, 'grad_norm': 1.3927854299545288, 'learning_rate': 5.0494505494505495e-05, 'epoch': 4.99}
********************on step end call back********************
Step 27440 finish
{'loss': 0.2738, 'grad_norm': 1.0089479684829712, 'learning_rate': 5.047619047619048e-05, 'epoch': 4.99}
********************on step end call back********************
Step 27450 finish
{'loss': 0.312, 'grad_norm': 1.4097763299942017, 'learning_rate': 5.045787545787546e-05, 'epoch': 4.99}
********************on step end call back********************
Step 27460 finish
{'loss': 0.2709, 'grad_norm': 1.0898958444595337, 'learning_rate': 5.043956043956044e-05, 'epoch': 4.99}
********************on step end call back********************
Step 27470 finish
{'loss': 0.2813, 'grad_norm': 1.2550913095474243, 'learning_rate': 5.042124542124542e-05, 'epoch': 4.99}
********************on step end call back********************
Step 27480 finish
{'loss': 0.298, 'grad_norm': 1.3386772871017456, 'learning_rate': 5.04029304029304e-05, 'epoch': 5.0}
********************on step end call back********************
Step 27490 finish
{'loss': 0.3099, 'grad_norm': 1.3375736474990845, 'learning_rate': 5.038461538461539e-05, 'epoch': 5.0}
********************on step end call back********************
Step 27500 finish
{'loss': 0.2498, 'grad_norm': 1.3275023698806763, 'learning_rate': 5.0366300366300366e-05, 'epoch': 5.0}
{'eval_loss': 0.34116390347480774, 'eval_accuracy': 0.8645833333333334, 'eval_runtime': 129.3222, 'eval_samples_per_second': 4.933, 'eval_steps_per_second': 4.933, 'epoch': 5.0}
********************save call back********************
********************on epoch end call back********************
Epoch 4.999977275309623 finish
********************on step end call back********************
Step 27510 finish
{'loss': 0.2199, 'grad_norm': 0.8549432158470154, 'learning_rate': 5.034798534798535e-05, 'epoch': 5.0}
********************on step end call back********************
Step 27520 finish
{'loss': 0.2327, 'grad_norm': 1.1251529455184937, 'learning_rate': 5.032967032967033e-05, 'epoch': 5.0}
********************on step end call back********************
Step 27530 finish
{'loss': 0.2402, 'grad_norm': 1.1462910175323486, 'learning_rate': 5.031135531135531e-05, 'epoch': 5.0}
********************on step end call back********************
Step 27540 finish
{'loss': 0.2215, 'grad_norm': 1.8174090385437012, 'learning_rate': 5.0293040293040295e-05, 'epoch': 5.01}
********************on step end call back********************
Step 27550 finish
{'loss': 0.2176, 'grad_norm': 0.9954920411109924, 'learning_rate': 5.0274725274725274e-05, 'epoch': 5.01}
[INFO|trainer.py:3376] 2024-03-23 22:24:31,083 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-23 22:24:31,083 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-23 22:24:31,083 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-23 22:26:40,325 >> Saving model checkpoint to ./output/tmp-checkpoint-27600
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-23 22:26:40,471 >> tokenizer config file saved in ./output/tmp-checkpoint-27600/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-23 22:26:40,471 >> Special tokens file saved in ./output/tmp-checkpoint-27600/special_tokens_map.json
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-23 22:35:15,796 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-23 22:35:15,796 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-23 22:35:15,796 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-23 22:37:24,738 >> Saving model checkpoint to ./output/tmp-checkpoint-27700
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-23 22:37:24,901 >> tokenizer config file saved in ./output/tmp-checkpoint-27700/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-23 22:37:24,901 >> Special tokens file saved in ./output/tmp-checkpoint-27700/special_tokens_map.json
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-23 22:45:53,634 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-23 22:45:53,634 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-23 22:45:53,634 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-23 22:48:02,847 >> Saving model checkpoint to ./output/tmp-checkpoint-27800
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-23 22:48:02,990 >> tokenizer config file saved in ./output/tmp-checkpoint-27800/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-23 22:48:02,990 >> Special tokens file saved in ./output/tmp-checkpoint-27800/special_tokens_map.json
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-23 22:56:44,235 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-23 22:56:44,235 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-23 22:56:44,235 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-23 22:58:53,625 >> Saving model checkpoint to ./output/tmp-checkpoint-27900
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-23 22:58:53,778 >> tokenizer config file saved in ./output/tmp-checkpoint-27900/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-23 22:58:53,778 >> Special tokens file saved in ./output/tmp-checkpoint-27900/special_tokens_map.json
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
********************on step end call back********************
Step 27560 finish
{'loss': 0.2175, 'grad_norm': 0.8830715417861938, 'learning_rate': 5.025641025641026e-05, 'epoch': 5.01}
********************on step end call back********************
Step 27570 finish
{'loss': 0.2159, 'grad_norm': 1.420525074005127, 'learning_rate': 5.023809523809524e-05, 'epoch': 5.01}
********************on step end call back********************
Step 27580 finish
{'loss': 0.2462, 'grad_norm': 1.1019097566604614, 'learning_rate': 5.0219780219780223e-05, 'epoch': 5.01}
********************on step end call back********************
Step 27590 finish
{'loss': 0.2154, 'grad_norm': 0.999660849571228, 'learning_rate': 5.02014652014652e-05, 'epoch': 5.02}
********************on step end call back********************
Step 27600 finish
{'loss': 0.2291, 'grad_norm': 1.1227283477783203, 'learning_rate': 5.018315018315019e-05, 'epoch': 5.02}
{'eval_loss': 0.3625183701515198, 'eval_accuracy': 0.8645833333333334, 'eval_runtime': 129.2409, 'eval_samples_per_second': 4.937, 'eval_steps_per_second': 4.937, 'epoch': 5.02}
********************save call back********************
********************on step end call back********************
Step 27610 finish
{'loss': 0.2192, 'grad_norm': 1.1543890237808228, 'learning_rate': 5.0164835164835166e-05, 'epoch': 5.02}
********************on step end call back********************
Step 27620 finish
{'loss': 0.2165, 'grad_norm': 1.1131008863449097, 'learning_rate': 5.0146520146520145e-05, 'epoch': 5.02}
********************on step end call back********************
Step 27630 finish
{'loss': 0.2281, 'grad_norm': 1.1284403800964355, 'learning_rate': 5.012820512820513e-05, 'epoch': 5.02}
********************on step end call back********************
Step 27640 finish
{'loss': 0.2121, 'grad_norm': 1.2114906311035156, 'learning_rate': 5.010989010989011e-05, 'epoch': 5.02}
********************on step end call back********************
Step 27650 finish
{'loss': 0.2331, 'grad_norm': 1.4294575452804565, 'learning_rate': 5.0091575091575095e-05, 'epoch': 5.03}
********************on step end call back********************
Step 27660 finish
{'loss': 0.2347, 'grad_norm': 1.4100629091262817, 'learning_rate': 5.0073260073260074e-05, 'epoch': 5.03}
********************on step end call back********************
Step 27670 finish
{'loss': 0.2178, 'grad_norm': 1.409806489944458, 'learning_rate': 5.005494505494506e-05, 'epoch': 5.03}
********************on step end call back********************
Step 27680 finish
{'loss': 0.2466, 'grad_norm': 1.3316330909729004, 'learning_rate': 5.003663003663004e-05, 'epoch': 5.03}
********************on step end call back********************
Step 27690 finish
{'loss': 0.2049, 'grad_norm': 0.8778414726257324, 'learning_rate': 5.0018315018315024e-05, 'epoch': 5.03}
********************on step end call back********************
Step 27700 finish
{'loss': 0.222, 'grad_norm': 1.4423201084136963, 'learning_rate': 5e-05, 'epoch': 5.04}
{'eval_loss': 0.36103400588035583, 'eval_accuracy': 0.8645833333333334, 'eval_runtime': 128.9406, 'eval_samples_per_second': 4.948, 'eval_steps_per_second': 4.948, 'epoch': 5.04}
********************save call back********************
********************on step end call back********************
Step 27710 finish
{'loss': 0.2324, 'grad_norm': 1.1774532794952393, 'learning_rate': 4.998168498168498e-05, 'epoch': 5.04}
********************on step end call back********************
Step 27720 finish
{'loss': 0.2193, 'grad_norm': 1.1473923921585083, 'learning_rate': 4.996336996336997e-05, 'epoch': 5.04}
********************on step end call back********************
Step 27730 finish
{'loss': 0.2116, 'grad_norm': 1.188077449798584, 'learning_rate': 4.9945054945054945e-05, 'epoch': 5.04}
********************on step end call back********************
Step 27740 finish
{'loss': 0.206, 'grad_norm': 1.051581859588623, 'learning_rate': 4.992673992673993e-05, 'epoch': 5.04}
********************on step end call back********************
Step 27750 finish
{'loss': 0.2564, 'grad_norm': 1.464599370956421, 'learning_rate': 4.990842490842491e-05, 'epoch': 5.04}
********************on step end call back********************
Step 27760 finish
{'loss': 0.2212, 'grad_norm': 1.0749281644821167, 'learning_rate': 4.9890109890109895e-05, 'epoch': 5.05}
********************on step end call back********************
Step 27770 finish
{'loss': 0.2366, 'grad_norm': 1.3717998266220093, 'learning_rate': 4.9871794871794874e-05, 'epoch': 5.05}
********************on step end call back********************
Step 27780 finish
{'loss': 0.237, 'grad_norm': 1.190854787826538, 'learning_rate': 4.985347985347986e-05, 'epoch': 5.05}
********************on step end call back********************
Step 27790 finish
{'loss': 0.2035, 'grad_norm': 0.778606116771698, 'learning_rate': 4.983516483516484e-05, 'epoch': 5.05}
********************on step end call back********************
Step 27800 finish
{'loss': 0.2423, 'grad_norm': 0.9756922125816345, 'learning_rate': 4.981684981684982e-05, 'epoch': 5.05}
{'eval_loss': 0.36554667353630066, 'eval_accuracy': 0.8645833333333334, 'eval_runtime': 129.2116, 'eval_samples_per_second': 4.938, 'eval_steps_per_second': 4.938, 'epoch': 5.05}
********************save call back********************
********************on step end call back********************
Step 27810 finish
{'loss': 0.2115, 'grad_norm': 1.0424467325210571, 'learning_rate': 4.97985347985348e-05, 'epoch': 5.06}
********************on step end call back********************
Step 27820 finish
{'loss': 0.2668, 'grad_norm': 1.1602927446365356, 'learning_rate': 4.978021978021978e-05, 'epoch': 5.06}
********************on step end call back********************
Step 27830 finish
{'loss': 0.2252, 'grad_norm': 1.500062108039856, 'learning_rate': 4.976190476190477e-05, 'epoch': 5.06}
********************on step end call back********************
Step 27840 finish
{'loss': 0.2473, 'grad_norm': 1.4057213068008423, 'learning_rate': 4.9743589743589746e-05, 'epoch': 5.06}
********************on step end call back********************
Step 27850 finish
{'loss': 0.2309, 'grad_norm': 1.0922555923461914, 'learning_rate': 4.972527472527473e-05, 'epoch': 5.06}
********************on step end call back********************
Step 27860 finish
{'loss': 0.2316, 'grad_norm': 1.2524889707565308, 'learning_rate': 4.970695970695971e-05, 'epoch': 5.06}
********************on step end call back********************
Step 27870 finish
{'loss': 0.2541, 'grad_norm': 1.170941710472107, 'learning_rate': 4.9688644688644695e-05, 'epoch': 5.07}
********************on step end call back********************
Step 27880 finish
{'loss': 0.2507, 'grad_norm': 1.0811494588851929, 'learning_rate': 4.9670329670329674e-05, 'epoch': 5.07}
********************on step end call back********************
Step 27890 finish
{'loss': 0.2472, 'grad_norm': 1.3428711891174316, 'learning_rate': 4.965201465201465e-05, 'epoch': 5.07}
********************on step end call back********************
Step 27900 finish
{'loss': 0.2432, 'grad_norm': 1.2010914087295532, 'learning_rate': 4.963369963369964e-05, 'epoch': 5.07}
{'eval_loss': 0.3591483533382416, 'eval_accuracy': 0.875, 'eval_runtime': 129.3892, 'eval_samples_per_second': 4.931, 'eval_steps_per_second': 4.931, 'epoch': 5.07}
********************save call back********************
********************on step end call back********************
Step 27910 finish
{'loss': 0.2104, 'grad_norm': 1.2663888931274414, 'learning_rate': 4.961538461538462e-05, 'epoch': 5.07}
********************on step end call back********************
Step 27920 finish
{'loss': 0.2146, 'grad_norm': 0.8990483283996582, 'learning_rate': 4.95970695970696e-05, 'epoch': 5.08}
********************on step end call back********************
Step 27930 finish
{'loss': 0.2442, 'grad_norm': 1.0565788745880127, 'learning_rate': 4.957875457875458e-05, 'epoch': 5.08}
********************on step end call back********************
Step 27940 finish
{'loss': 0.2298, 'grad_norm': 1.1464310884475708, 'learning_rate': 4.956043956043957e-05, 'epoch': 5.08}
********************on step end call back********************
[INFO|trainer.py:3376] 2024-03-23 23:09:06,141 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-23 23:09:06,142 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-23 23:09:06,142 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-23 23:11:16,405 >> Saving model checkpoint to ./output/tmp-checkpoint-28000
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-23 23:11:16,564 >> tokenizer config file saved in ./output/tmp-checkpoint-28000/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-23 23:11:16,564 >> Special tokens file saved in ./output/tmp-checkpoint-28000/special_tokens_map.json
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-23 23:19:52,240 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-23 23:19:52,240 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-23 23:19:52,240 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-23 23:22:02,492 >> Saving model checkpoint to ./output/tmp-checkpoint-28100
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-23 23:22:02,657 >> tokenizer config file saved in ./output/tmp-checkpoint-28100/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-23 23:22:02,657 >> Special tokens file saved in ./output/tmp-checkpoint-28100/special_tokens_map.json
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-23 23:30:47,817 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-23 23:30:47,818 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-23 23:30:47,818 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-23 23:32:57,457 >> Saving model checkpoint to ./output/tmp-checkpoint-28200
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-23 23:32:57,596 >> tokenizer config file saved in ./output/tmp-checkpoint-28200/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-23 23:32:57,597 >> Special tokens file saved in ./output/tmp-checkpoint-28200/special_tokens_map.json
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-23 23:41:39,741 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-23 23:41:39,741 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-23 23:41:39,741 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-23 23:43:49,849 >> Saving model checkpoint to ./output/tmp-checkpoint-28300
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-23 23:43:50,006 >> tokenizer config file saved in ./output/tmp-checkpoint-28300/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-23 23:43:50,007 >> Special tokens file saved in ./output/tmp-checkpoint-28300/special_tokens_map.json
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
Step 27950 finish
{'loss': 0.2523, 'grad_norm': 1.4374301433563232, 'learning_rate': 4.9542124542124546e-05, 'epoch': 5.08}
********************on step end call back********************
Step 27960 finish
{'loss': 0.2364, 'grad_norm': 1.18650221824646, 'learning_rate': 4.9523809523809525e-05, 'epoch': 5.08}
********************on step end call back********************
Step 27970 finish
{'loss': 0.214, 'grad_norm': 1.122043490409851, 'learning_rate': 4.950549450549451e-05, 'epoch': 5.08}
********************on step end call back********************
Step 27980 finish
{'loss': 0.211, 'grad_norm': 1.278657078742981, 'learning_rate': 4.948717948717949e-05, 'epoch': 5.09}
********************on step end call back********************
Step 27990 finish
{'loss': 0.2411, 'grad_norm': 1.4294872283935547, 'learning_rate': 4.9468864468864474e-05, 'epoch': 5.09}
********************on step end call back********************
Step 28000 finish
{'loss': 0.2312, 'grad_norm': 1.0867317914962769, 'learning_rate': 4.945054945054945e-05, 'epoch': 5.09}
{'eval_loss': 0.3650532364845276, 'eval_accuracy': 0.875, 'eval_runtime': 130.2626, 'eval_samples_per_second': 4.898, 'eval_steps_per_second': 4.898, 'epoch': 5.09}
********************save call back********************
********************on step end call back********************
Step 28010 finish
{'loss': 0.2133, 'grad_norm': 1.223649024963379, 'learning_rate': 4.943223443223444e-05, 'epoch': 5.09}
********************on step end call back********************
Step 28020 finish
{'loss': 0.2612, 'grad_norm': 1.5754389762878418, 'learning_rate': 4.941391941391942e-05, 'epoch': 5.09}
********************on step end call back********************
Step 28030 finish
{'loss': 0.2385, 'grad_norm': 1.3149747848510742, 'learning_rate': 4.93956043956044e-05, 'epoch': 5.1}
********************on step end call back********************
Step 28040 finish
{'loss': 0.2254, 'grad_norm': 1.2563775777816772, 'learning_rate': 4.937728937728938e-05, 'epoch': 5.1}
********************on step end call back********************
Step 28050 finish
{'loss': 0.2571, 'grad_norm': 1.3746860027313232, 'learning_rate': 4.935897435897436e-05, 'epoch': 5.1}
********************on step end call back********************
Step 28060 finish
{'loss': 0.2069, 'grad_norm': 0.8880413174629211, 'learning_rate': 4.9340659340659346e-05, 'epoch': 5.1}
********************on step end call back********************
Step 28070 finish
{'loss': 0.2027, 'grad_norm': 1.058877944946289, 'learning_rate': 4.9322344322344325e-05, 'epoch': 5.1}
********************on step end call back********************
Step 28080 finish
{'loss': 0.2304, 'grad_norm': 1.5684887170791626, 'learning_rate': 4.930402930402931e-05, 'epoch': 5.1}
********************on step end call back********************
Step 28090 finish
{'loss': 0.2452, 'grad_norm': 1.1733498573303223, 'learning_rate': 4.928571428571429e-05, 'epoch': 5.11}
********************on step end call back********************
Step 28100 finish
{'loss': 0.2637, 'grad_norm': 1.6236027479171753, 'learning_rate': 4.9267399267399275e-05, 'epoch': 5.11}
{'eval_loss': 0.36474600434303284, 'eval_accuracy': 0.875, 'eval_runtime': 130.2509, 'eval_samples_per_second': 4.898, 'eval_steps_per_second': 4.898, 'epoch': 5.11}
********************save call back********************
********************on step end call back********************
Step 28110 finish
{'loss': 0.2524, 'grad_norm': 1.1668744087219238, 'learning_rate': 4.9249084249084253e-05, 'epoch': 5.11}
********************on step end call back********************
Step 28120 finish
{'loss': 0.2451, 'grad_norm': 0.9423717260360718, 'learning_rate': 4.923076923076924e-05, 'epoch': 5.11}
********************on step end call back********************
Step 28130 finish
{'loss': 0.2441, 'grad_norm': 1.4725176095962524, 'learning_rate': 4.921245421245421e-05, 'epoch': 5.11}
********************on step end call back********************
Step 28140 finish
{'loss': 0.2417, 'grad_norm': 1.0837069749832153, 'learning_rate': 4.9194139194139196e-05, 'epoch': 5.12}
********************on step end call back********************
Step 28150 finish
{'loss': 0.2094, 'grad_norm': 1.2839264869689941, 'learning_rate': 4.9175824175824175e-05, 'epoch': 5.12}
********************on step end call back********************
Step 28160 finish
{'loss': 0.2651, 'grad_norm': 1.2923797369003296, 'learning_rate': 4.9157509157509154e-05, 'epoch': 5.12}
********************on step end call back********************
Step 28170 finish
{'loss': 0.2318, 'grad_norm': 1.2294018268585205, 'learning_rate': 4.913919413919414e-05, 'epoch': 5.12}
********************on step end call back********************
Step 28180 finish
{'loss': 0.2562, 'grad_norm': 1.1444014310836792, 'learning_rate': 4.912087912087912e-05, 'epoch': 5.12}
********************on step end call back********************
Step 28190 finish
{'loss': 0.2435, 'grad_norm': 1.350541591644287, 'learning_rate': 4.9102564102564104e-05, 'epoch': 5.12}
********************on step end call back********************
Step 28200 finish
{'loss': 0.2461, 'grad_norm': 1.324453592300415, 'learning_rate': 4.908424908424908e-05, 'epoch': 5.13}
{'eval_loss': 0.363864004611969, 'eval_accuracy': 0.8854166666666666, 'eval_runtime': 129.6388, 'eval_samples_per_second': 4.921, 'eval_steps_per_second': 4.921, 'epoch': 5.13}
********************save call back********************
********************on step end call back********************
Step 28210 finish
{'loss': 0.2005, 'grad_norm': 1.232742190361023, 'learning_rate': 4.906593406593407e-05, 'epoch': 5.13}
********************on step end call back********************
Step 28220 finish
{'loss': 0.2533, 'grad_norm': 0.9138016104698181, 'learning_rate': 4.904761904761905e-05, 'epoch': 5.13}
********************on step end call back********************
Step 28230 finish
{'loss': 0.2182, 'grad_norm': 1.434664011001587, 'learning_rate': 4.902930402930403e-05, 'epoch': 5.13}
********************on step end call back********************
Step 28240 finish
{'loss': 0.2395, 'grad_norm': 1.044034719467163, 'learning_rate': 4.901098901098901e-05, 'epoch': 5.13}
********************on step end call back********************
Step 28250 finish
{'loss': 0.2247, 'grad_norm': 0.9786983728408813, 'learning_rate': 4.899267399267399e-05, 'epoch': 5.14}
********************on step end call back********************
Step 28260 finish
{'loss': 0.25, 'grad_norm': 1.3526813983917236, 'learning_rate': 4.8974358974358975e-05, 'epoch': 5.14}
********************on step end call back********************
Step 28270 finish
{'loss': 0.2839, 'grad_norm': 1.0389342308044434, 'learning_rate': 4.8956043956043954e-05, 'epoch': 5.14}
********************on step end call back********************
Step 28280 finish
{'loss': 0.2711, 'grad_norm': 1.0968165397644043, 'learning_rate': 4.893772893772894e-05, 'epoch': 5.14}
********************on step end call back********************
Step 28290 finish
{'loss': 0.2416, 'grad_norm': 1.3492658138275146, 'learning_rate': 4.891941391941392e-05, 'epoch': 5.14}
********************on step end call back********************
Step 28300 finish
{'loss': 0.2377, 'grad_norm': 0.8290557265281677, 'learning_rate': 4.8901098901098904e-05, 'epoch': 5.14}
{'eval_loss': 0.3625798225402832, 'eval_accuracy': 0.875, 'eval_runtime': 130.1065, 'eval_samples_per_second': 4.904, 'eval_steps_per_second': 4.904, 'epoch': 5.14}
********************save call back********************
********************on step end call back********************
Step 28310 finish
{'loss': 0.2852, 'grad_norm': 1.4407552480697632, 'learning_rate': 4.888278388278388e-05, 'epoch': 5.15}
********************on step end call back********************
Step 28320 finish
{'loss': 0.243, 'grad_norm': 1.3356071710586548, 'learning_rate': 4.886446886446887e-05, 'epoch': 5.15}
********************on step end call back********************
Step 28330 finish
{'loss': 0.2594, 'grad_norm': 1.2594903707504272, 'learning_rate': 4.884615384615385e-05, 'epoch': 5.15}
********************on step end call back********************
Step 28340 finish
[INFO|trainer.py:3376] 2024-03-23 23:52:28,768 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-23 23:52:28,769 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-23 23:52:28,769 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-23 23:54:39,791 >> Saving model checkpoint to ./output/tmp-checkpoint-28400
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-23 23:54:39,944 >> tokenizer config file saved in ./output/tmp-checkpoint-28400/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-23 23:54:39,944 >> Special tokens file saved in ./output/tmp-checkpoint-28400/special_tokens_map.json
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-24 00:03:20,378 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-24 00:03:20,378 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-24 00:03:20,379 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-24 00:05:30,753 >> Saving model checkpoint to ./output/tmp-checkpoint-28500
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-24 00:05:30,919 >> tokenizer config file saved in ./output/tmp-checkpoint-28500/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-24 00:05:30,919 >> Special tokens file saved in ./output/tmp-checkpoint-28500/special_tokens_map.json
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-24 00:14:10,763 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-24 00:14:10,763 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-24 00:14:10,763 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-24 00:16:20,312 >> Saving model checkpoint to ./output/tmp-checkpoint-28600
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-24 00:16:20,462 >> tokenizer config file saved in ./output/tmp-checkpoint-28600/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-24 00:16:20,463 >> Special tokens file saved in ./output/tmp-checkpoint-28600/special_tokens_map.json
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-24 00:24:59,968 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-24 00:24:59,968 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-24 00:24:59,968 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-24 00:27:11,350 >> Saving model checkpoint to ./output/tmp-checkpoint-28700
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-24 00:27:11,497 >> tokenizer config file saved in ./output/tmp-checkpoint-28700/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-24 00:27:11,497 >> Special tokens file saved in ./output/tmp-checkpoint-28700/special_tokens_map.json
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
{'loss': 0.2552, 'grad_norm': 1.4524357318878174, 'learning_rate': 4.8827838827838826e-05, 'epoch': 5.15}
********************on step end call back********************
Step 28350 finish
{'loss': 0.2441, 'grad_norm': 1.1140061616897583, 'learning_rate': 4.880952380952381e-05, 'epoch': 5.15}
********************on step end call back********************
Step 28360 finish
{'loss': 0.2194, 'grad_norm': 0.9669395685195923, 'learning_rate': 4.879120879120879e-05, 'epoch': 5.16}
********************on step end call back********************
Step 28370 finish
{'loss': 0.2311, 'grad_norm': 1.0481576919555664, 'learning_rate': 4.8772893772893776e-05, 'epoch': 5.16}
********************on step end call back********************
Step 28380 finish
{'loss': 0.2388, 'grad_norm': 1.230154037475586, 'learning_rate': 4.8754578754578754e-05, 'epoch': 5.16}
********************on step end call back********************
Step 28390 finish
{'loss': 0.2211, 'grad_norm': 1.264138102531433, 'learning_rate': 4.873626373626374e-05, 'epoch': 5.16}
********************on step end call back********************
Step 28400 finish
{'loss': 0.2631, 'grad_norm': 1.2434399127960205, 'learning_rate': 4.871794871794872e-05, 'epoch': 5.16}
{'eval_loss': 0.3600255250930786, 'eval_accuracy': 0.8958333333333334, 'eval_runtime': 131.0212, 'eval_samples_per_second': 4.869, 'eval_steps_per_second': 4.869, 'epoch': 5.16}
********************save call back********************
********************on step end call back********************
Step 28410 finish
{'loss': 0.2721, 'grad_norm': 1.2226169109344482, 'learning_rate': 4.8699633699633704e-05, 'epoch': 5.16}
********************on step end call back********************
Step 28420 finish
{'loss': 0.2559, 'grad_norm': 1.355057716369629, 'learning_rate': 4.868131868131868e-05, 'epoch': 5.17}
********************on step end call back********************
Step 28430 finish
{'loss': 0.2133, 'grad_norm': 1.3331650495529175, 'learning_rate': 4.866300366300366e-05, 'epoch': 5.17}
********************on step end call back********************
Step 28440 finish
{'loss': 0.2267, 'grad_norm': 1.1518349647521973, 'learning_rate': 4.864468864468865e-05, 'epoch': 5.17}
********************on step end call back********************
Step 28450 finish
{'loss': 0.2398, 'grad_norm': 1.217569351196289, 'learning_rate': 4.8626373626373626e-05, 'epoch': 5.17}
********************on step end call back********************
Step 28460 finish
{'loss': 0.2442, 'grad_norm': 1.3617262840270996, 'learning_rate': 4.860805860805861e-05, 'epoch': 5.17}
********************on step end call back********************
Step 28470 finish
{'loss': 0.2636, 'grad_norm': 0.849305272102356, 'learning_rate': 4.858974358974359e-05, 'epoch': 5.18}
********************on step end call back********************
Step 28480 finish
{'loss': 0.2166, 'grad_norm': 1.1442105770111084, 'learning_rate': 4.8571428571428576e-05, 'epoch': 5.18}
********************on step end call back********************
Step 28490 finish
{'loss': 0.2319, 'grad_norm': 1.101086974143982, 'learning_rate': 4.8553113553113555e-05, 'epoch': 5.18}
********************on step end call back********************
Step 28500 finish
{'loss': 0.2165, 'grad_norm': 1.3968799114227295, 'learning_rate': 4.8534798534798533e-05, 'epoch': 5.18}
{'eval_loss': 0.3643397092819214, 'eval_accuracy': 0.8854166666666666, 'eval_runtime': 130.3738, 'eval_samples_per_second': 4.894, 'eval_steps_per_second': 4.894, 'epoch': 5.18}
********************save call back********************
********************on step end call back********************
Step 28510 finish
{'loss': 0.272, 'grad_norm': 1.3516186475753784, 'learning_rate': 4.851648351648352e-05, 'epoch': 5.18}
********************on step end call back********************
Step 28520 finish
{'loss': 0.2324, 'grad_norm': 0.9900206327438354, 'learning_rate': 4.84981684981685e-05, 'epoch': 5.18}
********************on step end call back********************
Step 28530 finish
{'loss': 0.2829, 'grad_norm': 1.135043740272522, 'learning_rate': 4.847985347985348e-05, 'epoch': 5.19}
********************on step end call back********************
Step 28540 finish
{'loss': 0.2544, 'grad_norm': 1.3308466672897339, 'learning_rate': 4.846153846153846e-05, 'epoch': 5.19}
********************on step end call back********************
Step 28550 finish
{'loss': 0.2296, 'grad_norm': 1.5287394523620605, 'learning_rate': 4.844322344322345e-05, 'epoch': 5.19}
********************on step end call back********************
Step 28560 finish
{'loss': 0.2539, 'grad_norm': 1.0169190168380737, 'learning_rate': 4.8424908424908426e-05, 'epoch': 5.19}
********************on step end call back********************
Step 28570 finish
{'loss': 0.214, 'grad_norm': 1.1680046319961548, 'learning_rate': 4.840659340659341e-05, 'epoch': 5.19}
********************on step end call back********************
Step 28580 finish
{'loss': 0.2251, 'grad_norm': 1.5414327383041382, 'learning_rate': 4.838827838827839e-05, 'epoch': 5.2}
********************on step end call back********************
Step 28590 finish
{'loss': 0.2107, 'grad_norm': 0.9655817151069641, 'learning_rate': 4.836996336996337e-05, 'epoch': 5.2}
********************on step end call back********************
Step 28600 finish
{'loss': 0.2648, 'grad_norm': 1.3318544626235962, 'learning_rate': 4.8351648351648355e-05, 'epoch': 5.2}
{'eval_loss': 0.3628968298435211, 'eval_accuracy': 0.875, 'eval_runtime': 129.5479, 'eval_samples_per_second': 4.925, 'eval_steps_per_second': 4.925, 'epoch': 5.2}
********************save call back********************
********************on step end call back********************
Step 28610 finish
{'loss': 0.2608, 'grad_norm': 1.411535382270813, 'learning_rate': 4.8333333333333334e-05, 'epoch': 5.2}
********************on step end call back********************
Step 28620 finish
{'loss': 0.258, 'grad_norm': 1.3315314054489136, 'learning_rate': 4.831501831501832e-05, 'epoch': 5.2}
********************on step end call back********************
Step 28630 finish
{'loss': 0.2338, 'grad_norm': 1.3974716663360596, 'learning_rate': 4.82967032967033e-05, 'epoch': 5.2}
********************on step end call back********************
Step 28640 finish
{'loss': 0.2321, 'grad_norm': 1.095031976699829, 'learning_rate': 4.8278388278388283e-05, 'epoch': 5.21}
********************on step end call back********************
Step 28650 finish
{'loss': 0.2337, 'grad_norm': 1.5691176652908325, 'learning_rate': 4.826007326007326e-05, 'epoch': 5.21}
********************on step end call back********************
Step 28660 finish
{'loss': 0.2667, 'grad_norm': 0.9938584566116333, 'learning_rate': 4.824175824175825e-05, 'epoch': 5.21}
********************on step end call back********************
Step 28670 finish
{'loss': 0.2717, 'grad_norm': 1.277506709098816, 'learning_rate': 4.8223443223443226e-05, 'epoch': 5.21}
********************on step end call back********************
Step 28680 finish
{'loss': 0.2246, 'grad_norm': 1.2536593675613403, 'learning_rate': 4.8205128205128205e-05, 'epoch': 5.21}
********************on step end call back********************
Step 28690 finish
{'loss': 0.2546, 'grad_norm': 1.346822738647461, 'learning_rate': 4.818681318681319e-05, 'epoch': 5.22}
********************on step end call back********************
Step 28700 finish
{'loss': 0.232, 'grad_norm': 1.2324836254119873, 'learning_rate': 4.816849816849817e-05, 'epoch': 5.22}
{'eval_loss': 0.36909398436546326, 'eval_accuracy': 0.8854166666666666, 'eval_runtime': 131.3816, 'eval_samples_per_second': 4.856, 'eval_steps_per_second': 4.856, 'epoch': 5.22}
********************save call back********************
********************on step end call back********************
Step 28710 finish
{'loss': 0.2415, 'grad_norm': 1.0293593406677246, 'learning_rate': 4.8150183150183155e-05, 'epoch': 5.22}
********************on step end call back********************
Step 28720 finish
{'loss': 0.2299, 'grad_norm': 1.3191192150115967, 'learning_rate': 4.8131868131868134e-05, 'epoch': 5.22}
********************on step end call back********************
Step 28730 finish
[INFO|trainer.py:3376] 2024-03-24 00:35:41,923 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-24 00:35:41,923 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-24 00:35:41,923 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-24 00:37:50,900 >> Saving model checkpoint to ./output/tmp-checkpoint-28800
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-24 00:37:51,040 >> tokenizer config file saved in ./output/tmp-checkpoint-28800/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-24 00:37:51,040 >> Special tokens file saved in ./output/tmp-checkpoint-28800/special_tokens_map.json
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-24 00:46:22,839 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-24 00:46:22,839 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-24 00:46:22,839 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-24 00:48:31,576 >> Saving model checkpoint to ./output/tmp-checkpoint-28900
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-24 00:48:31,708 >> tokenizer config file saved in ./output/tmp-checkpoint-28900/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-24 00:48:31,708 >> Special tokens file saved in ./output/tmp-checkpoint-28900/special_tokens_map.json
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-24 00:57:06,293 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-24 00:57:06,293 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-24 00:57:06,293 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-24 00:59:14,841 >> Saving model checkpoint to ./output/tmp-checkpoint-29000
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-24 00:59:14,974 >> tokenizer config file saved in ./output/tmp-checkpoint-29000/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-24 00:59:14,975 >> Special tokens file saved in ./output/tmp-checkpoint-29000/special_tokens_map.json
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-24 01:07:53,377 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-24 01:07:53,378 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-24 01:07:53,378 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-24 01:10:01,905 >> Saving model checkpoint to ./output/tmp-checkpoint-29100
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-24 01:10:02,038 >> tokenizer config file saved in ./output/tmp-checkpoint-29100/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-24 01:10:02,038 >> Special tokens file saved in ./output/tmp-checkpoint-29100/special_tokens_map.json
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
{'loss': 0.2217, 'grad_norm': 1.3244349956512451, 'learning_rate': 4.811355311355312e-05, 'epoch': 5.22}
********************on step end call back********************
Step 28740 finish
{'loss': 0.2187, 'grad_norm': 1.3499263525009155, 'learning_rate': 4.80952380952381e-05, 'epoch': 5.22}
********************on step end call back********************
Step 28750 finish
{'loss': 0.2364, 'grad_norm': 1.4881473779678345, 'learning_rate': 4.8076923076923084e-05, 'epoch': 5.23}
********************on step end call back********************
Step 28760 finish
{'loss': 0.2491, 'grad_norm': 1.3713090419769287, 'learning_rate': 4.805860805860806e-05, 'epoch': 5.23}
********************on step end call back********************
Step 28770 finish
{'loss': 0.2469, 'grad_norm': 1.1188737154006958, 'learning_rate': 4.804029304029304e-05, 'epoch': 5.23}
********************on step end call back********************
Step 28780 finish
{'loss': 0.2271, 'grad_norm': 1.404492735862732, 'learning_rate': 4.802197802197803e-05, 'epoch': 5.23}
********************on step end call back********************
Step 28790 finish
{'loss': 0.2261, 'grad_norm': 1.203031063079834, 'learning_rate': 4.8003663003663005e-05, 'epoch': 5.23}
********************on step end call back********************
Step 28800 finish
{'loss': 0.2273, 'grad_norm': 1.2722004652023315, 'learning_rate': 4.798534798534799e-05, 'epoch': 5.24}
{'eval_loss': 0.3558342456817627, 'eval_accuracy': 0.8958333333333334, 'eval_runtime': 128.9756, 'eval_samples_per_second': 4.947, 'eval_steps_per_second': 4.947, 'epoch': 5.24}
********************save call back********************
********************on step end call back********************
Step 28810 finish
{'loss': 0.2528, 'grad_norm': 1.2593955993652344, 'learning_rate': 4.796703296703297e-05, 'epoch': 5.24}
********************on step end call back********************
Step 28820 finish
{'loss': 0.2157, 'grad_norm': 1.2643232345581055, 'learning_rate': 4.7948717948717955e-05, 'epoch': 5.24}
********************on step end call back********************
Step 28830 finish
{'loss': 0.2489, 'grad_norm': 1.2436872720718384, 'learning_rate': 4.7930402930402934e-05, 'epoch': 5.24}
********************on step end call back********************
Step 28840 finish
{'loss': 0.2277, 'grad_norm': 1.8597184419631958, 'learning_rate': 4.791208791208792e-05, 'epoch': 5.24}
********************on step end call back********************
Step 28850 finish
{'loss': 0.2464, 'grad_norm': 1.2954187393188477, 'learning_rate': 4.78937728937729e-05, 'epoch': 5.24}
********************on step end call back********************
Step 28860 finish
{'loss': 0.2492, 'grad_norm': 1.3062220811843872, 'learning_rate': 4.787545787545788e-05, 'epoch': 5.25}
********************on step end call back********************
Step 28870 finish
{'loss': 0.2277, 'grad_norm': 1.4305644035339355, 'learning_rate': 4.785714285714286e-05, 'epoch': 5.25}
********************on step end call back********************
Step 28880 finish
{'loss': 0.2233, 'grad_norm': 1.3621529340744019, 'learning_rate': 4.783882783882784e-05, 'epoch': 5.25}
********************on step end call back********************
Step 28890 finish
{'loss': 0.2541, 'grad_norm': 1.1103237867355347, 'learning_rate': 4.782051282051283e-05, 'epoch': 5.25}
********************on step end call back********************
Step 28900 finish
{'loss': 0.2291, 'grad_norm': 1.345788598060608, 'learning_rate': 4.7802197802197806e-05, 'epoch': 5.25}
{'eval_loss': 0.37417522072792053, 'eval_accuracy': 0.875, 'eval_runtime': 128.736, 'eval_samples_per_second': 4.956, 'eval_steps_per_second': 4.956, 'epoch': 5.25}
********************save call back********************
********************on step end call back********************
Step 28910 finish
{'loss': 0.2307, 'grad_norm': 1.3707915544509888, 'learning_rate': 4.778388278388279e-05, 'epoch': 5.26}
********************on step end call back********************
Step 28920 finish
{'loss': 0.2221, 'grad_norm': 1.0941359996795654, 'learning_rate': 4.776556776556777e-05, 'epoch': 5.26}
********************on step end call back********************
Step 28930 finish
{'loss': 0.2494, 'grad_norm': 1.1035031080245972, 'learning_rate': 4.774725274725275e-05, 'epoch': 5.26}
********************on step end call back********************
Step 28940 finish
{'loss': 0.2377, 'grad_norm': 1.1916897296905518, 'learning_rate': 4.7728937728937734e-05, 'epoch': 5.26}
********************on step end call back********************
Step 28950 finish
{'loss': 0.2341, 'grad_norm': 1.4213194847106934, 'learning_rate': 4.771062271062271e-05, 'epoch': 5.26}
********************on step end call back********************
Step 28960 finish
{'loss': 0.2345, 'grad_norm': 1.0099750757217407, 'learning_rate': 4.76923076923077e-05, 'epoch': 5.26}
********************on step end call back********************
Step 28970 finish
{'loss': 0.2437, 'grad_norm': 1.2155184745788574, 'learning_rate': 4.767399267399268e-05, 'epoch': 5.27}
********************on step end call back********************
Step 28980 finish
{'loss': 0.2419, 'grad_norm': 1.2095469236373901, 'learning_rate': 4.7655677655677656e-05, 'epoch': 5.27}
********************on step end call back********************
Step 28990 finish
{'loss': 0.2329, 'grad_norm': 1.2723764181137085, 'learning_rate': 4.7637362637362635e-05, 'epoch': 5.27}
********************on step end call back********************
Step 29000 finish
{'loss': 0.232, 'grad_norm': 1.3354753255844116, 'learning_rate': 4.761904761904762e-05, 'epoch': 5.27}
{'eval_loss': 0.3580242097377777, 'eval_accuracy': 0.8854166666666666, 'eval_runtime': 128.5474, 'eval_samples_per_second': 4.963, 'eval_steps_per_second': 4.963, 'epoch': 5.27}
********************save call back********************
********************on step end call back********************
Step 29010 finish
{'loss': 0.225, 'grad_norm': 1.114149808883667, 'learning_rate': 4.76007326007326e-05, 'epoch': 5.27}
********************on step end call back********************
Step 29020 finish
{'loss': 0.2668, 'grad_norm': 1.2509185075759888, 'learning_rate': 4.7582417582417585e-05, 'epoch': 5.28}
********************on step end call back********************
Step 29030 finish
{'loss': 0.235, 'grad_norm': 1.027705430984497, 'learning_rate': 4.7564102564102563e-05, 'epoch': 5.28}
********************on step end call back********************
Step 29040 finish
{'loss': 0.266, 'grad_norm': 1.3565938472747803, 'learning_rate': 4.754578754578755e-05, 'epoch': 5.28}
********************on step end call back********************
Step 29050 finish
{'loss': 0.2466, 'grad_norm': 1.4745763540267944, 'learning_rate': 4.752747252747253e-05, 'epoch': 5.28}
********************on step end call back********************
Step 29060 finish
{'loss': 0.218, 'grad_norm': 0.8721505403518677, 'learning_rate': 4.7509157509157506e-05, 'epoch': 5.28}
********************on step end call back********************
Step 29070 finish
{'loss': 0.222, 'grad_norm': 1.2531131505966187, 'learning_rate': 4.749084249084249e-05, 'epoch': 5.28}
********************on step end call back********************
Step 29080 finish
{'loss': 0.2156, 'grad_norm': 1.3155245780944824, 'learning_rate': 4.747252747252747e-05, 'epoch': 5.29}
********************on step end call back********************
Step 29090 finish
{'loss': 0.228, 'grad_norm': 1.1637437343597412, 'learning_rate': 4.7454212454212456e-05, 'epoch': 5.29}
********************on step end call back********************
Step 29100 finish
{'loss': 0.2512, 'grad_norm': 1.3662495613098145, 'learning_rate': 4.7435897435897435e-05, 'epoch': 5.29}
{'eval_loss': 0.36479511857032776, 'eval_accuracy': 0.8854166666666666, 'eval_runtime': 128.5264, 'eval_samples_per_second': 4.964, 'eval_steps_per_second': 4.964, 'epoch': 5.29}
********************save call back********************
********************on step end call back********************
Step 29110 finish
{'loss': 0.2199, 'grad_norm': 1.315247654914856, 'learning_rate': 4.741758241758242e-05, 'epoch': 5.29}
********************on step end call back********************
Step 29120 finish
[INFO|trainer.py:3376] 2024-03-24 01:18:24,559 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-24 01:18:24,559 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-24 01:18:24,559 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-24 01:20:33,282 >> Saving model checkpoint to ./output/tmp-checkpoint-29200
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-24 01:20:33,401 >> tokenizer config file saved in ./output/tmp-checkpoint-29200/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-24 01:20:33,401 >> Special tokens file saved in ./output/tmp-checkpoint-29200/special_tokens_map.json
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-24 01:29:03,052 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-24 01:29:03,052 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-24 01:29:03,052 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-24 01:31:12,410 >> Saving model checkpoint to ./output/tmp-checkpoint-29300
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-24 01:31:12,548 >> tokenizer config file saved in ./output/tmp-checkpoint-29300/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-24 01:31:12,548 >> Special tokens file saved in ./output/tmp-checkpoint-29300/special_tokens_map.json
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-24 01:39:38,930 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-24 01:39:38,931 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-24 01:39:38,931 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-24 01:41:47,826 >> Saving model checkpoint to ./output/tmp-checkpoint-29400
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-24 01:41:47,966 >> tokenizer config file saved in ./output/tmp-checkpoint-29400/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-24 01:41:47,967 >> Special tokens file saved in ./output/tmp-checkpoint-29400/special_tokens_map.json
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-24 01:50:31,561 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-24 01:50:31,561 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-24 01:50:31,561 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-24 01:52:40,393 >> Saving model checkpoint to ./output/tmp-checkpoint-29500
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-24 01:52:40,537 >> tokenizer config file saved in ./output/tmp-checkpoint-29500/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-24 01:52:40,537 >> Special tokens file saved in ./output/tmp-checkpoint-29500/special_tokens_map.json
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
{'loss': 0.2494, 'grad_norm': 1.4747577905654907, 'learning_rate': 4.73992673992674e-05, 'epoch': 5.29}
********************on step end call back********************
Step 29130 finish
{'loss': 0.2329, 'grad_norm': 1.267196774482727, 'learning_rate': 4.738095238095238e-05, 'epoch': 5.3}
********************on step end call back********************
Step 29140 finish
{'loss': 0.2357, 'grad_norm': 0.9242668747901917, 'learning_rate': 4.7362637362637364e-05, 'epoch': 5.3}
********************on step end call back********************
Step 29150 finish
{'loss': 0.2313, 'grad_norm': 1.5649456977844238, 'learning_rate': 4.734432234432234e-05, 'epoch': 5.3}
********************on step end call back********************
Step 29160 finish
{'loss': 0.2454, 'grad_norm': 1.4672253131866455, 'learning_rate': 4.732600732600733e-05, 'epoch': 5.3}
********************on step end call back********************
Step 29170 finish
{'loss': 0.2038, 'grad_norm': 1.2139602899551392, 'learning_rate': 4.730769230769231e-05, 'epoch': 5.3}
********************on step end call back********************
Step 29180 finish
{'loss': 0.2497, 'grad_norm': 1.5596040487289429, 'learning_rate': 4.728937728937729e-05, 'epoch': 5.3}
********************on step end call back********************
Step 29190 finish
{'loss': 0.2379, 'grad_norm': 1.1175501346588135, 'learning_rate': 4.727106227106227e-05, 'epoch': 5.31}
********************on step end call back********************
Step 29200 finish
{'loss': 0.2555, 'grad_norm': 1.1262338161468506, 'learning_rate': 4.7252747252747257e-05, 'epoch': 5.31}
{'eval_loss': 0.36029255390167236, 'eval_accuracy': 0.8854166666666666, 'eval_runtime': 128.7218, 'eval_samples_per_second': 4.956, 'eval_steps_per_second': 4.956, 'epoch': 5.31}
********************save call back********************
********************on step end call back********************
Step 29210 finish
{'loss': 0.2444, 'grad_norm': 0.9951896667480469, 'learning_rate': 4.7234432234432235e-05, 'epoch': 5.31}
********************on step end call back********************
Step 29220 finish
{'loss': 0.2458, 'grad_norm': 1.704854130744934, 'learning_rate': 4.7216117216117214e-05, 'epoch': 5.31}
********************on step end call back********************
Step 29230 finish
{'loss': 0.2616, 'grad_norm': 1.303827166557312, 'learning_rate': 4.71978021978022e-05, 'epoch': 5.31}
********************on step end call back********************
Step 29240 finish
{'loss': 0.2295, 'grad_norm': 1.0749456882476807, 'learning_rate': 4.717948717948718e-05, 'epoch': 5.32}
********************on step end call back********************
Step 29250 finish
{'loss': 0.2621, 'grad_norm': 1.4812307357788086, 'learning_rate': 4.7161172161172164e-05, 'epoch': 5.32}
********************on step end call back********************
Step 29260 finish
{'loss': 0.2571, 'grad_norm': 1.1364606618881226, 'learning_rate': 4.714285714285714e-05, 'epoch': 5.32}
********************on step end call back********************
Step 29270 finish
{'loss': 0.2427, 'grad_norm': 1.1407338380813599, 'learning_rate': 4.712454212454213e-05, 'epoch': 5.32}
********************on step end call back********************
Step 29280 finish
{'loss': 0.2392, 'grad_norm': 1.1880557537078857, 'learning_rate': 4.710622710622711e-05, 'epoch': 5.32}
********************on step end call back********************
Step 29290 finish
{'loss': 0.2571, 'grad_norm': 1.8318860530853271, 'learning_rate': 4.708791208791209e-05, 'epoch': 5.32}
********************on step end call back********************
Step 29300 finish
{'loss': 0.2313, 'grad_norm': 1.50970458984375, 'learning_rate': 4.706959706959707e-05, 'epoch': 5.33}
{'eval_loss': 0.36154085397720337, 'eval_accuracy': 0.8645833333333334, 'eval_runtime': 129.3573, 'eval_samples_per_second': 4.932, 'eval_steps_per_second': 4.932, 'epoch': 5.33}
********************save call back********************
********************on step end call back********************
Step 29310 finish
{'loss': 0.2145, 'grad_norm': 1.1530194282531738, 'learning_rate': 4.705128205128205e-05, 'epoch': 5.33}
********************on step end call back********************
Step 29320 finish
{'loss': 0.2248, 'grad_norm': 1.2270983457565308, 'learning_rate': 4.7032967032967035e-05, 'epoch': 5.33}
********************on step end call back********************
Step 29330 finish
{'loss': 0.2205, 'grad_norm': 1.3288393020629883, 'learning_rate': 4.7014652014652014e-05, 'epoch': 5.33}
********************on step end call back********************
Step 29340 finish
{'loss': 0.2387, 'grad_norm': 1.3993327617645264, 'learning_rate': 4.6996336996337e-05, 'epoch': 5.33}
********************on step end call back********************
Step 29350 finish
{'loss': 0.2449, 'grad_norm': 1.288896918296814, 'learning_rate': 4.697802197802198e-05, 'epoch': 5.34}
********************on step end call back********************
Step 29360 finish
{'loss': 0.2445, 'grad_norm': 1.4847040176391602, 'learning_rate': 4.6959706959706964e-05, 'epoch': 5.34}
********************on step end call back********************
Step 29370 finish
{'loss': 0.2288, 'grad_norm': 1.1756346225738525, 'learning_rate': 4.694139194139194e-05, 'epoch': 5.34}
********************on step end call back********************
Step 29380 finish
{'loss': 0.2345, 'grad_norm': 1.030025839805603, 'learning_rate': 4.692307692307693e-05, 'epoch': 5.34}
********************on step end call back********************
Step 29390 finish
{'loss': 0.234, 'grad_norm': 1.1936930418014526, 'learning_rate': 4.690476190476191e-05, 'epoch': 5.34}
********************on step end call back********************
Step 29400 finish
{'loss': 0.2521, 'grad_norm': 1.4882415533065796, 'learning_rate': 4.6886446886446886e-05, 'epoch': 5.34}
{'eval_loss': 0.3629353940486908, 'eval_accuracy': 0.8645833333333334, 'eval_runtime': 128.895, 'eval_samples_per_second': 4.95, 'eval_steps_per_second': 4.95, 'epoch': 5.34}
********************save call back********************
********************on step end call back********************
Step 29410 finish
{'loss': 0.2269, 'grad_norm': 1.358396053314209, 'learning_rate': 4.686813186813187e-05, 'epoch': 5.35}
********************on step end call back********************
Step 29420 finish
{'loss': 0.2324, 'grad_norm': 1.4669915437698364, 'learning_rate': 4.684981684981685e-05, 'epoch': 5.35}
********************on step end call back********************
Step 29430 finish
{'loss': 0.2287, 'grad_norm': 0.9177419543266296, 'learning_rate': 4.6831501831501836e-05, 'epoch': 5.35}
********************on step end call back********************
Step 29440 finish
{'loss': 0.2573, 'grad_norm': 1.523733139038086, 'learning_rate': 4.6813186813186814e-05, 'epoch': 5.35}
********************on step end call back********************
Step 29450 finish
{'loss': 0.2401, 'grad_norm': 1.343919277191162, 'learning_rate': 4.67948717948718e-05, 'epoch': 5.35}
********************on step end call back********************
Step 29460 finish
{'loss': 0.2399, 'grad_norm': 0.8211895823478699, 'learning_rate': 4.677655677655678e-05, 'epoch': 5.36}
********************on step end call back********************
Step 29470 finish
{'loss': 0.2221, 'grad_norm': 1.2078195810317993, 'learning_rate': 4.6758241758241764e-05, 'epoch': 5.36}
********************on step end call back********************
Step 29480 finish
{'loss': 0.2445, 'grad_norm': 1.1527570486068726, 'learning_rate': 4.673992673992674e-05, 'epoch': 5.36}
********************on step end call back********************
Step 29490 finish
{'loss': 0.2586, 'grad_norm': 1.3302862644195557, 'learning_rate': 4.672161172161172e-05, 'epoch': 5.36}
********************on step end call back********************
Step 29500 finish
{'loss': 0.2394, 'grad_norm': 0.9596624970436096, 'learning_rate': 4.670329670329671e-05, 'epoch': 5.36}
{'eval_loss': 0.36455237865448, 'eval_accuracy': 0.875, 'eval_runtime': 128.8303, 'eval_samples_per_second': 4.952, 'eval_steps_per_second': 4.952, 'epoch': 5.36}
********************save call back********************
********************on step end call back********************
Step 29510 finish
[INFO|trainer.py:3376] 2024-03-24 02:01:15,519 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-24 02:01:15,519 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-24 02:01:15,519 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-24 02:03:24,323 >> Saving model checkpoint to ./output/tmp-checkpoint-29600
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-24 02:03:24,462 >> tokenizer config file saved in ./output/tmp-checkpoint-29600/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-24 02:03:24,462 >> Special tokens file saved in ./output/tmp-checkpoint-29600/special_tokens_map.json
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-24 02:11:58,626 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-24 02:11:58,626 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-24 02:11:58,626 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-24 02:14:07,597 >> Saving model checkpoint to ./output/tmp-checkpoint-29700
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-24 02:14:07,739 >> tokenizer config file saved in ./output/tmp-checkpoint-29700/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-24 02:14:07,739 >> Special tokens file saved in ./output/tmp-checkpoint-29700/special_tokens_map.json
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-24 02:22:44,469 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-24 02:22:44,470 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-24 02:22:44,470 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-24 02:24:53,383 >> Saving model checkpoint to ./output/tmp-checkpoint-29800
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-24 02:24:53,522 >> tokenizer config file saved in ./output/tmp-checkpoint-29800/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-24 02:24:53,522 >> Special tokens file saved in ./output/tmp-checkpoint-29800/special_tokens_map.json
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-24 02:33:33,314 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-24 02:33:33,314 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-24 02:33:33,314 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-24 02:35:41,966 >> Saving model checkpoint to ./output/tmp-checkpoint-29900
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-24 02:35:42,103 >> tokenizer config file saved in ./output/tmp-checkpoint-29900/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-24 02:35:42,103 >> Special tokens file saved in ./output/tmp-checkpoint-29900/special_tokens_map.json
{'loss': 0.2369, 'grad_norm': 1.2830835580825806, 'learning_rate': 4.6684981684981686e-05, 'epoch': 5.36}
********************on step end call back********************
Step 29520 finish
{'loss': 0.2372, 'grad_norm': 1.0454844236373901, 'learning_rate': 4.666666666666667e-05, 'epoch': 5.37}
********************on step end call back********************
Step 29530 finish
{'loss': 0.2461, 'grad_norm': 1.2031443119049072, 'learning_rate': 4.664835164835165e-05, 'epoch': 5.37}
********************on step end call back********************
Step 29540 finish
{'loss': 0.2309, 'grad_norm': 1.1676150560379028, 'learning_rate': 4.6630036630036636e-05, 'epoch': 5.37}
********************on step end call back********************
Step 29550 finish
{'loss': 0.2457, 'grad_norm': 1.2299118041992188, 'learning_rate': 4.6611721611721615e-05, 'epoch': 5.37}
********************on step end call back********************
Step 29560 finish
{'loss': 0.2508, 'grad_norm': 1.1636238098144531, 'learning_rate': 4.6593406593406593e-05, 'epoch': 5.37}
********************on step end call back********************
Step 29570 finish
{'loss': 0.2548, 'grad_norm': 1.3124514818191528, 'learning_rate': 4.657509157509158e-05, 'epoch': 5.38}
********************on step end call back********************
Step 29580 finish
{'loss': 0.2331, 'grad_norm': 1.1099883317947388, 'learning_rate': 4.655677655677656e-05, 'epoch': 5.38}
********************on step end call back********************
Step 29590 finish
{'loss': 0.2265, 'grad_norm': 1.287473201751709, 'learning_rate': 4.653846153846154e-05, 'epoch': 5.38}
********************on step end call back********************
Step 29600 finish
{'loss': 0.2506, 'grad_norm': 1.4289422035217285, 'learning_rate': 4.652014652014652e-05, 'epoch': 5.38}
{'eval_loss': 0.3597256541252136, 'eval_accuracy': 0.8958333333333334, 'eval_runtime': 128.8035, 'eval_samples_per_second': 4.953, 'eval_steps_per_second': 4.953, 'epoch': 5.38}
********************save call back********************
********************on step end call back********************
Step 29610 finish
{'loss': 0.2575, 'grad_norm': 1.1300873756408691, 'learning_rate': 4.650183150183151e-05, 'epoch': 5.38}
********************on step end call back********************
Step 29620 finish
{'loss': 0.2378, 'grad_norm': 1.3761072158813477, 'learning_rate': 4.6483516483516486e-05, 'epoch': 5.38}
********************on step end call back********************
Step 29630 finish
{'loss': 0.2205, 'grad_norm': 1.296121597290039, 'learning_rate': 4.646520146520147e-05, 'epoch': 5.39}
********************on step end call back********************
Step 29640 finish
{'loss': 0.2347, 'grad_norm': 1.3904186487197876, 'learning_rate': 4.644688644688645e-05, 'epoch': 5.39}
********************on step end call back********************
Step 29650 finish
{'loss': 0.1922, 'grad_norm': 1.1049644947052002, 'learning_rate': 4.642857142857143e-05, 'epoch': 5.39}
********************on step end call back********************
Step 29660 finish
{'loss': 0.243, 'grad_norm': 1.1999475955963135, 'learning_rate': 4.6410256410256415e-05, 'epoch': 5.39}
********************on step end call back********************
Step 29670 finish
{'loss': 0.204, 'grad_norm': 1.3160361051559448, 'learning_rate': 4.6391941391941394e-05, 'epoch': 5.39}
********************on step end call back********************
Step 29680 finish
{'loss': 0.2544, 'grad_norm': 1.1939499378204346, 'learning_rate': 4.637362637362638e-05, 'epoch': 5.4}
********************on step end call back********************
Step 29690 finish
{'loss': 0.2087, 'grad_norm': 1.1842085123062134, 'learning_rate': 4.635531135531136e-05, 'epoch': 5.4}
********************on step end call back********************
Step 29700 finish
{'loss': 0.2301, 'grad_norm': 1.26920485496521, 'learning_rate': 4.6336996336996343e-05, 'epoch': 5.4}
{'eval_loss': 0.369799941778183, 'eval_accuracy': 0.8854166666666666, 'eval_runtime': 128.9699, 'eval_samples_per_second': 4.947, 'eval_steps_per_second': 4.947, 'epoch': 5.4}
********************save call back********************
********************on step end call back********************
Step 29710 finish
{'loss': 0.2269, 'grad_norm': 1.150817632675171, 'learning_rate': 4.631868131868132e-05, 'epoch': 5.4}
********************on step end call back********************
Step 29720 finish
{'loss': 0.2302, 'grad_norm': 1.6402872800827026, 'learning_rate': 4.630036630036631e-05, 'epoch': 5.4}
********************on step end call back********************
Step 29730 finish
{'loss': 0.2289, 'grad_norm': 1.1015926599502563, 'learning_rate': 4.6282051282051287e-05, 'epoch': 5.4}
********************on step end call back********************
Step 29740 finish
{'loss': 0.2362, 'grad_norm': 1.3067340850830078, 'learning_rate': 4.6263736263736265e-05, 'epoch': 5.41}
********************on step end call back********************
Step 29750 finish
{'loss': 0.2421, 'grad_norm': 1.010596752166748, 'learning_rate': 4.624542124542125e-05, 'epoch': 5.41}
********************on step end call back********************
Step 29760 finish
{'loss': 0.2575, 'grad_norm': 1.3745697736740112, 'learning_rate': 4.622710622710623e-05, 'epoch': 5.41}
********************on step end call back********************
Step 29770 finish
{'loss': 0.2399, 'grad_norm': 1.294682502746582, 'learning_rate': 4.6208791208791215e-05, 'epoch': 5.41}
********************on step end call back********************
Step 29780 finish
{'loss': 0.2574, 'grad_norm': 1.32339346408844, 'learning_rate': 4.6190476190476194e-05, 'epoch': 5.41}
********************on step end call back********************
Step 29790 finish
{'loss': 0.2493, 'grad_norm': 1.7817734479904175, 'learning_rate': 4.617216117216118e-05, 'epoch': 5.42}
********************on step end call back********************
Step 29800 finish
{'loss': 0.2464, 'grad_norm': 1.4423470497131348, 'learning_rate': 4.615384615384616e-05, 'epoch': 5.42}
{'eval_loss': 0.3645722568035126, 'eval_accuracy': 0.875, 'eval_runtime': 128.9127, 'eval_samples_per_second': 4.949, 'eval_steps_per_second': 4.949, 'epoch': 5.42}
********************save call back********************
********************on step end call back********************
Step 29810 finish
{'loss': 0.2107, 'grad_norm': 1.1866518259048462, 'learning_rate': 4.6135531135531144e-05, 'epoch': 5.42}
********************on step end call back********************
Step 29820 finish
{'loss': 0.2726, 'grad_norm': 1.518038272857666, 'learning_rate': 4.611721611721612e-05, 'epoch': 5.42}
********************on step end call back********************
Step 29830 finish
{'loss': 0.2329, 'grad_norm': 1.1832685470581055, 'learning_rate': 4.60989010989011e-05, 'epoch': 5.42}
********************on step end call back********************
Step 29840 finish
{'loss': 0.2452, 'grad_norm': 1.4655678272247314, 'learning_rate': 4.608058608058608e-05, 'epoch': 5.42}
********************on step end call back********************
Step 29850 finish
{'loss': 0.2663, 'grad_norm': 1.4121333360671997, 'learning_rate': 4.606227106227106e-05, 'epoch': 5.43}
********************on step end call back********************
Step 29860 finish
{'loss': 0.2654, 'grad_norm': 1.1550225019454956, 'learning_rate': 4.6043956043956044e-05, 'epoch': 5.43}
********************on step end call back********************
Step 29870 finish
{'loss': 0.2572, 'grad_norm': 1.124957799911499, 'learning_rate': 4.602564102564102e-05, 'epoch': 5.43}
********************on step end call back********************
Step 29880 finish
{'loss': 0.2448, 'grad_norm': 1.2456897497177124, 'learning_rate': 4.600732600732601e-05, 'epoch': 5.43}
********************on step end call back********************
Step 29890 finish
{'loss': 0.2467, 'grad_norm': 1.1198416948318481, 'learning_rate': 4.598901098901099e-05, 'epoch': 5.43}
********************on step end call back********************
Step 29900 finish
{'loss': 0.2562, 'grad_norm': 1.478332281112671, 'learning_rate': 4.597069597069597e-05, 'epoch': 5.44}
{'eval_loss': 0.35724905133247375, 'eval_accuracy': 0.875, 'eval_runtime': 128.651, 'eval_samples_per_second': 4.959, 'eval_steps_per_second': 4.959, 'epoch': 5.44}
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-24 02:44:23,812 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-24 02:44:23,812 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-24 02:44:23,812 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-24 02:46:32,636 >> Saving model checkpoint to ./output/tmp-checkpoint-30000
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-24 02:46:32,773 >> tokenizer config file saved in ./output/tmp-checkpoint-30000/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-24 02:46:32,774 >> Special tokens file saved in ./output/tmp-checkpoint-30000/special_tokens_map.json
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-24 02:55:04,789 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-24 02:55:04,789 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-24 02:55:04,789 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-24 02:57:13,712 >> Saving model checkpoint to ./output/tmp-checkpoint-30100
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-24 02:57:13,850 >> tokenizer config file saved in ./output/tmp-checkpoint-30100/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-24 02:57:13,850 >> Special tokens file saved in ./output/tmp-checkpoint-30100/special_tokens_map.json
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-24 03:05:51,281 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-24 03:05:51,282 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-24 03:05:51,282 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-24 03:08:00,177 >> Saving model checkpoint to ./output/tmp-checkpoint-30200
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-24 03:08:00,318 >> tokenizer config file saved in ./output/tmp-checkpoint-30200/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-24 03:08:00,318 >> Special tokens file saved in ./output/tmp-checkpoint-30200/special_tokens_map.json
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-24 03:16:32,989 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-24 03:16:32,989 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-24 03:16:32,989 >>   Batch size = 1
********************save call back********************
********************on step end call back********************
Step 29910 finish
{'loss': 0.2517, 'grad_norm': 1.2449002265930176, 'learning_rate': 4.595238095238095e-05, 'epoch': 5.44}
********************on step end call back********************
Step 29920 finish
{'loss': 0.2485, 'grad_norm': 1.1844511032104492, 'learning_rate': 4.593406593406594e-05, 'epoch': 5.44}
********************on step end call back********************
Step 29930 finish
{'loss': 0.2508, 'grad_norm': 1.1705608367919922, 'learning_rate': 4.5915750915750916e-05, 'epoch': 5.44}
********************on step end call back********************
Step 29940 finish
{'loss': 0.2277, 'grad_norm': 1.1955187320709229, 'learning_rate': 4.5897435897435895e-05, 'epoch': 5.44}
********************on step end call back********************
Step 29950 finish
{'loss': 0.2199, 'grad_norm': 1.1382588148117065, 'learning_rate': 4.587912087912088e-05, 'epoch': 5.44}
********************on step end call back********************
Step 29960 finish
{'loss': 0.2455, 'grad_norm': 1.5388526916503906, 'learning_rate': 4.586080586080586e-05, 'epoch': 5.45}
********************on step end call back********************
Step 29970 finish
{'loss': 0.2338, 'grad_norm': 1.3261605501174927, 'learning_rate': 4.5842490842490844e-05, 'epoch': 5.45}
********************on step end call back********************
Step 29980 finish
{'loss': 0.2515, 'grad_norm': 1.4055148363113403, 'learning_rate': 4.582417582417582e-05, 'epoch': 5.45}
********************on step end call back********************
Step 29990 finish
{'loss': 0.2673, 'grad_norm': 1.1692856550216675, 'learning_rate': 4.580586080586081e-05, 'epoch': 5.45}
********************on step end call back********************
Step 30000 finish
{'loss': 0.2635, 'grad_norm': 1.2598973512649536, 'learning_rate': 4.578754578754579e-05, 'epoch': 5.45}
{'eval_loss': 0.361917108297348, 'eval_accuracy': 0.875, 'eval_runtime': 128.8235, 'eval_samples_per_second': 4.953, 'eval_steps_per_second': 4.953, 'epoch': 5.45}
********************save call back********************
********************on step end call back********************
Step 30010 finish
{'loss': 0.2477, 'grad_norm': 1.526053547859192, 'learning_rate': 4.576923076923077e-05, 'epoch': 5.46}
********************on step end call back********************
Step 30020 finish
{'loss': 0.2536, 'grad_norm': 1.5806690454483032, 'learning_rate': 4.575091575091575e-05, 'epoch': 5.46}
********************on step end call back********************
Step 30030 finish
{'loss': 0.2627, 'grad_norm': 1.3578397035598755, 'learning_rate': 4.573260073260073e-05, 'epoch': 5.46}
********************on step end call back********************
Step 30040 finish
{'loss': 0.2545, 'grad_norm': 1.0118186473846436, 'learning_rate': 4.5714285714285716e-05, 'epoch': 5.46}
********************on step end call back********************
Step 30050 finish
{'loss': 0.2284, 'grad_norm': 1.2366700172424316, 'learning_rate': 4.5695970695970695e-05, 'epoch': 5.46}
********************on step end call back********************
Step 30060 finish
{'loss': 0.2313, 'grad_norm': 1.4474637508392334, 'learning_rate': 4.567765567765568e-05, 'epoch': 5.46}
********************on step end call back********************
Step 30070 finish
{'loss': 0.2336, 'grad_norm': 1.3144207000732422, 'learning_rate': 4.565934065934066e-05, 'epoch': 5.47}
********************on step end call back********************
Step 30080 finish
{'loss': 0.2604, 'grad_norm': 1.2279354333877563, 'learning_rate': 4.5641025641025645e-05, 'epoch': 5.47}
********************on step end call back********************
Step 30090 finish
{'loss': 0.2211, 'grad_norm': 1.491405725479126, 'learning_rate': 4.5622710622710623e-05, 'epoch': 5.47}
********************on step end call back********************
Step 30100 finish
{'loss': 0.2463, 'grad_norm': 1.4195152521133423, 'learning_rate': 4.56043956043956e-05, 'epoch': 5.47}
{'eval_loss': 0.3577573597431183, 'eval_accuracy': 0.875, 'eval_runtime': 128.922, 'eval_samples_per_second': 4.949, 'eval_steps_per_second': 4.949, 'epoch': 5.47}
********************save call back********************
********************on step end call back********************
Step 30110 finish
{'loss': 0.2264, 'grad_norm': 1.1434239149093628, 'learning_rate': 4.558608058608059e-05, 'epoch': 5.47}
********************on step end call back********************
Step 30120 finish
{'loss': 0.2511, 'grad_norm': 1.4223703145980835, 'learning_rate': 4.5567765567765566e-05, 'epoch': 5.48}
********************on step end call back********************
Step 30130 finish
{'loss': 0.2374, 'grad_norm': 1.3463873863220215, 'learning_rate': 4.554945054945055e-05, 'epoch': 5.48}
********************on step end call back********************
Step 30140 finish
{'loss': 0.2508, 'grad_norm': 1.0151348114013672, 'learning_rate': 4.553113553113553e-05, 'epoch': 5.48}
********************on step end call back********************
Step 30150 finish
{'loss': 0.2375, 'grad_norm': 1.4906282424926758, 'learning_rate': 4.5512820512820516e-05, 'epoch': 5.48}
********************on step end call back********************
Step 30160 finish
{'loss': 0.2383, 'grad_norm': 1.2508105039596558, 'learning_rate': 4.5494505494505495e-05, 'epoch': 5.48}
********************on step end call back********************
Step 30170 finish
{'loss': 0.258, 'grad_norm': 1.5963499546051025, 'learning_rate': 4.547619047619048e-05, 'epoch': 5.48}
********************on step end call back********************
Step 30180 finish
{'loss': 0.2116, 'grad_norm': 1.2020478248596191, 'learning_rate': 4.545787545787546e-05, 'epoch': 5.49}
********************on step end call back********************
Step 30190 finish
{'loss': 0.2122, 'grad_norm': 0.6988877654075623, 'learning_rate': 4.543956043956044e-05, 'epoch': 5.49}
********************on step end call back********************
Step 30200 finish
{'loss': 0.2063, 'grad_norm': 1.3230783939361572, 'learning_rate': 4.5421245421245424e-05, 'epoch': 5.49}
{'eval_loss': 0.36249083280563354, 'eval_accuracy': 0.8854166666666666, 'eval_runtime': 128.895, 'eval_samples_per_second': 4.95, 'eval_steps_per_second': 4.95, 'epoch': 5.49}
********************save call back********************
********************on step end call back********************
Step 30210 finish
{'loss': 0.2666, 'grad_norm': 1.7066614627838135, 'learning_rate': 4.54029304029304e-05, 'epoch': 5.49}
********************on step end call back********************
Step 30220 finish
{'loss': 0.269, 'grad_norm': 1.4419794082641602, 'learning_rate': 4.538461538461539e-05, 'epoch': 5.49}
********************on step end call back********************
Step 30230 finish
{'loss': 0.2391, 'grad_norm': 1.287662386894226, 'learning_rate': 4.536630036630037e-05, 'epoch': 5.5}
********************on step end call back********************
Step 30240 finish
{'loss': 0.2504, 'grad_norm': 1.5137965679168701, 'learning_rate': 4.534798534798535e-05, 'epoch': 5.5}
********************on step end call back********************
Step 30250 finish
{'loss': 0.1923, 'grad_norm': 1.0352691411972046, 'learning_rate': 4.532967032967033e-05, 'epoch': 5.5}
********************on step end call back********************
Step 30260 finish
{'loss': 0.2352, 'grad_norm': 1.1526641845703125, 'learning_rate': 4.5311355311355317e-05, 'epoch': 5.5}
********************on step end call back********************
Step 30270 finish
{'loss': 0.2163, 'grad_norm': 1.3627830743789673, 'learning_rate': 4.5293040293040295e-05, 'epoch': 5.5}
********************on step end call back********************
Step 30280 finish
{'loss': 0.2302, 'grad_norm': 1.7318390607833862, 'learning_rate': 4.5274725274725274e-05, 'epoch': 5.5}
********************on step end call back********************
Step 30290 finish
{'loss': 0.2497, 'grad_norm': 1.0913835763931274, 'learning_rate': 4.525641025641026e-05, 'epoch': 5.51}
********************on step end call back********************
Step 30300 finish
{'loss': 0.2497, 'grad_norm': 1.3905136585235596, 'learning_rate': 4.523809523809524e-05, 'epoch': 5.51}
[INFO|trainer.py:3067] 2024-03-24 03:18:41,812 >> Saving model checkpoint to ./output/tmp-checkpoint-30300
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-24 03:18:41,953 >> tokenizer config file saved in ./output/tmp-checkpoint-30300/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-24 03:18:41,953 >> Special tokens file saved in ./output/tmp-checkpoint-30300/special_tokens_map.json
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-24 03:27:07,739 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-24 03:27:07,739 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-24 03:27:07,739 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-24 03:29:16,539 >> Saving model checkpoint to ./output/tmp-checkpoint-30400
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-24 03:29:16,680 >> tokenizer config file saved in ./output/tmp-checkpoint-30400/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-24 03:29:16,681 >> Special tokens file saved in ./output/tmp-checkpoint-30400/special_tokens_map.json
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-24 03:37:49,660 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-24 03:37:49,660 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-24 03:37:49,660 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-24 03:39:58,551 >> Saving model checkpoint to ./output/tmp-checkpoint-30500
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-24 03:39:58,693 >> tokenizer config file saved in ./output/tmp-checkpoint-30500/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-24 03:39:58,693 >> Special tokens file saved in ./output/tmp-checkpoint-30500/special_tokens_map.json
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-24 03:48:42,953 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-24 03:48:42,953 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-24 03:48:42,953 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-24 03:50:51,617 >> Saving model checkpoint to ./output/tmp-checkpoint-30600
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-24 03:50:51,755 >> tokenizer config file saved in ./output/tmp-checkpoint-30600/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-24 03:50:51,755 >> Special tokens file saved in ./output/tmp-checkpoint-30600/special_tokens_map.json
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
{'eval_loss': 0.3670978248119354, 'eval_accuracy': 0.875, 'eval_runtime': 128.822, 'eval_samples_per_second': 4.953, 'eval_steps_per_second': 4.953, 'epoch': 5.51}
********************save call back********************
********************on step end call back********************
Step 30310 finish
{'loss': 0.2383, 'grad_norm': 0.9468110203742981, 'learning_rate': 4.5219780219780224e-05, 'epoch': 5.51}
********************on step end call back********************
Step 30320 finish
{'loss': 0.2336, 'grad_norm': 1.2875707149505615, 'learning_rate': 4.52014652014652e-05, 'epoch': 5.51}
********************on step end call back********************
Step 30330 finish
{'loss': 0.2255, 'grad_norm': 1.2932428121566772, 'learning_rate': 4.518315018315019e-05, 'epoch': 5.51}
********************on step end call back********************
Step 30340 finish
{'loss': 0.2096, 'grad_norm': 1.2422791719436646, 'learning_rate': 4.516483516483517e-05, 'epoch': 5.52}
********************on step end call back********************
Step 30350 finish
{'loss': 0.2479, 'grad_norm': 1.0568304061889648, 'learning_rate': 4.514652014652015e-05, 'epoch': 5.52}
********************on step end call back********************
Step 30360 finish
{'loss': 0.2247, 'grad_norm': 1.1996784210205078, 'learning_rate': 4.512820512820513e-05, 'epoch': 5.52}
********************on step end call back********************
Step 30370 finish
{'loss': 0.221, 'grad_norm': 0.949616014957428, 'learning_rate': 4.510989010989011e-05, 'epoch': 5.52}
********************on step end call back********************
Step 30380 finish
{'loss': 0.2236, 'grad_norm': 1.5548204183578491, 'learning_rate': 4.5091575091575095e-05, 'epoch': 5.52}
********************on step end call back********************
Step 30390 finish
{'loss': 0.2552, 'grad_norm': 1.2883570194244385, 'learning_rate': 4.5073260073260074e-05, 'epoch': 5.52}
********************on step end call back********************
Step 30400 finish
{'loss': 0.2896, 'grad_norm': 1.4081612825393677, 'learning_rate': 4.505494505494506e-05, 'epoch': 5.53}
{'eval_loss': 0.3570282757282257, 'eval_accuracy': 0.875, 'eval_runtime': 128.7993, 'eval_samples_per_second': 4.953, 'eval_steps_per_second': 4.953, 'epoch': 5.53}
********************save call back********************
********************on step end call back********************
Step 30410 finish
{'loss': 0.2504, 'grad_norm': 1.2676787376403809, 'learning_rate': 4.503663003663004e-05, 'epoch': 5.53}
********************on step end call back********************
Step 30420 finish
{'loss': 0.2371, 'grad_norm': 1.14483642578125, 'learning_rate': 4.5018315018315024e-05, 'epoch': 5.53}
********************on step end call back********************
Step 30430 finish
{'loss': 0.269, 'grad_norm': 1.2956080436706543, 'learning_rate': 4.5e-05, 'epoch': 5.53}
********************on step end call back********************
Step 30440 finish
{'loss': 0.244, 'grad_norm': 1.298987627029419, 'learning_rate': 4.498168498168499e-05, 'epoch': 5.53}
********************on step end call back********************
Step 30450 finish
{'loss': 0.2568, 'grad_norm': 1.213154911994934, 'learning_rate': 4.496336996336997e-05, 'epoch': 5.54}
********************on step end call back********************
Step 30460 finish
{'loss': 0.242, 'grad_norm': 1.1530735492706299, 'learning_rate': 4.4945054945054946e-05, 'epoch': 5.54}
********************on step end call back********************
Step 30470 finish
{'loss': 0.2355, 'grad_norm': 1.3955273628234863, 'learning_rate': 4.492673992673993e-05, 'epoch': 5.54}
********************on step end call back********************
Step 30480 finish
{'loss': 0.2441, 'grad_norm': 1.1158006191253662, 'learning_rate': 4.490842490842491e-05, 'epoch': 5.54}
********************on step end call back********************
Step 30490 finish
{'loss': 0.2329, 'grad_norm': 1.4225645065307617, 'learning_rate': 4.4890109890109896e-05, 'epoch': 5.54}
********************on step end call back********************
Step 30500 finish
{'loss': 0.2402, 'grad_norm': 1.1809990406036377, 'learning_rate': 4.4871794871794874e-05, 'epoch': 5.54}
{'eval_loss': 0.36138439178466797, 'eval_accuracy': 0.875, 'eval_runtime': 128.8901, 'eval_samples_per_second': 4.95, 'eval_steps_per_second': 4.95, 'epoch': 5.54}
********************save call back********************
********************on step end call back********************
Step 30510 finish
{'loss': 0.269, 'grad_norm': 1.1331889629364014, 'learning_rate': 4.485347985347986e-05, 'epoch': 5.55}
********************on step end call back********************
Step 30520 finish
{'loss': 0.2404, 'grad_norm': 1.2064956426620483, 'learning_rate': 4.483516483516484e-05, 'epoch': 5.55}
********************on step end call back********************
Step 30530 finish
{'loss': 0.2793, 'grad_norm': 1.3579541444778442, 'learning_rate': 4.481684981684982e-05, 'epoch': 5.55}
********************on step end call back********************
Step 30540 finish
{'loss': 0.2645, 'grad_norm': 1.2126209735870361, 'learning_rate': 4.47985347985348e-05, 'epoch': 5.55}
********************on step end call back********************
Step 30550 finish
{'loss': 0.2157, 'grad_norm': 1.0759059190750122, 'learning_rate': 4.478021978021978e-05, 'epoch': 5.55}
********************on step end call back********************
Step 30560 finish
{'loss': 0.2682, 'grad_norm': 1.3031835556030273, 'learning_rate': 4.476190476190477e-05, 'epoch': 5.56}
********************on step end call back********************
Step 30570 finish
{'loss': 0.2472, 'grad_norm': 1.3550114631652832, 'learning_rate': 4.4743589743589746e-05, 'epoch': 5.56}
********************on step end call back********************
Step 30580 finish
{'loss': 0.2478, 'grad_norm': 1.2594435214996338, 'learning_rate': 4.472527472527473e-05, 'epoch': 5.56}
********************on step end call back********************
Step 30590 finish
{'loss': 0.2439, 'grad_norm': 1.3240734338760376, 'learning_rate': 4.470695970695971e-05, 'epoch': 5.56}
********************on step end call back********************
Step 30600 finish
{'loss': 0.2561, 'grad_norm': 1.4481971263885498, 'learning_rate': 4.4688644688644696e-05, 'epoch': 5.56}
{'eval_loss': 0.3597513437271118, 'eval_accuracy': 0.875, 'eval_runtime': 128.6628, 'eval_samples_per_second': 4.959, 'eval_steps_per_second': 4.959, 'epoch': 5.56}
********************save call back********************
********************on step end call back********************
Step 30610 finish
{'loss': 0.2403, 'grad_norm': 1.2632862329483032, 'learning_rate': 4.4670329670329675e-05, 'epoch': 5.56}
********************on step end call back********************
Step 30620 finish
{'loss': 0.2291, 'grad_norm': 1.1229583024978638, 'learning_rate': 4.4652014652014653e-05, 'epoch': 5.57}
********************on step end call back********************
Step 30630 finish
{'loss': 0.2206, 'grad_norm': 1.1394823789596558, 'learning_rate': 4.463369963369964e-05, 'epoch': 5.57}
********************on step end call back********************
Step 30640 finish
{'loss': 0.2402, 'grad_norm': 1.5499017238616943, 'learning_rate': 4.461538461538462e-05, 'epoch': 5.57}
********************on step end call back********************
Step 30650 finish
{'loss': 0.224, 'grad_norm': 1.004549503326416, 'learning_rate': 4.45970695970696e-05, 'epoch': 5.57}
********************on step end call back********************
Step 30660 finish
{'loss': 0.2634, 'grad_norm': 1.1897164583206177, 'learning_rate': 4.457875457875458e-05, 'epoch': 5.57}
********************on step end call back********************
Step 30670 finish
{'loss': 0.2507, 'grad_norm': 1.3469551801681519, 'learning_rate': 4.456043956043957e-05, 'epoch': 5.58}
********************on step end call back********************
Step 30680 finish
{'loss': 0.2458, 'grad_norm': 1.4084455966949463, 'learning_rate': 4.4542124542124546e-05, 'epoch': 5.58}
********************on step end call back********************
Step 30690 finish
{'loss': 0.2623, 'grad_norm': 1.720502495765686, 'learning_rate': 4.4523809523809525e-05, 'epoch': 5.58}
********************on step end call back********************
Step 30700 finish
[INFO|trainer.py:3376] 2024-03-24 03:59:17,250 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-24 03:59:17,250 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-24 03:59:17,250 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-24 04:01:26,106 >> Saving model checkpoint to ./output/tmp-checkpoint-30700
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-24 04:01:26,263 >> tokenizer config file saved in ./output/tmp-checkpoint-30700/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-24 04:01:26,263 >> Special tokens file saved in ./output/tmp-checkpoint-30700/special_tokens_map.json
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-24 04:09:51,815 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-24 04:09:51,815 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-24 04:09:51,815 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-24 04:12:00,429 >> Saving model checkpoint to ./output/tmp-checkpoint-30800
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-24 04:12:00,587 >> tokenizer config file saved in ./output/tmp-checkpoint-30800/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-24 04:12:00,587 >> Special tokens file saved in ./output/tmp-checkpoint-30800/special_tokens_map.json
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-24 04:20:34,390 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-24 04:20:34,390 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-24 04:20:34,390 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-24 04:22:43,272 >> Saving model checkpoint to ./output/tmp-checkpoint-30900
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-24 04:22:43,411 >> tokenizer config file saved in ./output/tmp-checkpoint-30900/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-24 04:22:43,411 >> Special tokens file saved in ./output/tmp-checkpoint-30900/special_tokens_map.json
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-24 04:31:18,462 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-24 04:31:18,462 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-24 04:31:18,462 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-24 04:33:27,503 >> Saving model checkpoint to ./output/tmp-checkpoint-31000
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-24 04:33:27,650 >> tokenizer config file saved in ./output/tmp-checkpoint-31000/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-24 04:33:27,650 >> Special tokens file saved in ./output/tmp-checkpoint-31000/special_tokens_map.json
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
{'loss': 0.2437, 'grad_norm': 1.376821517944336, 'learning_rate': 4.4505494505494504e-05, 'epoch': 5.58}
{'eval_loss': 0.35713285207748413, 'eval_accuracy': 0.875, 'eval_runtime': 128.8547, 'eval_samples_per_second': 4.951, 'eval_steps_per_second': 4.951, 'epoch': 5.58}
********************save call back********************
********************on step end call back********************
Step 30710 finish
{'loss': 0.2574, 'grad_norm': 1.2388359308242798, 'learning_rate': 4.448717948717949e-05, 'epoch': 5.58}
********************on step end call back********************
Step 30720 finish
{'loss': 0.2384, 'grad_norm': 1.1323163509368896, 'learning_rate': 4.446886446886447e-05, 'epoch': 5.58}
********************on step end call back********************
Step 30730 finish
{'loss': 0.2541, 'grad_norm': 1.5825417041778564, 'learning_rate': 4.445054945054945e-05, 'epoch': 5.59}
********************on step end call back********************
Step 30740 finish
{'loss': 0.2305, 'grad_norm': 1.168468952178955, 'learning_rate': 4.443223443223443e-05, 'epoch': 5.59}
********************on step end call back********************
Step 30750 finish
{'loss': 0.2644, 'grad_norm': 1.5732828378677368, 'learning_rate': 4.441391941391941e-05, 'epoch': 5.59}
********************on step end call back********************
Step 30760 finish
{'loss': 0.2398, 'grad_norm': 1.2926710844039917, 'learning_rate': 4.43956043956044e-05, 'epoch': 5.59}
********************on step end call back********************
Step 30770 finish
{'loss': 0.2515, 'grad_norm': 1.0149067640304565, 'learning_rate': 4.4377289377289375e-05, 'epoch': 5.59}
********************on step end call back********************
Step 30780 finish
{'loss': 0.238, 'grad_norm': 1.250846266746521, 'learning_rate': 4.435897435897436e-05, 'epoch': 5.6}
********************on step end call back********************
Step 30790 finish
{'loss': 0.2724, 'grad_norm': 1.2470005750656128, 'learning_rate': 4.434065934065934e-05, 'epoch': 5.6}
********************on step end call back********************
Step 30800 finish
{'loss': 0.2435, 'grad_norm': 1.101070761680603, 'learning_rate': 4.4322344322344325e-05, 'epoch': 5.6}
{'eval_loss': 0.3576939105987549, 'eval_accuracy': 0.8645833333333334, 'eval_runtime': 128.6134, 'eval_samples_per_second': 4.961, 'eval_steps_per_second': 4.961, 'epoch': 5.6}
********************save call back********************
********************on step end call back********************
Step 30810 finish
{'loss': 0.2529, 'grad_norm': 1.2131688594818115, 'learning_rate': 4.4304029304029304e-05, 'epoch': 5.6}
********************on step end call back********************
Step 30820 finish
{'loss': 0.2208, 'grad_norm': 1.3527129888534546, 'learning_rate': 4.428571428571428e-05, 'epoch': 5.6}
********************on step end call back********************
Step 30830 finish
{'loss': 0.2504, 'grad_norm': 1.2290998697280884, 'learning_rate': 4.426739926739927e-05, 'epoch': 5.6}
********************on step end call back********************
Step 30840 finish
{'loss': 0.2517, 'grad_norm': 1.2538542747497559, 'learning_rate': 4.424908424908425e-05, 'epoch': 5.61}
********************on step end call back********************
Step 30850 finish
{'loss': 0.2131, 'grad_norm': 1.7041748762130737, 'learning_rate': 4.423076923076923e-05, 'epoch': 5.61}
********************on step end call back********************
Step 30860 finish
{'loss': 0.2155, 'grad_norm': 1.1478543281555176, 'learning_rate': 4.421245421245421e-05, 'epoch': 5.61}
********************on step end call back********************
Step 30870 finish
{'loss': 0.2365, 'grad_norm': 1.0689356327056885, 'learning_rate': 4.41941391941392e-05, 'epoch': 5.61}
********************on step end call back********************
Step 30880 finish
{'loss': 0.234, 'grad_norm': 1.4304546117782593, 'learning_rate': 4.4175824175824176e-05, 'epoch': 5.61}
********************on step end call back********************
Step 30890 finish
{'loss': 0.2499, 'grad_norm': 1.4749501943588257, 'learning_rate': 4.415750915750916e-05, 'epoch': 5.62}
********************on step end call back********************
Step 30900 finish
{'loss': 0.2548, 'grad_norm': 1.2564102411270142, 'learning_rate': 4.413919413919414e-05, 'epoch': 5.62}
{'eval_loss': 0.3556043207645416, 'eval_accuracy': 0.8645833333333334, 'eval_runtime': 128.8809, 'eval_samples_per_second': 4.95, 'eval_steps_per_second': 4.95, 'epoch': 5.62}
********************save call back********************
********************on step end call back********************
Step 30910 finish
{'loss': 0.2445, 'grad_norm': 1.2578599452972412, 'learning_rate': 4.412087912087912e-05, 'epoch': 5.62}
********************on step end call back********************
Step 30920 finish
{'loss': 0.2197, 'grad_norm': 1.1839518547058105, 'learning_rate': 4.4102564102564104e-05, 'epoch': 5.62}
********************on step end call back********************
Step 30930 finish
{'loss': 0.267, 'grad_norm': 1.0938186645507812, 'learning_rate': 4.408424908424908e-05, 'epoch': 5.62}
********************on step end call back********************
Step 30940 finish
{'loss': 0.2657, 'grad_norm': 1.3307698965072632, 'learning_rate': 4.406593406593407e-05, 'epoch': 5.62}
********************on step end call back********************
Step 30950 finish
{'loss': 0.2258, 'grad_norm': 1.2383520603179932, 'learning_rate': 4.404761904761905e-05, 'epoch': 5.63}
********************on step end call back********************
Step 30960 finish
{'loss': 0.2259, 'grad_norm': 1.4605878591537476, 'learning_rate': 4.402930402930403e-05, 'epoch': 5.63}
********************on step end call back********************
Step 30970 finish
{'loss': 0.212, 'grad_norm': 1.4320992231369019, 'learning_rate': 4.401098901098901e-05, 'epoch': 5.63}
********************on step end call back********************
Step 30980 finish
{'loss': 0.2292, 'grad_norm': 0.9708892107009888, 'learning_rate': 4.3992673992674e-05, 'epoch': 5.63}
********************on step end call back********************
Step 30990 finish
{'loss': 0.2732, 'grad_norm': 1.2729300260543823, 'learning_rate': 4.3974358974358976e-05, 'epoch': 5.63}
********************on step end call back********************
Step 31000 finish
{'loss': 0.2381, 'grad_norm': 1.5149598121643066, 'learning_rate': 4.3956043956043955e-05, 'epoch': 5.64}
{'eval_loss': 0.36090323328971863, 'eval_accuracy': 0.8645833333333334, 'eval_runtime': 129.0402, 'eval_samples_per_second': 4.944, 'eval_steps_per_second': 4.944, 'epoch': 5.64}
********************save call back********************
********************on step end call back********************
Step 31010 finish
{'loss': 0.2474, 'grad_norm': 1.7120888233184814, 'learning_rate': 4.393772893772894e-05, 'epoch': 5.64}
********************on step end call back********************
Step 31020 finish
{'loss': 0.2441, 'grad_norm': 1.287402629852295, 'learning_rate': 4.391941391941392e-05, 'epoch': 5.64}
********************on step end call back********************
Step 31030 finish
{'loss': 0.241, 'grad_norm': 0.7840924263000488, 'learning_rate': 4.3901098901098904e-05, 'epoch': 5.64}
********************on step end call back********************
Step 31040 finish
{'loss': 0.2464, 'grad_norm': 1.2801882028579712, 'learning_rate': 4.388278388278388e-05, 'epoch': 5.64}
********************on step end call back********************
Step 31050 finish
{'loss': 0.2398, 'grad_norm': 0.9781440496444702, 'learning_rate': 4.386446886446887e-05, 'epoch': 5.64}
********************on step end call back********************
Step 31060 finish
{'loss': 0.2351, 'grad_norm': 1.503541111946106, 'learning_rate': 4.384615384615385e-05, 'epoch': 5.65}
********************on step end call back********************
Step 31070 finish
{'loss': 0.2562, 'grad_norm': 1.1607775688171387, 'learning_rate': 4.3827838827838826e-05, 'epoch': 5.65}
********************on step end call back********************
Step 31080 finish
{'loss': 0.2184, 'grad_norm': 1.2900104522705078, 'learning_rate': 4.380952380952381e-05, 'epoch': 5.65}
********************on step end call back********************
Step 31090 finish
[INFO|trainer.py:3376] 2024-03-24 04:42:05,047 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-24 04:42:05,047 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-24 04:42:05,047 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-24 04:44:13,863 >> Saving model checkpoint to ./output/tmp-checkpoint-31100
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-24 04:44:14,002 >> tokenizer config file saved in ./output/tmp-checkpoint-31100/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-24 04:44:14,002 >> Special tokens file saved in ./output/tmp-checkpoint-31100/special_tokens_map.json
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-24 04:52:40,310 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-24 04:52:40,310 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-24 04:52:40,310 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-24 04:54:49,454 >> Saving model checkpoint to ./output/tmp-checkpoint-31200
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-24 04:54:49,594 >> tokenizer config file saved in ./output/tmp-checkpoint-31200/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-24 04:54:49,594 >> Special tokens file saved in ./output/tmp-checkpoint-31200/special_tokens_map.json
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-24 05:03:25,609 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-24 05:03:25,610 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-24 05:03:25,610 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-24 05:05:34,562 >> Saving model checkpoint to ./output/tmp-checkpoint-31300
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-24 05:05:34,708 >> tokenizer config file saved in ./output/tmp-checkpoint-31300/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-24 05:05:34,708 >> Special tokens file saved in ./output/tmp-checkpoint-31300/special_tokens_map.json
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-24 05:14:13,695 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-24 05:14:13,695 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-24 05:14:13,695 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-24 05:16:22,673 >> Saving model checkpoint to ./output/tmp-checkpoint-31400
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-24 05:16:22,812 >> tokenizer config file saved in ./output/tmp-checkpoint-31400/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-24 05:16:22,812 >> Special tokens file saved in ./output/tmp-checkpoint-31400/special_tokens_map.json
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
{'loss': 0.2386, 'grad_norm': 1.7112274169921875, 'learning_rate': 4.379120879120879e-05, 'epoch': 5.65}
********************on step end call back********************
Step 31100 finish
{'loss': 0.2553, 'grad_norm': 1.4011454582214355, 'learning_rate': 4.3772893772893776e-05, 'epoch': 5.65}
{'eval_loss': 0.36285319924354553, 'eval_accuracy': 0.875, 'eval_runtime': 128.8155, 'eval_samples_per_second': 4.953, 'eval_steps_per_second': 4.953, 'epoch': 5.65}
********************save call back********************
********************on step end call back********************
Step 31110 finish
{'loss': 0.233, 'grad_norm': 1.5034996271133423, 'learning_rate': 4.3754578754578755e-05, 'epoch': 5.66}
********************on step end call back********************
Step 31120 finish
{'loss': 0.2366, 'grad_norm': 1.4865528345108032, 'learning_rate': 4.373626373626374e-05, 'epoch': 5.66}
********************on step end call back********************
Step 31130 finish
{'loss': 0.2401, 'grad_norm': 1.3260642290115356, 'learning_rate': 4.371794871794872e-05, 'epoch': 5.66}
********************on step end call back********************
Step 31140 finish
{'loss': 0.2464, 'grad_norm': 1.5039114952087402, 'learning_rate': 4.3699633699633705e-05, 'epoch': 5.66}
********************on step end call back********************
Step 31150 finish
{'loss': 0.2511, 'grad_norm': 1.303149700164795, 'learning_rate': 4.3681318681318683e-05, 'epoch': 5.66}
********************on step end call back********************
Step 31160 finish
{'loss': 0.2519, 'grad_norm': 1.2232931852340698, 'learning_rate': 4.366300366300366e-05, 'epoch': 5.66}
********************on step end call back********************
Step 31170 finish
{'loss': 0.2779, 'grad_norm': 1.263319969177246, 'learning_rate': 4.364468864468865e-05, 'epoch': 5.67}
********************on step end call back********************
Step 31180 finish
{'loss': 0.2316, 'grad_norm': 1.215716004371643, 'learning_rate': 4.3626373626373626e-05, 'epoch': 5.67}
********************on step end call back********************
Step 31190 finish
{'loss': 0.2344, 'grad_norm': 0.9758381843566895, 'learning_rate': 4.360805860805861e-05, 'epoch': 5.67}
********************on step end call back********************
Step 31200 finish
{'loss': 0.2179, 'grad_norm': 1.197821021080017, 'learning_rate': 4.358974358974359e-05, 'epoch': 5.67}
{'eval_loss': 0.35759127140045166, 'eval_accuracy': 0.875, 'eval_runtime': 129.1429, 'eval_samples_per_second': 4.94, 'eval_steps_per_second': 4.94, 'epoch': 5.67}
********************save call back********************
********************on step end call back********************
Step 31210 finish
{'loss': 0.2287, 'grad_norm': 1.3961222171783447, 'learning_rate': 4.3571428571428576e-05, 'epoch': 5.67}
********************on step end call back********************
Step 31220 finish
{'loss': 0.255, 'grad_norm': 1.2997026443481445, 'learning_rate': 4.3553113553113555e-05, 'epoch': 5.68}
********************on step end call back********************
Step 31230 finish
{'loss': 0.2573, 'grad_norm': 1.0677034854888916, 'learning_rate': 4.353479853479854e-05, 'epoch': 5.68}
********************on step end call back********************
Step 31240 finish
{'loss': 0.2585, 'grad_norm': 1.4327101707458496, 'learning_rate': 4.351648351648352e-05, 'epoch': 5.68}
********************on step end call back********************
Step 31250 finish
{'loss': 0.3016, 'grad_norm': 1.2586091756820679, 'learning_rate': 4.34981684981685e-05, 'epoch': 5.68}
********************on step end call back********************
Step 31260 finish
{'loss': 0.2417, 'grad_norm': 1.0341416597366333, 'learning_rate': 4.3479853479853484e-05, 'epoch': 5.68}
********************on step end call back********************
Step 31270 finish
{'loss': 0.2713, 'grad_norm': 1.6806252002716064, 'learning_rate': 4.346153846153846e-05, 'epoch': 5.68}
********************on step end call back********************
Step 31280 finish
{'loss': 0.2355, 'grad_norm': 1.2252962589263916, 'learning_rate': 4.344322344322345e-05, 'epoch': 5.69}
********************on step end call back********************
Step 31290 finish
{'loss': 0.2529, 'grad_norm': 2.019134283065796, 'learning_rate': 4.342490842490843e-05, 'epoch': 5.69}
********************on step end call back********************
Step 31300 finish
{'loss': 0.2237, 'grad_norm': 1.2017673254013062, 'learning_rate': 4.340659340659341e-05, 'epoch': 5.69}
{'eval_loss': 0.36073973774909973, 'eval_accuracy': 0.8854166666666666, 'eval_runtime': 128.9518, 'eval_samples_per_second': 4.948, 'eval_steps_per_second': 4.948, 'epoch': 5.69}
********************save call back********************
********************on step end call back********************
Step 31310 finish
{'loss': 0.2447, 'grad_norm': 1.1212953329086304, 'learning_rate': 4.338827838827839e-05, 'epoch': 5.69}
********************on step end call back********************
Step 31320 finish
{'loss': 0.2542, 'grad_norm': 1.1886581182479858, 'learning_rate': 4.3369963369963377e-05, 'epoch': 5.69}
********************on step end call back********************
Step 31330 finish
{'loss': 0.2294, 'grad_norm': 1.2876858711242676, 'learning_rate': 4.3351648351648355e-05, 'epoch': 5.7}
********************on step end call back********************
Step 31340 finish
{'loss': 0.2626, 'grad_norm': 1.4741119146347046, 'learning_rate': 4.3333333333333334e-05, 'epoch': 5.7}
********************on step end call back********************
Step 31350 finish
{'loss': 0.2258, 'grad_norm': 1.1558566093444824, 'learning_rate': 4.331501831501832e-05, 'epoch': 5.7}
********************on step end call back********************
Step 31360 finish
{'loss': 0.2394, 'grad_norm': 1.4128236770629883, 'learning_rate': 4.32967032967033e-05, 'epoch': 5.7}
********************on step end call back********************
Step 31370 finish
{'loss': 0.2262, 'grad_norm': 1.0929253101348877, 'learning_rate': 4.3278388278388284e-05, 'epoch': 5.7}
********************on step end call back********************
Step 31380 finish
{'loss': 0.2472, 'grad_norm': 1.4447931051254272, 'learning_rate': 4.326007326007326e-05, 'epoch': 5.7}
********************on step end call back********************
Step 31390 finish
{'loss': 0.2521, 'grad_norm': 1.2367489337921143, 'learning_rate': 4.324175824175825e-05, 'epoch': 5.71}
********************on step end call back********************
Step 31400 finish
{'loss': 0.2552, 'grad_norm': 1.4491498470306396, 'learning_rate': 4.322344322344323e-05, 'epoch': 5.71}
{'eval_loss': 0.3554486334323883, 'eval_accuracy': 0.8645833333333334, 'eval_runtime': 128.9767, 'eval_samples_per_second': 4.947, 'eval_steps_per_second': 4.947, 'epoch': 5.71}
********************save call back********************
********************on step end call back********************
Step 31410 finish
{'loss': 0.2099, 'grad_norm': 1.0438162088394165, 'learning_rate': 4.320512820512821e-05, 'epoch': 5.71}
********************on step end call back********************
Step 31420 finish
{'loss': 0.2418, 'grad_norm': 1.712472915649414, 'learning_rate': 4.318681318681319e-05, 'epoch': 5.71}
********************on step end call back********************
Step 31430 finish
{'loss': 0.2261, 'grad_norm': 1.5667747259140015, 'learning_rate': 4.316849816849817e-05, 'epoch': 5.71}
********************on step end call back********************
Step 31440 finish
{'loss': 0.2319, 'grad_norm': 1.6291941404342651, 'learning_rate': 4.3150183150183156e-05, 'epoch': 5.72}
********************on step end call back********************
Step 31450 finish
{'loss': 0.2289, 'grad_norm': 1.421994924545288, 'learning_rate': 4.3131868131868134e-05, 'epoch': 5.72}
********************on step end call back********************
Step 31460 finish
{'loss': 0.26, 'grad_norm': 1.2611379623413086, 'learning_rate': 4.311355311355312e-05, 'epoch': 5.72}
********************on step end call back********************
Step 31470 finish
{'loss': 0.2369, 'grad_norm': 1.1298766136169434, 'learning_rate': 4.30952380952381e-05, 'epoch': 5.72}
********************on step end call back********************
Step 31480 finish
[INFO|trainer.py:3376] 2024-03-24 05:25:04,575 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-24 05:25:04,575 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-24 05:25:04,575 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-24 05:27:13,718 >> Saving model checkpoint to ./output/tmp-checkpoint-31500
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-24 05:27:13,857 >> tokenizer config file saved in ./output/tmp-checkpoint-31500/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-24 05:27:13,857 >> Special tokens file saved in ./output/tmp-checkpoint-31500/special_tokens_map.json
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-24 05:35:57,595 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-24 05:35:57,595 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-24 05:35:57,595 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-24 05:38:06,660 >> Saving model checkpoint to ./output/tmp-checkpoint-31600
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-24 05:38:06,799 >> tokenizer config file saved in ./output/tmp-checkpoint-31600/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-24 05:38:06,799 >> Special tokens file saved in ./output/tmp-checkpoint-31600/special_tokens_map.json
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-24 05:46:45,990 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-24 05:46:45,990 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-24 05:46:45,990 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-24 05:48:54,988 >> Saving model checkpoint to ./output/tmp-checkpoint-31700
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-24 05:48:55,128 >> tokenizer config file saved in ./output/tmp-checkpoint-31700/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-24 05:48:55,128 >> Special tokens file saved in ./output/tmp-checkpoint-31700/special_tokens_map.json
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-24 05:57:37,300 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-24 05:57:37,301 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-24 05:57:37,301 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-24 05:59:46,419 >> Saving model checkpoint to ./output/tmp-checkpoint-31800
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-24 05:59:46,559 >> tokenizer config file saved in ./output/tmp-checkpoint-31800/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-24 05:59:46,559 >> Special tokens file saved in ./output/tmp-checkpoint-31800/special_tokens_map.json
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
{'loss': 0.2404, 'grad_norm': 1.4804795980453491, 'learning_rate': 4.3076923076923084e-05, 'epoch': 5.72}
********************on step end call back********************
Step 31490 finish
{'loss': 0.2721, 'grad_norm': 1.2975393533706665, 'learning_rate': 4.305860805860806e-05, 'epoch': 5.72}
********************on step end call back********************
Step 31500 finish
{'loss': 0.2203, 'grad_norm': 0.930333137512207, 'learning_rate': 4.304029304029304e-05, 'epoch': 5.73}
{'eval_loss': 0.35879409313201904, 'eval_accuracy': 0.8541666666666666, 'eval_runtime': 129.1421, 'eval_samples_per_second': 4.94, 'eval_steps_per_second': 4.94, 'epoch': 5.73}
********************save call back********************
********************on step end call back********************
Step 31510 finish
{'loss': 0.2547, 'grad_norm': 1.2411350011825562, 'learning_rate': 4.302197802197803e-05, 'epoch': 5.73}
********************on step end call back********************
Step 31520 finish
{'loss': 0.2805, 'grad_norm': 1.453546404838562, 'learning_rate': 4.3003663003663006e-05, 'epoch': 5.73}
********************on step end call back********************
Step 31530 finish
{'loss': 0.2784, 'grad_norm': 1.3943036794662476, 'learning_rate': 4.298534798534799e-05, 'epoch': 5.73}
********************on step end call back********************
Step 31540 finish
{'loss': 0.2536, 'grad_norm': 0.8814153671264648, 'learning_rate': 4.2967032967032963e-05, 'epoch': 5.73}
********************on step end call back********************
Step 31550 finish
{'loss': 0.2333, 'grad_norm': 1.0770959854125977, 'learning_rate': 4.294871794871795e-05, 'epoch': 5.74}
********************on step end call back********************
Step 31560 finish
{'loss': 0.2675, 'grad_norm': 1.3829309940338135, 'learning_rate': 4.293040293040293e-05, 'epoch': 5.74}
********************on step end call back********************
Step 31570 finish
{'loss': 0.2649, 'grad_norm': 1.1419323682785034, 'learning_rate': 4.291208791208791e-05, 'epoch': 5.74}
********************on step end call back********************
Step 31580 finish
{'loss': 0.2819, 'grad_norm': 1.312319040298462, 'learning_rate': 4.289377289377289e-05, 'epoch': 5.74}
********************on step end call back********************
Step 31590 finish
{'loss': 0.2406, 'grad_norm': 1.2853385210037231, 'learning_rate': 4.287545787545788e-05, 'epoch': 5.74}
********************on step end call back********************
Step 31600 finish
{'loss': 0.2572, 'grad_norm': 1.1808546781539917, 'learning_rate': 4.2857142857142856e-05, 'epoch': 5.74}
{'eval_loss': 0.36737003922462463, 'eval_accuracy': 0.875, 'eval_runtime': 129.0637, 'eval_samples_per_second': 4.943, 'eval_steps_per_second': 4.943, 'epoch': 5.74}
********************save call back********************
********************on step end call back********************
Step 31610 finish
{'loss': 0.2682, 'grad_norm': 1.3449324369430542, 'learning_rate': 4.283882783882784e-05, 'epoch': 5.75}
********************on step end call back********************
Step 31620 finish
{'loss': 0.2579, 'grad_norm': 1.0934042930603027, 'learning_rate': 4.282051282051282e-05, 'epoch': 5.75}
********************on step end call back********************
Step 31630 finish
{'loss': 0.2296, 'grad_norm': 1.0993123054504395, 'learning_rate': 4.28021978021978e-05, 'epoch': 5.75}
********************on step end call back********************
Step 31640 finish
{'loss': 0.2562, 'grad_norm': 1.261263370513916, 'learning_rate': 4.2783882783882785e-05, 'epoch': 5.75}
********************on step end call back********************
Step 31650 finish
{'loss': 0.2424, 'grad_norm': 1.280354619026184, 'learning_rate': 4.2765567765567764e-05, 'epoch': 5.75}
********************on step end call back********************
Step 31660 finish
{'loss': 0.2086, 'grad_norm': 1.3354164361953735, 'learning_rate': 4.274725274725275e-05, 'epoch': 5.76}
********************on step end call back********************
Step 31670 finish
{'loss': 0.2234, 'grad_norm': 0.9559174180030823, 'learning_rate': 4.272893772893773e-05, 'epoch': 5.76}
********************on step end call back********************
Step 31680 finish
{'loss': 0.2653, 'grad_norm': 1.2125664949417114, 'learning_rate': 4.2710622710622713e-05, 'epoch': 5.76}
********************on step end call back********************
Step 31690 finish
{'loss': 0.2173, 'grad_norm': 1.1757241487503052, 'learning_rate': 4.269230769230769e-05, 'epoch': 5.76}
********************on step end call back********************
Step 31700 finish
{'loss': 0.2627, 'grad_norm': 1.876289963722229, 'learning_rate': 4.267399267399267e-05, 'epoch': 5.76}
{'eval_loss': 0.3611988425254822, 'eval_accuracy': 0.875, 'eval_runtime': 128.9972, 'eval_samples_per_second': 4.946, 'eval_steps_per_second': 4.946, 'epoch': 5.76}
********************save call back********************
********************on step end call back********************
Step 31710 finish
{'loss': 0.2044, 'grad_norm': 1.2840622663497925, 'learning_rate': 4.2655677655677657e-05, 'epoch': 5.76}
********************on step end call back********************
Step 31720 finish
{'loss': 0.2446, 'grad_norm': 1.4759951829910278, 'learning_rate': 4.2637362637362635e-05, 'epoch': 5.77}
********************on step end call back********************
Step 31730 finish
{'loss': 0.2586, 'grad_norm': 1.357752799987793, 'learning_rate': 4.261904761904762e-05, 'epoch': 5.77}
********************on step end call back********************
Step 31740 finish
{'loss': 0.273, 'grad_norm': 1.9711493253707886, 'learning_rate': 4.26007326007326e-05, 'epoch': 5.77}
********************on step end call back********************
Step 31750 finish
{'loss': 0.2419, 'grad_norm': 1.3191055059432983, 'learning_rate': 4.2582417582417585e-05, 'epoch': 5.77}
********************on step end call back********************
Step 31760 finish
{'loss': 0.2556, 'grad_norm': 1.3321449756622314, 'learning_rate': 4.2564102564102564e-05, 'epoch': 5.77}
********************on step end call back********************
Step 31770 finish
{'loss': 0.2511, 'grad_norm': 1.4133996963500977, 'learning_rate': 4.254578754578755e-05, 'epoch': 5.78}
********************on step end call back********************
Step 31780 finish
{'loss': 0.1922, 'grad_norm': 1.1597821712493896, 'learning_rate': 4.252747252747253e-05, 'epoch': 5.78}
********************on step end call back********************
Step 31790 finish
{'loss': 0.2793, 'grad_norm': 1.41635262966156, 'learning_rate': 4.250915750915751e-05, 'epoch': 5.78}
********************on step end call back********************
Step 31800 finish
{'loss': 0.255, 'grad_norm': 1.0510742664337158, 'learning_rate': 4.249084249084249e-05, 'epoch': 5.78}
{'eval_loss': 0.35314324498176575, 'eval_accuracy': 0.8645833333333334, 'eval_runtime': 129.1174, 'eval_samples_per_second': 4.941, 'eval_steps_per_second': 4.941, 'epoch': 5.78}
********************save call back********************
********************on step end call back********************
Step 31810 finish
{'loss': 0.2595, 'grad_norm': 1.1898812055587769, 'learning_rate': 4.247252747252747e-05, 'epoch': 5.78}
********************on step end call back********************
Step 31820 finish
{'loss': 0.2625, 'grad_norm': 1.2104852199554443, 'learning_rate': 4.245421245421246e-05, 'epoch': 5.78}
********************on step end call back********************
Step 31830 finish
{'loss': 0.2714, 'grad_norm': 1.2272140979766846, 'learning_rate': 4.2435897435897435e-05, 'epoch': 5.79}
********************on step end call back********************
Step 31840 finish
{'loss': 0.2264, 'grad_norm': 1.3380852937698364, 'learning_rate': 4.241758241758242e-05, 'epoch': 5.79}
********************on step end call back********************
Step 31850 finish
{'loss': 0.2311, 'grad_norm': 1.0089257955551147, 'learning_rate': 4.23992673992674e-05, 'epoch': 5.79}
********************on step end call back********************
Step 31860 finish
{'loss': 0.2313, 'grad_norm': 0.9708094000816345, 'learning_rate': 4.2380952380952385e-05, 'epoch': 5.79}
********************on step end call back********************
Step 31870 finish
[INFO|trainer.py:3376] 2024-03-24 06:08:21,773 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-24 06:08:21,774 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-24 06:08:21,774 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-24 06:10:30,874 >> Saving model checkpoint to ./output/tmp-checkpoint-31900
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-24 06:10:31,355 >> tokenizer config file saved in ./output/tmp-checkpoint-31900/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-24 06:10:31,356 >> Special tokens file saved in ./output/tmp-checkpoint-31900/special_tokens_map.json
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-24 06:19:03,181 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-24 06:19:03,182 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-24 06:19:03,182 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-24 06:21:12,090 >> Saving model checkpoint to ./output/tmp-checkpoint-32000
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-24 06:21:12,228 >> tokenizer config file saved in ./output/tmp-checkpoint-32000/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-24 06:21:12,228 >> Special tokens file saved in ./output/tmp-checkpoint-32000/special_tokens_map.json
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-24 06:29:55,611 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-24 06:29:55,611 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-24 06:29:55,611 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-24 06:32:04,442 >> Saving model checkpoint to ./output/tmp-checkpoint-32100
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-24 06:32:04,580 >> tokenizer config file saved in ./output/tmp-checkpoint-32100/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-24 06:32:04,580 >> Special tokens file saved in ./output/tmp-checkpoint-32100/special_tokens_map.json
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-24 06:40:37,571 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-24 06:40:37,571 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-24 06:40:37,571 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-24 06:42:46,463 >> Saving model checkpoint to ./output/tmp-checkpoint-32200
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-24 06:42:46,601 >> tokenizer config file saved in ./output/tmp-checkpoint-32200/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-24 06:42:46,601 >> Special tokens file saved in ./output/tmp-checkpoint-32200/special_tokens_map.json
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
{'loss': 0.2228, 'grad_norm': 1.228740930557251, 'learning_rate': 4.2362637362637364e-05, 'epoch': 5.79}
********************on step end call back********************
Step 31880 finish
{'loss': 0.2685, 'grad_norm': 1.1798207759857178, 'learning_rate': 4.234432234432234e-05, 'epoch': 5.8}
********************on step end call back********************
Step 31890 finish
{'loss': 0.2591, 'grad_norm': 1.2319486141204834, 'learning_rate': 4.232600732600733e-05, 'epoch': 5.8}
********************on step end call back********************
Step 31900 finish
{'loss': 0.216, 'grad_norm': 1.1298341751098633, 'learning_rate': 4.230769230769231e-05, 'epoch': 5.8}
{'eval_loss': 0.3654285669326782, 'eval_accuracy': 0.8645833333333334, 'eval_runtime': 129.0993, 'eval_samples_per_second': 4.942, 'eval_steps_per_second': 4.942, 'epoch': 5.8}
********************save call back********************
********************on step end call back********************
Step 31910 finish
{'loss': 0.2782, 'grad_norm': 0.8727369904518127, 'learning_rate': 4.228937728937729e-05, 'epoch': 5.8}
********************on step end call back********************
Step 31920 finish
{'loss': 0.2529, 'grad_norm': 1.2504886388778687, 'learning_rate': 4.227106227106227e-05, 'epoch': 5.8}
********************on step end call back********************
Step 31930 finish
{'loss': 0.2225, 'grad_norm': 1.2723579406738281, 'learning_rate': 4.225274725274726e-05, 'epoch': 5.8}
********************on step end call back********************
Step 31940 finish
{'loss': 0.2373, 'grad_norm': 1.0924738645553589, 'learning_rate': 4.2234432234432236e-05, 'epoch': 5.81}
********************on step end call back********************
Step 31950 finish
{'loss': 0.2555, 'grad_norm': 1.2474980354309082, 'learning_rate': 4.221611721611722e-05, 'epoch': 5.81}
********************on step end call back********************
Step 31960 finish
{'loss': 0.234, 'grad_norm': 1.5087103843688965, 'learning_rate': 4.21978021978022e-05, 'epoch': 5.81}
********************on step end call back********************
Step 31970 finish
{'loss': 0.2284, 'grad_norm': 1.241240382194519, 'learning_rate': 4.217948717948718e-05, 'epoch': 5.81}
********************on step end call back********************
Step 31980 finish
{'loss': 0.2562, 'grad_norm': 1.0301746129989624, 'learning_rate': 4.2161172161172164e-05, 'epoch': 5.81}
********************on step end call back********************
Step 31990 finish
{'loss': 0.2769, 'grad_norm': 1.1706256866455078, 'learning_rate': 4.214285714285714e-05, 'epoch': 5.82}
********************on step end call back********************
Step 32000 finish
{'loss': 0.2719, 'grad_norm': 1.1255038976669312, 'learning_rate': 4.212454212454213e-05, 'epoch': 5.82}
{'eval_loss': 0.3625715374946594, 'eval_accuracy': 0.875, 'eval_runtime': 128.9073, 'eval_samples_per_second': 4.949, 'eval_steps_per_second': 4.949, 'epoch': 5.82}
********************save call back********************
********************on step end call back********************
Step 32010 finish
{'loss': 0.2577, 'grad_norm': 0.9922735095024109, 'learning_rate': 4.210622710622711e-05, 'epoch': 5.82}
********************on step end call back********************
Step 32020 finish
{'loss': 0.2652, 'grad_norm': 1.4100168943405151, 'learning_rate': 4.208791208791209e-05, 'epoch': 5.82}
********************on step end call back********************
Step 32030 finish
{'loss': 0.24, 'grad_norm': 1.078399419784546, 'learning_rate': 4.206959706959707e-05, 'epoch': 5.82}
********************on step end call back********************
Step 32040 finish
{'loss': 0.2576, 'grad_norm': 1.150762677192688, 'learning_rate': 4.205128205128206e-05, 'epoch': 5.82}
********************on step end call back********************
Step 32050 finish
{'loss': 0.2657, 'grad_norm': 1.3137540817260742, 'learning_rate': 4.2032967032967036e-05, 'epoch': 5.83}
********************on step end call back********************
Step 32060 finish
{'loss': 0.2421, 'grad_norm': 1.2211484909057617, 'learning_rate': 4.2014652014652015e-05, 'epoch': 5.83}
********************on step end call back********************
Step 32070 finish
{'loss': 0.2678, 'grad_norm': 1.5083404779434204, 'learning_rate': 4.1996336996337e-05, 'epoch': 5.83}
********************on step end call back********************
Step 32080 finish
{'loss': 0.2111, 'grad_norm': 1.0167967081069946, 'learning_rate': 4.197802197802198e-05, 'epoch': 5.83}
********************on step end call back********************
Step 32090 finish
{'loss': 0.2583, 'grad_norm': 1.3294155597686768, 'learning_rate': 4.1959706959706964e-05, 'epoch': 5.83}
********************on step end call back********************
Step 32100 finish
{'loss': 0.2568, 'grad_norm': 1.7328462600708008, 'learning_rate': 4.194139194139194e-05, 'epoch': 5.84}
{'eval_loss': 0.36677122116088867, 'eval_accuracy': 0.875, 'eval_runtime': 128.8308, 'eval_samples_per_second': 4.952, 'eval_steps_per_second': 4.952, 'epoch': 5.84}
********************save call back********************
********************on step end call back********************
Step 32110 finish
{'loss': 0.2392, 'grad_norm': 1.3943172693252563, 'learning_rate': 4.192307692307693e-05, 'epoch': 5.84}
********************on step end call back********************
Step 32120 finish
{'loss': 0.2505, 'grad_norm': 1.0929615497589111, 'learning_rate': 4.190476190476191e-05, 'epoch': 5.84}
********************on step end call back********************
Step 32130 finish
{'loss': 0.2516, 'grad_norm': 1.0965368747711182, 'learning_rate': 4.1886446886446886e-05, 'epoch': 5.84}
********************on step end call back********************
Step 32140 finish
{'loss': 0.2389, 'grad_norm': 1.234734058380127, 'learning_rate': 4.186813186813187e-05, 'epoch': 5.84}
********************on step end call back********************
Step 32150 finish
{'loss': 0.2348, 'grad_norm': 0.9799031615257263, 'learning_rate': 4.184981684981685e-05, 'epoch': 5.84}
********************on step end call back********************
Step 32160 finish
{'loss': 0.2562, 'grad_norm': 1.2098798751831055, 'learning_rate': 4.1831501831501836e-05, 'epoch': 5.85}
********************on step end call back********************
Step 32170 finish
{'loss': 0.2324, 'grad_norm': 1.358238935470581, 'learning_rate': 4.1813186813186815e-05, 'epoch': 5.85}
********************on step end call back********************
Step 32180 finish
{'loss': 0.2792, 'grad_norm': 1.2618178129196167, 'learning_rate': 4.17948717948718e-05, 'epoch': 5.85}
********************on step end call back********************
Step 32190 finish
{'loss': 0.2451, 'grad_norm': 1.366829752922058, 'learning_rate': 4.177655677655678e-05, 'epoch': 5.85}
********************on step end call back********************
Step 32200 finish
{'loss': 0.2548, 'grad_norm': 1.3561432361602783, 'learning_rate': 4.1758241758241765e-05, 'epoch': 5.85}
{'eval_loss': 0.3573705554008484, 'eval_accuracy': 0.875, 'eval_runtime': 128.8911, 'eval_samples_per_second': 4.95, 'eval_steps_per_second': 4.95, 'epoch': 5.85}
********************save call back********************
********************on step end call back********************
Step 32210 finish
{'loss': 0.2554, 'grad_norm': 1.1672141551971436, 'learning_rate': 4.1739926739926743e-05, 'epoch': 5.86}
********************on step end call back********************
Step 32220 finish
{'loss': 0.2339, 'grad_norm': 1.0431010723114014, 'learning_rate': 4.172161172161172e-05, 'epoch': 5.86}
********************on step end call back********************
Step 32230 finish
{'loss': 0.2698, 'grad_norm': 1.312703013420105, 'learning_rate': 4.170329670329671e-05, 'epoch': 5.86}
********************on step end call back********************
Step 32240 finish
{'loss': 0.2576, 'grad_norm': 1.3497694730758667, 'learning_rate': 4.1684981684981687e-05, 'epoch': 5.86}
********************on step end call back********************
Step 32250 finish
{'loss': 0.2643, 'grad_norm': 1.2354905605316162, 'learning_rate': 4.166666666666667e-05, 'epoch': 5.86}
********************on step end call back********************
Step 32260 finish
[INFO|trainer.py:3376] 2024-03-24 06:51:28,626 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-24 06:51:28,626 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-24 06:51:28,626 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-24 06:53:37,424 >> Saving model checkpoint to ./output/tmp-checkpoint-32300
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-24 06:53:37,561 >> tokenizer config file saved in ./output/tmp-checkpoint-32300/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-24 06:53:37,561 >> Special tokens file saved in ./output/tmp-checkpoint-32300/special_tokens_map.json
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-24 07:02:13,006 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-24 07:02:13,006 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-24 07:02:13,006 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-24 07:04:21,852 >> Saving model checkpoint to ./output/tmp-checkpoint-32400
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-24 07:04:21,989 >> tokenizer config file saved in ./output/tmp-checkpoint-32400/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-24 07:04:21,989 >> Special tokens file saved in ./output/tmp-checkpoint-32400/special_tokens_map.json
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-24 07:12:50,827 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-24 07:12:50,827 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-24 07:12:50,827 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-24 07:14:59,552 >> Saving model checkpoint to ./output/tmp-checkpoint-32500
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-24 07:14:59,690 >> tokenizer config file saved in ./output/tmp-checkpoint-32500/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-24 07:14:59,690 >> Special tokens file saved in ./output/tmp-checkpoint-32500/special_tokens_map.json
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-24 07:23:32,649 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-24 07:23:32,649 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-24 07:23:32,649 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-24 07:25:41,540 >> Saving model checkpoint to ./output/tmp-checkpoint-32600
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-24 07:25:41,695 >> tokenizer config file saved in ./output/tmp-checkpoint-32600/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-24 07:25:41,695 >> Special tokens file saved in ./output/tmp-checkpoint-32600/special_tokens_map.json
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
{'loss': 0.2555, 'grad_norm': 1.1238415241241455, 'learning_rate': 4.164835164835165e-05, 'epoch': 5.86}
********************on step end call back********************
Step 32270 finish
{'loss': 0.2338, 'grad_norm': 1.4459755420684814, 'learning_rate': 4.1630036630036636e-05, 'epoch': 5.87}
********************on step end call back********************
Step 32280 finish
{'loss': 0.2427, 'grad_norm': 1.4084091186523438, 'learning_rate': 4.1611721611721615e-05, 'epoch': 5.87}
********************on step end call back********************
Step 32290 finish
{'loss': 0.2228, 'grad_norm': 1.1980321407318115, 'learning_rate': 4.15934065934066e-05, 'epoch': 5.87}
********************on step end call back********************
Step 32300 finish
{'loss': 0.2661, 'grad_norm': 0.9623695015907288, 'learning_rate': 4.157509157509158e-05, 'epoch': 5.87}
{'eval_loss': 0.36232680082321167, 'eval_accuracy': 0.8645833333333334, 'eval_runtime': 128.7975, 'eval_samples_per_second': 4.954, 'eval_steps_per_second': 4.954, 'epoch': 5.87}
********************save call back********************
********************on step end call back********************
Step 32310 finish
{'loss': 0.2401, 'grad_norm': 1.2905181646347046, 'learning_rate': 4.155677655677656e-05, 'epoch': 5.87}
********************on step end call back********************
Step 32320 finish
{'loss': 0.2463, 'grad_norm': 1.1582525968551636, 'learning_rate': 4.1538461538461544e-05, 'epoch': 5.88}
********************on step end call back********************
Step 32330 finish
{'loss': 0.23, 'grad_norm': 1.2113134860992432, 'learning_rate': 4.152014652014652e-05, 'epoch': 5.88}
********************on step end call back********************
Step 32340 finish
{'loss': 0.2224, 'grad_norm': 1.674574851989746, 'learning_rate': 4.150183150183151e-05, 'epoch': 5.88}
********************on step end call back********************
Step 32350 finish
{'loss': 0.2811, 'grad_norm': 1.3699086904525757, 'learning_rate': 4.148351648351649e-05, 'epoch': 5.88}
********************on step end call back********************
Step 32360 finish
{'loss': 0.2512, 'grad_norm': 1.3621801137924194, 'learning_rate': 4.146520146520147e-05, 'epoch': 5.88}
********************on step end call back********************
Step 32370 finish
{'loss': 0.2568, 'grad_norm': 1.4184049367904663, 'learning_rate': 4.144688644688645e-05, 'epoch': 5.88}
********************on step end call back********************
Step 32380 finish
{'loss': 0.254, 'grad_norm': 1.1510487794876099, 'learning_rate': 4.1428571428571437e-05, 'epoch': 5.89}
********************on step end call back********************
Step 32390 finish
{'loss': 0.2557, 'grad_norm': 1.1925947666168213, 'learning_rate': 4.1410256410256415e-05, 'epoch': 5.89}
********************on step end call back********************
Step 32400 finish
{'loss': 0.2351, 'grad_norm': 1.316712498664856, 'learning_rate': 4.1391941391941394e-05, 'epoch': 5.89}
{'eval_loss': 0.3624142110347748, 'eval_accuracy': 0.8645833333333334, 'eval_runtime': 128.8454, 'eval_samples_per_second': 4.952, 'eval_steps_per_second': 4.952, 'epoch': 5.89}
********************save call back********************
********************on step end call back********************
Step 32410 finish
{'loss': 0.231, 'grad_norm': 1.2269866466522217, 'learning_rate': 4.137362637362637e-05, 'epoch': 5.89}
********************on step end call back********************
Step 32420 finish
{'loss': 0.2411, 'grad_norm': 0.934049665927887, 'learning_rate': 4.135531135531135e-05, 'epoch': 5.89}
********************on step end call back********************
Step 32430 finish
{'loss': 0.2265, 'grad_norm': 1.477666974067688, 'learning_rate': 4.133699633699634e-05, 'epoch': 5.9}
********************on step end call back********************
Step 32440 finish
{'loss': 0.2486, 'grad_norm': 1.5323668718338013, 'learning_rate': 4.1318681318681316e-05, 'epoch': 5.9}
********************on step end call back********************
Step 32450 finish
{'loss': 0.2161, 'grad_norm': 0.9362382888793945, 'learning_rate': 4.13003663003663e-05, 'epoch': 5.9}
********************on step end call back********************
Step 32460 finish
{'loss': 0.2517, 'grad_norm': 1.491382122039795, 'learning_rate': 4.128205128205128e-05, 'epoch': 5.9}
********************on step end call back********************
Step 32470 finish
{'loss': 0.2589, 'grad_norm': 1.7070001363754272, 'learning_rate': 4.1263736263736266e-05, 'epoch': 5.9}
********************on step end call back********************
Step 32480 finish
{'loss': 0.2492, 'grad_norm': 1.2762260437011719, 'learning_rate': 4.1245421245421244e-05, 'epoch': 5.9}
********************on step end call back********************
Step 32490 finish
{'loss': 0.2519, 'grad_norm': 1.5081650018692017, 'learning_rate': 4.122710622710623e-05, 'epoch': 5.91}
********************on step end call back********************
Step 32500 finish
{'loss': 0.2599, 'grad_norm': 1.2384179830551147, 'learning_rate': 4.120879120879121e-05, 'epoch': 5.91}
{'eval_loss': 0.3600156307220459, 'eval_accuracy': 0.8645833333333334, 'eval_runtime': 128.7238, 'eval_samples_per_second': 4.956, 'eval_steps_per_second': 4.956, 'epoch': 5.91}
********************save call back********************
********************on step end call back********************
Step 32510 finish
{'loss': 0.2068, 'grad_norm': 1.1869611740112305, 'learning_rate': 4.119047619047619e-05, 'epoch': 5.91}
********************on step end call back********************
Step 32520 finish
{'loss': 0.2219, 'grad_norm': 1.0589866638183594, 'learning_rate': 4.117216117216117e-05, 'epoch': 5.91}
********************on step end call back********************
Step 32530 finish
{'loss': 0.2587, 'grad_norm': 1.365638017654419, 'learning_rate': 4.115384615384615e-05, 'epoch': 5.91}
********************on step end call back********************
Step 32540 finish
{'loss': 0.2458, 'grad_norm': 1.1957379579544067, 'learning_rate': 4.113553113553114e-05, 'epoch': 5.92}
********************on step end call back********************
Step 32550 finish
{'loss': 0.2448, 'grad_norm': 1.3149690628051758, 'learning_rate': 4.1117216117216116e-05, 'epoch': 5.92}
********************on step end call back********************
Step 32560 finish
{'loss': 0.2269, 'grad_norm': 1.1123030185699463, 'learning_rate': 4.10989010989011e-05, 'epoch': 5.92}
********************on step end call back********************
Step 32570 finish
{'loss': 0.2476, 'grad_norm': 1.4106312990188599, 'learning_rate': 4.108058608058608e-05, 'epoch': 5.92}
********************on step end call back********************
Step 32580 finish
{'loss': 0.2412, 'grad_norm': 1.2371257543563843, 'learning_rate': 4.1062271062271066e-05, 'epoch': 5.92}
********************on step end call back********************
Step 32590 finish
{'loss': 0.2693, 'grad_norm': 1.407456636428833, 'learning_rate': 4.1043956043956045e-05, 'epoch': 5.92}
********************on step end call back********************
Step 32600 finish
{'loss': 0.2373, 'grad_norm': 1.213632345199585, 'learning_rate': 4.1025641025641023e-05, 'epoch': 5.93}
{'eval_loss': 0.36006438732147217, 'eval_accuracy': 0.8645833333333334, 'eval_runtime': 128.8902, 'eval_samples_per_second': 4.95, 'eval_steps_per_second': 4.95, 'epoch': 5.93}
********************save call back********************
********************on step end call back********************
Step 32610 finish
{'loss': 0.2363, 'grad_norm': 1.137972116470337, 'learning_rate': 4.100732600732601e-05, 'epoch': 5.93}
********************on step end call back********************
Step 32620 finish
{'loss': 0.2541, 'grad_norm': 1.1775755882263184, 'learning_rate': 4.098901098901099e-05, 'epoch': 5.93}
********************on step end call back********************
Step 32630 finish
{'loss': 0.2496, 'grad_norm': 1.6706013679504395, 'learning_rate': 4.097069597069597e-05, 'epoch': 5.93}
********************on step end call back********************
Step 32640 finish
{'loss': 0.2392, 'grad_norm': 1.3810310363769531, 'learning_rate': 4.095238095238095e-05, 'epoch': 5.93}
********************on step end call back********************
Step 32650 finish
[INFO|trainer.py:3376] 2024-03-24 07:34:17,351 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-24 07:34:17,351 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-24 07:34:17,351 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-24 07:36:26,140 >> Saving model checkpoint to ./output/tmp-checkpoint-32700
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-24 07:36:26,277 >> tokenizer config file saved in ./output/tmp-checkpoint-32700/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-24 07:36:26,277 >> Special tokens file saved in ./output/tmp-checkpoint-32700/special_tokens_map.json
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-24 07:45:05,749 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-24 07:45:05,749 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-24 07:45:05,749 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-24 07:47:14,528 >> Saving model checkpoint to ./output/tmp-checkpoint-32800
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-24 07:47:14,665 >> tokenizer config file saved in ./output/tmp-checkpoint-32800/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-24 07:47:14,666 >> Special tokens file saved in ./output/tmp-checkpoint-32800/special_tokens_map.json
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-24 07:55:48,598 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-24 07:55:48,598 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-24 07:55:48,598 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-24 07:57:57,327 >> Saving model checkpoint to ./output/tmp-checkpoint-32900
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-24 07:57:57,466 >> tokenizer config file saved in ./output/tmp-checkpoint-32900/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-24 07:57:57,467 >> Special tokens file saved in ./output/tmp-checkpoint-32900/special_tokens_map.json
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-24 08:06:30,280 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-24 08:06:30,280 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-24 08:06:30,280 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-24 08:08:39,287 >> Saving model checkpoint to ./output/tmp-checkpoint-33000
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-24 08:08:39,424 >> tokenizer config file saved in ./output/tmp-checkpoint-33000/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-24 08:08:39,425 >> Special tokens file saved in ./output/tmp-checkpoint-33000/special_tokens_map.json
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
{'loss': 0.2493, 'grad_norm': 1.4905115365982056, 'learning_rate': 4.093406593406594e-05, 'epoch': 5.94}
********************on step end call back********************
Step 32660 finish
{'loss': 0.2558, 'grad_norm': 1.469964861869812, 'learning_rate': 4.0915750915750916e-05, 'epoch': 5.94}
********************on step end call back********************
Step 32670 finish
{'loss': 0.2354, 'grad_norm': 1.5881075859069824, 'learning_rate': 4.0897435897435895e-05, 'epoch': 5.94}
********************on step end call back********************
Step 32680 finish
{'loss': 0.2424, 'grad_norm': 1.3850774765014648, 'learning_rate': 4.087912087912088e-05, 'epoch': 5.94}
********************on step end call back********************
Step 32690 finish
{'loss': 0.2222, 'grad_norm': 1.239870548248291, 'learning_rate': 4.086080586080586e-05, 'epoch': 5.94}
********************on step end call back********************
Step 32700 finish
{'loss': 0.2512, 'grad_norm': 1.1419802904129028, 'learning_rate': 4.0842490842490845e-05, 'epoch': 5.94}
{'eval_loss': 0.3593383729457855, 'eval_accuracy': 0.8645833333333334, 'eval_runtime': 128.7872, 'eval_samples_per_second': 4.954, 'eval_steps_per_second': 4.954, 'epoch': 5.94}
********************save call back********************
********************on step end call back********************
Step 32710 finish
{'loss': 0.2487, 'grad_norm': 1.2917197942733765, 'learning_rate': 4.0824175824175824e-05, 'epoch': 5.95}
********************on step end call back********************
Step 32720 finish
{'loss': 0.2062, 'grad_norm': 1.0852736234664917, 'learning_rate': 4.080586080586081e-05, 'epoch': 5.95}
********************on step end call back********************
Step 32730 finish
{'loss': 0.2383, 'grad_norm': 1.283246397972107, 'learning_rate': 4.078754578754579e-05, 'epoch': 5.95}
********************on step end call back********************
Step 32740 finish
{'loss': 0.2363, 'grad_norm': 1.1256992816925049, 'learning_rate': 4.0769230769230773e-05, 'epoch': 5.95}
********************on step end call back********************
Step 32750 finish
{'loss': 0.2415, 'grad_norm': 1.4773597717285156, 'learning_rate': 4.075091575091575e-05, 'epoch': 5.95}
********************on step end call back********************
Step 32760 finish
{'loss': 0.2655, 'grad_norm': 1.3732457160949707, 'learning_rate': 4.073260073260073e-05, 'epoch': 5.96}
********************on step end call back********************
Step 32770 finish
{'loss': 0.239, 'grad_norm': 1.0987550020217896, 'learning_rate': 4.0714285714285717e-05, 'epoch': 5.96}
********************on step end call back********************
Step 32780 finish
{'loss': 0.24, 'grad_norm': 1.4181915521621704, 'learning_rate': 4.0695970695970695e-05, 'epoch': 5.96}
********************on step end call back********************
Step 32790 finish
{'loss': 0.2729, 'grad_norm': 1.3756439685821533, 'learning_rate': 4.067765567765568e-05, 'epoch': 5.96}
********************on step end call back********************
Step 32800 finish
{'loss': 0.2428, 'grad_norm': 1.2619760036468506, 'learning_rate': 4.065934065934066e-05, 'epoch': 5.96}
{'eval_loss': 0.3618738651275635, 'eval_accuracy': 0.8645833333333334, 'eval_runtime': 128.7784, 'eval_samples_per_second': 4.954, 'eval_steps_per_second': 4.954, 'epoch': 5.96}
********************save call back********************
********************on step end call back********************
Step 32810 finish
{'loss': 0.2381, 'grad_norm': 1.0564357042312622, 'learning_rate': 4.0641025641025645e-05, 'epoch': 5.96}
********************on step end call back********************
Step 32820 finish
{'loss': 0.2428, 'grad_norm': 1.4563921689987183, 'learning_rate': 4.0622710622710624e-05, 'epoch': 5.97}
********************on step end call back********************
Step 32830 finish
{'loss': 0.2159, 'grad_norm': 1.1022160053253174, 'learning_rate': 4.060439560439561e-05, 'epoch': 5.97}
********************on step end call back********************
Step 32840 finish
{'loss': 0.2388, 'grad_norm': 1.2529395818710327, 'learning_rate': 4.058608058608059e-05, 'epoch': 5.97}
********************on step end call back********************
Step 32850 finish
{'loss': 0.2492, 'grad_norm': 1.3772531747817993, 'learning_rate': 4.056776556776557e-05, 'epoch': 5.97}
********************on step end call back********************
Step 32860 finish
{'loss': 0.2503, 'grad_norm': 1.2181899547576904, 'learning_rate': 4.054945054945055e-05, 'epoch': 5.97}
********************on step end call back********************
Step 32870 finish
{'loss': 0.2441, 'grad_norm': 1.21678626537323, 'learning_rate': 4.053113553113553e-05, 'epoch': 5.98}
********************on step end call back********************
Step 32880 finish
{'loss': 0.262, 'grad_norm': 1.089930534362793, 'learning_rate': 4.051282051282052e-05, 'epoch': 5.98}
********************on step end call back********************
Step 32890 finish
{'loss': 0.2086, 'grad_norm': 1.116600751876831, 'learning_rate': 4.0494505494505496e-05, 'epoch': 5.98}
********************on step end call back********************
Step 32900 finish
{'loss': 0.2542, 'grad_norm': 1.8135626316070557, 'learning_rate': 4.047619047619048e-05, 'epoch': 5.98}
{'eval_loss': 0.3633406460285187, 'eval_accuracy': 0.8645833333333334, 'eval_runtime': 128.7279, 'eval_samples_per_second': 4.956, 'eval_steps_per_second': 4.956, 'epoch': 5.98}
********************save call back********************
********************on step end call back********************
Step 32910 finish
{'loss': 0.2336, 'grad_norm': 1.374842882156372, 'learning_rate': 4.045787545787546e-05, 'epoch': 5.98}
********************on step end call back********************
Step 32920 finish
{'loss': 0.2275, 'grad_norm': 1.3395311832427979, 'learning_rate': 4.0439560439560445e-05, 'epoch': 5.98}
********************on step end call back********************
Step 32930 finish
{'loss': 0.2455, 'grad_norm': 1.429398775100708, 'learning_rate': 4.0421245421245424e-05, 'epoch': 5.99}
********************on step end call back********************
Step 32940 finish
{'loss': 0.2322, 'grad_norm': 1.6575837135314941, 'learning_rate': 4.04029304029304e-05, 'epoch': 5.99}
********************on step end call back********************
Step 32950 finish
{'loss': 0.2338, 'grad_norm': 1.245490312576294, 'learning_rate': 4.038461538461539e-05, 'epoch': 5.99}
********************on step end call back********************
Step 32960 finish
{'loss': 0.2425, 'grad_norm': 0.9952256679534912, 'learning_rate': 4.036630036630037e-05, 'epoch': 5.99}
********************on step end call back********************
Step 32970 finish
{'loss': 0.2134, 'grad_norm': 1.118920922279358, 'learning_rate': 4.034798534798535e-05, 'epoch': 5.99}
********************on step end call back********************
Step 32980 finish
{'loss': 0.223, 'grad_norm': 0.9141085743904114, 'learning_rate': 4.032967032967033e-05, 'epoch': 6.0}
********************on step end call back********************
Step 32990 finish
{'loss': 0.236, 'grad_norm': 1.1101126670837402, 'learning_rate': 4.031135531135532e-05, 'epoch': 6.0}
********************on step end call back********************
Step 33000 finish
{'loss': 0.2409, 'grad_norm': 1.6533559560775757, 'learning_rate': 4.0293040293040296e-05, 'epoch': 6.0}
{'eval_loss': 0.35440871119499207, 'eval_accuracy': 0.875, 'eval_runtime': 129.0052, 'eval_samples_per_second': 4.946, 'eval_steps_per_second': 4.946, 'epoch': 6.0}
********************save call back********************
********************on epoch end call back********************
Epoch 5.999863651857743 finish
********************on step end call back********************
Step 33010 finish
{'loss': 0.2324, 'grad_norm': 1.0886166095733643, 'learning_rate': 4.027472527472528e-05, 'epoch': 6.0}
********************on step end call back********************
Step 33020 finish
{'loss': 0.21, 'grad_norm': 1.0772638320922852, 'learning_rate': 4.025641025641026e-05, 'epoch': 6.0}
********************on step end call back********************
Step 33030 finish
{'loss': 0.2351, 'grad_norm': 0.8289417624473572, 'learning_rate': 4.023809523809524e-05, 'epoch': 6.0}
[INFO|trainer.py:3376] 2024-03-24 08:17:26,522 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-24 08:17:26,522 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-24 08:17:26,522 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-24 08:19:38,551 >> Saving model checkpoint to ./output/tmp-checkpoint-33100
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-24 08:19:38,689 >> tokenizer config file saved in ./output/tmp-checkpoint-33100/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-24 08:19:38,689 >> Special tokens file saved in ./output/tmp-checkpoint-33100/special_tokens_map.json
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-24 08:28:11,518 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-24 08:28:11,518 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-24 08:28:11,518 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-24 08:30:20,296 >> Saving model checkpoint to ./output/tmp-checkpoint-33200
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-24 08:30:20,435 >> tokenizer config file saved in ./output/tmp-checkpoint-33200/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-24 08:30:20,435 >> Special tokens file saved in ./output/tmp-checkpoint-33200/special_tokens_map.json
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-24 08:38:48,807 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-24 08:38:48,807 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-24 08:38:48,808 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-24 08:40:57,582 >> Saving model checkpoint to ./output/tmp-checkpoint-33300
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-24 08:40:57,720 >> tokenizer config file saved in ./output/tmp-checkpoint-33300/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-24 08:40:57,720 >> Special tokens file saved in ./output/tmp-checkpoint-33300/special_tokens_map.json
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-24 08:49:31,949 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-24 08:49:31,949 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-24 08:49:31,949 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-24 08:51:41,403 >> Saving model checkpoint to ./output/tmp-checkpoint-33400
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-24 08:51:41,546 >> tokenizer config file saved in ./output/tmp-checkpoint-33400/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-24 08:51:41,546 >> Special tokens file saved in ./output/tmp-checkpoint-33400/special_tokens_map.json
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
********************on step end call back********************
Step 33040 finish
{'loss': 0.2229, 'grad_norm': 0.9790392518043518, 'learning_rate': 4.0219780219780224e-05, 'epoch': 6.01}
********************on step end call back********************
Step 33050 finish
{'loss': 0.1992, 'grad_norm': 1.1568015813827515, 'learning_rate': 4.02014652014652e-05, 'epoch': 6.01}
********************on step end call back********************
Step 33060 finish
{'loss': 0.1978, 'grad_norm': 1.024288535118103, 'learning_rate': 4.018315018315019e-05, 'epoch': 6.01}
********************on step end call back********************
Step 33070 finish
{'loss': 0.2021, 'grad_norm': 1.1546884775161743, 'learning_rate': 4.016483516483517e-05, 'epoch': 6.01}
********************on step end call back********************
Step 33080 finish
{'loss': 0.2222, 'grad_norm': 1.3333443403244019, 'learning_rate': 4.014652014652015e-05, 'epoch': 6.01}
********************on step end call back********************
Step 33090 finish
{'loss': 0.2162, 'grad_norm': 1.1681901216506958, 'learning_rate': 4.012820512820513e-05, 'epoch': 6.02}
********************on step end call back********************
Step 33100 finish
{'loss': 0.207, 'grad_norm': 1.1078628301620483, 'learning_rate': 4.010989010989011e-05, 'epoch': 6.02}
{'eval_loss': 0.38187375664711, 'eval_accuracy': 0.8645833333333334, 'eval_runtime': 132.0279, 'eval_samples_per_second': 4.832, 'eval_steps_per_second': 4.832, 'epoch': 6.02}
********************save call back********************
********************on step end call back********************
Step 33110 finish
{'loss': 0.1734, 'grad_norm': 1.4330071210861206, 'learning_rate': 4.0091575091575096e-05, 'epoch': 6.02}
********************on step end call back********************
Step 33120 finish
{'loss': 0.198, 'grad_norm': 1.0713386535644531, 'learning_rate': 4.0073260073260075e-05, 'epoch': 6.02}
********************on step end call back********************
Step 33130 finish
{'loss': 0.1829, 'grad_norm': 0.9207273721694946, 'learning_rate': 4.005494505494506e-05, 'epoch': 6.02}
********************on step end call back********************
Step 33140 finish
{'loss': 0.2097, 'grad_norm': 1.2841423749923706, 'learning_rate': 4.003663003663004e-05, 'epoch': 6.02}
********************on step end call back********************
Step 33150 finish
{'loss': 0.1929, 'grad_norm': 1.287048101425171, 'learning_rate': 4.0018315018315025e-05, 'epoch': 6.03}
********************on step end call back********************
Step 33160 finish
{'loss': 0.2102, 'grad_norm': 1.0625605583190918, 'learning_rate': 4e-05, 'epoch': 6.03}
********************on step end call back********************
Step 33170 finish
{'loss': 0.2123, 'grad_norm': 1.5894516706466675, 'learning_rate': 3.998168498168499e-05, 'epoch': 6.03}
********************on step end call back********************
Step 33180 finish
{'loss': 0.2083, 'grad_norm': 1.199906587600708, 'learning_rate': 3.996336996336997e-05, 'epoch': 6.03}
********************on step end call back********************
Step 33190 finish
{'loss': 0.1991, 'grad_norm': 1.0913159847259521, 'learning_rate': 3.9945054945054946e-05, 'epoch': 6.03}
********************on step end call back********************
Step 33200 finish
{'loss': 0.1995, 'grad_norm': 1.4306058883666992, 'learning_rate': 3.992673992673993e-05, 'epoch': 6.04}
{'eval_loss': 0.38110557198524475, 'eval_accuracy': 0.875, 'eval_runtime': 128.7769, 'eval_samples_per_second': 4.954, 'eval_steps_per_second': 4.954, 'epoch': 6.04}
********************save call back********************
********************on step end call back********************
Step 33210 finish
{'loss': 0.1936, 'grad_norm': 1.270841121673584, 'learning_rate': 3.990842490842491e-05, 'epoch': 6.04}
********************on step end call back********************
Step 33220 finish
{'loss': 0.2138, 'grad_norm': 1.5744619369506836, 'learning_rate': 3.9890109890109896e-05, 'epoch': 6.04}
********************on step end call back********************
Step 33230 finish
{'loss': 0.2301, 'grad_norm': 1.6141051054000854, 'learning_rate': 3.9871794871794875e-05, 'epoch': 6.04}
********************on step end call back********************
Step 33240 finish
{'loss': 0.1834, 'grad_norm': 1.2895995378494263, 'learning_rate': 3.985347985347986e-05, 'epoch': 6.04}
********************on step end call back********************
Step 33250 finish
{'loss': 0.1934, 'grad_norm': 0.9731007218360901, 'learning_rate': 3.983516483516483e-05, 'epoch': 6.04}
********************on step end call back********************
Step 33260 finish
{'loss': 0.1829, 'grad_norm': 1.445206880569458, 'learning_rate': 3.981684981684982e-05, 'epoch': 6.05}
********************on step end call back********************
Step 33270 finish
{'loss': 0.1944, 'grad_norm': 1.3239389657974243, 'learning_rate': 3.97985347985348e-05, 'epoch': 6.05}
********************on step end call back********************
Step 33280 finish
{'loss': 0.1899, 'grad_norm': 1.417236089706421, 'learning_rate': 3.978021978021978e-05, 'epoch': 6.05}
********************on step end call back********************
Step 33290 finish
{'loss': 0.2247, 'grad_norm': 1.1759892702102661, 'learning_rate': 3.976190476190476e-05, 'epoch': 6.05}
********************on step end call back********************
Step 33300 finish
{'loss': 0.1918, 'grad_norm': 1.085282325744629, 'learning_rate': 3.974358974358974e-05, 'epoch': 6.05}
{'eval_loss': 0.37883225083351135, 'eval_accuracy': 0.8958333333333334, 'eval_runtime': 128.7734, 'eval_samples_per_second': 4.954, 'eval_steps_per_second': 4.954, 'epoch': 6.05}
********************save call back********************
********************on step end call back********************
Step 33310 finish
{'loss': 0.2104, 'grad_norm': 1.2484238147735596, 'learning_rate': 3.9725274725274725e-05, 'epoch': 6.06}
********************on step end call back********************
Step 33320 finish
{'loss': 0.1846, 'grad_norm': 1.125028371810913, 'learning_rate': 3.9706959706959704e-05, 'epoch': 6.06}
********************on step end call back********************
Step 33330 finish
{'loss': 0.1979, 'grad_norm': 0.944279670715332, 'learning_rate': 3.968864468864469e-05, 'epoch': 6.06}
********************on step end call back********************
Step 33340 finish
{'loss': 0.1999, 'grad_norm': 1.2074174880981445, 'learning_rate': 3.967032967032967e-05, 'epoch': 6.06}
********************on step end call back********************
Step 33350 finish
{'loss': 0.1703, 'grad_norm': 1.6433608531951904, 'learning_rate': 3.9652014652014654e-05, 'epoch': 6.06}
********************on step end call back********************
Step 33360 finish
{'loss': 0.2166, 'grad_norm': 1.1174641847610474, 'learning_rate': 3.963369963369963e-05, 'epoch': 6.06}
********************on step end call back********************
Step 33370 finish
{'loss': 0.2018, 'grad_norm': 1.0359348058700562, 'learning_rate': 3.961538461538462e-05, 'epoch': 6.07}
********************on step end call back********************
Step 33380 finish
{'loss': 0.1915, 'grad_norm': 1.1012293100357056, 'learning_rate': 3.95970695970696e-05, 'epoch': 6.07}
********************on step end call back********************
Step 33390 finish
{'loss': 0.2028, 'grad_norm': 1.2251418828964233, 'learning_rate': 3.9578754578754576e-05, 'epoch': 6.07}
********************on step end call back********************
Step 33400 finish
{'loss': 0.212, 'grad_norm': 1.0344219207763672, 'learning_rate': 3.956043956043956e-05, 'epoch': 6.07}
{'eval_loss': 0.3708548843860626, 'eval_accuracy': 0.8854166666666666, 'eval_runtime': 129.4536, 'eval_samples_per_second': 4.928, 'eval_steps_per_second': 4.928, 'epoch': 6.07}
********************save call back********************
********************on step end call back********************
Step 33410 finish
{'loss': 0.1947, 'grad_norm': 1.4446684122085571, 'learning_rate': 3.954212454212454e-05, 'epoch': 6.07}
********************on step end call back********************
Step 33420 finish
{'loss': 0.189, 'grad_norm': 1.238693118095398, 'learning_rate': 3.9523809523809526e-05, 'epoch': 6.08}
********************on step end call back********************
Step 33430 finish[INFO|trainer.py:3376] 2024-03-24 09:00:26,249 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-24 09:00:26,249 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-24 09:00:26,249 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-24 09:02:35,151 >> Saving model checkpoint to ./output/tmp-checkpoint-33500
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-24 09:02:35,291 >> tokenizer config file saved in ./output/tmp-checkpoint-33500/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-24 09:02:35,291 >> Special tokens file saved in ./output/tmp-checkpoint-33500/special_tokens_map.json
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-24 09:11:03,223 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-24 09:11:03,223 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-24 09:11:03,223 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-24 09:13:12,057 >> Saving model checkpoint to ./output/tmp-checkpoint-33600
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-24 09:13:12,196 >> tokenizer config file saved in ./output/tmp-checkpoint-33600/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-24 09:13:12,196 >> Special tokens file saved in ./output/tmp-checkpoint-33600/special_tokens_map.json
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-24 09:21:52,098 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-24 09:21:52,098 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-24 09:21:52,098 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-24 09:24:00,990 >> Saving model checkpoint to ./output/tmp-checkpoint-33700
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-24 09:24:01,129 >> tokenizer config file saved in ./output/tmp-checkpoint-33700/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-24 09:24:01,129 >> Special tokens file saved in ./output/tmp-checkpoint-33700/special_tokens_map.json
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-24 09:32:35,378 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-24 09:32:35,378 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-24 09:32:35,378 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-24 09:34:44,252 >> Saving model checkpoint to ./output/tmp-checkpoint-33800
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-24 09:34:44,394 >> tokenizer config file saved in ./output/tmp-checkpoint-33800/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-24 09:34:44,394 >> Special tokens file saved in ./output/tmp-checkpoint-33800/special_tokens_map.json
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(

{'loss': 0.2392, 'grad_norm': 0.8645321130752563, 'learning_rate': 3.9505494505494504e-05, 'epoch': 6.08}
********************on step end call back********************
Step 33440 finish
{'loss': 0.2201, 'grad_norm': 1.322867512702942, 'learning_rate': 3.948717948717949e-05, 'epoch': 6.08}
********************on step end call back********************
Step 33450 finish
{'loss': 0.1897, 'grad_norm': 1.2201944589614868, 'learning_rate': 3.946886446886447e-05, 'epoch': 6.08}
********************on step end call back********************
Step 33460 finish
{'loss': 0.2472, 'grad_norm': 1.2633028030395508, 'learning_rate': 3.9450549450549454e-05, 'epoch': 6.08}
********************on step end call back********************
Step 33470 finish
{'loss': 0.2151, 'grad_norm': 1.1934529542922974, 'learning_rate': 3.943223443223443e-05, 'epoch': 6.08}
********************on step end call back********************
Step 33480 finish
{'loss': 0.2264, 'grad_norm': 1.082322120666504, 'learning_rate': 3.941391941391941e-05, 'epoch': 6.09}
********************on step end call back********************
Step 33490 finish
{'loss': 0.1944, 'grad_norm': 1.0020673274993896, 'learning_rate': 3.93956043956044e-05, 'epoch': 6.09}
********************on step end call back********************
Step 33500 finish
{'loss': 0.2074, 'grad_norm': 1.2725512981414795, 'learning_rate': 3.9377289377289376e-05, 'epoch': 6.09}
{'eval_loss': 0.3708164691925049, 'eval_accuracy': 0.8645833333333334, 'eval_runtime': 128.9013, 'eval_samples_per_second': 4.95, 'eval_steps_per_second': 4.95, 'epoch': 6.09}
********************save call back********************
********************on step end call back********************
Step 33510 finish
{'loss': 0.1961, 'grad_norm': 0.9388559460639954, 'learning_rate': 3.935897435897436e-05, 'epoch': 6.09}
********************on step end call back********************
Step 33520 finish
{'loss': 0.2026, 'grad_norm': 1.0017375946044922, 'learning_rate': 3.934065934065934e-05, 'epoch': 6.09}
********************on step end call back********************
Step 33530 finish
{'loss': 0.215, 'grad_norm': 1.290284514427185, 'learning_rate': 3.9322344322344326e-05, 'epoch': 6.1}
********************on step end call back********************
Step 33540 finish
{'loss': 0.1841, 'grad_norm': 1.1605169773101807, 'learning_rate': 3.9304029304029304e-05, 'epoch': 6.1}
********************on step end call back********************
Step 33550 finish
{'loss': 0.1806, 'grad_norm': 1.262880563735962, 'learning_rate': 3.928571428571429e-05, 'epoch': 6.1}
********************on step end call back********************
Step 33560 finish
{'loss': 0.2039, 'grad_norm': 1.4537720680236816, 'learning_rate': 3.926739926739927e-05, 'epoch': 6.1}
********************on step end call back********************
Step 33570 finish
{'loss': 0.1839, 'grad_norm': 0.9768860340118408, 'learning_rate': 3.924908424908425e-05, 'epoch': 6.1}
********************on step end call back********************
Step 33580 finish
{'loss': 0.1876, 'grad_norm': 1.4279451370239258, 'learning_rate': 3.923076923076923e-05, 'epoch': 6.1}
********************on step end call back********************
Step 33590 finish
{'loss': 0.2139, 'grad_norm': 1.0104197263717651, 'learning_rate': 3.921245421245421e-05, 'epoch': 6.11}
********************on step end call back********************
Step 33600 finish
{'loss': 0.1936, 'grad_norm': 1.2089812755584717, 'learning_rate': 3.91941391941392e-05, 'epoch': 6.11}
{'eval_loss': 0.38038918375968933, 'eval_accuracy': 0.8541666666666666, 'eval_runtime': 128.8335, 'eval_samples_per_second': 4.952, 'eval_steps_per_second': 4.952, 'epoch': 6.11}
********************save call back********************
********************on step end call back********************
Step 33610 finish
{'loss': 0.2148, 'grad_norm': 1.3516550064086914, 'learning_rate': 3.9175824175824176e-05, 'epoch': 6.11}
********************on step end call back********************
Step 33620 finish
{'loss': 0.2219, 'grad_norm': 1.338710904121399, 'learning_rate': 3.915750915750916e-05, 'epoch': 6.11}
********************on step end call back********************
Step 33630 finish
{'loss': 0.1939, 'grad_norm': 1.3662848472595215, 'learning_rate': 3.913919413919414e-05, 'epoch': 6.11}
********************on step end call back********************
Step 33640 finish
{'loss': 0.2079, 'grad_norm': 1.2166516780853271, 'learning_rate': 3.912087912087912e-05, 'epoch': 6.12}
********************on step end call back********************
Step 33650 finish
{'loss': 0.2134, 'grad_norm': 1.0979421138763428, 'learning_rate': 3.9102564102564105e-05, 'epoch': 6.12}
********************on step end call back********************
Step 33660 finish
{'loss': 0.2011, 'grad_norm': 1.1675662994384766, 'learning_rate': 3.9084249084249083e-05, 'epoch': 6.12}
********************on step end call back********************
Step 33670 finish
{'loss': 0.2236, 'grad_norm': 1.1056355237960815, 'learning_rate': 3.906593406593407e-05, 'epoch': 6.12}
********************on step end call back********************
Step 33680 finish
{'loss': 0.1909, 'grad_norm': 1.200234055519104, 'learning_rate': 3.904761904761905e-05, 'epoch': 6.12}
********************on step end call back********************
Step 33690 finish
{'loss': 0.2064, 'grad_norm': 0.8418554663658142, 'learning_rate': 3.902930402930403e-05, 'epoch': 6.12}
********************on step end call back********************
Step 33700 finish
{'loss': 0.2136, 'grad_norm': 1.1674132347106934, 'learning_rate': 3.901098901098901e-05, 'epoch': 6.13}
{'eval_loss': 0.3770604133605957, 'eval_accuracy': 0.875, 'eval_runtime': 128.8903, 'eval_samples_per_second': 4.95, 'eval_steps_per_second': 4.95, 'epoch': 6.13}
********************save call back********************
********************on step end call back********************
Step 33710 finish
{'loss': 0.1877, 'grad_norm': 1.0597416162490845, 'learning_rate': 3.8992673992674e-05, 'epoch': 6.13}
********************on step end call back********************
Step 33720 finish
{'loss': 0.179, 'grad_norm': 1.3335535526275635, 'learning_rate': 3.8974358974358976e-05, 'epoch': 6.13}
********************on step end call back********************
Step 33730 finish
{'loss': 0.2339, 'grad_norm': 1.122536540031433, 'learning_rate': 3.8956043956043955e-05, 'epoch': 6.13}
********************on step end call back********************
Step 33740 finish
{'loss': 0.211, 'grad_norm': 1.0015885829925537, 'learning_rate': 3.893772893772894e-05, 'epoch': 6.13}
********************on step end call back********************
Step 33750 finish
{'loss': 0.1858, 'grad_norm': 1.2030316591262817, 'learning_rate': 3.891941391941392e-05, 'epoch': 6.14}
********************on step end call back********************
Step 33760 finish
{'loss': 0.2002, 'grad_norm': 1.4965434074401855, 'learning_rate': 3.8901098901098905e-05, 'epoch': 6.14}
********************on step end call back********************
Step 33770 finish
{'loss': 0.2088, 'grad_norm': 1.43781578540802, 'learning_rate': 3.8882783882783884e-05, 'epoch': 6.14}
********************on step end call back********************
Step 33780 finish
{'loss': 0.2, 'grad_norm': 0.9235832691192627, 'learning_rate': 3.886446886446887e-05, 'epoch': 6.14}
********************on step end call back********************
Step 33790 finish
{'loss': 0.195, 'grad_norm': 1.2847474813461304, 'learning_rate': 3.884615384615385e-05, 'epoch': 6.14}
********************on step end call back********************
Step 33800 finish
{'loss': 0.2041, 'grad_norm': 0.9918239116668701, 'learning_rate': 3.8827838827838833e-05, 'epoch': 6.14}
{'eval_loss': 0.3714103102684021, 'eval_accuracy': 0.8541666666666666, 'eval_runtime': 128.8733, 'eval_samples_per_second': 4.951, 'eval_steps_per_second': 4.951, 'epoch': 6.14}
********************save call back********************
********************on step end call back********************
Step 33810 finish
{'loss': 0.2123, 'grad_norm': 1.0777636766433716, 'learning_rate': 3.880952380952381e-05, 'epoch': 6.15}
********************on step end call back********************
Step 33820 finish
[INFO|trainer.py:3376] 2024-03-24 09:43:21,524 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-24 09:43:21,524 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-24 09:43:21,524 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-24 09:45:32,031 >> Saving model checkpoint to ./output/tmp-checkpoint-33900
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-24 09:45:32,185 >> tokenizer config file saved in ./output/tmp-checkpoint-33900/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-24 09:45:32,185 >> Special tokens file saved in ./output/tmp-checkpoint-33900/special_tokens_map.json
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-24 09:54:08,974 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-24 09:54:08,974 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-24 09:54:08,974 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-24 09:56:18,167 >> Saving model checkpoint to ./output/tmp-checkpoint-34000
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-24 09:56:18,308 >> tokenizer config file saved in ./output/tmp-checkpoint-34000/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-24 09:56:18,308 >> Special tokens file saved in ./output/tmp-checkpoint-34000/special_tokens_map.json
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-24 10:04:47,841 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-24 10:04:47,841 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-24 10:04:47,841 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-24 10:06:56,768 >> Saving model checkpoint to ./output/tmp-checkpoint-34100
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-24 10:06:56,907 >> tokenizer config file saved in ./output/tmp-checkpoint-34100/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-24 10:06:56,907 >> Special tokens file saved in ./output/tmp-checkpoint-34100/special_tokens_map.json
[INFO|trainer.py:3159] 2024-03-24 10:06:57,145 >> Deleting older checkpoint [output/checkpoint-24100] due to args.save_total_limit
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-24 10:15:25,964 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-24 10:15:25,965 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-24 10:15:25,965 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-24 10:17:34,856 >> Saving model checkpoint to ./output/tmp-checkpoint-34200
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-24 10:17:35,152 >> tokenizer config file saved in ./output/tmp-checkpoint-34200/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-24 10:17:35,152 >> Special tokens file saved in ./output/tmp-checkpoint-34200/special_tokens_map.json
[INFO|trainer.py:3159] 2024-03-24 10:17:35,389 >> Deleting older checkpoint [output/checkpoint-24200] due to args.save_total_limit
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
{'loss': 0.1973, 'grad_norm': 1.0374348163604736, 'learning_rate': 3.879120879120879e-05, 'epoch': 6.15}
********************on step end call back********************
Step 33830 finish
{'loss': 0.2142, 'grad_norm': 1.1655641794204712, 'learning_rate': 3.8772893772893777e-05, 'epoch': 6.15}
********************on step end call back********************
Step 33840 finish
{'loss': 0.2362, 'grad_norm': 1.5035377740859985, 'learning_rate': 3.8754578754578755e-05, 'epoch': 6.15}
********************on step end call back********************
Step 33850 finish
{'loss': 0.2093, 'grad_norm': 1.4412883520126343, 'learning_rate': 3.873626373626374e-05, 'epoch': 6.15}
********************on step end call back********************
Step 33860 finish
{'loss': 0.1955, 'grad_norm': 1.3566263914108276, 'learning_rate': 3.871794871794872e-05, 'epoch': 6.16}
********************on step end call back********************
Step 33870 finish
{'loss': 0.2056, 'grad_norm': 1.1517137289047241, 'learning_rate': 3.8699633699633705e-05, 'epoch': 6.16}
********************on step end call back********************
Step 33880 finish
{'loss': 0.1974, 'grad_norm': 1.839701533317566, 'learning_rate': 3.8681318681318684e-05, 'epoch': 6.16}
********************on step end call back********************
Step 33890 finish
{'loss': 0.2027, 'grad_norm': 1.0327589511871338, 'learning_rate': 3.866300366300367e-05, 'epoch': 6.16}
********************on step end call back********************
Step 33900 finish
{'loss': 0.2155, 'grad_norm': 1.33100163936615, 'learning_rate': 3.864468864468865e-05, 'epoch': 6.16}
{'eval_loss': 0.3756246566772461, 'eval_accuracy': 0.8958333333333334, 'eval_runtime': 130.5064, 'eval_samples_per_second': 4.889, 'eval_steps_per_second': 4.889, 'epoch': 6.16}
********************save call back********************
********************on step end call back********************
Step 33910 finish
{'loss': 0.219, 'grad_norm': 1.2711987495422363, 'learning_rate': 3.862637362637363e-05, 'epoch': 6.16}
********************on step end call back********************
Step 33920 finish
{'loss': 0.1881, 'grad_norm': 1.3635627031326294, 'learning_rate': 3.860805860805861e-05, 'epoch': 6.17}
********************on step end call back********************
Step 33930 finish
{'loss': 0.1993, 'grad_norm': 1.6344786882400513, 'learning_rate': 3.858974358974359e-05, 'epoch': 6.17}
********************on step end call back********************
Step 33940 finish
{'loss': 0.201, 'grad_norm': 1.2929705381393433, 'learning_rate': 3.857142857142858e-05, 'epoch': 6.17}
********************on step end call back********************
Step 33950 finish
{'loss': 0.2009, 'grad_norm': 1.2801140546798706, 'learning_rate': 3.8553113553113556e-05, 'epoch': 6.17}
********************on step end call back********************
Step 33960 finish
{'loss': 0.2211, 'grad_norm': 1.0857707262039185, 'learning_rate': 3.853479853479854e-05, 'epoch': 6.17}
********************on step end call back********************
Step 33970 finish
{'loss': 0.1884, 'grad_norm': 1.4165503978729248, 'learning_rate': 3.851648351648352e-05, 'epoch': 6.18}
********************on step end call back********************
Step 33980 finish
{'loss': 0.2042, 'grad_norm': 0.8722745776176453, 'learning_rate': 3.8498168498168505e-05, 'epoch': 6.18}
********************on step end call back********************
Step 33990 finish
{'loss': 0.1818, 'grad_norm': 1.1423249244689941, 'learning_rate': 3.8479853479853484e-05, 'epoch': 6.18}
********************on step end call back********************
Step 34000 finish
{'loss': 0.192, 'grad_norm': 1.1942604780197144, 'learning_rate': 3.846153846153846e-05, 'epoch': 6.18}
{'eval_loss': 0.38208824396133423, 'eval_accuracy': 0.8854166666666666, 'eval_runtime': 129.1914, 'eval_samples_per_second': 4.938, 'eval_steps_per_second': 4.938, 'epoch': 6.18}
********************save call back********************
********************on step end call back********************
Step 34010 finish
{'loss': 0.2248, 'grad_norm': 1.3390707969665527, 'learning_rate': 3.844322344322345e-05, 'epoch': 6.18}
********************on step end call back********************
Step 34020 finish
{'loss': 0.1829, 'grad_norm': 1.171433687210083, 'learning_rate': 3.842490842490843e-05, 'epoch': 6.18}
********************on step end call back********************
Step 34030 finish
{'loss': 0.212, 'grad_norm': 1.0179405212402344, 'learning_rate': 3.840659340659341e-05, 'epoch': 6.19}
********************on step end call back********************
Step 34040 finish
{'loss': 0.2354, 'grad_norm': 1.698415756225586, 'learning_rate': 3.838827838827839e-05, 'epoch': 6.19}
********************on step end call back********************
Step 34050 finish
{'loss': 0.2164, 'grad_norm': 1.320180892944336, 'learning_rate': 3.836996336996338e-05, 'epoch': 6.19}
********************on step end call back********************
Step 34060 finish
{'loss': 0.1874, 'grad_norm': 1.200075626373291, 'learning_rate': 3.8351648351648356e-05, 'epoch': 6.19}
********************on step end call back********************
Step 34070 finish
{'loss': 0.1938, 'grad_norm': 1.2967135906219482, 'learning_rate': 3.8333333333333334e-05, 'epoch': 6.19}
********************on step end call back********************
Step 34080 finish
{'loss': 0.2016, 'grad_norm': 1.3404209613800049, 'learning_rate': 3.831501831501832e-05, 'epoch': 6.2}
********************on step end call back********************
Step 34090 finish
{'loss': 0.1995, 'grad_norm': 1.5215741395950317, 'learning_rate': 3.82967032967033e-05, 'epoch': 6.2}
********************on step end call back********************
Step 34100 finish
{'loss': 0.2101, 'grad_norm': 1.1176773309707642, 'learning_rate': 3.827838827838828e-05, 'epoch': 6.2}
{'eval_loss': 0.3782969117164612, 'eval_accuracy': 0.90625, 'eval_runtime': 128.9256, 'eval_samples_per_second': 4.949, 'eval_steps_per_second': 4.949, 'epoch': 6.2}
********************save call back********************
********************on step end call back********************
Step 34110 finish
{'loss': 0.2132, 'grad_norm': 0.9334336519241333, 'learning_rate': 3.8260073260073256e-05, 'epoch': 6.2}
********************on step end call back********************
Step 34120 finish
{'loss': 0.2052, 'grad_norm': 1.033989667892456, 'learning_rate': 3.824175824175824e-05, 'epoch': 6.2}
********************on step end call back********************
Step 34130 finish
{'loss': 0.2267, 'grad_norm': 1.1893908977508545, 'learning_rate': 3.822344322344322e-05, 'epoch': 6.2}
********************on step end call back********************
Step 34140 finish
{'loss': 0.2022, 'grad_norm': 1.4589228630065918, 'learning_rate': 3.8205128205128206e-05, 'epoch': 6.21}
********************on step end call back********************
Step 34150 finish
{'loss': 0.2116, 'grad_norm': 1.0436514616012573, 'learning_rate': 3.8186813186813185e-05, 'epoch': 6.21}
********************on step end call back********************
Step 34160 finish
{'loss': 0.2074, 'grad_norm': 1.0429840087890625, 'learning_rate': 3.816849816849817e-05, 'epoch': 6.21}
********************on step end call back********************
Step 34170 finish
{'loss': 0.1951, 'grad_norm': 1.1749006509780884, 'learning_rate': 3.815018315018315e-05, 'epoch': 6.21}
********************on step end call back********************
Step 34180 finish
{'loss': 0.2112, 'grad_norm': 1.357250452041626, 'learning_rate': 3.8131868131868135e-05, 'epoch': 6.21}
********************on step end call back********************
Step 34190 finish
{'loss': 0.2229, 'grad_norm': 1.2341469526290894, 'learning_rate': 3.8113553113553113e-05, 'epoch': 6.22}
********************on step end call back********************
Step 34200 finish
{'loss': 0.2099, 'grad_norm': 1.0870243310928345, 'learning_rate': 3.809523809523809e-05, 'epoch': 6.22}
{'eval_loss': 0.37329521775245667, 'eval_accuracy': 0.8854166666666666, 'eval_runtime': 128.8906, 'eval_samples_per_second': 4.95, 'eval_steps_per_second': 4.95, 'epoch': 6.22}
********************save call back********************
********************on step end call back********************
Step 34210 finish
[INFO|trainer.py:3376] 2024-03-24 10:26:04,243 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-24 10:26:04,243 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-24 10:26:04,243 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-24 10:28:13,258 >> Saving model checkpoint to ./output/tmp-checkpoint-34300
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-24 10:28:13,549 >> tokenizer config file saved in ./output/tmp-checkpoint-34300/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-24 10:28:13,550 >> Special tokens file saved in ./output/tmp-checkpoint-34300/special_tokens_map.json
[INFO|trainer.py:3159] 2024-03-24 10:28:13,919 >> Deleting older checkpoint [output/checkpoint-24300] due to args.save_total_limit
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-24 10:36:52,416 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-24 10:36:52,416 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-24 10:36:52,416 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-24 10:39:01,428 >> Saving model checkpoint to ./output/tmp-checkpoint-34400
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-24 10:39:01,595 >> tokenizer config file saved in ./output/tmp-checkpoint-34400/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-24 10:39:01,595 >> Special tokens file saved in ./output/tmp-checkpoint-34400/special_tokens_map.json
[INFO|trainer.py:3159] 2024-03-24 10:39:01,834 >> Deleting older checkpoint [output/checkpoint-24400] due to args.save_total_limit
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-24 10:47:42,619 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-24 10:47:42,620 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-24 10:47:42,620 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-24 10:49:51,752 >> Saving model checkpoint to ./output/tmp-checkpoint-34500
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-24 10:49:51,999 >> tokenizer config file saved in ./output/tmp-checkpoint-34500/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-24 10:49:52,000 >> Special tokens file saved in ./output/tmp-checkpoint-34500/special_tokens_map.json
[INFO|trainer.py:3159] 2024-03-24 10:49:52,246 >> Deleting older checkpoint [output/checkpoint-24500] due to args.save_total_limit
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-24 10:58:31,064 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-24 10:58:31,064 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-24 10:58:31,064 >>   Batch size = 1
{'loss': 0.2135, 'grad_norm': 1.1153510808944702, 'learning_rate': 3.807692307692308e-05, 'epoch': 6.22}
********************on step end call back********************
Step 34220 finish
{'loss': 0.1961, 'grad_norm': 1.0564883947372437, 'learning_rate': 3.8058608058608057e-05, 'epoch': 6.22}
********************on step end call back********************
Step 34230 finish
{'loss': 0.188, 'grad_norm': 1.0788706541061401, 'learning_rate': 3.804029304029304e-05, 'epoch': 6.22}
********************on step end call back********************
Step 34240 finish
{'loss': 0.188, 'grad_norm': 1.2400606870651245, 'learning_rate': 3.802197802197802e-05, 'epoch': 6.22}
********************on step end call back********************
Step 34250 finish
{'loss': 0.2272, 'grad_norm': 1.400878667831421, 'learning_rate': 3.8003663003663006e-05, 'epoch': 6.23}
********************on step end call back********************
Step 34260 finish
{'loss': 0.2059, 'grad_norm': 0.9928277730941772, 'learning_rate': 3.7985347985347985e-05, 'epoch': 6.23}
********************on step end call back********************
Step 34270 finish
{'loss': 0.2117, 'grad_norm': 1.1188533306121826, 'learning_rate': 3.7967032967032964e-05, 'epoch': 6.23}
********************on step end call back********************
Step 34280 finish
{'loss': 0.2068, 'grad_norm': 1.2586874961853027, 'learning_rate': 3.794871794871795e-05, 'epoch': 6.23}
********************on step end call back********************
Step 34290 finish
{'loss': 0.1774, 'grad_norm': 1.5800056457519531, 'learning_rate': 3.793040293040293e-05, 'epoch': 6.23}
********************on step end call back********************
Step 34300 finish
{'loss': 0.2117, 'grad_norm': 0.9400688409805298, 'learning_rate': 3.7912087912087914e-05, 'epoch': 6.24}
{'eval_loss': 0.3748065233230591, 'eval_accuracy': 0.8854166666666666, 'eval_runtime': 129.0142, 'eval_samples_per_second': 4.945, 'eval_steps_per_second': 4.945, 'epoch': 6.24}
********************save call back********************
********************on step end call back********************
Step 34310 finish
{'loss': 0.2163, 'grad_norm': 1.2706937789916992, 'learning_rate': 3.789377289377289e-05, 'epoch': 6.24}
********************on step end call back********************
Step 34320 finish
{'loss': 0.1891, 'grad_norm': 1.5688891410827637, 'learning_rate': 3.787545787545788e-05, 'epoch': 6.24}
********************on step end call back********************
Step 34330 finish
{'loss': 0.2158, 'grad_norm': 1.3493903875350952, 'learning_rate': 3.785714285714286e-05, 'epoch': 6.24}
********************on step end call back********************
Step 34340 finish
{'loss': 0.2095, 'grad_norm': 1.357438564300537, 'learning_rate': 3.783882783882784e-05, 'epoch': 6.24}
********************on step end call back********************
Step 34350 finish
{'loss': 0.2144, 'grad_norm': 1.0252673625946045, 'learning_rate': 3.782051282051282e-05, 'epoch': 6.24}
********************on step end call back********************
Step 34360 finish
{'loss': 0.2092, 'grad_norm': 1.0149495601654053, 'learning_rate': 3.78021978021978e-05, 'epoch': 6.25}
********************on step end call back********************
Step 34370 finish
{'loss': 0.2012, 'grad_norm': 1.2185957431793213, 'learning_rate': 3.7783882783882785e-05, 'epoch': 6.25}
********************on step end call back********************
Step 34380 finish
{'loss': 0.239, 'grad_norm': 1.8521580696105957, 'learning_rate': 3.7765567765567764e-05, 'epoch': 6.25}
********************on step end call back********************
Step 34390 finish
{'loss': 0.1903, 'grad_norm': 1.3004512786865234, 'learning_rate': 3.774725274725275e-05, 'epoch': 6.25}
********************on step end call back********************
Step 34400 finish
{'loss': 0.2048, 'grad_norm': 1.0437744855880737, 'learning_rate': 3.772893772893773e-05, 'epoch': 6.25}
{'eval_loss': 0.3750438988208771, 'eval_accuracy': 0.8958333333333334, 'eval_runtime': 129.0103, 'eval_samples_per_second': 4.945, 'eval_steps_per_second': 4.945, 'epoch': 6.25}
********************save call back********************
********************on step end call back********************
Step 34410 finish
{'loss': 0.2107, 'grad_norm': 1.161908745765686, 'learning_rate': 3.7710622710622714e-05, 'epoch': 6.26}
********************on step end call back********************
Step 34420 finish
{'loss': 0.2152, 'grad_norm': 1.0421550273895264, 'learning_rate': 3.769230769230769e-05, 'epoch': 6.26}
********************on step end call back********************
Step 34430 finish
{'loss': 0.2471, 'grad_norm': 1.0535476207733154, 'learning_rate': 3.767399267399268e-05, 'epoch': 6.26}
********************on step end call back********************
Step 34440 finish
{'loss': 0.1791, 'grad_norm': 1.0582630634307861, 'learning_rate': 3.765567765567766e-05, 'epoch': 6.26}
********************on step end call back********************
Step 34450 finish
{'loss': 0.2068, 'grad_norm': 1.5746331214904785, 'learning_rate': 3.7637362637362636e-05, 'epoch': 6.26}
********************on step end call back********************
Step 34460 finish
{'loss': 0.1879, 'grad_norm': 1.4039390087127686, 'learning_rate': 3.761904761904762e-05, 'epoch': 6.26}
********************on step end call back********************
Step 34470 finish
{'loss': 0.1957, 'grad_norm': 1.18514883518219, 'learning_rate': 3.76007326007326e-05, 'epoch': 6.27}
********************on step end call back********************
Step 34480 finish
{'loss': 0.2027, 'grad_norm': 1.325654149055481, 'learning_rate': 3.7582417582417586e-05, 'epoch': 6.27}
********************on step end call back********************
Step 34490 finish
{'loss': 0.2129, 'grad_norm': 1.4057255983352661, 'learning_rate': 3.7564102564102564e-05, 'epoch': 6.27}
********************on step end call back********************
Step 34500 finish
{'loss': 0.2055, 'grad_norm': 1.229668378829956, 'learning_rate': 3.754578754578755e-05, 'epoch': 6.27}
{'eval_loss': 0.3782777190208435, 'eval_accuracy': 0.8645833333333334, 'eval_runtime': 129.1315, 'eval_samples_per_second': 4.941, 'eval_steps_per_second': 4.941, 'epoch': 6.27}
********************save call back********************
********************on step end call back********************
Step 34510 finish
{'loss': 0.1928, 'grad_norm': 1.7231321334838867, 'learning_rate': 3.752747252747253e-05, 'epoch': 6.27}
********************on step end call back********************
Step 34520 finish
{'loss': 0.1912, 'grad_norm': 1.5452977418899536, 'learning_rate': 3.7509157509157514e-05, 'epoch': 6.28}
********************on step end call back********************
Step 34530 finish
{'loss': 0.1966, 'grad_norm': 1.0787540674209595, 'learning_rate': 3.749084249084249e-05, 'epoch': 6.28}
********************on step end call back********************
Step 34540 finish
{'loss': 0.1845, 'grad_norm': 1.1738179922103882, 'learning_rate': 3.747252747252747e-05, 'epoch': 6.28}
********************on step end call back********************
Step 34550 finish
{'loss': 0.2205, 'grad_norm': 1.1379129886627197, 'learning_rate': 3.745421245421246e-05, 'epoch': 6.28}
********************on step end call back********************
Step 34560 finish
{'loss': 0.2539, 'grad_norm': 1.2949968576431274, 'learning_rate': 3.7435897435897436e-05, 'epoch': 6.28}
********************on step end call back********************
Step 34570 finish
{'loss': 0.1845, 'grad_norm': 1.2404851913452148, 'learning_rate': 3.741758241758242e-05, 'epoch': 6.28}
********************on step end call back********************
Step 34580 finish
{'loss': 0.1895, 'grad_norm': 1.2023167610168457, 'learning_rate': 3.73992673992674e-05, 'epoch': 6.29}
********************on step end call back********************
Step 34590 finish
{'loss': 0.2356, 'grad_norm': 1.4575163125991821, 'learning_rate': 3.7380952380952386e-05, 'epoch': 6.29}
********************on step end call back********************
Step 34600 finish
{'loss': 0.2085, 'grad_norm': 1.1416258811950684, 'learning_rate': 3.7362637362637365e-05, 'epoch': 6.29}
[INFO|trainer.py:3067] 2024-03-24 11:00:40,232 >> Saving model checkpoint to ./output/tmp-checkpoint-34600
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-24 11:00:40,374 >> tokenizer config file saved in ./output/tmp-checkpoint-34600/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-24 11:00:40,375 >> Special tokens file saved in ./output/tmp-checkpoint-34600/special_tokens_map.json
[INFO|trainer.py:3159] 2024-03-24 11:00:40,606 >> Deleting older checkpoint [output/checkpoint-24600] due to args.save_total_limit
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-24 11:09:15,887 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-24 11:09:15,887 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-24 11:09:15,887 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-24 11:11:25,154 >> Saving model checkpoint to ./output/tmp-checkpoint-34700
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-24 11:11:25,300 >> tokenizer config file saved in ./output/tmp-checkpoint-34700/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-24 11:11:25,300 >> Special tokens file saved in ./output/tmp-checkpoint-34700/special_tokens_map.json
[INFO|trainer.py:3159] 2024-03-24 11:11:25,537 >> Deleting older checkpoint [output/checkpoint-24700] due to args.save_total_limit
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-24 11:20:10,449 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-24 11:20:10,449 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-24 11:20:10,449 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-24 11:22:19,624 >> Saving model checkpoint to ./output/tmp-checkpoint-34800
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-24 11:22:19,817 >> tokenizer config file saved in ./output/tmp-checkpoint-34800/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-24 11:22:19,817 >> Special tokens file saved in ./output/tmp-checkpoint-34800/special_tokens_map.json
[INFO|trainer.py:3159] 2024-03-24 11:22:20,048 >> Deleting older checkpoint [output/checkpoint-24800] due to args.save_total_limit
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-24 11:30:57,199 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-24 11:30:57,199 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-24 11:30:57,199 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-24 11:33:06,355 >> Saving model checkpoint to ./output/tmp-checkpoint-34900
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-24 11:33:06,530 >> tokenizer config file saved in ./output/tmp-checkpoint-34900/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-24 11:33:06,531 >> Special tokens file saved in ./output/tmp-checkpoint-34900/special_tokens_map.json
[INFO|trainer.py:3159] 2024-03-24 11:33:06,757 >> Deleting older checkpoint [output/checkpoint-24900] due to args.save_total_limit
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
{'eval_loss': 0.3710038363933563, 'eval_accuracy': 0.8645833333333334, 'eval_runtime': 129.1664, 'eval_samples_per_second': 4.939, 'eval_steps_per_second': 4.939, 'epoch': 6.29}
********************save call back********************
********************on step end call back********************
Step 34610 finish
{'loss': 0.2067, 'grad_norm': 1.2187551259994507, 'learning_rate': 3.734432234432235e-05, 'epoch': 6.29}
********************on step end call back********************
Step 34620 finish
{'loss': 0.1747, 'grad_norm': 1.0046391487121582, 'learning_rate': 3.732600732600733e-05, 'epoch': 6.29}
********************on step end call back********************
Step 34630 finish
{'loss': 0.1928, 'grad_norm': 0.8595934510231018, 'learning_rate': 3.730769230769231e-05, 'epoch': 6.3}
********************on step end call back********************
Step 34640 finish
{'loss': 0.225, 'grad_norm': 1.244345784187317, 'learning_rate': 3.728937728937729e-05, 'epoch': 6.3}
********************on step end call back********************
Step 34650 finish
{'loss': 0.2019, 'grad_norm': 1.5182521343231201, 'learning_rate': 3.727106227106227e-05, 'epoch': 6.3}
********************on step end call back********************
Step 34660 finish
{'loss': 0.2203, 'grad_norm': 1.274865746498108, 'learning_rate': 3.725274725274726e-05, 'epoch': 6.3}
********************on step end call back********************
Step 34670 finish
{'loss': 0.2134, 'grad_norm': 1.1640607118606567, 'learning_rate': 3.7234432234432236e-05, 'epoch': 6.3}
********************on step end call back********************
Step 34680 finish
{'loss': 0.212, 'grad_norm': 1.1932873725891113, 'learning_rate': 3.721611721611722e-05, 'epoch': 6.3}
********************on step end call back********************
Step 34690 finish
{'loss': 0.2125, 'grad_norm': 1.2756403684616089, 'learning_rate': 3.71978021978022e-05, 'epoch': 6.31}
********************on step end call back********************
Step 34700 finish
{'loss': 0.2022, 'grad_norm': 1.2474547624588013, 'learning_rate': 3.717948717948718e-05, 'epoch': 6.31}
{'eval_loss': 0.37674131989479065, 'eval_accuracy': 0.875, 'eval_runtime': 129.2661, 'eval_samples_per_second': 4.936, 'eval_steps_per_second': 4.936, 'epoch': 6.31}
********************save call back********************
********************on step end call back********************
Step 34710 finish
{'loss': 0.2027, 'grad_norm': 1.4075634479522705, 'learning_rate': 3.7161172161172165e-05, 'epoch': 6.31}
********************on step end call back********************
Step 34720 finish
{'loss': 0.2197, 'grad_norm': 1.165365219116211, 'learning_rate': 3.7142857142857143e-05, 'epoch': 6.31}
********************on step end call back********************
Step 34730 finish
{'loss': 0.21, 'grad_norm': 1.4881311655044556, 'learning_rate': 3.712454212454213e-05, 'epoch': 6.31}
********************on step end call back********************
Step 34740 finish
{'loss': 0.1996, 'grad_norm': 1.2128626108169556, 'learning_rate': 3.710622710622711e-05, 'epoch': 6.32}
********************on step end call back********************
Step 34750 finish
{'loss': 0.2026, 'grad_norm': 1.2269492149353027, 'learning_rate': 3.708791208791209e-05, 'epoch': 6.32}
********************on step end call back********************
Step 34760 finish
{'loss': 0.2422, 'grad_norm': 1.1135704517364502, 'learning_rate': 3.706959706959707e-05, 'epoch': 6.32}
********************on step end call back********************
Step 34770 finish
{'loss': 0.1989, 'grad_norm': 1.1281816959381104, 'learning_rate': 3.705128205128206e-05, 'epoch': 6.32}
********************on step end call back********************
Step 34780 finish
{'loss': 0.2011, 'grad_norm': 1.698261022567749, 'learning_rate': 3.7032967032967036e-05, 'epoch': 6.32}
********************on step end call back********************
Step 34790 finish
{'loss': 0.2338, 'grad_norm': 1.0557341575622559, 'learning_rate': 3.7014652014652015e-05, 'epoch': 6.32}
********************on step end call back********************
Step 34800 finish
{'loss': 0.2308, 'grad_norm': 1.0954407453536987, 'learning_rate': 3.6996336996337e-05, 'epoch': 6.33}
{'eval_loss': 0.37173202633857727, 'eval_accuracy': 0.875, 'eval_runtime': 129.1736, 'eval_samples_per_second': 4.939, 'eval_steps_per_second': 4.939, 'epoch': 6.33}
********************save call back********************
********************on step end call back********************
Step 34810 finish
{'loss': 0.2309, 'grad_norm': 1.2833725214004517, 'learning_rate': 3.697802197802198e-05, 'epoch': 6.33}
********************on step end call back********************
Step 34820 finish
{'loss': 0.1974, 'grad_norm': 0.9960132241249084, 'learning_rate': 3.6959706959706965e-05, 'epoch': 6.33}
********************on step end call back********************
Step 34830 finish
{'loss': 0.2046, 'grad_norm': 1.2669599056243896, 'learning_rate': 3.6941391941391944e-05, 'epoch': 6.33}
********************on step end call back********************
Step 34840 finish
{'loss': 0.1946, 'grad_norm': 0.9248341917991638, 'learning_rate': 3.692307692307693e-05, 'epoch': 6.33}
********************on step end call back********************
Step 34850 finish
{'loss': 0.1962, 'grad_norm': 1.2000434398651123, 'learning_rate': 3.690476190476191e-05, 'epoch': 6.34}
********************on step end call back********************
Step 34860 finish
{'loss': 0.2337, 'grad_norm': 1.2291401624679565, 'learning_rate': 3.6886446886446894e-05, 'epoch': 6.34}
********************on step end call back********************
Step 34870 finish
{'loss': 0.221, 'grad_norm': 1.323078989982605, 'learning_rate': 3.686813186813187e-05, 'epoch': 6.34}
********************on step end call back********************
Step 34880 finish
{'loss': 0.228, 'grad_norm': 1.43739652633667, 'learning_rate': 3.684981684981685e-05, 'epoch': 6.34}
********************on step end call back********************
Step 34890 finish
{'loss': 0.1929, 'grad_norm': 1.1263450384140015, 'learning_rate': 3.6831501831501837e-05, 'epoch': 6.34}
********************on step end call back********************
Step 34900 finish
{'loss': 0.2167, 'grad_norm': 1.3120052814483643, 'learning_rate': 3.6813186813186815e-05, 'epoch': 6.34}
{'eval_loss': 0.3793572783470154, 'eval_accuracy': 0.8854166666666666, 'eval_runtime': 129.1549, 'eval_samples_per_second': 4.94, 'eval_steps_per_second': 4.94, 'epoch': 6.34}
********************save call back********************
********************on step end call back********************
Step 34910 finish
{'loss': 0.217, 'grad_norm': 1.220043420791626, 'learning_rate': 3.67948717948718e-05, 'epoch': 6.35}
********************on step end call back********************
Step 34920 finish
{'loss': 0.2073, 'grad_norm': 0.9724994897842407, 'learning_rate': 3.677655677655678e-05, 'epoch': 6.35}
********************on step end call back********************
Step 34930 finish
{'loss': 0.2006, 'grad_norm': 1.303147554397583, 'learning_rate': 3.6758241758241765e-05, 'epoch': 6.35}
********************on step end call back********************
Step 34940 finish
{'loss': 0.2115, 'grad_norm': 1.2675576210021973, 'learning_rate': 3.6739926739926744e-05, 'epoch': 6.35}
********************on step end call back********************
Step 34950 finish
{'loss': 0.1922, 'grad_norm': 1.229538917541504, 'learning_rate': 3.672161172161173e-05, 'epoch': 6.35}
********************on step end call back********************
Step 34960 finish
{'loss': 0.2014, 'grad_norm': 1.1640512943267822, 'learning_rate': 3.67032967032967e-05, 'epoch': 6.36}
********************on step end call back********************
Step 34970 finish
{'loss': 0.228, 'grad_norm': 1.2277319431304932, 'learning_rate': 3.668498168498169e-05, 'epoch': 6.36}
********************on step end call back********************
Step 34980 finish
{'loss': 0.2045, 'grad_norm': 1.1369205713272095, 'learning_rate': 3.6666666666666666e-05, 'epoch': 6.36}
********************on step end call back********************
Step 34990 finish
{'loss': 0.19, 'grad_norm': 0.8711759448051453, 'learning_rate': 3.6648351648351644e-05, 'epoch': 6.36}
********************on step end call back********************
Step 35000 finish
[INFO|trainer.py:3376] 2024-03-24 11:41:47,624 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-24 11:41:47,625 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-24 11:41:47,625 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-24 11:43:56,822 >> Saving model checkpoint to ./output/tmp-checkpoint-35000
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-24 11:43:56,968 >> tokenizer config file saved in ./output/tmp-checkpoint-35000/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-24 11:43:56,968 >> Special tokens file saved in ./output/tmp-checkpoint-35000/special_tokens_map.json
[INFO|trainer.py:3159] 2024-03-24 11:43:57,196 >> Deleting older checkpoint [output/checkpoint-25000] due to args.save_total_limit
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-24 11:52:35,291 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-24 11:52:35,292 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-24 11:52:35,292 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-24 11:54:44,112 >> Saving model checkpoint to ./output/tmp-checkpoint-35100
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-24 11:54:44,387 >> tokenizer config file saved in ./output/tmp-checkpoint-35100/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-24 11:54:44,388 >> Special tokens file saved in ./output/tmp-checkpoint-35100/special_tokens_map.json
[INFO|trainer.py:3159] 2024-03-24 11:54:44,620 >> Deleting older checkpoint [output/checkpoint-25100] due to args.save_total_limit
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-24 12:03:15,661 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-24 12:03:15,661 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-24 12:03:15,661 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-24 12:05:24,545 >> Saving model checkpoint to ./output/tmp-checkpoint-35200
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-24 12:05:24,826 >> tokenizer config file saved in ./output/tmp-checkpoint-35200/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-24 12:05:24,826 >> Special tokens file saved in ./output/tmp-checkpoint-35200/special_tokens_map.json
[INFO|trainer.py:3159] 2024-03-24 12:05:25,134 >> Deleting older checkpoint [output/checkpoint-25200] due to args.save_total_limit
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-24 12:14:06,029 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-24 12:14:06,029 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-24 12:14:06,029 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-24 12:16:14,838 >> Saving model checkpoint to ./output/tmp-checkpoint-35300
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-24 12:16:15,010 >> tokenizer config file saved in ./output/tmp-checkpoint-35300/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-24 12:16:15,010 >> Special tokens file saved in ./output/tmp-checkpoint-35300/special_tokens_map.json
[INFO|trainer.py:3159] 2024-03-24 12:16:15,283 >> Deleting older checkpoint [output/checkpoint-25300] due to args.save_total_limit
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
{'loss': 0.2037, 'grad_norm': 1.196094274520874, 'learning_rate': 3.663003663003663e-05, 'epoch': 6.36}
{'eval_loss': 0.3827749788761139, 'eval_accuracy': 0.875, 'eval_runtime': 129.1962, 'eval_samples_per_second': 4.938, 'eval_steps_per_second': 4.938, 'epoch': 6.36}
********************save call back********************
********************on step end call back********************
Step 35010 finish
{'loss': 0.2098, 'grad_norm': 1.206791877746582, 'learning_rate': 3.661172161172161e-05, 'epoch': 6.36}
********************on step end call back********************
Step 35020 finish
{'loss': 0.2058, 'grad_norm': 1.0167503356933594, 'learning_rate': 3.6593406593406594e-05, 'epoch': 6.37}
********************on step end call back********************
Step 35030 finish
{'loss': 0.191, 'grad_norm': 1.2150603532791138, 'learning_rate': 3.657509157509157e-05, 'epoch': 6.37}
********************on step end call back********************
Step 35040 finish
{'loss': 0.2079, 'grad_norm': 1.533663272857666, 'learning_rate': 3.655677655677656e-05, 'epoch': 6.37}
********************on step end call back********************
Step 35050 finish
{'loss': 0.1916, 'grad_norm': 1.2149577140808105, 'learning_rate': 3.653846153846154e-05, 'epoch': 6.37}
********************on step end call back********************
Step 35060 finish
{'loss': 0.2361, 'grad_norm': 1.4280258417129517, 'learning_rate': 3.652014652014652e-05, 'epoch': 6.37}
********************on step end call back********************
Step 35070 finish
{'loss': 0.1903, 'grad_norm': 1.2776343822479248, 'learning_rate': 3.65018315018315e-05, 'epoch': 6.38}
********************on step end call back********************
Step 35080 finish
{'loss': 0.204, 'grad_norm': 1.344854712486267, 'learning_rate': 3.648351648351648e-05, 'epoch': 6.38}
********************on step end call back********************
Step 35090 finish
{'loss': 0.2064, 'grad_norm': 1.7004350423812866, 'learning_rate': 3.6465201465201466e-05, 'epoch': 6.38}
********************on step end call back********************
Step 35100 finish
{'loss': 0.2078, 'grad_norm': 0.7991491556167603, 'learning_rate': 3.6446886446886445e-05, 'epoch': 6.38}
{'eval_loss': 0.38214677572250366, 'eval_accuracy': 0.8854166666666666, 'eval_runtime': 128.8192, 'eval_samples_per_second': 4.953, 'eval_steps_per_second': 4.953, 'epoch': 6.38}
********************save call back********************
********************on step end call back********************
Step 35110 finish
{'loss': 0.2148, 'grad_norm': 1.1142280101776123, 'learning_rate': 3.642857142857143e-05, 'epoch': 6.38}
********************on step end call back********************
Step 35120 finish
{'loss': 0.21, 'grad_norm': 1.2382218837738037, 'learning_rate': 3.641025641025641e-05, 'epoch': 6.38}
********************on step end call back********************
Step 35130 finish
{'loss': 0.1987, 'grad_norm': 1.3269877433776855, 'learning_rate': 3.6391941391941395e-05, 'epoch': 6.39}
********************on step end call back********************
Step 35140 finish
{'loss': 0.2124, 'grad_norm': 1.57101309299469, 'learning_rate': 3.637362637362637e-05, 'epoch': 6.39}
********************on step end call back********************
Step 35150 finish
{'loss': 0.2007, 'grad_norm': 1.5412917137145996, 'learning_rate': 3.635531135531136e-05, 'epoch': 6.39}
********************on step end call back********************
Step 35160 finish
{'loss': 0.2006, 'grad_norm': 0.9287168383598328, 'learning_rate': 3.633699633699634e-05, 'epoch': 6.39}
********************on step end call back********************
Step 35170 finish
{'loss': 0.2024, 'grad_norm': 1.0468264818191528, 'learning_rate': 3.6318681318681316e-05, 'epoch': 6.39}
********************on step end call back********************
Step 35180 finish
{'loss': 0.2347, 'grad_norm': 1.4235373735427856, 'learning_rate': 3.63003663003663e-05, 'epoch': 6.4}
********************on step end call back********************
Step 35190 finish
{'loss': 0.2143, 'grad_norm': 0.9894235134124756, 'learning_rate': 3.628205128205128e-05, 'epoch': 6.4}
********************on step end call back********************
Step 35200 finish
{'loss': 0.196, 'grad_norm': 0.7491705417633057, 'learning_rate': 3.6263736263736266e-05, 'epoch': 6.4}
{'eval_loss': 0.36829298734664917, 'eval_accuracy': 0.8645833333333334, 'eval_runtime': 128.8824, 'eval_samples_per_second': 4.95, 'eval_steps_per_second': 4.95, 'epoch': 6.4}
********************save call back********************
********************on step end call back********************
Step 35210 finish
{'loss': 0.1975, 'grad_norm': 1.109071969985962, 'learning_rate': 3.6245421245421245e-05, 'epoch': 6.4}
********************on step end call back********************
Step 35220 finish
{'loss': 0.2058, 'grad_norm': 1.2830824851989746, 'learning_rate': 3.622710622710623e-05, 'epoch': 6.4}
********************on step end call back********************
Step 35230 finish
{'loss': 0.2124, 'grad_norm': 1.382686734199524, 'learning_rate': 3.620879120879121e-05, 'epoch': 6.4}
********************on step end call back********************
Step 35240 finish
{'loss': 0.2061, 'grad_norm': 1.4129838943481445, 'learning_rate': 3.619047619047619e-05, 'epoch': 6.41}
********************on step end call back********************
Step 35250 finish
{'loss': 0.2179, 'grad_norm': 1.1871745586395264, 'learning_rate': 3.6172161172161173e-05, 'epoch': 6.41}
********************on step end call back********************
Step 35260 finish
{'loss': 0.2249, 'grad_norm': 1.2652603387832642, 'learning_rate': 3.615384615384615e-05, 'epoch': 6.41}
********************on step end call back********************
Step 35270 finish
{'loss': 0.205, 'grad_norm': 1.1454353332519531, 'learning_rate': 3.613553113553114e-05, 'epoch': 6.41}
********************on step end call back********************
Step 35280 finish
{'loss': 0.2014, 'grad_norm': 1.1012648344039917, 'learning_rate': 3.6117216117216117e-05, 'epoch': 6.41}
********************on step end call back********************
Step 35290 finish
{'loss': 0.216, 'grad_norm': 1.3001338243484497, 'learning_rate': 3.60989010989011e-05, 'epoch': 6.42}
********************on step end call back********************
Step 35300 finish
{'loss': 0.2052, 'grad_norm': 1.0509462356567383, 'learning_rate': 3.608058608058608e-05, 'epoch': 6.42}
{'eval_loss': 0.3769769072532654, 'eval_accuracy': 0.875, 'eval_runtime': 128.808, 'eval_samples_per_second': 4.953, 'eval_steps_per_second': 4.953, 'epoch': 6.42}
********************save call back********************
********************on step end call back********************
Step 35310 finish
{'loss': 0.2484, 'grad_norm': 1.1920967102050781, 'learning_rate': 3.6062271062271066e-05, 'epoch': 6.42}
********************on step end call back********************
Step 35320 finish
{'loss': 0.2023, 'grad_norm': 1.056187391281128, 'learning_rate': 3.6043956043956045e-05, 'epoch': 6.42}
********************on step end call back********************
Step 35330 finish
{'loss': 0.2061, 'grad_norm': 1.3073800802230835, 'learning_rate': 3.6025641025641024e-05, 'epoch': 6.42}
********************on step end call back********************
Step 35340 finish
{'loss': 0.1997, 'grad_norm': 1.3777481317520142, 'learning_rate': 3.600732600732601e-05, 'epoch': 6.42}
********************on step end call back********************
Step 35350 finish
{'loss': 0.2259, 'grad_norm': 1.2488987445831299, 'learning_rate': 3.598901098901099e-05, 'epoch': 6.43}
********************on step end call back********************
Step 35360 finish
{'loss': 0.2044, 'grad_norm': 1.2499611377716064, 'learning_rate': 3.5970695970695974e-05, 'epoch': 6.43}
********************on step end call back********************
Step 35370 finish
{'loss': 0.2043, 'grad_norm': 1.7002015113830566, 'learning_rate': 3.595238095238095e-05, 'epoch': 6.43}
********************on step end call back********************
Step 35380 finish
{'loss': 0.2036, 'grad_norm': 1.2252863645553589, 'learning_rate': 3.593406593406594e-05, 'epoch': 6.43}
********************on step end call back********************
Step 35390 finish
[INFO|trainer.py:3376] 2024-03-24 12:24:50,552 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-24 12:24:50,552 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-24 12:24:50,552 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-24 12:26:59,611 >> Saving model checkpoint to ./output/tmp-checkpoint-35400
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-24 12:26:59,781 >> tokenizer config file saved in ./output/tmp-checkpoint-35400/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-24 12:26:59,782 >> Special tokens file saved in ./output/tmp-checkpoint-35400/special_tokens_map.json
[INFO|trainer.py:3159] 2024-03-24 12:27:00,009 >> Deleting older checkpoint [output/checkpoint-25400] due to args.save_total_limit
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-24 12:35:36,520 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-24 12:35:36,520 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-24 12:35:36,520 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-24 12:37:45,394 >> Saving model checkpoint to ./output/tmp-checkpoint-35500
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-24 12:37:45,528 >> tokenizer config file saved in ./output/tmp-checkpoint-35500/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-24 12:37:45,528 >> Special tokens file saved in ./output/tmp-checkpoint-35500/special_tokens_map.json
[INFO|trainer.py:3159] 2024-03-24 12:37:45,754 >> Deleting older checkpoint [output/checkpoint-25500] due to args.save_total_limit
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-24 12:46:16,016 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-24 12:46:16,016 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-24 12:46:16,016 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-24 12:48:24,597 >> Saving model checkpoint to ./output/tmp-checkpoint-35600
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-24 12:48:24,736 >> tokenizer config file saved in ./output/tmp-checkpoint-35600/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-24 12:48:24,736 >> Special tokens file saved in ./output/tmp-checkpoint-35600/special_tokens_map.json
[INFO|trainer.py:3159] 2024-03-24 12:48:24,966 >> Deleting older checkpoint [output/checkpoint-25600] due to args.save_total_limit
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-24 12:56:54,082 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-24 12:56:54,083 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-24 12:56:54,083 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-24 12:59:02,563 >> Saving model checkpoint to ./output/tmp-checkpoint-35700
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-24 12:59:02,699 >> tokenizer config file saved in ./output/tmp-checkpoint-35700/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-24 12:59:02,699 >> Special tokens file saved in ./output/tmp-checkpoint-35700/special_tokens_map.json
[INFO|trainer.py:3159] 2024-03-24 12:59:02,925 >> Deleting older checkpoint [output/checkpoint-25700] due to args.save_total_limit
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
{'loss': 0.2101, 'grad_norm': 1.3732869625091553, 'learning_rate': 3.591575091575092e-05, 'epoch': 6.43}
********************on step end call back********************
Step 35400 finish
{'loss': 0.2083, 'grad_norm': 1.2309937477111816, 'learning_rate': 3.58974358974359e-05, 'epoch': 6.44}
{'eval_loss': 0.3746258616447449, 'eval_accuracy': 0.875, 'eval_runtime': 129.057, 'eval_samples_per_second': 4.944, 'eval_steps_per_second': 4.944, 'epoch': 6.44}
********************save call back********************
********************on step end call back********************
Step 35410 finish
{'loss': 0.1843, 'grad_norm': 1.027748703956604, 'learning_rate': 3.587912087912088e-05, 'epoch': 6.44}
********************on step end call back********************
Step 35420 finish
{'loss': 0.203, 'grad_norm': 1.412980556488037, 'learning_rate': 3.586080586080586e-05, 'epoch': 6.44}
********************on step end call back********************
Step 35430 finish
{'loss': 0.1942, 'grad_norm': 1.1058381795883179, 'learning_rate': 3.5842490842490845e-05, 'epoch': 6.44}
********************on step end call back********************
Step 35440 finish
{'loss': 0.2071, 'grad_norm': 1.3249019384384155, 'learning_rate': 3.5824175824175824e-05, 'epoch': 6.44}
********************on step end call back********************
Step 35450 finish
{'loss': 0.2145, 'grad_norm': 1.2224675416946411, 'learning_rate': 3.580586080586081e-05, 'epoch': 6.44}
********************on step end call back********************
Step 35460 finish
{'loss': 0.2295, 'grad_norm': 1.3362082242965698, 'learning_rate': 3.578754578754579e-05, 'epoch': 6.45}
********************on step end call back********************
Step 35470 finish
{'loss': 0.2279, 'grad_norm': 1.0292315483093262, 'learning_rate': 3.5769230769230774e-05, 'epoch': 6.45}
********************on step end call back********************
Step 35480 finish
{'loss': 0.1998, 'grad_norm': 1.2791391611099243, 'learning_rate': 3.575091575091575e-05, 'epoch': 6.45}
********************on step end call back********************
Step 35490 finish
{'loss': 0.2351, 'grad_norm': 0.9805885553359985, 'learning_rate': 3.573260073260074e-05, 'epoch': 6.45}
********************on step end call back********************
Step 35500 finish
{'loss': 0.1921, 'grad_norm': 1.074379324913025, 'learning_rate': 3.571428571428572e-05, 'epoch': 6.45}
{'eval_loss': 0.3737751841545105, 'eval_accuracy': 0.8854166666666666, 'eval_runtime': 128.8728, 'eval_samples_per_second': 4.951, 'eval_steps_per_second': 4.951, 'epoch': 6.45}
********************save call back********************
********************on step end call back********************
Step 35510 finish
{'loss': 0.2061, 'grad_norm': 1.088676929473877, 'learning_rate': 3.5695970695970696e-05, 'epoch': 6.46}
********************on step end call back********************
Step 35520 finish
{'loss': 0.202, 'grad_norm': 1.5076231956481934, 'learning_rate': 3.567765567765568e-05, 'epoch': 6.46}
********************on step end call back********************
Step 35530 finish
{'loss': 0.1857, 'grad_norm': 0.8678986430168152, 'learning_rate': 3.565934065934066e-05, 'epoch': 6.46}
********************on step end call back********************
Step 35540 finish
{'loss': 0.1891, 'grad_norm': 1.3725543022155762, 'learning_rate': 3.5641025641025646e-05, 'epoch': 6.46}
********************on step end call back********************
Step 35550 finish
{'loss': 0.236, 'grad_norm': 1.0103824138641357, 'learning_rate': 3.5622710622710624e-05, 'epoch': 6.46}
********************on step end call back********************
Step 35560 finish
{'loss': 0.2292, 'grad_norm': 1.027217149734497, 'learning_rate': 3.560439560439561e-05, 'epoch': 6.46}
********************on step end call back********************
Step 35570 finish
{'loss': 0.2257, 'grad_norm': 1.3981513977050781, 'learning_rate': 3.558608058608059e-05, 'epoch': 6.47}
********************on step end call back********************
Step 35580 finish
{'loss': 0.205, 'grad_norm': 1.0698654651641846, 'learning_rate': 3.5567765567765574e-05, 'epoch': 6.47}
********************on step end call back********************
Step 35590 finish
{'loss': 0.212, 'grad_norm': 1.0426019430160522, 'learning_rate': 3.554945054945055e-05, 'epoch': 6.47}
********************on step end call back********************
Step 35600 finish
{'loss': 0.208, 'grad_norm': 1.0355674028396606, 'learning_rate': 3.553113553113553e-05, 'epoch': 6.47}
{'eval_loss': 0.3776116967201233, 'eval_accuracy': 0.875, 'eval_runtime': 128.5797, 'eval_samples_per_second': 4.962, 'eval_steps_per_second': 4.962, 'epoch': 6.47}
********************save call back********************
********************on step end call back********************
Step 35610 finish
{'loss': 0.2171, 'grad_norm': 1.199506402015686, 'learning_rate': 3.551282051282052e-05, 'epoch': 6.47}
********************on step end call back********************
Step 35620 finish
{'loss': 0.2006, 'grad_norm': 1.3634451627731323, 'learning_rate': 3.5494505494505496e-05, 'epoch': 6.48}
********************on step end call back********************
Step 35630 finish
{'loss': 0.23, 'grad_norm': 1.5311758518218994, 'learning_rate': 3.547619047619048e-05, 'epoch': 6.48}
********************on step end call back********************
Step 35640 finish
{'loss': 0.2095, 'grad_norm': 0.9954770803451538, 'learning_rate': 3.545787545787546e-05, 'epoch': 6.48}
********************on step end call back********************
Step 35650 finish
{'loss': 0.2126, 'grad_norm': 1.1813125610351562, 'learning_rate': 3.5439560439560446e-05, 'epoch': 6.48}
********************on step end call back********************
Step 35660 finish
{'loss': 0.2159, 'grad_norm': 1.4019981622695923, 'learning_rate': 3.5421245421245425e-05, 'epoch': 6.48}
********************on step end call back********************
Step 35670 finish
{'loss': 0.1815, 'grad_norm': 1.5414130687713623, 'learning_rate': 3.54029304029304e-05, 'epoch': 6.48}
********************on step end call back********************
Step 35680 finish
{'loss': 0.1961, 'grad_norm': 0.9962263703346252, 'learning_rate': 3.538461538461539e-05, 'epoch': 6.49}
********************on step end call back********************
Step 35690 finish
{'loss': 0.2251, 'grad_norm': 1.4876376390457153, 'learning_rate': 3.536630036630037e-05, 'epoch': 6.49}
********************on step end call back********************
Step 35700 finish
{'loss': 0.1978, 'grad_norm': 1.0600029230117798, 'learning_rate': 3.534798534798535e-05, 'epoch': 6.49}
{'eval_loss': 0.3748302161693573, 'eval_accuracy': 0.875, 'eval_runtime': 128.4794, 'eval_samples_per_second': 4.966, 'eval_steps_per_second': 4.966, 'epoch': 6.49}
********************save call back********************
********************on step end call back********************
Step 35710 finish
{'loss': 0.1995, 'grad_norm': 1.1827505826950073, 'learning_rate': 3.532967032967033e-05, 'epoch': 6.49}
********************on step end call back********************
Step 35720 finish
{'loss': 0.2215, 'grad_norm': 0.8763139247894287, 'learning_rate': 3.531135531135532e-05, 'epoch': 6.49}
********************on step end call back********************
Step 35730 finish
{'loss': 0.2132, 'grad_norm': 1.3008050918579102, 'learning_rate': 3.5293040293040296e-05, 'epoch': 6.5}
********************on step end call back********************
Step 35740 finish
{'loss': 0.2025, 'grad_norm': 1.1982269287109375, 'learning_rate': 3.527472527472528e-05, 'epoch': 6.5}
********************on step end call back********************
Step 35750 finish
{'loss': 0.1916, 'grad_norm': 1.2842377424240112, 'learning_rate': 3.525641025641026e-05, 'epoch': 6.5}
********************on step end call back********************
Step 35760 finish
{'loss': 0.2047, 'grad_norm': 1.4163206815719604, 'learning_rate': 3.523809523809524e-05, 'epoch': 6.5}
********************on step end call back********************
Step 35770 finish
{'loss': 0.2186, 'grad_norm': 1.34294855594635, 'learning_rate': 3.5219780219780225e-05, 'epoch': 6.5}
********************on step end call back********************
Step 35780 finish
[INFO|trainer.py:3376] 2024-03-24 13:07:36,484 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-24 13:07:36,484 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-24 13:07:36,484 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-24 13:09:45,010 >> Saving model checkpoint to ./output/tmp-checkpoint-35800
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-24 13:09:45,146 >> tokenizer config file saved in ./output/tmp-checkpoint-35800/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-24 13:09:45,147 >> Special tokens file saved in ./output/tmp-checkpoint-35800/special_tokens_map.json
[INFO|trainer.py:3159] 2024-03-24 13:09:45,372 >> Deleting older checkpoint [output/checkpoint-25800] due to args.save_total_limit
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-24 13:18:24,387 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-24 13:18:24,387 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-24 13:18:24,387 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-24 13:20:32,968 >> Saving model checkpoint to ./output/tmp-checkpoint-35900
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-24 13:20:33,107 >> tokenizer config file saved in ./output/tmp-checkpoint-35900/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-24 13:20:33,108 >> Special tokens file saved in ./output/tmp-checkpoint-35900/special_tokens_map.json
[INFO|trainer.py:3159] 2024-03-24 13:20:33,339 >> Deleting older checkpoint [output/checkpoint-25900] due to args.save_total_limit
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-24 13:29:13,584 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-24 13:29:13,584 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-24 13:29:13,584 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-24 13:31:22,204 >> Saving model checkpoint to ./output/tmp-checkpoint-36000
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-24 13:31:22,343 >> tokenizer config file saved in ./output/tmp-checkpoint-36000/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-24 13:31:22,343 >> Special tokens file saved in ./output/tmp-checkpoint-36000/special_tokens_map.json
[INFO|trainer.py:3159] 2024-03-24 13:31:22,595 >> Deleting older checkpoint [output/checkpoint-26000] due to args.save_total_limit
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-03-24 13:40:00,610 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-24 13:40:00,610 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-24 13:40:00,610 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-24 13:42:09,393 >> Saving model checkpoint to ./output/tmp-checkpoint-36100
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-24 13:42:09,540 >> tokenizer config file saved in ./output/tmp-checkpoint-36100/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-24 13:42:09,540 >> Special tokens file saved in ./output/tmp-checkpoint-36100/special_tokens_map.json
[INFO|trainer.py:3159] 2024-03-24 13:42:09,811 >> Deleting older checkpoint [output/checkpoint-26100] due to args.save_total_limit
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
{'loss': 0.2286, 'grad_norm': 1.101991891860962, 'learning_rate': 3.5201465201465203e-05, 'epoch': 6.5}
********************on step end call back********************
Step 35790 finish
{'loss': 0.2096, 'grad_norm': 1.0662331581115723, 'learning_rate': 3.518315018315019e-05, 'epoch': 6.51}
********************on step end call back********************
Step 35800 finish
{'loss': 0.214, 'grad_norm': 1.3264663219451904, 'learning_rate': 3.516483516483517e-05, 'epoch': 6.51}
{'eval_loss': 0.3793773055076599, 'eval_accuracy': 0.875, 'eval_runtime': 128.5247, 'eval_samples_per_second': 4.964, 'eval_steps_per_second': 4.964, 'epoch': 6.51}
********************save call back********************
********************on step end call back********************
Step 35810 finish
{'loss': 0.2009, 'grad_norm': 0.9244502782821655, 'learning_rate': 3.5146520146520147e-05, 'epoch': 6.51}
********************on step end call back********************
Step 35820 finish
{'loss': 0.214, 'grad_norm': 1.079209327697754, 'learning_rate': 3.5128205128205125e-05, 'epoch': 6.51}
********************on step end call back********************
Step 35830 finish
{'loss': 0.2413, 'grad_norm': 1.3061237335205078, 'learning_rate': 3.510989010989011e-05, 'epoch': 6.51}
********************on step end call back********************
Step 35840 finish
{'loss': 0.211, 'grad_norm': 1.2754392623901367, 'learning_rate': 3.509157509157509e-05, 'epoch': 6.52}
********************on step end call back********************
Step 35850 finish
{'loss': 0.2086, 'grad_norm': 1.1459802389144897, 'learning_rate': 3.5073260073260075e-05, 'epoch': 6.52}
********************on step end call back********************
Step 35860 finish
{'loss': 0.196, 'grad_norm': 1.1285595893859863, 'learning_rate': 3.5054945054945054e-05, 'epoch': 6.52}
********************on step end call back********************
Step 35870 finish
{'loss': 0.2105, 'grad_norm': 1.4340486526489258, 'learning_rate': 3.503663003663003e-05, 'epoch': 6.52}
********************on step end call back********************
Step 35880 finish
{'loss': 0.2241, 'grad_norm': 1.2498363256454468, 'learning_rate': 3.501831501831502e-05, 'epoch': 6.52}
********************on step end call back********************
Step 35890 finish
{'loss': 0.2197, 'grad_norm': 1.432754397392273, 'learning_rate': 3.5e-05, 'epoch': 6.52}
********************on step end call back********************
Step 35900 finish
{'loss': 0.1896, 'grad_norm': 1.0633748769760132, 'learning_rate': 3.498168498168498e-05, 'epoch': 6.53}
{'eval_loss': 0.376913845539093, 'eval_accuracy': 0.875, 'eval_runtime': 128.5798, 'eval_samples_per_second': 4.962, 'eval_steps_per_second': 4.962, 'epoch': 6.53}
********************save call back********************
********************on step end call back********************
Step 35910 finish
{'loss': 0.2265, 'grad_norm': 1.0917154550552368, 'learning_rate': 3.496336996336996e-05, 'epoch': 6.53}
********************on step end call back********************
Step 35920 finish
{'loss': 0.226, 'grad_norm': 1.1661251783370972, 'learning_rate': 3.494505494505495e-05, 'epoch': 6.53}
********************on step end call back********************
Step 35930 finish
{'loss': 0.1953, 'grad_norm': 1.4796416759490967, 'learning_rate': 3.4926739926739926e-05, 'epoch': 6.53}
********************on step end call back********************
Step 35940 finish
{'loss': 0.2245, 'grad_norm': 1.3937119245529175, 'learning_rate': 3.490842490842491e-05, 'epoch': 6.53}
********************on step end call back********************
Step 35950 finish
{'loss': 0.1946, 'grad_norm': 1.190722107887268, 'learning_rate': 3.489010989010989e-05, 'epoch': 6.54}
********************on step end call back********************
Step 35960 finish
{'loss': 0.2083, 'grad_norm': 1.4150195121765137, 'learning_rate': 3.487179487179487e-05, 'epoch': 6.54}
********************on step end call back********************
Step 35970 finish
{'loss': 0.2095, 'grad_norm': 1.7713730335235596, 'learning_rate': 3.4853479853479854e-05, 'epoch': 6.54}
********************on step end call back********************
Step 35980 finish
{'loss': 0.2098, 'grad_norm': 1.1421966552734375, 'learning_rate': 3.483516483516483e-05, 'epoch': 6.54}
********************on step end call back********************
Step 35990 finish
{'loss': 0.2262, 'grad_norm': 1.1589734554290771, 'learning_rate': 3.481684981684982e-05, 'epoch': 6.54}
********************on step end call back********************
Step 36000 finish
{'loss': 0.2166, 'grad_norm': 1.3667103052139282, 'learning_rate': 3.47985347985348e-05, 'epoch': 6.54}
{'eval_loss': 0.3837796151638031, 'eval_accuracy': 0.875, 'eval_runtime': 128.6183, 'eval_samples_per_second': 4.96, 'eval_steps_per_second': 4.96, 'epoch': 6.54}
********************save call back********************
********************on step end call back********************
Step 36010 finish
{'loss': 0.2148, 'grad_norm': 1.1581716537475586, 'learning_rate': 3.478021978021978e-05, 'epoch': 6.55}
********************on step end call back********************
Step 36020 finish
{'loss': 0.2163, 'grad_norm': 1.3023070096969604, 'learning_rate': 3.476190476190476e-05, 'epoch': 6.55}
********************on step end call back********************
Step 36030 finish
{'loss': 0.2087, 'grad_norm': 1.2568620443344116, 'learning_rate': 3.474358974358975e-05, 'epoch': 6.55}
********************on step end call back********************
Step 36040 finish
{'loss': 0.2238, 'grad_norm': 1.3232547044754028, 'learning_rate': 3.4725274725274726e-05, 'epoch': 6.55}
********************on step end call back********************
Step 36050 finish
{'loss': 0.1914, 'grad_norm': 0.9317625164985657, 'learning_rate': 3.4706959706959704e-05, 'epoch': 6.55}
********************on step end call back********************
Step 36060 finish
{'loss': 0.2068, 'grad_norm': 0.999305248260498, 'learning_rate': 3.468864468864469e-05, 'epoch': 6.56}
********************on step end call back********************
Step 36070 finish
{'loss': 0.2237, 'grad_norm': 0.9603312611579895, 'learning_rate': 3.467032967032967e-05, 'epoch': 6.56}
********************on step end call back********************
Step 36080 finish
{'loss': 0.2302, 'grad_norm': 1.4032171964645386, 'learning_rate': 3.4652014652014654e-05, 'epoch': 6.56}
********************on step end call back********************
Step 36090 finish
{'loss': 0.1913, 'grad_norm': 0.9439430832862854, 'learning_rate': 3.463369963369963e-05, 'epoch': 6.56}
********************on step end call back********************
Step 36100 finish
{'loss': 0.1984, 'grad_norm': 1.100322961807251, 'learning_rate': 3.461538461538462e-05, 'epoch': 6.56}
{'eval_loss': 0.3796536326408386, 'eval_accuracy': 0.875, 'eval_runtime': 128.7816, 'eval_samples_per_second': 4.954, 'eval_steps_per_second': 4.954, 'epoch': 6.56}
********************save call back********************
********************on step end call back********************
Step 36110 finish
{'loss': 0.1876, 'grad_norm': 1.2536040544509888, 'learning_rate': 3.45970695970696e-05, 'epoch': 6.56}
********************on step end call back********************
Step 36120 finish
{'loss': 0.2246, 'grad_norm': 1.4743109941482544, 'learning_rate': 3.457875457875458e-05, 'epoch': 6.57}
********************on step end call back********************
Step 36130 finish
{'loss': 0.2072, 'grad_norm': 1.1378568410873413, 'learning_rate': 3.456043956043956e-05, 'epoch': 6.57}
********************on step end call back********************
Step 36140 finish
{'loss': 0.2, 'grad_norm': 1.1702898740768433, 'learning_rate': 3.454212454212454e-05, 'epoch': 6.57}
********************on step end call back********************
Step 36150 finish
{'loss': 0.2063, 'grad_norm': 1.5134670734405518, 'learning_rate': 3.4523809523809526e-05, 'epoch': 6.57}
********************on step end call back********************
Step 36160 finish
{'loss': 0.1978, 'grad_norm': 1.4832831621170044, 'learning_rate': 3.4505494505494505e-05, 'epoch': 6.57}
********************on step end call back********************
Step 36170 finish
{'loss': 0.1803, 'grad_norm': 1.020260214805603, 'learning_rate': 3.448717948717949e-05, 'epoch': 6.58}
[INFO|trainer.py:3376] 2024-03-24 13:50:36,865 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-03-24 13:50:36,865 >>   Num examples = 638
[INFO|trainer.py:3381] 2024-03-24 13:50:36,866 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-03-24 13:52:45,813 >> Saving model checkpoint to ./output/tmp-checkpoint-36200
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-03-24 13:52:45,962 >> tokenizer config file saved in ./output/tmp-checkpoint-36200/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-03-24 13:52:45,962 >> Special tokens file saved in ./output/tmp-checkpoint-36200/special_tokens_map.json
[INFO|trainer.py:3159] 2024-03-24 13:52:46,231 >> Deleting older checkpoint [output/checkpoint-26200] due to args.save_total_limit
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
