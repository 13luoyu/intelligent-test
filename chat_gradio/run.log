The model is automatically converting to bf16 for faster inference. If you want to disable the automatic precision, please manually add bf16/fp16/fp32=True to "AutoModelForCausalLM.from_pretrained".
Try importing flash-attention for faster inference...
Warning: import flash_attn rms_norm fail, please install FlashAttention layer_norm to get higher efficiency https://github.com/Dao-AILab/flash-attention/tree/main/csrc/layer_norm
Loading checkpoint shards:   0%|          | 0/15 [00:00<?, ?it/s]Loading checkpoint shards:   7%|▋         | 1/15 [00:02<00:31,  2.23s/it]Loading checkpoint shards:  13%|█▎        | 2/15 [00:03<00:24,  1.88s/it]Loading checkpoint shards:  20%|██        | 3/15 [00:05<00:20,  1.68s/it]Loading checkpoint shards:  27%|██▋       | 4/15 [00:06<00:16,  1.47s/it]Loading checkpoint shards:  33%|███▎      | 5/15 [00:08<00:15,  1.58s/it]Loading checkpoint shards:  40%|████      | 6/15 [00:08<00:10,  1.17s/it]Loading checkpoint shards:  47%|████▋     | 7/15 [00:08<00:07,  1.11it/s]Loading checkpoint shards:  53%|█████▎    | 8/15 [00:09<00:05,  1.37it/s]Loading checkpoint shards:  60%|██████    | 9/15 [00:09<00:03,  1.63it/s]Loading checkpoint shards:  67%|██████▋   | 10/15 [00:10<00:02,  1.82it/s]Loading checkpoint shards:  73%|███████▎  | 11/15 [00:10<00:01,  2.27it/s]Loading checkpoint shards:  80%|████████  | 12/15 [00:10<00:01,  2.95it/s]Loading checkpoint shards:  87%|████████▋ | 13/15 [00:10<00:00,  3.71it/s]Loading checkpoint shards:  93%|█████████▎| 14/15 [00:10<00:00,  4.54it/s]Loading checkpoint shards: 100%|██████████| 15/15 [00:10<00:00,  5.36it/s]Loading checkpoint shards: 100%|██████████| 15/15 [00:10<00:00,  1.40it/s]
WARNING:root:Some parameters are on the meta device device because they were offloaded to the cpu.
