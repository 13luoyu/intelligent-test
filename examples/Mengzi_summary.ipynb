{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "badaca7f-1fc3-4f9e-9d68-3aa37603447d",
   "metadata": {
    "tags": []
   },
   "source": [
    "自动文摘的目的是通过对原文本进行压缩、提炼，为用户供简明扼要的文字描述。自动文摘是一个信息压缩过程，将输入的一篇或多篇文档内容总结为一段简要描述，该过程不可避免有信息损失，但是要求保留尽可能多的重要信息，自动文摘也是自然语言生成领域中一个重要任务。\n",
    "下面我们以文本摘要任务为例，展示孟子预训练模型在下游任务上微调的流程，整体流程可以分为5部分：\n",
    "\n",
    "- 数据加载\n",
    "- 数据预处理\n",
    "- 模型训练\n",
    "- 模型推理\n",
    "- 评测\n",
    "\n",
    "下面我们以中文科学文献数据（CSL）文本摘要数据为例进行演示，数据下载地址：https://github.com/CLUEbenchmark/CLGE\n",
    "\n",
    "下载的原始数据：训练集(3,000)，验证集(500)，测试集(500)，但测试集没有摘要标注结果，所以这里我们简单地把验证集当作测试集，从训练集中划出500条作为开发集。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9318f008-84ce-4d04-886d-a91ca1cb6b15",
   "metadata": {},
   "source": [
    "## 依赖环境\n",
    "代码使用以下环境运行\n",
    "- torch==1.8.0\n",
    "- transformers==4.12.5\n",
    "- sentencepiece==0.1.95\n",
    "- rouge==1.0.1\n",
    "- nltk==3.6.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb9bdb4-e9b9-40e6-901e-e16a12fed49e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1. 数据加载\n",
    "\n",
    "CSL数据以json的形式存储，通过如下方式可以将数据加载进内存。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94f0f96e-5856-47e5-96e4-505a51df983f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, TrainingArguments, Trainer\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "878a541b-a822-429b-a2a8-9a0b775ac51c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading...: 100%|██████████| 3000/3000 [00:00<00:00, 466102.83it/s]\n",
      "Reading...: 100%|██████████| 500/500 [00:00<00:00, 481993.11it/s]\n"
     ]
    }
   ],
   "source": [
    "def read_json(input_file: str) -> list:\n",
    "    '''\n",
    "    读取json文件，每行是一个json字段\n",
    "\n",
    "    Args:\n",
    "        input_file:文件名\n",
    "\n",
    "    Returns:\n",
    "        lines\n",
    "    '''\n",
    "    with open(input_file, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "    return list(map(json.loads, tqdm(lines, desc='Reading...')))\n",
    "\n",
    "trainset = read_json(\"csl_title_public/csl_title_train.json\") \n",
    "test = read_json(\"csl_title_public/csl_title_dev.json\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9f545e-ffbc-43f7-bdfc-204408194c83",
   "metadata": {},
   "source": [
    "下面展示数据集的具体信息："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "53e04393-9e66-42dc-b707-b31e1a3789f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集大小：2500个训练样本\n",
      "开发集大小：500个训练样本\n",
      "每个训练样本的原始格式如下：\n",
      " {'id': 747, 'title': 'SLP和遗传算法结合在车间设备布局中的应用', 'abst': '用经典的系统布置设计结合遗传算法求解车间设备布局,以高效率获得满意的设计结果,弥补传统SLP设计过程中手工操作的繁琐迭代、易受主观影响、结果不稳定等缺点。并且通过对遗传算法的改进,增强了算法的全局和局部搜索能力。最后,通过实例验证了其有效性。'}\n"
     ]
    }
   ],
   "source": [
    "random.shuffle(trainset)\n",
    "train = trainset[:2500]\n",
    "dev = trainset[2500:]\n",
    "print('训练集大小：%d个训练样本'%(len(train)))\n",
    "print('开发集大小：%d个训练样本'%(len(dev)))\n",
    "print('每个训练样本的原始格式如下：\\n',train[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11cc3d96-7cc3-4a06-97cd-9e59a08d4035",
   "metadata": {},
   "source": [
    "### 可以看出每条原始数据包含3个字段，分别是id，title，abst，其中id是唯一标识，abst是文本摘要任务的输入，title是文本摘要任务的输出。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed38e107-447d-4476-90a8-b1ee809c75dd",
   "metadata": {},
   "source": [
    "## 2. 数据预处理"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4437e1d-85ab-46f3-aa99-eb2e6d1e6447",
   "metadata": {},
   "source": [
    "数据预处理的目的是将原始数据处理为模型可以接受的输入形式，相当于在原始数据和模型输入之间建立管道。\n",
    "模型输入，可接受的字段为input_ids、labels，其中input_ids为输入文本的tokenized表示，可以直接通过transformers提供的Tokenizer进行转换；labels为模型期望输出文本的tokenized表示。\n",
    "通过定义DataCollatorForSeq2Seq数据预处理类，将其传递给data_collator完成上述流程，数据预处理代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "af4bb983-ebaf-4df4-ae42-1892be31bbb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"mengzi-t5-base\" # huggingface下载模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e83eec31-4ee5-471c-82be-752cc89d2c30",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### 加载预训练模型，包括分词器tokenizer和model。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af548e66-c49d-41d9-842b-0958b54183b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Mengzi_tokenizer = T5Tokenizer.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d43d696d-a974-41dd-a31d-7b8928754d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Mengzi_model = T5ForConditionalGeneration.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f9290477-3a9c-4264-9566-9f6df5663a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqDataset:\n",
    "    def __init__(self, data):\n",
    "        self.datas = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.datas)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.datas[index]\n",
    "\n",
    "class DataCollatorForSeq2Seq:\n",
    "    def __init__(self, tokenizer, padding: bool = True, max_length: int = 512):\n",
    "        self.tokenizer = tokenizer\n",
    "        #self.model = model\n",
    "        self.padding = padding\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        features = self.collator_fn(batch)\n",
    "        return features\n",
    "\n",
    "\n",
    "    def preprocess(self, item):\n",
    "        source = item[\"abst\"]\n",
    "        target = item[\"title\"]\n",
    "        return source, target\n",
    "\n",
    "    def collator_fn(self, batch):\n",
    "        results = map(self.preprocess, batch)\n",
    "        inputs, targets = zip(*results)\n",
    "\n",
    "        input_tensor = self.tokenizer(inputs,\n",
    "                                      truncation=True,\n",
    "                                      padding=True,\n",
    "                                      max_length=self.max_length,\n",
    "                                      return_tensors=\"pt\",\n",
    "                                      )\n",
    "\n",
    "        target_tensor = self.tokenizer(targets,\n",
    "                                       truncation=True,\n",
    "                                       padding=True,\n",
    "                                       max_length=self.max_length,\n",
    "                                       return_tensors=\"pt\",\n",
    "                                       )\n",
    "\n",
    "        input_tensor[\"labels\"] = target_tensor[\"input_ids\"]\n",
    "\n",
    "        if \"token_type_ids\" in input_tensor:\n",
    "            del input_tensor[\"token_type_ids\"]\n",
    "        return input_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8e12554a-00dc-4d0a-a874-06711381cd11",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = Seq2SeqDataset(train)\n",
    "devset = Seq2SeqDataset(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b48f1d71-8a04-4386-bc43-c1ac1b7db6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "collator = DataCollatorForSeq2Seq(Mengzi_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eccbf5a-8206-4eaf-895f-2f0492c91df2",
   "metadata": {},
   "source": [
    "## 3. 模型训练\n",
    "\n",
    "训练模型前需要指定模型训练的超参数，包括训练的轮数、学习率和学习率管理策略等等：可以通过实例化TrainingArguments类来，并将其传递给Trainer来传入这些超参数。\n",
    "然后通过huggingface定义的trainer.train()方法来进行训练。\n",
    "训练完成后通过trainer.save_model()方法来保存最佳模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0449453c-f27a-4ab8-944f-f99d61c024eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"test\" # 模型checkpoint的保存目录\n",
    "training_args = TrainingArguments(\n",
    "        num_train_epochs=3,\n",
    "        per_device_train_batch_size=8, # batch_size需要根据自己GPU的显存进行设置，2080,8G显存，batch_size设置为2可以跑起来。\n",
    "        logging_steps=10,\n",
    "        #fp16=True,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        eval_steps=100,\n",
    "        load_best_model_at_end=True,\n",
    "        learning_rate=1e-5,\n",
    "        #warmup_steps=100,\n",
    "        output_dir=\"test\",\n",
    "        save_total_limit=5,\n",
    "        lr_scheduler_type='constant',\n",
    "        gradient_accumulation_steps=1,\n",
    "        dataloader_num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "351a0034-eda7-4664-a180-1a9ea67beba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Arguments ...\n",
      "TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=False,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_steps=100,\n",
      "evaluation_strategy=steps,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "greater_is_better=False,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=1e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=True,\n",
      "local_rank=-1,\n",
      "log_level=passive,\n",
      "log_level_replica=passive,\n",
      "log_on_each_node=True,\n",
      "logging_dir=test/runs/Mar06_21-15-43_luoyu,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=10,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_type=constant,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=loss,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=3,\n",
      "optim=adamw_hf,\n",
      "optim_args=None,\n",
      "output_dir=test,\n",
      "overwrite_output_dir=False,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=8,\n",
      "per_device_train_batch_size=8,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=test,\n",
      "save_on_each_node=False,\n",
      "save_steps=500,\n",
      "save_strategy=steps,\n",
      "save_total_limit=5,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      "xpu_backend=None,\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/asus/miniconda3/envs/mengzi/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 2500\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 939\n",
      "  Number of trainable parameters = 247577856\n",
      "The following columns in the training set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: id, title, abst. If id, title, abst are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'abst'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 13\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[39mprint\u001b[39m(training_args)\n\u001b[1;32m      4\u001b[0m trainer \u001b[39m=\u001b[39m Trainer(\n\u001b[1;32m      5\u001b[0m     tokenizer\u001b[39m=\u001b[39mMengzi_tokenizer,\n\u001b[1;32m      6\u001b[0m     model\u001b[39m=\u001b[39mMengzi_model,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     10\u001b[0m     eval_dataset\u001b[39m=\u001b[39mdevset\n\u001b[1;32m     11\u001b[0m )\n\u001b[0;32m---> 13\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n\u001b[1;32m     14\u001b[0m trainer\u001b[39m.\u001b[39msave_model(\u001b[39m\"\u001b[39m\u001b[39mtest/best\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39m# 保存最好的模型\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/mengzi/lib/python3.9/site-packages/transformers/trainer.py:1543\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1538\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_wrapped \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\n\u001b[1;32m   1540\u001b[0m inner_training_loop \u001b[39m=\u001b[39m find_executable_batch_size(\n\u001b[1;32m   1541\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inner_training_loop, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_batch_size, args\u001b[39m.\u001b[39mauto_find_batch_size\n\u001b[1;32m   1542\u001b[0m )\n\u001b[0;32m-> 1543\u001b[0m \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[1;32m   1544\u001b[0m     args\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m   1545\u001b[0m     resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[1;32m   1546\u001b[0m     trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[1;32m   1547\u001b[0m     ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[1;32m   1548\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/mengzi/lib/python3.9/site-packages/transformers/trainer.py:1765\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1762\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_load_rng_state(resume_from_checkpoint)\n\u001b[1;32m   1764\u001b[0m step \u001b[39m=\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m\n\u001b[0;32m-> 1765\u001b[0m \u001b[39mfor\u001b[39;00m step, inputs \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(epoch_iterator):\n\u001b[1;32m   1766\u001b[0m \n\u001b[1;32m   1767\u001b[0m     \u001b[39m# Skip past any already trained steps if resuming training\u001b[39;00m\n\u001b[1;32m   1768\u001b[0m     \u001b[39mif\u001b[39;00m steps_trained_in_current_epoch \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m   1769\u001b[0m         steps_trained_in_current_epoch \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/mengzi/lib/python3.9/site-packages/torch/utils/data/dataloader.py:628\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    625\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    626\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    627\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 628\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    629\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    631\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    632\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/miniconda3/envs/mengzi/lib/python3.9/site-packages/torch/utils/data/dataloader.py:671\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    669\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    670\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 671\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    672\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    673\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/miniconda3/envs/mengzi/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:61\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     60\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 61\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcollate_fn(data)\n",
      "File \u001b[0;32m~/miniconda3/envs/mengzi/lib/python3.9/site-packages/transformers/trainer_utils.py:700\u001b[0m, in \u001b[0;36mRemoveColumnsCollator.__call__\u001b[0;34m(self, features)\u001b[0m\n\u001b[1;32m    698\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, features: List[\u001b[39mdict\u001b[39m]):\n\u001b[1;32m    699\u001b[0m     features \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_remove_columns(feature) \u001b[39mfor\u001b[39;00m feature \u001b[39min\u001b[39;00m features]\n\u001b[0;32m--> 700\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdata_collator(features)\n",
      "Cell \u001b[0;32mIn[10], line 19\u001b[0m, in \u001b[0;36mDataCollatorForSeq2Seq.__call__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, batch):\n\u001b[0;32m---> 19\u001b[0m     features \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcollator_fn(batch)\n\u001b[1;32m     20\u001b[0m     \u001b[39mreturn\u001b[39;00m features\n",
      "Cell \u001b[0;32mIn[10], line 30\u001b[0m, in \u001b[0;36mDataCollatorForSeq2Seq.collator_fn\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcollator_fn\u001b[39m(\u001b[39mself\u001b[39m, batch):\n\u001b[1;32m     29\u001b[0m     results \u001b[39m=\u001b[39m \u001b[39mmap\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpreprocess, batch)\n\u001b[0;32m---> 30\u001b[0m     inputs, targets \u001b[39m=\u001b[39m \u001b[39mzip\u001b[39;49m(\u001b[39m*\u001b[39;49mresults)\n\u001b[1;32m     32\u001b[0m     input_tensor \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokenizer(inputs,\n\u001b[1;32m     33\u001b[0m                                   truncation\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m     34\u001b[0m                                   padding\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m     35\u001b[0m                                   max_length\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_length,\n\u001b[1;32m     36\u001b[0m                                   return_tensors\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     37\u001b[0m                                   )\n\u001b[1;32m     39\u001b[0m     target_tensor \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokenizer(targets,\n\u001b[1;32m     40\u001b[0m                                    truncation\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m     41\u001b[0m                                    padding\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m     42\u001b[0m                                    max_length\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_length,\n\u001b[1;32m     43\u001b[0m                                    return_tensors\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     44\u001b[0m                                    )\n",
      "Cell \u001b[0;32mIn[10], line 24\u001b[0m, in \u001b[0;36mDataCollatorForSeq2Seq.preprocess\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpreprocess\u001b[39m(\u001b[39mself\u001b[39m, item):\n\u001b[0;32m---> 24\u001b[0m     source \u001b[39m=\u001b[39m item[\u001b[39m\"\u001b[39;49m\u001b[39mabst\u001b[39;49m\u001b[39m\"\u001b[39;49m]\n\u001b[1;32m     25\u001b[0m     target \u001b[39m=\u001b[39m item[\u001b[39m\"\u001b[39m\u001b[39mtitle\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m     26\u001b[0m     \u001b[39mreturn\u001b[39;00m source, target\n",
      "\u001b[0;31mKeyError\u001b[0m: 'abst'"
     ]
    }
   ],
   "source": [
    "print('Training Arguments ...')\n",
    "print(training_args)\n",
    "\n",
    "trainer = Trainer(\n",
    "    tokenizer=Mengzi_tokenizer,\n",
    "    model=Mengzi_model,\n",
    "    args=training_args,\n",
    "    data_collator=collator,\n",
    "    train_dataset=trainset,\n",
    "    eval_dataset=devset\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_model(\"test/best\") # 保存最好的模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef383301-4332-4092-840e-9899798fa98a",
   "metadata": {},
   "source": [
    "## 4. 模型推理\n",
    "\n",
    "最佳模型保存在了\"test/best\"位置，我们可以加载最佳模型并利用其进行摘要生成。\n",
    "下面是我们利用模型进行推理的一种实现方式，将希望简化的文本tokenized后传入模型，得到经过tokenizer解码后即可获得摘要后的文本。当然，读者也可以利用自己熟悉的方式进行生成。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84d29708-29f3-45e3-9d13-e93d3ab24ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(items):\n",
    "    inputs = []\n",
    "    titles = []\n",
    "    for item in items:\n",
    "        inputs.append(item[\"abst\"])\n",
    "        titles.append(item[\"title\"])\n",
    "    return inputs, titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0f06189-1ec3-4b8b-be41-f27bebb5e3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = \"test/best\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(best_model)\n",
    "model = T5ForConditionalGeneration.from_pretrained(best_model).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3a4834d-ae9e-40c9-97f6-373585ece54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(sources, batch_size=8):\n",
    "    model.eval() # 将模型转换为评估模式\n",
    "    \n",
    "    kwargs = {\"num_beams\":4}\n",
    "    \n",
    "    outputs = []\n",
    "    for start in tqdm(range(0, len(sources), batch_size)):\n",
    "        batch = sources[start:start+batch_size]\n",
    "        \n",
    "        input_tensor = tokenizer(batch, return_tensors=\"pt\", truncation=True, padding=True, max_length=512).input_ids.cuda()\n",
    "        \n",
    "        outputs.extend(model.generate(input_ids=input_tensor, **kwargs))\n",
    "    return tokenizer.batch_decode(outputs, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "449b1a65-18ca-4e78-9b1e-25df3e7677f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs, refs = preprocess(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "668efe77-fa52-49f7-9946-c3d7a6ced3e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'抽象了一种基于中心的战术应用场景与业务,并将网络编码技术应用于此类场景的实时数据多播业务中。在分析基于中心网络与Many-to-all业务模式特性的基础上,提出了仅在中心节点进行编码操作的传输策略以及相应的贪心算法。分析了网络编码多播策略的理论增益上界,仿真试验表明该贪心算法能够获得与理论相近的性能增益。最后的分析与仿真试验表明,在这种有中心网络的实时数据多播应用中,所提出的多播策略的实时性能要明显优于传统传输策略。'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dc14f77c-53ae-4ed2-9bab-9bc497a9b01c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'网络编码在实时战术数据多播中的应用'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "refs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "95aafa6d-45e7-4b04-a56a-403ac45529bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 63/63 [00:25<00:00,  2.49it/s]\n"
     ]
    }
   ],
   "source": [
    "generations = predict(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ca784168-6500-4390-9e88-5b050f703ae2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'基于中心网络的实时数据多播应用'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generations[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc461586-8d0e-42f8-8a95-a514cadafc8a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 5. 生成结果的评测\n",
    "\n",
    "采用自动文摘任务上常用的自动评测指标Rouge-1, Rouge-2, Rouge-L对生成文本的质量进行评测。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b4d4f2be-b4a4-404a-9717-ad6923a4215a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge import Rouge \n",
    "\n",
    "hypothesis = \"the #### transcript is a written version of each day 's cnn student news program use this transcript to help students with reading comprehension and vocabulary use the weekly newsquiz to test your knowledge of storie s you saw on cnn student news\"\n",
    "\n",
    "reference = \"this page includes the show transcript use the transcript to help students with reading comprehension and vocabulary at the bottom of the page , comment for a chance to be mentioned on cnn student news . you must be a teacher or a student age # # or older to request a mention on the cnn student news roll call . the weekly newsquiz tests students ' knowledge of even ts in the news\"\n",
    "\n",
    "rouge = Rouge()\n",
    "scores = rouge.get_scores(hypothesis, reference)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4153cc33-6496-4a6d-888f-9b4e20d61c90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'rouge-1': {'r': 0.4583333333333333,\n",
       "   'p': 0.6285714285714286,\n",
       "   'f': 0.5301204770503702},\n",
       "  'rouge-2': {'r': 0.21739130434782608, 'p': 0.375, 'f': 0.2752293531520916},\n",
       "  'rouge-l': {'r': 0.4166666666666667,\n",
       "   'p': 0.5714285714285714,\n",
       "   'f': 0.4819277059660328}}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8f9e06f0-36c0-412e-8355-dd5257d74088",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge import Rouge\n",
    "rouge = Rouge()\n",
    "\n",
    "def rouge_score(candidate, reference):\n",
    "    text1 = \" \".join(list(candidate))\n",
    "    text2 = \" \".join(list(reference))\n",
    "    score = rouge.get_scores(text1, text2)\n",
    "    return score\n",
    "\n",
    "def compute_rouge(preds, refs):\n",
    "    r1=[]\n",
    "    r2=[]\n",
    "    R_L=[]\n",
    "    for pred, ref in zip(preds, refs):\n",
    "        scores = rouge_score(pred, ref)\n",
    "        r1.append(scores[0][\"rouge-1\"][\"f\"])\n",
    "        r2.append(scores[0][\"rouge-2\"][\"f\"])\n",
    "        R_L.append(scores[0][\"rouge-l\"][\"f\"])\n",
    "    return sum(r1)/len(r1), sum(r2)/len(r2), sum(R_L)/len(R_L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0e34b7f4-04c6-4b43-85bf-ac3461be3390",
   "metadata": {},
   "outputs": [],
   "source": [
    "R_1, R_2, R_L = compute_rouge(generations, refs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
