/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/scipy/__init__.py:132: UserWarning: A NumPy version >=1.21.6 and <1.28.0 is required for this version of SciPy (detected version 1.21.5)
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
Some weights of the model checkpoint at ../model/mengzi-bert-base-fin were not used when initializing BertForTokenClassification: ['pos_transform.dense.bias', 'pos_head.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'pos_transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'sop.cls.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.weight', 'pos_transform.LayerNorm.bias', 'sop.cls.bias', 'pos_head.weight', 'cls.predictions.decoder.weight', 'pos_transform.LayerNorm.weight']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at ../model/mengzi-bert-base-fin and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
{'loss': 0.9071, 'learning_rate': 2e-05, 'epoch': 0.02}
{'loss': 0.49, 'learning_rate': 2e-05, 'epoch': 0.03}
{'loss': 0.397, 'learning_rate': 2e-05, 'epoch': 0.05}
{'loss': 0.3739, 'learning_rate': 2e-05, 'epoch': 0.07}
{'loss': 0.3379, 'learning_rate': 2e-05, 'epoch': 0.09}
{'loss': 0.3274, 'learning_rate': 2e-05, 'epoch': 0.1}
{'loss': 0.3056, 'learning_rate': 2e-05, 'epoch': 0.12}
{'loss': 0.3194, 'learning_rate': 2e-05, 'epoch': 0.14}
{'loss': 0.3076, 'learning_rate': 2e-05, 'epoch': 0.16}
{'loss': 0.2816, 'learning_rate': 2e-05, 'epoch': 0.17}
{'loss': 0.2707, 'learning_rate': 2e-05, 'epoch': 0.19}
{'loss': 0.2703, 'learning_rate': 2e-05, 'epoch': 0.21}
{'loss': 0.2347, 'learning_rate': 2e-05, 'epoch': 0.23}
{'loss': 0.2627, 'learning_rate': 2e-05, 'epoch': 0.24}
{'loss': 0.2902, 'learning_rate': 2e-05, 'epoch': 0.26}
{'loss': 0.259, 'learning_rate': 2e-05, 'epoch': 0.28}
{'loss': 0.2491, 'learning_rate': 2e-05, 'epoch': 0.3}
{'loss': 0.2438, 'learning_rate': 2e-05, 'epoch': 0.31}
{'loss': 0.223, 'learning_rate': 2e-05, 'epoch': 0.33}
{'loss': 0.2395, 'learning_rate': 2e-05, 'epoch': 0.35}
{'loss': 0.2116, 'learning_rate': 2e-05, 'epoch': 0.37}
{'loss': 0.2218, 'learning_rate': 2e-05, 'epoch': 0.38}
{'loss': 0.2262, 'learning_rate': 2e-05, 'epoch': 0.4}
{'loss': 0.2094, 'learning_rate': 2e-05, 'epoch': 0.42}
{'loss': 0.2241, 'learning_rate': 2e-05, 'epoch': 0.44}
{'loss': 0.2124, 'learning_rate': 2e-05, 'epoch': 0.45}
{'loss': 0.2084, 'learning_rate': 2e-05, 'epoch': 0.47}
{'loss': 0.1862, 'learning_rate': 2e-05, 'epoch': 0.49}
{'loss': 0.2019, 'learning_rate': 2e-05, 'epoch': 0.51}
{'loss': 0.184, 'learning_rate': 2e-05, 'epoch': 0.52}
{'loss': 0.1798, 'learning_rate': 2e-05, 'epoch': 0.54}
{'loss': 0.1887, 'learning_rate': 2e-05, 'epoch': 0.56}
{'loss': 0.182, 'learning_rate': 2e-05, 'epoch': 0.58}
{'loss': 0.1781, 'learning_rate': 2e-05, 'epoch': 0.59}
{'loss': 0.1836, 'learning_rate': 2e-05, 'epoch': 0.61}
{'loss': 0.1748, 'learning_rate': 2e-05, 'epoch': 0.63}
{'loss': 0.1772, 'learning_rate': 2e-05, 'epoch': 0.65}
{'loss': 0.1726, 'learning_rate': 2e-05, 'epoch': 0.66}
{'loss': 0.1687, 'learning_rate': 2e-05, 'epoch': 0.68}
{'loss': 0.1523, 'learning_rate': 2e-05, 'epoch': 0.7}
{'loss': 0.1504, 'learning_rate': 2e-05, 'epoch': 0.72}
{'loss': 0.1519, 'learning_rate': 2e-05, 'epoch': 0.73}
{'loss': 0.1492, 'learning_rate': 2e-05, 'epoch': 0.75}
{'loss': 0.1757, 'learning_rate': 2e-05, 'epoch': 0.77}
{'loss': 0.1538, 'learning_rate': 2e-05, 'epoch': 0.79}
{'loss': 0.1367, 'learning_rate': 2e-05, 'epoch': 0.8}
{'loss': 0.1417, 'learning_rate': 2e-05, 'epoch': 0.82}
{'loss': 0.1586, 'learning_rate': 2e-05, 'epoch': 0.84}
{'loss': 0.1518, 'learning_rate': 2e-05, 'epoch': 0.85}
{'loss': 0.1425, 'learning_rate': 2e-05, 'epoch': 0.87}
{'loss': 0.1389, 'learning_rate': 2e-05, 'epoch': 0.89}
{'loss': 0.1423, 'learning_rate': 2e-05, 'epoch': 0.91}
{'loss': 0.1534, 'learning_rate': 2e-05, 'epoch': 0.92}
{'loss': 0.1422, 'learning_rate': 2e-05, 'epoch': 0.94}
{'loss': 0.155, 'learning_rate': 2e-05, 'epoch': 0.96}
{'loss': 0.1426, 'learning_rate': 2e-05, 'epoch': 0.98}
{'loss': 0.1434, 'learning_rate': 2e-05, 'epoch': 0.99}
{'eval_loss': 0.05855708569288254, 'eval_runtime': 4.4089, 'eval_samples_per_second': 144.707, 'eval_steps_per_second': 18.145, 'epoch': 1.0}
{'loss': 0.1291, 'learning_rate': 2e-05, 'epoch': 1.01}
{'loss': 0.1269, 'learning_rate': 2e-05, 'epoch': 1.03}
{'loss': 0.1389, 'learning_rate': 2e-05, 'epoch': 1.05}
{'loss': 0.1231, 'learning_rate': 2e-05, 'epoch': 1.06}
{'loss': 0.1135, 'learning_rate': 2e-05, 'epoch': 1.08}
{'loss': 0.1301, 'learning_rate': 2e-05, 'epoch': 1.1}
{'loss': 0.1165, 'learning_rate': 2e-05, 'epoch': 1.12}
{'loss': 0.1218, 'learning_rate': 2e-05, 'epoch': 1.13}
{'loss': 0.13, 'learning_rate': 2e-05, 'epoch': 1.15}
{'loss': 0.1252, 'learning_rate': 2e-05, 'epoch': 1.17}
{'loss': 0.1208, 'learning_rate': 2e-05, 'epoch': 1.19}
{'loss': 0.117, 'learning_rate': 2e-05, 'epoch': 1.2}
{'loss': 0.1348, 'learning_rate': 2e-05, 'epoch': 1.22}
{'loss': 0.1056, 'learning_rate': 2e-05, 'epoch': 1.24}
{'loss': 0.1253, 'learning_rate': 2e-05, 'epoch': 1.26}
{'loss': 0.1204, 'learning_rate': 2e-05, 'epoch': 1.27}
{'loss': 0.1085, 'learning_rate': 2e-05, 'epoch': 1.29}
{'loss': 0.1186, 'learning_rate': 2e-05, 'epoch': 1.31}
{'loss': 0.1109, 'learning_rate': 2e-05, 'epoch': 1.33}
{'loss': 0.1021, 'learning_rate': 2e-05, 'epoch': 1.34}
{'loss': 0.1137, 'learning_rate': 2e-05, 'epoch': 1.36}
{'loss': 0.1076, 'learning_rate': 2e-05, 'epoch': 1.38}
{'loss': 0.1157, 'learning_rate': 2e-05, 'epoch': 1.4}
{'loss': 0.118, 'learning_rate': 2e-05, 'epoch': 1.41}
{'loss': 0.1031, 'learning_rate': 2e-05, 'epoch': 1.43}
{'loss': 0.1278, 'learning_rate': 2e-05, 'epoch': 1.45}
{'loss': 0.1084, 'learning_rate': 2e-05, 'epoch': 1.47}
{'loss': 0.109, 'learning_rate': 2e-05, 'epoch': 1.48}
{'loss': 0.1099, 'learning_rate': 2e-05, 'epoch': 1.5}
{'loss': 0.1252, 'learning_rate': 2e-05, 'epoch': 1.52}
{'loss': 0.1058, 'learning_rate': 2e-05, 'epoch': 1.54}
{'loss': 0.1155, 'learning_rate': 2e-05, 'epoch': 1.55}
{'loss': 0.1101, 'learning_rate': 2e-05, 'epoch': 1.57}
{'loss': 0.1036, 'learning_rate': 2e-05, 'epoch': 1.59}
{'loss': 0.1027, 'learning_rate': 2e-05, 'epoch': 1.61}
{'loss': 0.1099, 'learning_rate': 2e-05, 'epoch': 1.62}
{'loss': 0.1189, 'learning_rate': 2e-05, 'epoch': 1.64}
{'loss': 0.1122, 'learning_rate': 2e-05, 'epoch': 1.66}
{'loss': 0.1101, 'learning_rate': 2e-05, 'epoch': 1.68}
{'loss': 0.1121, 'learning_rate': 2e-05, 'epoch': 1.69}
{'loss': 0.1184, 'learning_rate': 2e-05, 'epoch': 1.71}
{'loss': 0.1006, 'learning_rate': 2e-05, 'epoch': 1.73}
{'loss': 0.0927, 'learning_rate': 2e-05, 'epoch': 1.74}
{'loss': 0.1069, 'learning_rate': 2e-05, 'epoch': 1.76}
{'loss': 0.0988, 'learning_rate': 2e-05, 'epoch': 1.78}
{'loss': 0.1004, 'learning_rate': 2e-05, 'epoch': 1.8}
{'loss': 0.1058, 'learning_rate': 2e-05, 'epoch': 1.81}
{'loss': 0.1038, 'learning_rate': 2e-05, 'epoch': 1.83}
{'loss': 0.1101, 'learning_rate': 2e-05, 'epoch': 1.85}
{'loss': 0.1215, 'learning_rate': 2e-05, 'epoch': 1.87}
{'loss': 0.1062, 'learning_rate': 2e-05, 'epoch': 1.88}
{'loss': 0.0874, 'learning_rate': 2e-05, 'epoch': 1.9}
{'loss': 0.1036, 'learning_rate': 2e-05, 'epoch': 1.92}
{'loss': 0.1056, 'learning_rate': 2e-05, 'epoch': 1.94}
{'loss': 0.1107, 'learning_rate': 2e-05, 'epoch': 1.95}
{'loss': 0.0937, 'learning_rate': 2e-05, 'epoch': 1.97}
{'loss': 0.0948, 'learning_rate': 2e-05, 'epoch': 1.99}
{'eval_loss': 0.04791798070073128, 'eval_runtime': 4.4007, 'eval_samples_per_second': 144.976, 'eval_steps_per_second': 18.179, 'epoch': 2.0}
Terminated
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/scipy/__init__.py:132: UserWarning: A NumPy version >=1.21.6 and <1.28.0 is required for this version of SciPy (detected version 1.21.5)
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
Some weights of the model checkpoint at ../model/mengzi-bert-base-fin were not used when initializing BertForTokenClassification: ['pos_transform.LayerNorm.bias', 'sop.cls.weight', 'pos_transform.dense.bias', 'cls.predictions.transform.dense.weight', 'pos_head.bias', 'pos_transform.dense.weight', 'cls.predictions.decoder.weight', 'sop.cls.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'pos_transform.LayerNorm.weight', 'pos_head.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at ../model/mengzi-bert-base-fin and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/torch/cuda/__init__.py:546: UserWarning: Can't initialize NVML
  warnings.warn("Can't initialize NVML")
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
