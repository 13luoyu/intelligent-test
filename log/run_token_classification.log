原始数据（规则筛选或信息抽取）：训练集有数据2995条，验证集有数据333条。
原始数据（规则筛选或信息抽取）：训练集有数据1197条，验证集有数据134条。
规则：训练集有数据570条，验证集有数据64条。
Building prefix dict from /home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/synonyms/data/vocab.txt ...
Loading model from cache /tmp/jieba.u2936b6aa2669ea7ea53bb471c04c76db.cache
Loading model cost 0.957 seconds.
Prefix dict has been built successfully.
Simbert不能正常使用，除非你安装：bert4keras、tensorflow ，为了安装快捷，没有默认安装.... No module named 'bert4keras'
smart_open library not found; falling back to local-filesystem-only

 Synonyms: v3.18.0, Project home: https://github.com/chatopera/Synonyms/

 Project Sponsored by Chatopera

  deliver your chatbots with Chatopera Cloud Services --> https://bot.chatopera.com

>> Synonyms load wordseg dict [/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/synonyms/data/vocab.txt] ... 
>> Synonyms on loading stopwords [/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/synonyms/data/stopwords.txt] ...
>> Synonyms on loading vectors [/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/synonyms/data/words.vector.gz] ...
Some weights of the model checkpoint at ../model/mengzi-bert-base-fin were not used when initializing BertForTokenClassification: ['pos_transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.weight', 'pos_transform.LayerNorm.weight', 'pos_head.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'sop.cls.weight', 'cls.predictions.bias', 'sop.cls.bias', 'pos_head.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'pos_transform.dense.bias', 'pos_transform.dense.weight']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at ../model/mengzi-bert-base-fin and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
{'loss': 0.9993, 'learning_rate': 9.994044073853485e-06, 'epoch': 0.01}
{'loss': 0.5858, 'learning_rate': 9.98808814770697e-06, 'epoch': 0.02}
{'loss': 0.504, 'learning_rate': 9.982132221560452e-06, 'epoch': 0.04}
{'loss': 0.4411, 'learning_rate': 9.976176295413937e-06, 'epoch': 0.05}
{'loss': 0.4237, 'learning_rate': 9.970220369267421e-06, 'epoch': 0.06}
{'loss': 0.3608, 'learning_rate': 9.964264443120906e-06, 'epoch': 0.07}
{'loss': 0.368, 'learning_rate': 9.95830851697439e-06, 'epoch': 0.08}
{'loss': 0.358, 'learning_rate': 9.952352590827875e-06, 'epoch': 0.1}
{'loss': 0.3619, 'learning_rate': 9.94639666468136e-06, 'epoch': 0.11}
{'loss': 0.3431, 'learning_rate': 9.940440738534844e-06, 'epoch': 0.12}
{'loss': 0.3301, 'learning_rate': 9.934484812388328e-06, 'epoch': 0.13}
{'loss': 0.3096, 'learning_rate': 9.92852888624181e-06, 'epoch': 0.14}
{'loss': 0.3037, 'learning_rate': 9.922572960095295e-06, 'epoch': 0.15}
{'loss': 0.3089, 'learning_rate': 9.91661703394878e-06, 'epoch': 0.17}
{'loss': 0.3265, 'learning_rate': 9.910661107802264e-06, 'epoch': 0.18}
{'loss': 0.2886, 'learning_rate': 9.904705181655749e-06, 'epoch': 0.19}
{'loss': 0.2949, 'learning_rate': 9.898749255509233e-06, 'epoch': 0.2}
{'loss': 0.278, 'learning_rate': 9.892793329362716e-06, 'epoch': 0.21}
{'loss': 0.2836, 'learning_rate': 9.8868374032162e-06, 'epoch': 0.23}
{'loss': 0.2667, 'learning_rate': 9.880881477069685e-06, 'epoch': 0.24}
{'loss': 0.2564, 'learning_rate': 9.87492555092317e-06, 'epoch': 0.25}
{'loss': 0.2692, 'learning_rate': 9.868969624776654e-06, 'epoch': 0.26}
{'loss': 0.2651, 'learning_rate': 9.863013698630138e-06, 'epoch': 0.27}
{'loss': 0.2564, 'learning_rate': 9.857057772483623e-06, 'epoch': 0.29}
{'loss': 0.2676, 'learning_rate': 9.851101846337107e-06, 'epoch': 0.3}
{'loss': 0.2606, 'learning_rate': 9.845145920190591e-06, 'epoch': 0.31}
{'loss': 0.2627, 'learning_rate': 9.839189994044074e-06, 'epoch': 0.32}
{'loss': 0.2428, 'learning_rate': 9.833234067897559e-06, 'epoch': 0.33}
{'loss': 0.2333, 'learning_rate': 9.827278141751043e-06, 'epoch': 0.35}
{'loss': 0.2153, 'learning_rate': 9.821322215604528e-06, 'epoch': 0.36}
{'loss': 0.2216, 'learning_rate': 9.815366289458012e-06, 'epoch': 0.37}
{'loss': 0.2426, 'learning_rate': 9.809410363311496e-06, 'epoch': 0.38}
{'loss': 0.2443, 'learning_rate': 9.80345443716498e-06, 'epoch': 0.39}
{'loss': 0.2331, 'learning_rate': 9.797498511018464e-06, 'epoch': 0.41}
{'loss': 0.2165, 'learning_rate': 9.791542584871948e-06, 'epoch': 0.42}
{'loss': 0.2348, 'learning_rate': 9.785586658725433e-06, 'epoch': 0.43}
{'loss': 0.2383, 'learning_rate': 9.779630732578917e-06, 'epoch': 0.44}
{'loss': 0.2245, 'learning_rate': 9.7736748064324e-06, 'epoch': 0.45}
{'loss': 0.2175, 'learning_rate': 9.767718880285886e-06, 'epoch': 0.46}
{'loss': 0.2164, 'learning_rate': 9.76176295413937e-06, 'epoch': 0.48}
{'loss': 0.2058, 'learning_rate': 9.755807027992855e-06, 'epoch': 0.49}
{'loss': 0.1872, 'learning_rate': 9.749851101846338e-06, 'epoch': 0.5}
{'loss': 0.1941, 'learning_rate': 9.743895175699822e-06, 'epoch': 0.51}
{'loss': 0.2094, 'learning_rate': 9.737939249553306e-06, 'epoch': 0.52}
{'loss': 0.2075, 'learning_rate': 9.731983323406791e-06, 'epoch': 0.54}
{'loss': 0.2237, 'learning_rate': 9.726027397260275e-06, 'epoch': 0.55}
{'loss': 0.2067, 'learning_rate': 9.72007147111376e-06, 'epoch': 0.56}
{'loss': 0.2115, 'learning_rate': 9.714115544967243e-06, 'epoch': 0.57}
{'loss': 0.2114, 'learning_rate': 9.708159618820727e-06, 'epoch': 0.58}
{'loss': 0.1754, 'learning_rate': 9.702203692674211e-06, 'epoch': 0.6}
{'loss': 0.2008, 'learning_rate': 9.696247766527696e-06, 'epoch': 0.61}
{'loss': 0.1955, 'learning_rate': 9.69029184038118e-06, 'epoch': 0.62}
{'loss': 0.1924, 'learning_rate': 9.684335914234663e-06, 'epoch': 0.63}
{'loss': 0.1976, 'learning_rate': 9.67837998808815e-06, 'epoch': 0.64}
{'loss': 0.1861, 'learning_rate': 9.672424061941634e-06, 'epoch': 0.66}
{'loss': 0.1949, 'learning_rate': 9.666468135795118e-06, 'epoch': 0.67}
{'loss': 0.1947, 'learning_rate': 9.660512209648601e-06, 'epoch': 0.68}
{'loss': 0.182, 'learning_rate': 9.654556283502085e-06, 'epoch': 0.69}
{'loss': 0.1939, 'learning_rate': 9.64860035735557e-06, 'epoch': 0.7}
{'loss': 0.1779, 'learning_rate': 9.642644431209054e-06, 'epoch': 0.71}
{'loss': 0.1813, 'learning_rate': 9.636688505062539e-06, 'epoch': 0.73}
{'loss': 0.1795, 'learning_rate': 9.630732578916021e-06, 'epoch': 0.74}
{'loss': 0.1803, 'learning_rate': 9.624776652769506e-06, 'epoch': 0.75}
{'loss': 0.1769, 'learning_rate': 9.61882072662299e-06, 'epoch': 0.76}
{'loss': 0.1612, 'learning_rate': 9.612864800476475e-06, 'epoch': 0.77}
{'loss': 0.1768, 'learning_rate': 9.60690887432996e-06, 'epoch': 0.79}
{'loss': 0.179, 'learning_rate': 9.600952948183444e-06, 'epoch': 0.8}
{'loss': 0.1662, 'learning_rate': 9.594997022036926e-06, 'epoch': 0.81}
{'loss': 0.1821, 'learning_rate': 9.589041095890411e-06, 'epoch': 0.82}
{'loss': 0.1641, 'learning_rate': 9.583085169743897e-06, 'epoch': 0.83}
{'loss': 0.1787, 'learning_rate': 9.57712924359738e-06, 'epoch': 0.85}
{'loss': 0.1743, 'learning_rate': 9.571173317450864e-06, 'epoch': 0.86}
{'loss': 0.1689, 'learning_rate': 9.565217391304349e-06, 'epoch': 0.87}
{'loss': 0.155, 'learning_rate': 9.559261465157833e-06, 'epoch': 0.88}
{'loss': 0.1515, 'learning_rate': 9.553305539011318e-06, 'epoch': 0.89}
{'loss': 0.153, 'learning_rate': 9.547349612864802e-06, 'epoch': 0.91}
{'loss': 0.1577, 'learning_rate': 9.541393686718285e-06, 'epoch': 0.92}
{'loss': 0.174, 'learning_rate': 9.53543776057177e-06, 'epoch': 0.93}
{'loss': 0.1809, 'learning_rate': 9.529481834425254e-06, 'epoch': 0.94}
{'loss': 0.1554, 'learning_rate': 9.523525908278738e-06, 'epoch': 0.95}
{'loss': 0.1471, 'learning_rate': 9.517569982132223e-06, 'epoch': 0.96}
{'loss': 0.1473, 'learning_rate': 9.511614055985707e-06, 'epoch': 0.98}
{'loss': 0.1542, 'learning_rate': 9.50565812983919e-06, 'epoch': 0.99}
{'eval_loss': 0.0622333325445652, 'eval_runtime': 4.5888, 'eval_samples_per_second': 138.162, 'eval_steps_per_second': 17.434, 'epoch': 1.0}
