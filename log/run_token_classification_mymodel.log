/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/scipy/__init__.py:132: UserWarning: A NumPy version >=1.21.6 and <1.28.0 is required for this version of SciPy (detected version 1.21.5)
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
Some weights of the model checkpoint at ../train/model were not used when initializing BertForTokenClassification: ['model.encoder.layer.11.self.key.bias', 'model.encoder.layer.5.attention_output.LayerNorm.weight', 'model.encoder.layer.7.output.dense.bias', 'model.encoder.layer.1.attention_output.LayerNorm.weight', 'model.encoder.layer.5.intermediate.dense.weight', 'model.encoder.layer.8.self.key.weight', 'model.encoder.layer.4.attention_output.LayerNorm.weight', 'model.encoder.layer.3.self.value.weight', 'model.encoder.layer.8.output.dense.bias', 'model.encoder.layer.3.output.dense.weight', 'model.encoder.layer.11.self.query.bias', 'model.encoder.layer.9.attention_output.LayerNorm.bias', 'model.encoder.layer.0.self.query.weight', 'model.encoder.layer.10.intermediate.dense.bias', 'model.encoder.layer.7.self.query.bias', 'model.encoder.layer.3.attention_output.LayerNorm.bias', 'model.encoder.layer.1.attention_output.LayerNorm.bias', 'model.encoder.layer.0.output.dense.bias', 'model.encoder.layer.1.output.LayerNorm.weight', 'model.encoder.layer.5.self.query.weight', 'model.encoder.layer.0.self.value.weight', 'model.encoder.layer.3.self.key.bias', 'model.encoder.layer.1.self.query.bias', 'model.encoder.layer.4.self.query.weight', 'model.encoder.layer.6.self.query.weight', 'model.encoder.layer.7.attention_output.LayerNorm.weight', 'model.encoder.layer.8.self.value.bias', 'model.encoder.layer.4.intermediate.dense.weight', 'model.encoder.layer.9.self.key.bias', 'model.encoder.layer.10.output.LayerNorm.weight', 'model.encoder.layer.11.attention_output.LayerNorm.weight', 'model.encoder.layer.2.attention_output.LayerNorm.bias', 'model.encoder.layer.0.self.key.weight', 'model.encoder.layer.3.output.LayerNorm.weight', 'model.encoder.layer.11.self.value.weight', 'model.encoder.layer.4.self.query.bias', 'model.encoder.layer.6.self.key.bias', 'model.encoder.layer.11.output.dense.bias', 'model.encoder.layer.0.attention_output.LayerNorm.weight', 'model.encoder.layer.9.self.key.weight', 'model.encoder.layer.10.self.query.weight', 'model.embeddings.token_type_embeddings.weight', 'model.encoder.layer.0.self.value.bias', 'model.encoder.layer.4.attention_output.dense.weight', 'model.encoder.layer.0.output.dense.weight', 'model.encoder.layer.2.attention_output.dense.bias', 'model.encoder.layer.7.self.key.bias', 'model.encoder.layer.1.output.LayerNorm.bias', 'model.encoder.layer.5.output.LayerNorm.weight', 'model.encoder.layer.9.attention_output.dense.bias', 'model.encoder.layer.6.output.dense.bias', 'model.encoder.layer.3.self.value.bias', 'model.encoder.layer.9.output.LayerNorm.weight', 'model.encoder.layer.7.intermediate.dense.weight', 'model.encoder.layer.9.self.query.bias', 'model.encoder.layer.7.attention_output.dense.bias', 'model.encoder.layer.5.self.query.bias', 'model.encoder.layer.6.attention_output.dense.bias', 'model.encoder.layer.2.self.query.bias', 'model.encoder.layer.8.output.dense.weight', 'model.encoder.layer.11.output.dense.weight', 'model.encoder.layer.4.self.key.weight', 'model.encoder.layer.8.output.LayerNorm.bias', 'model.encoder.layer.5.intermediate.dense.bias', 'model.encoder.layer.8.attention_output.LayerNorm.bias', 'model.encoder.layer.0.self.query.bias', 'model.encoder.layer.2.self.query.weight', 'model.encoder.layer.11.self.value.bias', 'model.encoder.layer.2.attention_output.LayerNorm.weight', 'model.encoder.layer.2.attention_output.dense.weight', 'model.encoder.layer.5.output.dense.weight', 'model.encoder.layer.6.attention_output.dense.weight', 'model.encoder.layer.1.attention_output.dense.weight', 'model.embeddings.LayerNorm.weight', 'model.encoder.layer.9.output.LayerNorm.bias', 'model.encoder.layer.9.self.value.bias', 'model.encoder.layer.5.output.LayerNorm.bias', 'model.encoder.layer.2.output.LayerNorm.bias', 'model.encoder.layer.5.attention_output.dense.bias', 'model.encoder.layer.8.self.value.weight', 'model.encoder.layer.4.self.key.bias', 'model.encoder.layer.1.self.key.bias', 'model.encoder.layer.5.self.value.bias', 'model.encoder.layer.7.self.key.weight', 'model.encoder.layer.1.intermediate.dense.bias', 'model.encoder.layer.4.output.dense.weight', 'model.encoder.layer.4.self.value.weight', 'model.encoder.layer.1.output.dense.bias', 'model.encoder.layer.3.output.dense.bias', 'model.encoder.layer.4.self.value.bias', 'model.encoder.layer.9.output.dense.bias', 'model.encoder.layer.10.attention_output.dense.bias', 'model.encoder.layer.0.attention_output.dense.bias', 'model.encoder.layer.4.attention_output.dense.bias', 'model.encoder.layer.11.output.LayerNorm.weight', 'model.encoder.layer.3.self.query.bias', 'model.encoder.layer.3.self.query.weight', 'model.encoder.layer.9.self.value.weight', 'model.encoder.layer.11.self.key.weight', 'model.encoder.layer.3.attention_output.LayerNorm.weight', 'model.encoder.layer.5.self.key.bias', 'model.encoder.layer.0.intermediate.dense.bias', 'model.encoder.layer.7.attention_output.LayerNorm.bias', 'model.encoder.layer.10.output.dense.bias', 'model.encoder.layer.11.attention_output.dense.bias', 'model.encoder.layer.8.output.LayerNorm.weight', 'model.encoder.layer.7.intermediate.dense.bias', 'model.encoder.layer.11.output.LayerNorm.bias', 'model.encoder.layer.2.self.value.bias', 'model.encoder.layer.7.self.value.weight', 'model.encoder.layer.3.intermediate.dense.weight', 'model.encoder.layer.8.self.query.weight', 'model.encoder.layer.1.self.value.weight', 'model.encoder.layer.3.attention_output.dense.weight', 'model.embeddings.word_embeddings.weight', 'model.encoder.layer.10.attention_output.dense.weight', 'model.encoder.layer.2.output.dense.bias', 'model.encoder.layer.11.intermediate.dense.bias', 'model.encoder.layer.6.self.value.weight', 'model.encoder.layer.0.attention_output.LayerNorm.bias', 'model.encoder.layer.11.attention_output.dense.weight', 'model.encoder.layer.7.attention_output.dense.weight', 'model.encoder.layer.1.self.key.weight', 'model.encoder.layer.2.self.value.weight', 'model.encoder.layer.11.intermediate.dense.weight', 'model.encoder.layer.5.output.dense.bias', 'model.encoder.layer.5.attention_output.dense.weight', 'model.encoder.layer.8.attention_output.dense.weight', 'model.encoder.layer.6.intermediate.dense.bias', 'model.encoder.layer.8.self.key.bias', 'model.encoder.layer.9.self.query.weight', 'model.encoder.layer.9.attention_output.LayerNorm.weight', 'model.encoder.layer.10.self.value.weight', 'model.encoder.layer.10.attention_output.LayerNorm.bias', 'model.encoder.layer.6.output.LayerNorm.bias', 'model.encoder.layer.6.self.value.bias', 'model.encoder.layer.10.output.dense.weight', 'model.encoder.layer.1.intermediate.dense.weight', 'model.encoder.layer.2.output.LayerNorm.weight', 'model.encoder.layer.3.intermediate.dense.bias', 'model.encoder.layer.0.output.LayerNorm.weight', 'model.encoder.layer.1.attention_output.dense.bias', 'model.encoder.layer.2.output.dense.weight', 'model.encoder.layer.8.attention_output.LayerNorm.weight', 'model.encoder.layer.4.output.LayerNorm.bias', 'model.encoder.layer.5.self.value.weight', 'model.encoder.layer.8.self.query.bias', 'model.encoder.layer.9.output.dense.weight', 'model.encoder.layer.1.self.value.bias', 'model.embeddings.LayerNorm.bias', 'model.encoder.layer.11.self.query.weight', 'model.encoder.layer.10.self.key.bias', 'model.encoder.layer.6.attention_output.LayerNorm.bias', 'model.encoder.layer.10.self.value.bias', 'model.encoder.layer.7.output.LayerNorm.bias', 'model.encoder.layer.0.attention_output.dense.weight', 'model.encoder.layer.9.intermediate.dense.bias', 'model.encoder.layer.6.output.LayerNorm.weight', 'model.encoder.layer.1.output.dense.weight', 'model.encoder.layer.4.intermediate.dense.bias', 'model.encoder.layer.8.intermediate.dense.bias', 'model.encoder.layer.8.attention_output.dense.bias', 'model.encoder.layer.7.self.value.bias', 'model.encoder.layer.7.self.query.weight', 'model.encoder.layer.8.intermediate.dense.weight', 'model.encoder.layer.5.attention_output.LayerNorm.bias', 'model.encoder.layer.6.self.key.weight', 'model.encoder.layer.9.attention_output.dense.weight', 'model.encoder.layer.1.self.query.weight', 'model.encoder.layer.4.attention_output.LayerNorm.bias', 'model.encoder.layer.6.self.query.bias', 'model.encoder.layer.4.output.dense.bias', 'model.encoder.layer.0.output.LayerNorm.bias', 'model.encoder.layer.7.output.dense.weight', 'model.embeddings.position_embeddings.weight', 'model.encoder.layer.10.intermediate.dense.weight', 'model.encoder.layer.9.intermediate.dense.weight', 'model.encoder.layer.3.output.LayerNorm.bias', 'model.encoder.layer.3.self.key.weight', 'model.encoder.layer.0.intermediate.dense.weight', 'model.encoder.layer.10.attention_output.LayerNorm.weight', 'model.encoder.layer.10.self.query.bias', 'model.encoder.layer.10.self.key.weight', 'model.encoder.layer.2.intermediate.dense.weight', 'model.encoder.layer.2.self.key.bias', 'model.encoder.layer.3.attention_output.dense.bias', 'model.encoder.layer.6.intermediate.dense.weight', 'model.encoder.layer.6.output.dense.weight', 'model.encoder.layer.6.attention_output.LayerNorm.weight', 'model.encoder.layer.10.output.LayerNorm.bias', 'model.encoder.layer.2.intermediate.dense.bias', 'model.encoder.layer.11.attention_output.LayerNorm.bias', 'model.encoder.layer.2.self.key.weight', 'model.encoder.layer.7.output.LayerNorm.weight', 'model.encoder.layer.4.output.LayerNorm.weight', 'model.encoder.layer.0.self.key.bias', 'model.encoder.layer.5.self.key.weight']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at ../train/model and are newly initialized: ['encoder.layer.10.intermediate.dense.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.7.output.LayerNorm.bias', 'embeddings.word_embeddings.weight', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.0.intermediate.dense.bias', 'embeddings.LayerNorm.weight', 'classifier.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.0.output.dense.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.0.output.dense.bias', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.0.attention.self.key.weight', 'embeddings.position_embeddings.weight', 'encoder.layer.3.output.dense.weight', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.9.output.dense.weight', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.8.output.dense.weight', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.2.attention.self.key.bias', 'embeddings.LayerNorm.bias', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.10.output.dense.weight', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.5.output.dense.bias', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.4.output.dense.weight', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.7.intermediate.dense.weight', 'classifier.weight', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.4.output.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.11.output.dense.bias', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.output.dense.bias', 'embeddings.token_type_embeddings.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.3.output.dense.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.11.attention.output.LayerNorm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
{'loss': 10.1823, 'learning_rate': 0.09990313831848122, 'epoch': 0.02}
{'loss': 1.4453, 'learning_rate': 0.09980627663696243, 'epoch': 0.04}
{'loss': 1.0487, 'learning_rate': 0.09970941495544364, 'epoch': 0.06}
{'loss': 0.9758, 'learning_rate': 0.09961255327392483, 'epoch': 0.08}
{'loss': 0.9878, 'learning_rate': 0.09951569159240604, 'epoch': 0.1}
{'loss': 0.9605, 'learning_rate': 0.09941882991088725, 'epoch': 0.12}
{'loss': 0.9148, 'learning_rate': 0.09932196822936847, 'epoch': 0.14}
{'loss': 0.9675, 'learning_rate': 0.09922510654784968, 'epoch': 0.15}
{'loss': 0.9093, 'learning_rate': 0.09912824486633089, 'epoch': 0.17}
{'loss': 0.9276, 'learning_rate': 0.09903138318481208, 'epoch': 0.19}
{'loss': 0.9228, 'learning_rate': 0.0989345215032933, 'epoch': 0.21}
{'loss': 3.3768, 'learning_rate': 0.0988376598217745, 'epoch': 0.23}
{'loss': 7.7194, 'learning_rate': 0.09874079814025571, 'epoch': 0.25}
{'loss': 4.6125, 'learning_rate': 0.09864393645873692, 'epoch': 0.27}
{'loss': 2.9112, 'learning_rate': 0.09854707477721814, 'epoch': 0.29}
{'loss': 2.1084, 'learning_rate': 0.09845021309569935, 'epoch': 0.31}
{'loss': 1.9677, 'learning_rate': 0.09835335141418056, 'epoch': 0.33}
{'loss': 1.4555, 'learning_rate': 0.09825648973266177, 'epoch': 0.35}
{'loss': 1.1921, 'learning_rate': 0.09815962805114298, 'epoch': 0.37}
{'loss': 1.0332, 'learning_rate': 0.09806276636962419, 'epoch': 0.39}
{'loss': 0.954, 'learning_rate': 0.0979659046881054, 'epoch': 0.41}
{'loss': 0.9337, 'learning_rate': 0.09786904300658661, 'epoch': 0.43}
{'loss': 0.9718, 'learning_rate': 0.09777218132506782, 'epoch': 0.45}
{'loss': 0.9247, 'learning_rate': 0.09767531964354902, 'epoch': 0.46}
{'loss': 0.9056, 'learning_rate': 0.09757845796203023, 'epoch': 0.48}
{'loss': 0.9892, 'learning_rate': 0.09748159628051144, 'epoch': 0.5}
{'loss': 0.9295, 'learning_rate': 0.09738473459899265, 'epoch': 0.52}
{'loss': 0.996, 'learning_rate': 0.09728787291747386, 'epoch': 0.54}
{'loss': 0.9774, 'learning_rate': 0.09719101123595507, 'epoch': 0.56}
{'loss': 0.9752, 'learning_rate': 0.09709414955443627, 'epoch': 0.58}
{'loss': 1.0636, 'learning_rate': 0.09699728787291748, 'epoch': 0.6}
{'loss': 0.9525, 'learning_rate': 0.09690042619139869, 'epoch': 0.62}
{'loss': 0.9759, 'learning_rate': 0.0968035645098799, 'epoch': 0.64}
{'loss': 1.029, 'learning_rate': 0.09670670282836111, 'epoch': 0.66}
{'loss': 1.0083, 'learning_rate': 0.09660984114684232, 'epoch': 0.68}
{'loss': 1.0244, 'learning_rate': 0.09651297946532353, 'epoch': 0.7}
{'loss': 1.0115, 'learning_rate': 0.09641611778380473, 'epoch': 0.72}
{'loss': 1.0027, 'learning_rate': 0.09631925610228594, 'epoch': 0.74}
{'loss': 0.9532, 'learning_rate': 0.09622239442076715, 'epoch': 0.76}
{'loss': 0.998, 'learning_rate': 0.09612553273924836, 'epoch': 0.77}
{'loss': 0.9525, 'learning_rate': 0.09602867105772957, 'epoch': 0.79}
{'loss': 0.9894, 'learning_rate': 0.09593180937621078, 'epoch': 0.81}
{'loss': 1.7956, 'learning_rate': 0.09583494769469199, 'epoch': 0.83}
{'loss': 4.6732, 'learning_rate': 0.09573808601317318, 'epoch': 0.85}
{'loss': 2.6414, 'learning_rate': 0.0956412243316544, 'epoch': 0.87}
{'loss': 3.2588, 'learning_rate': 0.0955443626501356, 'epoch': 0.89}
{'loss': 2.8871, 'learning_rate': 0.09544750096861682, 'epoch': 0.91}
{'loss': 2.0516, 'learning_rate': 0.09535063928709803, 'epoch': 0.93}
{'loss': 2.1004, 'learning_rate': 0.09525377760557924, 'epoch': 0.95}
{'loss': 1.4033, 'learning_rate': 0.09515691592406045, 'epoch': 0.97}
{'loss': 1.1627, 'learning_rate': 0.09506005424254166, 'epoch': 0.99}
{'eval_loss': 0.6300641298294067, 'eval_runtime': 4.4556, 'eval_samples_per_second': 143.189, 'eval_steps_per_second': 17.955, 'epoch': 1.0}
{'loss': 1.0382, 'learning_rate': 0.09496319256102287, 'epoch': 1.01}
{'loss': 0.941, 'learning_rate': 0.09486633087950408, 'epoch': 1.03}
{'loss': 0.9544, 'learning_rate': 0.09476946919798529, 'epoch': 1.05}
{'loss': 0.8797, 'learning_rate': 0.0946726075164665, 'epoch': 1.07}
{'loss': 0.9624, 'learning_rate': 0.09457574583494771, 'epoch': 1.08}
{'loss': 0.9702, 'learning_rate': 0.09447888415342891, 'epoch': 1.1}
{'loss': 0.9608, 'learning_rate': 0.09438202247191012, 'epoch': 1.12}
{'loss': 0.9398, 'learning_rate': 0.09428516079039133, 'epoch': 1.14}
{'loss': 1.0072, 'learning_rate': 0.09418829910887254, 'epoch': 1.16}
{'loss': 0.9874, 'learning_rate': 0.09409143742735375, 'epoch': 1.18}
{'loss': 0.8991, 'learning_rate': 0.09399457574583496, 'epoch': 1.2}
{'loss': 0.954, 'learning_rate': 0.09389771406431616, 'epoch': 1.22}
{'loss': 0.9852, 'learning_rate': 0.09380085238279737, 'epoch': 1.24}
{'loss': 0.9749, 'learning_rate': 0.09370399070127858, 'epoch': 1.26}
{'loss': 1.018, 'learning_rate': 0.09360712901975979, 'epoch': 1.28}
{'loss': 0.9675, 'learning_rate': 0.093510267338241, 'epoch': 1.3}
{'loss': 0.9515, 'learning_rate': 0.09341340565672221, 'epoch': 1.32}
{'loss': 1.0628, 'learning_rate': 0.09331654397520342, 'epoch': 1.34}
{'loss': 1.5262, 'learning_rate': 0.09321968229368462, 'epoch': 1.36}
{'loss': 2.9538, 'learning_rate': 0.09312282061216583, 'epoch': 1.38}
{'loss': 3.3461, 'learning_rate': 0.09302595893064704, 'epoch': 1.39}
{'loss': 2.238, 'learning_rate': 0.09292909724912825, 'epoch': 1.41}
{'loss': 1.5149, 'learning_rate': 0.09283223556760946, 'epoch': 1.43}
{'loss': 1.5402, 'learning_rate': 0.09273537388609067, 'epoch': 1.45}
{'loss': 1.3222, 'learning_rate': 0.09263851220457188, 'epoch': 1.47}
{'loss': 1.054, 'learning_rate': 0.09254165052305308, 'epoch': 1.49}
{'loss': 0.9915, 'learning_rate': 0.09244478884153429, 'epoch': 1.51}
{'loss': 0.941, 'learning_rate': 0.0923479271600155, 'epoch': 1.53}
{'loss': 0.919, 'learning_rate': 0.09225106547849671, 'epoch': 1.55}
{'loss': 0.9292, 'learning_rate': 0.09215420379697792, 'epoch': 1.57}
{'loss': 0.9371, 'learning_rate': 0.09205734211545913, 'epoch': 1.59}
{'loss': 0.9142, 'learning_rate': 0.09196048043394034, 'epoch': 1.61}
{'loss': 0.9033, 'learning_rate': 0.09186361875242155, 'epoch': 1.63}
{'loss': 0.972, 'learning_rate': 0.09176675707090276, 'epoch': 1.65}
{'loss': 1.2015, 'learning_rate': 0.09166989538938397, 'epoch': 1.67}
{'loss': 1.0279, 'learning_rate': 0.09157303370786518, 'epoch': 1.69}
{'loss': 0.9156, 'learning_rate': 0.09147617202634639, 'epoch': 1.7}
{'loss': 1.0469, 'learning_rate': 0.0913793103448276, 'epoch': 1.72}
{'loss': 0.9528, 'learning_rate': 0.0912824486633088, 'epoch': 1.74}
{'loss': 0.9837, 'learning_rate': 0.09118558698179001, 'epoch': 1.76}
{'loss': 0.9085, 'learning_rate': 0.09108872530027122, 'epoch': 1.78}
{'loss': 0.9821, 'learning_rate': 0.09099186361875243, 'epoch': 1.8}
{'loss': 0.9777, 'learning_rate': 0.09089500193723364, 'epoch': 1.82}
{'loss': 1.0785, 'learning_rate': 0.09079814025571485, 'epoch': 1.84}
{'loss': 1.5656, 'learning_rate': 0.09070127857419605, 'epoch': 1.86}
{'loss': 1.9944, 'learning_rate': 0.09060441689267726, 'epoch': 1.88}
{'loss': 1.5938, 'learning_rate': 0.09050755521115847, 'epoch': 1.9}
{'loss': 2.2644, 'learning_rate': 0.09041069352963968, 'epoch': 1.92}
{'loss': 1.8255, 'learning_rate': 0.09031383184812089, 'epoch': 1.94}
{'loss': 1.1923, 'learning_rate': 0.0902169701666021, 'epoch': 1.96}
{'loss': 1.0577, 'learning_rate': 0.09012010848508331, 'epoch': 1.98}
{'loss': 0.9817, 'learning_rate': 0.09002324680356451, 'epoch': 2.0}
{'eval_loss': 0.5496692657470703, 'eval_runtime': 4.4654, 'eval_samples_per_second': 142.876, 'eval_steps_per_second': 17.916, 'epoch': 2.0}
{'loss': 0.981, 'learning_rate': 0.08992638512204572, 'epoch': 2.01}
{'loss': 0.9555, 'learning_rate': 0.08982952344052693, 'epoch': 2.03}
{'loss': 0.9617, 'learning_rate': 0.08973266175900814, 'epoch': 2.05}
{'loss': 0.9216, 'learning_rate': 0.08963580007748935, 'epoch': 2.07}
{'loss': 0.9236, 'learning_rate': 0.08953893839597056, 'epoch': 2.09}
{'loss': 0.8812, 'learning_rate': 0.08944207671445177, 'epoch': 2.11}
{'loss': 0.946, 'learning_rate': 0.08934521503293297, 'epoch': 2.13}
{'loss': 1.0073, 'learning_rate': 0.08924835335141418, 'epoch': 2.15}
{'loss': 0.9842, 'learning_rate': 0.08915149166989539, 'epoch': 2.17}
{'loss': 0.9982, 'learning_rate': 0.0890546299883766, 'epoch': 2.19}
{'loss': 0.9505, 'learning_rate': 0.08895776830685781, 'epoch': 2.21}
{'loss': 0.943, 'learning_rate': 0.08886090662533902, 'epoch': 2.23}
{'loss': 0.9593, 'learning_rate': 0.08876404494382023, 'epoch': 2.25}
{'loss': 0.9258, 'learning_rate': 0.08866718326230144, 'epoch': 2.27}
{'loss': 0.9641, 'learning_rate': 0.08857032158078265, 'epoch': 2.29}
{'loss': 0.9532, 'learning_rate': 0.08847345989926386, 'epoch': 2.31}
{'loss': 0.9946, 'learning_rate': 0.08837659821774507, 'epoch': 2.32}
{'loss': 1.2401, 'learning_rate': 0.08827973653622628, 'epoch': 2.34}
{'loss': 1.6934, 'learning_rate': 0.0881828748547075, 'epoch': 2.36}
{'loss': 1.9003, 'learning_rate': 0.08808601317318869, 'epoch': 2.38}
{'loss': 1.8132, 'learning_rate': 0.0879891514916699, 'epoch': 2.4}
{'loss': 2.1495, 'learning_rate': 0.08789228981015111, 'epoch': 2.42}
{'loss': 1.7869, 'learning_rate': 0.08779542812863232, 'epoch': 2.44}
{'loss': 1.2973, 'learning_rate': 0.08769856644711353, 'epoch': 2.46}
{'loss': 1.0565, 'learning_rate': 0.08760170476559474, 'epoch': 2.48}
{'loss': 0.9591, 'learning_rate': 0.08750484308407595, 'epoch': 2.5}
{'loss': 0.9601, 'learning_rate': 0.08740798140255715, 'epoch': 2.52}
{'loss': 0.9438, 'learning_rate': 0.08731111972103836, 'epoch': 2.54}
{'loss': 0.9621, 'learning_rate': 0.08721425803951957, 'epoch': 2.56}
{'loss': 0.9201, 'learning_rate': 0.08711739635800078, 'epoch': 2.58}
{'loss': 0.8898, 'learning_rate': 0.08702053467648199, 'epoch': 2.6}
{'loss': 0.9205, 'learning_rate': 0.0869236729949632, 'epoch': 2.62}
{'loss': 0.9553, 'learning_rate': 0.0868268113134444, 'epoch': 2.63}
{'loss': 0.9214, 'learning_rate': 0.08672994963192561, 'epoch': 2.65}
{'loss': 0.9253, 'learning_rate': 0.08663308795040682, 'epoch': 2.67}
{'loss': 0.9469, 'learning_rate': 0.08653622626888803, 'epoch': 2.69}
{'loss': 0.978, 'learning_rate': 0.08643936458736924, 'epoch': 2.71}
{'loss': 0.9449, 'learning_rate': 0.08634250290585045, 'epoch': 2.73}
{'loss': 0.9319, 'learning_rate': 0.08624564122433166, 'epoch': 2.75}
{'loss': 1.0849, 'learning_rate': 0.08614877954281286, 'epoch': 2.77}
{'loss': 1.2381, 'learning_rate': 0.08605191786129407, 'epoch': 2.79}
{'loss': 4.6739, 'learning_rate': 0.08595505617977528, 'epoch': 2.81}
{'loss': 2.6728, 'learning_rate': 0.08585819449825649, 'epoch': 2.83}
{'loss': 2.5758, 'learning_rate': 0.0857613328167377, 'epoch': 2.85}
{'loss': 1.578, 'learning_rate': 0.08566447113521891, 'epoch': 2.87}
{'loss': 1.2765, 'learning_rate': 0.08556760945370012, 'epoch': 2.89}
{'loss': 1.027, 'learning_rate': 0.08547074777218133, 'epoch': 2.91}
{'loss': 0.9099, 'learning_rate': 0.08537388609066254, 'epoch': 2.93}
{'loss': 0.9495, 'learning_rate': 0.08527702440914375, 'epoch': 2.94}
{'loss': 0.9319, 'learning_rate': 0.08518016272762496, 'epoch': 2.96}
{'loss': 0.9383, 'learning_rate': 0.08508330104610617, 'epoch': 2.98}
{'eval_loss': 0.5912693738937378, 'eval_runtime': 4.448, 'eval_samples_per_second': 143.436, 'eval_steps_per_second': 17.986, 'epoch': 3.0}
{'loss': 0.9505, 'learning_rate': 0.08498643936458739, 'epoch': 3.0}
{'loss': 0.9865, 'learning_rate': 0.08488957768306858, 'epoch': 3.02}
{'loss': 0.9289, 'learning_rate': 0.08479271600154979, 'epoch': 3.04}
{'loss': 0.9454, 'learning_rate': 0.084695854320031, 'epoch': 3.06}
{'loss': 0.9482, 'learning_rate': 0.08459899263851221, 'epoch': 3.08}
{'loss': 0.9458, 'learning_rate': 0.08450213095699342, 'epoch': 3.1}
{'loss': 0.9675, 'learning_rate': 0.08440526927547463, 'epoch': 3.12}
{'loss': 0.9466, 'learning_rate': 0.08430840759395584, 'epoch': 3.14}
{'loss': 0.9994, 'learning_rate': 0.08421154591243704, 'epoch': 3.16}
{'loss': 1.0326, 'learning_rate': 0.08411468423091825, 'epoch': 3.18}
{'loss': 0.9163, 'learning_rate': 0.08401782254939946, 'epoch': 3.2}
{'loss': 0.9867, 'learning_rate': 0.08392096086788067, 'epoch': 3.22}
{'loss': 0.9576, 'learning_rate': 0.08382409918636188, 'epoch': 3.24}
{'loss': 0.9667, 'learning_rate': 0.0837272375048431, 'epoch': 3.25}
{'loss': 0.9966, 'learning_rate': 0.08363037582332429, 'epoch': 3.27}
{'loss': 1.0872, 'learning_rate': 0.0835335141418055, 'epoch': 3.29}
{'loss': 0.9795, 'learning_rate': 0.08343665246028671, 'epoch': 3.31}
{'loss': 0.9433, 'learning_rate': 0.08333979077876792, 'epoch': 3.33}
{'loss': 1.225, 'learning_rate': 0.08324292909724913, 'epoch': 3.35}
{'loss': 1.0567, 'learning_rate': 0.08314606741573034, 'epoch': 3.37}
{'loss': 0.9381, 'learning_rate': 0.08304920573421155, 'epoch': 3.39}
{'loss': 0.9776, 'learning_rate': 0.08295234405269275, 'epoch': 3.41}
{'loss': 0.9208, 'learning_rate': 0.08285548237117396, 'epoch': 3.43}
{'loss': 0.9304, 'learning_rate': 0.08275862068965517, 'epoch': 3.45}
{'loss': 0.9332, 'learning_rate': 0.08266175900813638, 'epoch': 3.47}
{'loss': 0.8981, 'learning_rate': 0.08256489732661759, 'epoch': 3.49}
{'loss': 0.9786, 'learning_rate': 0.0824680356450988, 'epoch': 3.51}
{'loss': 1.0007, 'learning_rate': 0.08237117396358001, 'epoch': 3.53}
{'loss': 0.9872, 'learning_rate': 0.08227431228206122, 'epoch': 3.55}
{'loss': 0.9543, 'learning_rate': 0.08217745060054243, 'epoch': 3.56}
{'loss': 1.0541, 'learning_rate': 0.08208058891902364, 'epoch': 3.58}
{'loss': 1.2738, 'learning_rate': 0.08198372723750486, 'epoch': 3.6}
{'loss': 2.3883, 'learning_rate': 0.08188686555598607, 'epoch': 3.62}
{'loss': 2.8681, 'learning_rate': 0.08179000387446728, 'epoch': 3.64}
{'loss': 1.5431, 'learning_rate': 0.08169314219294847, 'epoch': 3.66}
{'loss': 1.2509, 'learning_rate': 0.08159628051142968, 'epoch': 3.68}
{'loss': 1.0379, 'learning_rate': 0.0814994188299109, 'epoch': 3.7}
{'loss': 0.9302, 'learning_rate': 0.0814025571483921, 'epoch': 3.72}
{'loss': 0.977, 'learning_rate': 0.08130569546687332, 'epoch': 3.74}
{'loss': 0.9502, 'learning_rate': 0.08120883378535453, 'epoch': 3.76}
{'loss': 0.9679, 'learning_rate': 0.08111197210383574, 'epoch': 3.78}
{'loss': 0.9232, 'learning_rate': 0.08101511042231693, 'epoch': 3.8}
{'loss': 0.9333, 'learning_rate': 0.08091824874079814, 'epoch': 3.82}
{'loss': 0.9074, 'learning_rate': 0.08082138705927935, 'epoch': 3.84}
{'loss': 0.8947, 'learning_rate': 0.08072452537776056, 'epoch': 3.86}
{'loss': 0.9645, 'learning_rate': 0.08062766369624177, 'epoch': 3.87}
{'loss': 0.9738, 'learning_rate': 0.08053080201472299, 'epoch': 3.89}
{'loss': 1.0544, 'learning_rate': 0.08043394033320418, 'epoch': 3.91}
{'loss': 0.9302, 'learning_rate': 0.08033707865168539, 'epoch': 3.93}
{'loss': 1.0192, 'learning_rate': 0.0802402169701666, 'epoch': 3.95}
{'loss': 0.9629, 'learning_rate': 0.08014335528864781, 'epoch': 3.97}
{'loss': 0.9066, 'learning_rate': 0.08004649360712902, 'epoch': 3.99}
{'eval_loss': 0.5512017607688904, 'eval_runtime': 4.4483, 'eval_samples_per_second': 143.425, 'eval_steps_per_second': 17.984, 'epoch': 4.0}
{'loss': 0.9356, 'learning_rate': 0.07994963192561023, 'epoch': 4.01}
{'loss': 0.9264, 'learning_rate': 0.07985277024409144, 'epoch': 4.03}
{'loss': 0.9554, 'learning_rate': 0.07975590856257264, 'epoch': 4.05}
{'loss': 0.9188, 'learning_rate': 0.07965904688105385, 'epoch': 4.07}
{'loss': 0.9256, 'learning_rate': 0.07956218519953506, 'epoch': 4.09}
{'loss': 1.0115, 'learning_rate': 0.07946532351801627, 'epoch': 4.11}
{'loss': 1.4131, 'learning_rate': 0.07936846183649748, 'epoch': 4.13}
{'loss': 2.2146, 'learning_rate': 0.0792716001549787, 'epoch': 4.15}
{'loss': 2.2293, 'learning_rate': 0.0791747384734599, 'epoch': 4.17}
{'loss': 1.2397, 'learning_rate': 0.07907787679194112, 'epoch': 4.18}
{'loss': 1.008, 'learning_rate': 0.07898101511042233, 'epoch': 4.2}
{'loss': 0.9937, 'learning_rate': 0.07888415342890354, 'epoch': 4.22}
{'loss': 0.946, 'learning_rate': 0.07878729174738475, 'epoch': 4.24}
{'loss': 0.9402, 'learning_rate': 0.07869043006586596, 'epoch': 4.26}
{'loss': 0.8769, 'learning_rate': 0.07859356838434717, 'epoch': 4.28}
{'loss': 0.8881, 'learning_rate': 0.07849670670282836, 'epoch': 4.3}
{'loss': 0.8779, 'learning_rate': 0.07839984502130957, 'epoch': 4.32}
{'loss': 0.9437, 'learning_rate': 0.07830298333979079, 'epoch': 4.34}
{'loss': 0.9549, 'learning_rate': 0.078206121658272, 'epoch': 4.36}
{'loss': 1.0001, 'learning_rate': 0.0781092599767532, 'epoch': 4.38}
{'loss': 0.9805, 'learning_rate': 0.07801239829523442, 'epoch': 4.4}
{'loss': 0.961, 'learning_rate': 0.07791553661371563, 'epoch': 4.42}
{'loss': 0.9558, 'learning_rate': 0.07781867493219682, 'epoch': 4.44}
{'loss': 1.053, 'learning_rate': 0.07772181325067803, 'epoch': 4.46}
{'loss': 0.9873, 'learning_rate': 0.07762495156915925, 'epoch': 4.48}
{'loss': 0.9783, 'learning_rate': 0.07752808988764046, 'epoch': 4.49}
{'loss': 0.932, 'learning_rate': 0.07743122820612167, 'epoch': 4.51}
{'loss': 0.9231, 'learning_rate': 0.07733436652460288, 'epoch': 4.53}
{'loss': 0.9512, 'learning_rate': 0.07723750484308407, 'epoch': 4.55}
{'loss': 0.9693, 'learning_rate': 0.07714064316156528, 'epoch': 4.57}
{'loss': 0.9395, 'learning_rate': 0.0770437814800465, 'epoch': 4.59}
{'loss': 0.9402, 'learning_rate': 0.0769469197985277, 'epoch': 4.61}
{'loss': 0.975, 'learning_rate': 0.07685005811700892, 'epoch': 4.63}
{'loss': 1.0282, 'learning_rate': 0.07675319643549013, 'epoch': 4.65}
{'loss': 0.9426, 'learning_rate': 0.07665633475397134, 'epoch': 4.67}
{'loss': 0.9867, 'learning_rate': 0.07655947307245253, 'epoch': 4.69}
{'loss': 0.9463, 'learning_rate': 0.07646261139093374, 'epoch': 4.71}
{'loss': 0.997, 'learning_rate': 0.07636574970941495, 'epoch': 4.73}
{'loss': 0.9618, 'learning_rate': 0.07626888802789616, 'epoch': 4.75}
{'loss': 0.9223, 'learning_rate': 0.07617202634637738, 'epoch': 4.77}
{'loss': 0.937, 'learning_rate': 0.07607516466485859, 'epoch': 4.78}
{'loss': 0.9637, 'learning_rate': 0.0759783029833398, 'epoch': 4.8}
{'loss': 0.8984, 'learning_rate': 0.075881441301821, 'epoch': 4.82}
{'loss': 0.9883, 'learning_rate': 0.07578457962030222, 'epoch': 4.84}
{'loss': 1.0198, 'learning_rate': 0.07568771793878343, 'epoch': 4.86}
{'loss': 1.338, 'learning_rate': 0.07559085625726464, 'epoch': 4.88}
{'loss': 2.1253, 'learning_rate': 0.07549399457574585, 'epoch': 4.9}
{'loss': 2.3079, 'learning_rate': 0.07539713289422706, 'epoch': 4.92}
{'loss': 2.0462, 'learning_rate': 0.07530027121270826, 'epoch': 4.94}
{'loss': 1.2993, 'learning_rate': 0.07520340953118947, 'epoch': 4.96}
{'loss': 1.1021, 'learning_rate': 0.07510654784967068, 'epoch': 4.98}
{'loss': 0.9544, 'learning_rate': 0.07500968616815189, 'epoch': 5.0}
{'eval_loss': 0.5617254972457886, 'eval_runtime': 4.4511, 'eval_samples_per_second': 143.334, 'eval_steps_per_second': 17.973, 'epoch': 5.0}
