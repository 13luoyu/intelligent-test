Some weights of the model checkpoint at ../model/mengzi-bert-base-fin were not used when initializing BertForTokenClassification: ['pos_transform.LayerNorm.bias', 'pos_transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'sop.cls.weight', 'cls.predictions.decoder.weight', 'sop.cls.bias', 'pos_transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'pos_head.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'pos_head.bias', 'pos_transform.LayerNorm.weight']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at ../model/mengzi-bert-base-fin and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
{'loss': 0.9295, 'learning_rate': 1e-05, 'epoch': 5.56}
{'loss': 0.3142, 'learning_rate': 1e-05, 'epoch': 11.11}
{'loss': 0.1939, 'learning_rate': 1e-05, 'epoch': 16.67}
{'loss': 0.1353, 'learning_rate': 1e-05, 'epoch': 22.22}
{'loss': 0.0958, 'learning_rate': 1e-05, 'epoch': 27.78}
{'eval_loss': 0.07226108014583588, 'eval_runtime': 1.0657, 'eval_samples_per_second': 135.125, 'eval_steps_per_second': 16.891, 'epoch': 27.78}
{'loss': 0.0773, 'learning_rate': 1e-05, 'epoch': 33.33}
{'loss': 0.0648, 'learning_rate': 1e-05, 'epoch': 38.89}
{'loss': 0.0512, 'learning_rate': 1e-05, 'epoch': 44.44}
{'loss': 0.0412, 'learning_rate': 1e-05, 'epoch': 50.0}
{'loss': 0.0371, 'learning_rate': 1e-05, 'epoch': 55.56}
{'eval_loss': 0.025140851736068726, 'eval_runtime': 1.0901, 'eval_samples_per_second': 132.093, 'eval_steps_per_second': 16.512, 'epoch': 55.56}
{'loss': 0.0315, 'learning_rate': 1e-05, 'epoch': 61.11}
{'loss': 0.0286, 'learning_rate': 1e-05, 'epoch': 66.67}
{'loss': 0.024, 'learning_rate': 1e-05, 'epoch': 72.22}
{'loss': 0.0215, 'learning_rate': 1e-05, 'epoch': 77.78}
{'loss': 0.018, 'learning_rate': 1e-05, 'epoch': 83.33}
{'eval_loss': 0.01213744468986988, 'eval_runtime': 1.0891, 'eval_samples_per_second': 132.219, 'eval_steps_per_second': 16.527, 'epoch': 83.33}
{'loss': 0.0164, 'learning_rate': 1e-05, 'epoch': 88.89}
{'loss': 0.0142, 'learning_rate': 1e-05, 'epoch': 94.44}
{'loss': 0.013, 'learning_rate': 1e-05, 'epoch': 100.0}
{'train_runtime': 336.4588, 'train_samples_per_second': 42.799, 'train_steps_per_second': 5.35, 'train_loss': 0.1170710179540846, 'epoch': 100.0}
Some weights of the model checkpoint at ../model/mengzi-bert-base-fin were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'pos_head.bias', 'pos_transform.dense.bias', 'pos_transform.LayerNorm.weight', 'pos_head.weight', 'cls.predictions.decoder.weight', 'pos_transform.dense.weight', 'sop.cls.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.weight', 'sop.cls.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'pos_transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at ../model/mengzi-bert-base-fin and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
{'loss': 0.8123, 'learning_rate': 9.444444444444445e-06, 'epoch': 5.56}
{'loss': 0.3185, 'learning_rate': 8.888888888888888e-06, 'epoch': 11.11}
{'loss': 0.2239, 'learning_rate': 8.333333333333334e-06, 'epoch': 16.67}
{'loss': 0.1545, 'learning_rate': 7.77777777777778e-06, 'epoch': 22.22}
{'loss': 0.118, 'learning_rate': 7.222222222222223e-06, 'epoch': 27.78}
{'eval_loss': 0.08730516582727432, 'eval_runtime': 1.0886, 'eval_samples_per_second': 132.283, 'eval_steps_per_second': 16.535, 'epoch': 27.78}
{'loss': 0.0967, 'learning_rate': 6.666666666666667e-06, 'epoch': 33.33}
{'loss': 0.0797, 'learning_rate': 6.111111111111112e-06, 'epoch': 38.89}
{'loss': 0.0677, 'learning_rate': 5.555555555555557e-06, 'epoch': 44.44}
{'loss': 0.0603, 'learning_rate': 5e-06, 'epoch': 50.0}
{'loss': 0.0534, 'learning_rate': 4.444444444444444e-06, 'epoch': 55.56}
{'eval_loss': 0.039935093373060226, 'eval_runtime': 1.0957, 'eval_samples_per_second': 131.422, 'eval_steps_per_second': 16.428, 'epoch': 55.56}
{'loss': 0.0497, 'learning_rate': 3.88888888888889e-06, 'epoch': 61.11}
{'loss': 0.0454, 'learning_rate': 3.3333333333333333e-06, 'epoch': 66.67}
{'loss': 0.0419, 'learning_rate': 2.7777777777777783e-06, 'epoch': 72.22}
{'loss': 0.0401, 'learning_rate': 2.222222222222222e-06, 'epoch': 77.78}
{'loss': 0.0381, 'learning_rate': 1.6666666666666667e-06, 'epoch': 83.33}
{'eval_loss': 0.028613507747650146, 'eval_runtime': 1.0862, 'eval_samples_per_second': 132.572, 'eval_steps_per_second': 16.572, 'epoch': 83.33}
{'loss': 0.0375, 'learning_rate': 1.111111111111111e-06, 'epoch': 88.89}
{'loss': 0.0361, 'learning_rate': 5.555555555555555e-07, 'epoch': 94.44}
{'loss': 0.0349, 'learning_rate': 0.0, 'epoch': 100.0}
{'train_runtime': 338.1325, 'train_samples_per_second': 42.587, 'train_steps_per_second': 5.323, 'train_loss': 0.12827023612128363, 'epoch': 100.0}
Some weights of the model checkpoint at ../model/mengzi-bert-base-fin were not used when initializing BertForTokenClassification: ['pos_head.bias', 'pos_transform.LayerNorm.bias', 'pos_transform.dense.bias', 'cls.predictions.transform.dense.weight', 'sop.cls.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'pos_head.weight', 'cls.predictions.bias', 'pos_transform.dense.weight', 'sop.cls.bias', 'cls.predictions.transform.LayerNorm.bias', 'pos_transform.LayerNorm.weight']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at ../model/mengzi-bert-base-fin and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
{'loss': 0.9418, 'learning_rate': 9.924038765061042e-06, 'epoch': 5.56}
{'loss': 0.3034, 'learning_rate': 9.698463103929542e-06, 'epoch': 11.11}
{'loss': 0.1848, 'learning_rate': 9.330127018922195e-06, 'epoch': 16.67}
{'loss': 0.1265, 'learning_rate': 8.83022221559489e-06, 'epoch': 22.22}
{'loss': 0.0967, 'learning_rate': 8.213938048432697e-06, 'epoch': 27.78}
{'eval_loss': 0.07126767933368683, 'eval_runtime': 1.0803, 'eval_samples_per_second': 133.292, 'eval_steps_per_second': 16.662, 'epoch': 27.78}
{'loss': 0.0785, 'learning_rate': 7.500000000000001e-06, 'epoch': 33.33}
{'loss': 0.0652, 'learning_rate': 6.710100716628345e-06, 'epoch': 38.89}
{'loss': 0.0551, 'learning_rate': 5.8682408883346535e-06, 'epoch': 44.44}
{'loss': 0.0492, 'learning_rate': 5e-06, 'epoch': 50.0}
{'loss': 0.0431, 'learning_rate': 4.131759111665349e-06, 'epoch': 55.56}
{'eval_loss': 0.03209081292152405, 'eval_runtime': 1.082, 'eval_samples_per_second': 133.084, 'eval_steps_per_second': 16.636, 'epoch': 55.56}
{'loss': 0.0412, 'learning_rate': 3.289899283371657e-06, 'epoch': 61.11}
{'loss': 0.0371, 'learning_rate': 2.5000000000000015e-06, 'epoch': 66.67}
{'loss': 0.034, 'learning_rate': 1.7860619515673034e-06, 'epoch': 72.22}
{'loss': 0.0348, 'learning_rate': 1.1697777844051105e-06, 'epoch': 77.78}
{'loss': 0.0322, 'learning_rate': 6.698729810778065e-07, 'epoch': 83.33}
{'eval_loss': 0.02439028024673462, 'eval_runtime': 1.0865, 'eval_samples_per_second': 132.536, 'eval_steps_per_second': 16.567, 'epoch': 83.33}
{'loss': 0.0331, 'learning_rate': 3.015368960704584e-07, 'epoch': 88.89}
{'loss': 0.0327, 'learning_rate': 7.59612349389599e-08, 'epoch': 94.44}
{'loss': 0.0313, 'learning_rate': 0.0, 'epoch': 100.0}
{'train_runtime': 336.2356, 'train_samples_per_second': 42.827, 'train_steps_per_second': 5.353, 'train_loss': 0.12338236901495192, 'epoch': 100.0}
Some weights of the model checkpoint at ../model/mengzi-bert-base-fin were not used when initializing BertForTokenClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'pos_transform.dense.weight', 'sop.cls.weight', 'pos_transform.LayerNorm.bias', 'sop.cls.bias', 'cls.predictions.decoder.weight', 'pos_head.bias', 'pos_transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'pos_transform.dense.bias', 'cls.predictions.bias', 'pos_head.weight']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at ../model/mengzi-bert-base-fin and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
{'loss': 0.9402, 'learning_rate': 9.924038765061042e-06, 'epoch': 5.56}
{'loss': 0.3256, 'learning_rate': 9.698463103929542e-06, 'epoch': 11.11}
{'loss': 0.2198, 'learning_rate': 9.330127018922195e-06, 'epoch': 16.67}
{'loss': 0.1501, 'learning_rate': 8.83022221559489e-06, 'epoch': 22.22}
{'loss': 0.115, 'learning_rate': 8.213938048432697e-06, 'epoch': 27.78}
{'eval_loss': 0.08139640837907791, 'eval_runtime': 1.0829, 'eval_samples_per_second': 132.982, 'eval_steps_per_second': 16.623, 'epoch': 27.78}
{'loss': 0.0885, 'learning_rate': 7.500000000000001e-06, 'epoch': 33.33}
{'loss': 0.0741, 'learning_rate': 6.710100716628345e-06, 'epoch': 38.89}
{'loss': 0.0642, 'learning_rate': 5.8682408883346535e-06, 'epoch': 44.44}
{'loss': 0.0562, 'learning_rate': 5e-06, 'epoch': 50.0}
{'loss': 0.0508, 'learning_rate': 4.131759111665349e-06, 'epoch': 55.56}
{'eval_loss': 0.03692339360713959, 'eval_runtime': 1.0832, 'eval_samples_per_second': 132.934, 'eval_steps_per_second': 16.617, 'epoch': 55.56}
{'loss': 0.046, 'learning_rate': 3.289899283371657e-06, 'epoch': 61.11}
{'loss': 0.0442, 'learning_rate': 2.5000000000000015e-06, 'epoch': 66.67}
{'loss': 0.0405, 'learning_rate': 1.7860619515673034e-06, 'epoch': 72.22}
{'loss': 0.0395, 'learning_rate': 1.1697777844051105e-06, 'epoch': 77.78}
{'loss': 0.0388, 'learning_rate': 6.698729810778065e-07, 'epoch': 83.33}
{'eval_loss': 0.028721651062369347, 'eval_runtime': 1.0834, 'eval_samples_per_second': 132.918, 'eval_steps_per_second': 16.615, 'epoch': 83.33}
{'loss': 0.0371, 'learning_rate': 3.015368960704584e-07, 'epoch': 88.89}
{'loss': 0.0384, 'learning_rate': 7.59612349389599e-08, 'epoch': 94.44}
{'loss': 0.0369, 'learning_rate': 0.0, 'epoch': 100.0}
{'train_runtime': 335.4639, 'train_samples_per_second': 42.926, 'train_steps_per_second': 5.366, 'train_loss': 0.13366218513912625, 'epoch': 100.0}
Some weights of the model checkpoint at ../model/mengzi-bert-base-fin were not used when initializing BertForTokenClassification: ['cls.predictions.transform.LayerNorm.bias', 'pos_transform.dense.bias', 'pos_transform.LayerNorm.weight', 'pos_transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'pos_head.weight', 'sop.cls.weight', 'pos_transform.dense.weight', 'sop.cls.bias', 'cls.predictions.transform.dense.bias', 'pos_head.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at ../model/mengzi-bert-base-fin and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
{'loss': 0.8628, 'learning_rate': 9.450000000000001e-06, 'epoch': 5.56}
{'loss': 0.317, 'learning_rate': 8.9e-06, 'epoch': 11.11}
{'loss': 0.2109, 'learning_rate': 8.350000000000001e-06, 'epoch': 16.67}
{'loss': 0.1477, 'learning_rate': 7.800000000000002e-06, 'epoch': 22.22}
{'loss': 0.1119, 'learning_rate': 7.25e-06, 'epoch': 27.78}
{'eval_loss': 0.08598732948303223, 'eval_runtime': 1.0902, 'eval_samples_per_second': 132.087, 'eval_steps_per_second': 16.511, 'epoch': 27.78}
{'loss': 0.0922, 'learning_rate': 6.700000000000001e-06, 'epoch': 33.33}
{'loss': 0.0767, 'learning_rate': 6.15e-06, 'epoch': 38.89}
{'loss': 0.0654, 'learning_rate': 5.600000000000001e-06, 'epoch': 44.44}
{'loss': 0.0583, 'learning_rate': 5.050000000000001e-06, 'epoch': 50.0}
{'loss': 0.0516, 'learning_rate': 4.499999999999999e-06, 'epoch': 55.56}
{'eval_loss': 0.03752487897872925, 'eval_runtime': 1.0812, 'eval_samples_per_second': 133.191, 'eval_steps_per_second': 16.649, 'epoch': 55.56}
{'loss': 0.0463, 'learning_rate': 3.9499999999999995e-06, 'epoch': 61.11}
{'loss': 0.043, 'learning_rate': 3.4000000000000005e-06, 'epoch': 66.67}
{'loss': 0.0391, 'learning_rate': 2.85e-06, 'epoch': 72.22}
{'loss': 0.0377, 'learning_rate': 2.2999999999999996e-06, 'epoch': 77.78}
{'loss': 0.0357, 'learning_rate': 1.7499999999999998e-06, 'epoch': 83.33}
{'eval_loss': 0.025796284899115562, 'eval_runtime': 1.085, 'eval_samples_per_second': 132.723, 'eval_steps_per_second': 16.59, 'epoch': 83.33}
{'loss': 0.0337, 'learning_rate': 1.2000000000000004e-06, 'epoch': 88.89}
{'loss': 0.0329, 'learning_rate': 6.500000000000002e-07, 'epoch': 94.44}
{'loss': 0.0323, 'learning_rate': 1e-07, 'epoch': 100.0}
{'train_runtime': 336.1521, 'train_samples_per_second': 42.838, 'train_steps_per_second': 5.355, 'train_loss': 0.12750663187768724, 'epoch': 100.0}
Some weights of the model checkpoint at ../model/mengzi-bert-base-fin were not used when initializing BertForTokenClassification: ['pos_transform.dense.bias', 'pos_head.bias', 'pos_head.weight', 'sop.cls.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'sop.cls.bias', 'cls.predictions.decoder.bias', 'pos_transform.LayerNorm.weight', 'cls.predictions.bias', 'pos_transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'pos_transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at ../model/mengzi-bert-base-fin and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
{'loss': 0.8847, 'learning_rate': 1e-05, 'epoch': 5.56}
{'loss': 0.3119, 'learning_rate': 1e-05, 'epoch': 11.11}
{'loss': 0.1988, 'learning_rate': 1e-05, 'epoch': 16.67}
{'loss': 0.1331, 'learning_rate': 1e-05, 'epoch': 22.22}
{'loss': 0.0983, 'learning_rate': 1e-05, 'epoch': 27.78}
{'eval_loss': 0.07108572125434875, 'eval_runtime': 1.0913, 'eval_samples_per_second': 131.951, 'eval_steps_per_second': 16.494, 'epoch': 27.78}
{'loss': 0.0776, 'learning_rate': 1e-05, 'epoch': 33.33}
{'loss': 0.064, 'learning_rate': 1e-05, 'epoch': 38.89}
{'loss': 0.0512, 'learning_rate': 1e-05, 'epoch': 44.44}
{'loss': 0.0427, 'learning_rate': 1e-05, 'epoch': 50.0}
{'loss': 0.0369, 'learning_rate': 1e-05, 'epoch': 55.56}
{'eval_loss': 0.026623232290148735, 'eval_runtime': 1.0857, 'eval_samples_per_second': 132.637, 'eval_steps_per_second': 16.58, 'epoch': 55.56}
{'loss': 0.0318, 'learning_rate': 1e-05, 'epoch': 61.11}
{'loss': 0.0273, 'learning_rate': 1e-05, 'epoch': 66.67}
{'loss': 0.0238, 'learning_rate': 1e-05, 'epoch': 72.22}
{'loss': 0.0209, 'learning_rate': 1e-05, 'epoch': 77.78}
{'loss': 0.018, 'learning_rate': 1e-05, 'epoch': 83.33}
{'eval_loss': 0.011746014468371868, 'eval_runtime': 1.0777, 'eval_samples_per_second': 133.621, 'eval_steps_per_second': 16.703, 'epoch': 83.33}
{'loss': 0.0163, 'learning_rate': 1e-05, 'epoch': 88.89}
{'loss': 0.0141, 'learning_rate': 1e-05, 'epoch': 94.44}
{'loss': 0.0122, 'learning_rate': 1e-05, 'epoch': 100.0}
{'train_runtime': 335.7847, 'train_samples_per_second': 42.885, 'train_steps_per_second': 5.361, 'train_loss': 0.1146419870853424, 'epoch': 100.0}
Some weights of the model checkpoint at ../model/bert_FinBERT were not used when initializing BertForTokenClassification: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at ../model/bert_FinBERT and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
{'loss': 0.6637, 'learning_rate': 1e-05, 'epoch': 5.56}
{'loss': 0.1899, 'learning_rate': 1e-05, 'epoch': 11.11}
{'loss': 0.101, 'learning_rate': 1e-05, 'epoch': 16.67}
{'loss': 0.0639, 'learning_rate': 1e-05, 'epoch': 22.22}
{'loss': 0.0438, 'learning_rate': 1e-05, 'epoch': 27.78}
{'eval_loss': 0.027775049209594727, 'eval_runtime': 1.0868, 'eval_samples_per_second': 132.499, 'eval_steps_per_second': 16.562, 'epoch': 27.78}
{'loss': 0.0327, 'learning_rate': 1e-05, 'epoch': 33.33}
{'loss': 0.0228, 'learning_rate': 1e-05, 'epoch': 38.89}
{'loss': 0.0192, 'learning_rate': 1e-05, 'epoch': 44.44}
{'loss': 0.0144, 'learning_rate': 1e-05, 'epoch': 50.0}
{'loss': 0.0107, 'learning_rate': 1e-05, 'epoch': 55.56}
{'eval_loss': 0.005460226908326149, 'eval_runtime': 1.0852, 'eval_samples_per_second': 132.698, 'eval_steps_per_second': 16.587, 'epoch': 55.56}
{'loss': 0.0089, 'learning_rate': 1e-05, 'epoch': 61.11}
{'loss': 0.0074, 'learning_rate': 1e-05, 'epoch': 66.67}
{'loss': 0.0059, 'learning_rate': 1e-05, 'epoch': 72.22}
{'loss': 0.0049, 'learning_rate': 1e-05, 'epoch': 77.78}
{'loss': 0.004, 'learning_rate': 1e-05, 'epoch': 83.33}
{'eval_loss': 0.0014736338052898645, 'eval_runtime': 1.0793, 'eval_samples_per_second': 133.419, 'eval_steps_per_second': 16.677, 'epoch': 83.33}
{'loss': 0.0034, 'learning_rate': 1e-05, 'epoch': 88.89}
{'loss': 0.0025, 'learning_rate': 1e-05, 'epoch': 94.44}
{'loss': 0.002, 'learning_rate': 1e-05, 'epoch': 100.0}
{'train_runtime': 334.7957, 'train_samples_per_second': 43.011, 'train_steps_per_second': 5.376, 'train_loss': 0.06672885258164671, 'epoch': 100.0}
Some weights of the model checkpoint at ../model/bert_FinBERT were not used when initializing BertForTokenClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at ../model/bert_FinBERT and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
{'loss': 0.6066, 'learning_rate': 9.444444444444445e-06, 'epoch': 5.56}
{'loss': 0.184, 'learning_rate': 8.888888888888888e-06, 'epoch': 11.11}
{'loss': 0.0994, 'learning_rate': 8.333333333333334e-06, 'epoch': 16.67}
{'loss': 0.0678, 'learning_rate': 7.77777777777778e-06, 'epoch': 22.22}
{'loss': 0.0479, 'learning_rate': 7.222222222222223e-06, 'epoch': 27.78}
{'eval_loss': 0.031496331095695496, 'eval_runtime': 1.086, 'eval_samples_per_second': 132.593, 'eval_steps_per_second': 16.574, 'epoch': 27.78}
{'loss': 0.0356, 'learning_rate': 6.666666666666667e-06, 'epoch': 33.33}
{'loss': 0.0297, 'learning_rate': 6.111111111111112e-06, 'epoch': 38.89}
{'loss': 0.0247, 'learning_rate': 5.555555555555557e-06, 'epoch': 44.44}
{'loss': 0.0199, 'learning_rate': 5e-06, 'epoch': 50.0}
{'loss': 0.0173, 'learning_rate': 4.444444444444444e-06, 'epoch': 55.56}
{'eval_loss': 0.010530084371566772, 'eval_runtime': 1.095, 'eval_samples_per_second': 131.506, 'eval_steps_per_second': 16.438, 'epoch': 55.56}
{'loss': 0.0155, 'learning_rate': 3.88888888888889e-06, 'epoch': 61.11}
{'loss': 0.0135, 'learning_rate': 3.3333333333333333e-06, 'epoch': 66.67}
{'loss': 0.0123, 'learning_rate': 2.7777777777777783e-06, 'epoch': 72.22}
{'loss': 0.0107, 'learning_rate': 2.222222222222222e-06, 'epoch': 77.78}
{'loss': 0.0101, 'learning_rate': 1.6666666666666667e-06, 'epoch': 83.33}
{'eval_loss': 0.005294503644108772, 'eval_runtime': 1.0868, 'eval_samples_per_second': 132.497, 'eval_steps_per_second': 16.562, 'epoch': 83.33}
{'loss': 0.0096, 'learning_rate': 1.111111111111111e-06, 'epoch': 88.89}
{'loss': 0.0088, 'learning_rate': 5.555555555555555e-07, 'epoch': 94.44}
{'loss': 0.0091, 'learning_rate': 0.0, 'epoch': 100.0}
{'train_runtime': 335.0878, 'train_samples_per_second': 42.974, 'train_steps_per_second': 5.372, 'train_loss': 0.06791956557167901, 'epoch': 100.0}
Some weights of the model checkpoint at ../model/bert_FinBERT were not used when initializing BertForTokenClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at ../model/bert_FinBERT and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
{'loss': 0.6639, 'learning_rate': 9.924038765061042e-06, 'epoch': 5.56}
{'loss': 0.1838, 'learning_rate': 9.698463103929542e-06, 'epoch': 11.11}
{'loss': 0.0966, 'learning_rate': 9.330127018922195e-06, 'epoch': 16.67}
{'loss': 0.0615, 'learning_rate': 8.83022221559489e-06, 'epoch': 22.22}
{'loss': 0.0454, 'learning_rate': 8.213938048432697e-06, 'epoch': 27.78}
{'eval_loss': 0.028452787548303604, 'eval_runtime': 1.0921, 'eval_samples_per_second': 131.853, 'eval_steps_per_second': 16.482, 'epoch': 27.78}
{'loss': 0.0335, 'learning_rate': 7.500000000000001e-06, 'epoch': 33.33}
{'loss': 0.026, 'learning_rate': 6.710100716628345e-06, 'epoch': 38.89}
{'loss': 0.0209, 'learning_rate': 5.8682408883346535e-06, 'epoch': 44.44}
{'loss': 0.0183, 'learning_rate': 5e-06, 'epoch': 50.0}
{'loss': 0.0151, 'learning_rate': 4.131759111665349e-06, 'epoch': 55.56}
{'eval_loss': 0.008125526830554008, 'eval_runtime': 1.0839, 'eval_samples_per_second': 132.857, 'eval_steps_per_second': 16.607, 'epoch': 55.56}
{'loss': 0.0131, 'learning_rate': 3.289899283371657e-06, 'epoch': 61.11}
{'loss': 0.011, 'learning_rate': 2.5000000000000015e-06, 'epoch': 66.67}
{'loss': 0.0103, 'learning_rate': 1.7860619515673034e-06, 'epoch': 72.22}
{'loss': 0.0098, 'learning_rate': 1.1697777844051105e-06, 'epoch': 77.78}
{'loss': 0.0092, 'learning_rate': 6.698729810778065e-07, 'epoch': 83.33}
{'eval_loss': 0.004726396407932043, 'eval_runtime': 1.0889, 'eval_samples_per_second': 132.246, 'eval_steps_per_second': 16.531, 'epoch': 83.33}
{'loss': 0.0085, 'learning_rate': 3.015368960704584e-07, 'epoch': 88.89}
{'loss': 0.0089, 'learning_rate': 7.59612349389599e-08, 'epoch': 94.44}
{'loss': 0.0086, 'learning_rate': 0.0, 'epoch': 100.0}
{'train_runtime': 335.4217, 'train_samples_per_second': 42.931, 'train_steps_per_second': 5.366, 'train_loss': 0.06912098871337043, 'epoch': 100.0}
Some weights of the model checkpoint at ../model/bert_FinBERT were not used when initializing BertForTokenClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at ../model/bert_FinBERT and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
{'loss': 0.6742, 'learning_rate': 9.924038765061042e-06, 'epoch': 5.56}
{'loss': 0.1928, 'learning_rate': 9.698463103929542e-06, 'epoch': 11.11}
{'loss': 0.1022, 'learning_rate': 9.330127018922195e-06, 'epoch': 16.67}
{'loss': 0.0649, 'learning_rate': 8.83022221559489e-06, 'epoch': 22.22}
{'loss': 0.0477, 'learning_rate': 8.213938048432697e-06, 'epoch': 27.78}
{'eval_loss': 0.03065919503569603, 'eval_runtime': 1.0994, 'eval_samples_per_second': 130.983, 'eval_steps_per_second': 16.373, 'epoch': 27.78}
{'loss': 0.0363, 'learning_rate': 7.500000000000001e-06, 'epoch': 33.33}
{'loss': 0.0273, 'learning_rate': 6.710100716628345e-06, 'epoch': 38.89}
{'loss': 0.0225, 'learning_rate': 5.8682408883346535e-06, 'epoch': 44.44}
{'loss': 0.019, 'learning_rate': 5e-06, 'epoch': 50.0}
{'loss': 0.0161, 'learning_rate': 4.131759111665349e-06, 'epoch': 55.56}
{'eval_loss': 0.00937497429549694, 'eval_runtime': 1.0866, 'eval_samples_per_second': 132.52, 'eval_steps_per_second': 16.565, 'epoch': 55.56}
{'loss': 0.0137, 'learning_rate': 3.289899283371657e-06, 'epoch': 61.11}
{'loss': 0.0126, 'learning_rate': 2.5000000000000015e-06, 'epoch': 66.67}
{'loss': 0.0114, 'learning_rate': 1.7860619515673034e-06, 'epoch': 72.22}
{'loss': 0.0104, 'learning_rate': 1.1697777844051105e-06, 'epoch': 77.78}
{'loss': 0.0101, 'learning_rate': 6.698729810778065e-07, 'epoch': 83.33}
{'eval_loss': 0.0054094670340418816, 'eval_runtime': 1.088, 'eval_samples_per_second': 132.347, 'eval_steps_per_second': 16.543, 'epoch': 83.33}
{'loss': 0.0099, 'learning_rate': 3.015368960704584e-07, 'epoch': 88.89}
{'loss': 0.0096, 'learning_rate': 7.59612349389599e-08, 'epoch': 94.44}
{'loss': 0.0096, 'learning_rate': 0.0, 'epoch': 100.0}
{'train_runtime': 335.4712, 'train_samples_per_second': 42.925, 'train_steps_per_second': 5.366, 'train_loss': 0.07168029460642072, 'epoch': 100.0}
Some weights of the model checkpoint at ../model/bert_FinBERT were not used when initializing BertForTokenClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at ../model/bert_FinBERT and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
{'loss': 0.5643, 'learning_rate': 9.450000000000001e-06, 'epoch': 5.56}
{'loss': 0.1802, 'learning_rate': 8.9e-06, 'epoch': 11.11}
{'loss': 0.1023, 'learning_rate': 8.350000000000001e-06, 'epoch': 16.67}
{'loss': 0.0673, 'learning_rate': 7.800000000000002e-06, 'epoch': 22.22}
{'loss': 0.0496, 'learning_rate': 7.25e-06, 'epoch': 27.78}
{'eval_loss': 0.031959712505340576, 'eval_runtime': 1.0913, 'eval_samples_per_second': 131.953, 'eval_steps_per_second': 16.494, 'epoch': 27.78}
{'loss': 0.0379, 'learning_rate': 6.700000000000001e-06, 'epoch': 33.33}
{'loss': 0.031, 'learning_rate': 6.15e-06, 'epoch': 38.89}
{'loss': 0.0254, 'learning_rate': 5.600000000000001e-06, 'epoch': 44.44}
{'loss': 0.0206, 'learning_rate': 5.050000000000001e-06, 'epoch': 50.0}
{'loss': 0.0182, 'learning_rate': 4.499999999999999e-06, 'epoch': 55.56}
{'eval_loss': 0.010196947492659092, 'eval_runtime': 1.0848, 'eval_samples_per_second': 132.742, 'eval_steps_per_second': 16.593, 'epoch': 55.56}
{'loss': 0.0153, 'learning_rate': 3.9499999999999995e-06, 'epoch': 61.11}
{'loss': 0.0142, 'learning_rate': 3.4000000000000005e-06, 'epoch': 66.67}
{'loss': 0.0117, 'learning_rate': 2.85e-06, 'epoch': 72.22}
{'loss': 0.0114, 'learning_rate': 2.2999999999999996e-06, 'epoch': 77.78}
{'loss': 0.0102, 'learning_rate': 1.7499999999999998e-06, 'epoch': 83.33}
{'eval_loss': 0.0050387573428452015, 'eval_runtime': 1.0949, 'eval_samples_per_second': 131.519, 'eval_steps_per_second': 16.44, 'epoch': 83.33}
{'loss': 0.0094, 'learning_rate': 1.2000000000000004e-06, 'epoch': 88.89}
{'loss': 0.0092, 'learning_rate': 6.500000000000002e-07, 'epoch': 94.44}
{'loss': 0.0085, 'learning_rate': 1e-07, 'epoch': 100.0}
{'train_runtime': 335.6529, 'train_samples_per_second': 42.901, 'train_steps_per_second': 5.363, 'train_loss': 0.0659250326289071, 'epoch': 100.0}
Some weights of the model checkpoint at ../model/bert_FinBERT were not used when initializing BertForTokenClassification: ['cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at ../model/bert_FinBERT and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
{'loss': 0.6997, 'learning_rate': 1e-05, 'epoch': 5.56}
{'loss': 0.1886, 'learning_rate': 1e-05, 'epoch': 11.11}
{'loss': 0.1001, 'learning_rate': 1e-05, 'epoch': 16.67}
{'loss': 0.0617, 'learning_rate': 1e-05, 'epoch': 22.22}
{'loss': 0.0444, 'learning_rate': 1e-05, 'epoch': 27.78}
{'eval_loss': 0.02826496586203575, 'eval_runtime': 1.0893, 'eval_samples_per_second': 132.199, 'eval_steps_per_second': 16.525, 'epoch': 27.78}
{'loss': 0.0335, 'learning_rate': 1e-05, 'epoch': 33.33}
{'loss': 0.0247, 'learning_rate': 1e-05, 'epoch': 38.89}
{'loss': 0.0184, 'learning_rate': 1e-05, 'epoch': 44.44}
{'loss': 0.0153, 'learning_rate': 1e-05, 'epoch': 50.0}
{'loss': 0.0121, 'learning_rate': 1e-05, 'epoch': 55.56}
{'eval_loss': 0.006168138235807419, 'eval_runtime': 1.0857, 'eval_samples_per_second': 132.633, 'eval_steps_per_second': 16.579, 'epoch': 55.56}
{'loss': 0.0094, 'learning_rate': 1e-05, 'epoch': 61.11}
{'loss': 0.0074, 'learning_rate': 1e-05, 'epoch': 66.67}
{'loss': 0.0062, 'learning_rate': 1e-05, 'epoch': 72.22}
{'loss': 0.005, 'learning_rate': 1e-05, 'epoch': 77.78}
{'loss': 0.0038, 'learning_rate': 1e-05, 'epoch': 83.33}
{'eval_loss': 0.0014962088316679, 'eval_runtime': 1.0964, 'eval_samples_per_second': 131.345, 'eval_steps_per_second': 16.418, 'epoch': 83.33}
{'loss': 0.003, 'learning_rate': 1e-05, 'epoch': 88.89}
{'loss': 0.0027, 'learning_rate': 1e-05, 'epoch': 94.44}
{'loss': 0.0024, 'learning_rate': 1e-05, 'epoch': 100.0}
{'train_runtime': 335.4474, 'train_samples_per_second': 42.928, 'train_steps_per_second': 5.366, 'train_loss': 0.06880563444561429, 'epoch': 100.0}
