Some weights of the model checkpoint at ../model/mengzi-bert-base-fin were not used when initializing BertForTokenClassification: ['cls.predictions.decoder.bias', 'pos_transform.dense.weight', 'pos_transform.LayerNorm.weight', 'pos_transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'sop.cls.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'pos_head.weight', 'cls.predictions.transform.dense.weight', 'pos_head.bias', 'sop.cls.bias', 'pos_transform.dense.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at ../model/mengzi-bert-base-fin and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
{'loss': 1.1357, 'learning_rate': 1e-05, 'epoch': 20.0}
{'loss': 0.242, 'learning_rate': 1e-05, 'epoch': 40.0}
{'loss': 0.1495, 'learning_rate': 1e-05, 'epoch': 60.0}
{'loss': 0.1034, 'learning_rate': 1e-05, 'epoch': 80.0}
{'loss': 0.0746, 'learning_rate': 1e-05, 'epoch': 100.0}
{'eval_loss': 0.05131637677550316, 'eval_runtime': 0.3376, 'eval_samples_per_second': 115.512, 'eval_steps_per_second': 14.809, 'epoch': 100.0}
{'train_runtime': 97.3849, 'train_samples_per_second': 40.047, 'train_steps_per_second': 5.134, 'train_loss': 0.3410424852371216, 'epoch': 100.0}
Some weights of the model checkpoint at ../model/mengzi-bert-base-fin were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.decoder.bias', 'pos_transform.LayerNorm.weight', 'pos_head.weight', 'cls.predictions.decoder.weight', 'pos_transform.dense.weight', 'sop.cls.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'pos_transform.LayerNorm.bias', 'pos_head.bias', 'pos_transform.dense.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'sop.cls.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at ../model/mengzi-bert-base-fin and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
{'loss': 1.0544, 'learning_rate': 8.000000000000001e-06, 'epoch': 20.0}
{'loss': 0.2648, 'learning_rate': 6e-06, 'epoch': 40.0}
{'loss': 0.1751, 'learning_rate': 4.000000000000001e-06, 'epoch': 60.0}
{'loss': 0.1417, 'learning_rate': 2.0000000000000003e-06, 'epoch': 80.0}
{'loss': 0.1285, 'learning_rate': 0.0, 'epoch': 100.0}
{'eval_loss': 0.105768583714962, 'eval_runtime': 0.3349, 'eval_samples_per_second': 116.458, 'eval_steps_per_second': 14.93, 'epoch': 100.0}
{'train_runtime': 96.9495, 'train_samples_per_second': 40.227, 'train_steps_per_second': 5.157, 'train_loss': 0.3529086742401123, 'epoch': 100.0}
Some weights of the model checkpoint at ../model/mengzi-bert-base-fin were not used when initializing BertForTokenClassification: ['pos_transform.LayerNorm.bias', 'pos_transform.LayerNorm.weight', 'pos_transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'sop.cls.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'pos_head.bias', 'cls.predictions.decoder.weight', 'sop.cls.bias', 'cls.predictions.transform.dense.bias', 'pos_transform.dense.weight', 'pos_head.weight']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at ../model/mengzi-bert-base-fin and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
{'loss': 1.1155, 'learning_rate': 9.045084971874738e-06, 'epoch': 20.0}
{'loss': 0.2571, 'learning_rate': 6.545084971874738e-06, 'epoch': 40.0}
{'loss': 0.1695, 'learning_rate': 3.4549150281252635e-06, 'epoch': 60.0}
{'loss': 0.1423, 'learning_rate': 9.549150281252633e-07, 'epoch': 80.0}
{'loss': 0.1351, 'learning_rate': 0.0, 'epoch': 100.0}
{'eval_loss': 0.11578685790300369, 'eval_runtime': 0.3348, 'eval_samples_per_second': 116.483, 'eval_steps_per_second': 14.934, 'epoch': 100.0}
{'train_runtime': 97.7044, 'train_samples_per_second': 39.916, 'train_steps_per_second': 5.117, 'train_loss': 0.3639148292541504, 'epoch': 100.0}
Some weights of the model checkpoint at ../model/mengzi-bert-base-fin were not used when initializing BertForTokenClassification: ['cls.predictions.transform.LayerNorm.bias', 'pos_head.weight', 'pos_transform.dense.bias', 'pos_head.bias', 'pos_transform.dense.weight', 'cls.predictions.transform.dense.bias', 'pos_transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'pos_transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'sop.cls.weight', 'sop.cls.bias', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at ../model/mengzi-bert-base-fin and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
{'loss': 0.9999, 'learning_rate': 9.045084971874738e-06, 'epoch': 20.0}
{'loss': 0.2333, 'learning_rate': 6.545084971874738e-06, 'epoch': 40.0}
{'loss': 0.1551, 'learning_rate': 3.4549150281252635e-06, 'epoch': 60.0}
{'loss': 0.1307, 'learning_rate': 9.549150281252633e-07, 'epoch': 80.0}
{'loss': 0.1247, 'learning_rate': 0.0, 'epoch': 100.0}
{'eval_loss': 0.10638498514890671, 'eval_runtime': 0.3356, 'eval_samples_per_second': 116.213, 'eval_steps_per_second': 14.899, 'epoch': 100.0}
{'train_runtime': 97.2042, 'train_samples_per_second': 40.122, 'train_steps_per_second': 5.144, 'train_loss': 0.32874110221862796, 'epoch': 100.0}
Some weights of the model checkpoint at ../model/mengzi-bert-base-fin were not used when initializing BertForTokenClassification: ['cls.predictions.transform.dense.weight', 'pos_transform.dense.bias', 'pos_transform.dense.weight', 'pos_head.weight', 'cls.predictions.decoder.bias', 'sop.cls.weight', 'cls.predictions.transform.LayerNorm.weight', 'sop.cls.bias', 'cls.predictions.transform.LayerNorm.bias', 'pos_transform.LayerNorm.bias', 'pos_head.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'pos_transform.LayerNorm.weight']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at ../model/mengzi-bert-base-fin and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
{'loss': 1.07, 'learning_rate': 8.020000000000001e-06, 'epoch': 20.0}
{'loss': 0.2822, 'learning_rate': 6.040000000000001e-06, 'epoch': 40.0}
{'loss': 0.206, 'learning_rate': 4.06e-06, 'epoch': 60.0}
{'loss': 0.1677, 'learning_rate': 2.0799999999999996e-06, 'epoch': 80.0}
{'loss': 0.1524, 'learning_rate': 1e-07, 'epoch': 100.0}
{'eval_loss': 0.1299624741077423, 'eval_runtime': 0.3353, 'eval_samples_per_second': 116.3, 'eval_steps_per_second': 14.91, 'epoch': 100.0}
{'train_runtime': 97.1582, 'train_samples_per_second': 40.141, 'train_steps_per_second': 5.146, 'train_loss': 0.37565241813659667, 'epoch': 100.0}
Some weights of the model checkpoint at ../model/mengzi-bert-base-fin were not used when initializing BertForTokenClassification: ['pos_head.bias', 'pos_head.weight', 'pos_transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'sop.cls.weight', 'cls.predictions.transform.LayerNorm.bias', 'pos_transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'sop.cls.bias', 'pos_transform.dense.bias', 'pos_transform.LayerNorm.weight', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at ../model/mengzi-bert-base-fin and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
{'loss': 1.0015, 'learning_rate': 1e-05, 'epoch': 20.0}
{'loss': 0.2336, 'learning_rate': 1e-05, 'epoch': 40.0}
{'loss': 0.1375, 'learning_rate': 1e-05, 'epoch': 60.0}
{'loss': 0.0958, 'learning_rate': 1e-05, 'epoch': 80.0}
{'loss': 0.0692, 'learning_rate': 1e-05, 'epoch': 100.0}
{'eval_loss': 0.046387672424316406, 'eval_runtime': 0.3349, 'eval_samples_per_second': 116.458, 'eval_steps_per_second': 14.93, 'epoch': 100.0}
{'train_runtime': 97.0457, 'train_samples_per_second': 40.187, 'train_steps_per_second': 5.152, 'train_loss': 0.3075003185272217, 'epoch': 100.0}
Some weights of the model checkpoint at ../model/bert_FinBERT were not used when initializing BertForTokenClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at ../model/bert_FinBERT and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
{'loss': 0.6032, 'learning_rate': 1e-05, 'epoch': 20.0}
{'loss': 0.0843, 'learning_rate': 1e-05, 'epoch': 40.0}
{'loss': 0.0342, 'learning_rate': 1e-05, 'epoch': 60.0}
{'loss': 0.0172, 'learning_rate': 1e-05, 'epoch': 80.0}
{'loss': 0.0094, 'learning_rate': 1e-05, 'epoch': 100.0}
{'eval_loss': 0.003183398861438036, 'eval_runtime': 0.3334, 'eval_samples_per_second': 116.985, 'eval_steps_per_second': 14.998, 'epoch': 100.0}
{'train_runtime': 96.7691, 'train_samples_per_second': 40.302, 'train_steps_per_second': 5.167, 'train_loss': 0.1496531480550766, 'epoch': 100.0}
Some weights of the model checkpoint at ../model/bert_FinBERT were not used when initializing BertForTokenClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at ../model/bert_FinBERT and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
{'loss': 0.6193, 'learning_rate': 8.000000000000001e-06, 'epoch': 20.0}
{'loss': 0.0957, 'learning_rate': 6e-06, 'epoch': 40.0}
{'loss': 0.0518, 'learning_rate': 4.000000000000001e-06, 'epoch': 60.0}
{'loss': 0.0353, 'learning_rate': 2.0000000000000003e-06, 'epoch': 80.0}
{'loss': 0.0293, 'learning_rate': 0.0, 'epoch': 100.0}
{'eval_loss': 0.018439073115587234, 'eval_runtime': 0.3508, 'eval_samples_per_second': 111.175, 'eval_steps_per_second': 14.253, 'epoch': 100.0}
{'train_runtime': 98.6454, 'train_samples_per_second': 39.536, 'train_steps_per_second': 5.069, 'train_loss': 0.16626850032806398, 'epoch': 100.0}
Some weights of the model checkpoint at ../model/bert_FinBERT were not used when initializing BertForTokenClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at ../model/bert_FinBERT and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
{'loss': 0.6084, 'learning_rate': 9.045084971874738e-06, 'epoch': 20.0}
{'loss': 0.0913, 'learning_rate': 6.545084971874738e-06, 'epoch': 40.0}
{'loss': 0.0466, 'learning_rate': 3.4549150281252635e-06, 'epoch': 60.0}
{'loss': 0.0339, 'learning_rate': 9.549150281252633e-07, 'epoch': 80.0}
{'loss': 0.0306, 'learning_rate': 0.0, 'epoch': 100.0}
{'eval_loss': 0.020586468279361725, 'eval_runtime': 0.3482, 'eval_samples_per_second': 112.011, 'eval_steps_per_second': 14.36, 'epoch': 100.0}
{'train_runtime': 98.2943, 'train_samples_per_second': 39.677, 'train_steps_per_second': 5.087, 'train_loss': 0.16214199352264405, 'epoch': 100.0}
Some weights of the model checkpoint at ../model/bert_FinBERT were not used when initializing BertForTokenClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at ../model/bert_FinBERT and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
{'loss': 0.5459, 'learning_rate': 9.045084971874738e-06, 'epoch': 20.0}
{'loss': 0.0829, 'learning_rate': 6.545084971874738e-06, 'epoch': 40.0}
{'loss': 0.0435, 'learning_rate': 3.4549150281252635e-06, 'epoch': 60.0}
{'loss': 0.0315, 'learning_rate': 9.549150281252633e-07, 'epoch': 80.0}
{'loss': 0.0285, 'learning_rate': 0.0, 'epoch': 100.0}
{'eval_loss': 0.018736839294433594, 'eval_runtime': 0.3396, 'eval_samples_per_second': 114.857, 'eval_steps_per_second': 14.725, 'epoch': 100.0}
{'train_runtime': 98.39, 'train_samples_per_second': 39.638, 'train_steps_per_second': 5.082, 'train_loss': 0.14646002340316772, 'epoch': 100.0}
Some weights of the model checkpoint at ../model/bert_FinBERT were not used when initializing BertForTokenClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at ../model/bert_FinBERT and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
{'loss': 0.6591, 'learning_rate': 8.020000000000001e-06, 'epoch': 20.0}
{'loss': 0.0959, 'learning_rate': 6.040000000000001e-06, 'epoch': 40.0}
{'loss': 0.0516, 'learning_rate': 4.06e-06, 'epoch': 60.0}
{'loss': 0.0358, 'learning_rate': 2.0799999999999996e-06, 'epoch': 80.0}
{'loss': 0.0301, 'learning_rate': 1e-07, 'epoch': 100.0}
{'eval_loss': 0.01966891624033451, 'eval_runtime': 0.3408, 'eval_samples_per_second': 114.447, 'eval_steps_per_second': 14.673, 'epoch': 100.0}
{'train_runtime': 98.2077, 'train_samples_per_second': 39.712, 'train_steps_per_second': 5.091, 'train_loss': 0.17449197673797606, 'epoch': 100.0}
Some weights of the model checkpoint at ../model/bert_FinBERT were not used when initializing BertForTokenClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at ../model/bert_FinBERT and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
{'loss': 0.6353, 'learning_rate': 1e-05, 'epoch': 20.0}
{'loss': 0.0784, 'learning_rate': 1e-05, 'epoch': 40.0}
{'loss': 0.0331, 'learning_rate': 1e-05, 'epoch': 60.0}
{'loss': 0.0167, 'learning_rate': 1e-05, 'epoch': 80.0}
{'loss': 0.009, 'learning_rate': 1e-05, 'epoch': 100.0}
{'eval_loss': 0.003036229172721505, 'eval_runtime': 0.3411, 'eval_samples_per_second': 114.342, 'eval_steps_per_second': 14.659, 'epoch': 100.0}
{'train_runtime': 99.5775, 'train_samples_per_second': 39.165, 'train_steps_per_second': 5.021, 'train_loss': 0.15450619864463805, 'epoch': 100.0}
