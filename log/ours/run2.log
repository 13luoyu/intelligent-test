Some weights of the model checkpoint at ../model/mengzi-bert-base-fin were not used when initializing BertForSequenceClassification: ['sop.cls.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'pos_head.bias', 'pos_transform.dense.bias', 'pos_transform.LayerNorm.bias', 'pos_head.weight', 'cls.predictions.decoder.bias', 'sop.cls.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'pos_transform.dense.weight', 'cls.predictions.transform.dense.weight', 'pos_transform.LayerNorm.weight']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ../model/mengzi-bert-base-fin and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
{'loss': 0.3095, 'learning_rate': 1e-05, 'epoch': 1.92}
{'loss': 0.0712, 'learning_rate': 1e-05, 'epoch': 3.85}
{'loss': 0.0088, 'learning_rate': 1e-05, 'epoch': 5.77}
{'loss': 0.0018, 'learning_rate': 1e-05, 'epoch': 7.69}
{'loss': 0.001, 'learning_rate': 1e-05, 'epoch': 9.62}
{'eval_loss': 0.000695814669597894, 'eval_runtime': 2.9428, 'eval_samples_per_second': 140.683, 'eval_steps_per_second': 17.67, 'epoch': 9.62}
{'loss': 0.0007, 'learning_rate': 1e-05, 'epoch': 11.54}
{'loss': 0.0004, 'learning_rate': 1e-05, 'epoch': 13.46}
{'loss': 0.0005, 'learning_rate': 1e-05, 'epoch': 15.38}
{'loss': 0.0005, 'learning_rate': 1e-05, 'epoch': 17.31}
{'loss': 0.0003, 'learning_rate': 1e-05, 'epoch': 19.23}
{'eval_loss': 0.00020852686429861933, 'eval_runtime': 2.9488, 'eval_samples_per_second': 140.398, 'eval_steps_per_second': 17.634, 'epoch': 19.23}
{'loss': 0.0002, 'learning_rate': 1e-05, 'epoch': 21.15}
{'loss': 0.0002, 'learning_rate': 1e-05, 'epoch': 23.08}
{'loss': 0.0002, 'learning_rate': 1e-05, 'epoch': 25.0}
{'loss': 0.0001, 'learning_rate': 1e-05, 'epoch': 26.92}
{'loss': 0.0001, 'learning_rate': 1e-05, 'epoch': 28.85}
{'eval_loss': 9.250449511455372e-05, 'eval_runtime': 2.9488, 'eval_samples_per_second': 140.396, 'eval_steps_per_second': 17.634, 'epoch': 28.85}
{'loss': 0.0001, 'learning_rate': 1e-05, 'epoch': 30.77}
{'loss': 0.0001, 'learning_rate': 1e-05, 'epoch': 32.69}
{'loss': 0.0001, 'learning_rate': 1e-05, 'epoch': 34.62}
{'loss': 0.0001, 'learning_rate': 1e-05, 'epoch': 36.54}
{'loss': 0.0001, 'learning_rate': 1e-05, 'epoch': 38.46}
{'eval_loss': 5.128638804308139e-05, 'eval_runtime': 2.9536, 'eval_samples_per_second': 140.169, 'eval_steps_per_second': 17.606, 'epoch': 38.46}
{'loss': 0.0, 'learning_rate': 1e-05, 'epoch': 40.38}
{'loss': 0.0001, 'learning_rate': 1e-05, 'epoch': 42.31}
{'loss': 0.0, 'learning_rate': 1e-05, 'epoch': 44.23}
{'loss': 0.0, 'learning_rate': 1e-05, 'epoch': 46.15}
{'loss': 0.0001, 'learning_rate': 1e-05, 'epoch': 48.08}
{'eval_loss': 3.062844552914612e-05, 'eval_runtime': 2.9537, 'eval_samples_per_second': 140.165, 'eval_steps_per_second': 17.605, 'epoch': 48.08}
{'loss': 0.0, 'learning_rate': 1e-05, 'epoch': 50.0}
{'loss': 0.0, 'learning_rate': 1e-05, 'epoch': 51.92}
{'loss': 0.0, 'learning_rate': 1e-05, 'epoch': 53.85}
{'loss': 0.0, 'learning_rate': 1e-05, 'epoch': 55.77}
{'loss': 0.0, 'learning_rate': 1e-05, 'epoch': 57.69}
{'eval_loss': 1.85094031621702e-05, 'eval_runtime': 2.9514, 'eval_samples_per_second': 140.273, 'eval_steps_per_second': 17.619, 'epoch': 57.69}
{'loss': 0.0, 'learning_rate': 1e-05, 'epoch': 59.62}
{'loss': 0.0, 'learning_rate': 1e-05, 'epoch': 61.54}
{'loss': 0.0, 'learning_rate': 1e-05, 'epoch': 63.46}
{'loss': 0.0, 'learning_rate': 1e-05, 'epoch': 65.38}
{'loss': 0.0, 'learning_rate': 1e-05, 'epoch': 67.31}
{'eval_loss': 1.3003042113268748e-05, 'eval_runtime': 2.9497, 'eval_samples_per_second': 140.352, 'eval_steps_per_second': 17.629, 'epoch': 67.31}
{'loss': 0.0, 'learning_rate': 1e-05, 'epoch': 69.23}
{'loss': 0.0, 'learning_rate': 1e-05, 'epoch': 71.15}
{'loss': 0.0, 'learning_rate': 1e-05, 'epoch': 73.08}
{'loss': 0.0, 'learning_rate': 1e-05, 'epoch': 75.0}
{'loss': 0.0, 'learning_rate': 1e-05, 'epoch': 76.92}
{'eval_loss': 9.20257116376888e-06, 'eval_runtime': 2.9542, 'eval_samples_per_second': 140.141, 'eval_steps_per_second': 17.602, 'epoch': 76.92}
{'loss': 0.0, 'learning_rate': 1e-05, 'epoch': 78.85}
{'loss': 0.0, 'learning_rate': 1e-05, 'epoch': 80.77}
{'loss': 0.0, 'learning_rate': 1e-05, 'epoch': 82.69}
{'loss': 0.0, 'learning_rate': 1e-05, 'epoch': 84.62}
{'loss': 0.0, 'learning_rate': 1e-05, 'epoch': 86.54}
{'eval_loss': 6.134320301498519e-06, 'eval_runtime': 2.954, 'eval_samples_per_second': 140.151, 'eval_steps_per_second': 17.603, 'epoch': 86.54}
{'loss': 0.0, 'learning_rate': 1e-05, 'epoch': 88.46}
{'loss': 0.0, 'learning_rate': 1e-05, 'epoch': 90.38}
{'loss': 0.0, 'learning_rate': 1e-05, 'epoch': 92.31}
{'loss': 0.0, 'learning_rate': 1e-05, 'epoch': 94.23}
{'loss': 0.0, 'learning_rate': 1e-05, 'epoch': 96.15}
{'eval_loss': 4.472520231502131e-06, 'eval_runtime': 2.959, 'eval_samples_per_second': 139.913, 'eval_steps_per_second': 17.574, 'epoch': 96.15}
{'loss': 0.0, 'learning_rate': 1e-05, 'epoch': 98.08}
{'loss': 0.0, 'learning_rate': 1e-05, 'epoch': 100.0}
{'train_runtime': 945.7614, 'train_samples_per_second': 43.774, 'train_steps_per_second': 5.498, 'train_loss': 0.007627631090939618, 'epoch': 100.0}
Some weights of the model checkpoint at ../model/mengzi-bert-base-fin were not used when initializing BertForSequenceClassification: ['sop.cls.weight', 'pos_head.bias', 'pos_head.weight', 'pos_transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'pos_transform.dense.weight', 'pos_transform.LayerNorm.bias', 'sop.cls.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'pos_transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ../model/mengzi-bert-base-fin and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
{'loss': 0.3535, 'learning_rate': 9.807692307692308e-06, 'epoch': 1.92}
{'loss': 0.1474, 'learning_rate': 9.615384615384616e-06, 'epoch': 3.85}
{'loss': 0.0586, 'learning_rate': 9.423076923076923e-06, 'epoch': 5.77}
{'loss': 0.0092, 'learning_rate': 9.230769230769232e-06, 'epoch': 7.69}
{'loss': 0.0064, 'learning_rate': 9.03846153846154e-06, 'epoch': 9.62}
{'eval_loss': 0.001065339078195393, 'eval_runtime': 2.947, 'eval_samples_per_second': 140.481, 'eval_steps_per_second': 17.645, 'epoch': 9.62}
{'loss': 0.0155, 'learning_rate': 8.846153846153847e-06, 'epoch': 11.54}
{'loss': 0.0006, 'learning_rate': 8.653846153846155e-06, 'epoch': 13.46}
{'loss': 0.0008, 'learning_rate': 8.461538461538462e-06, 'epoch': 15.38}
{'loss': 0.0004, 'learning_rate': 8.26923076923077e-06, 'epoch': 17.31}
{'loss': 0.0003, 'learning_rate': 8.076923076923077e-06, 'epoch': 19.23}
{'eval_loss': 0.00030129338847473264, 'eval_runtime': 2.9606, 'eval_samples_per_second': 139.837, 'eval_steps_per_second': 17.564, 'epoch': 19.23}
{'loss': 0.0003, 'learning_rate': 7.884615384615384e-06, 'epoch': 21.15}
{'loss': 0.0003, 'learning_rate': 7.692307692307694e-06, 'epoch': 23.08}
{'loss': 0.0002, 'learning_rate': 7.500000000000001e-06, 'epoch': 25.0}
{'loss': 0.0002, 'learning_rate': 7.307692307692308e-06, 'epoch': 26.92}
{'loss': 0.0002, 'learning_rate': 7.115384615384616e-06, 'epoch': 28.85}
{'eval_loss': 0.00014325990923680365, 'eval_runtime': 2.9606, 'eval_samples_per_second': 139.838, 'eval_steps_per_second': 17.564, 'epoch': 28.85}
{'loss': 0.0002, 'learning_rate': 6.923076923076923e-06, 'epoch': 30.77}
{'loss': 0.0001, 'learning_rate': 6.730769230769232e-06, 'epoch': 32.69}
{'loss': 0.0002, 'learning_rate': 6.538461538461539e-06, 'epoch': 34.62}
{'loss': 0.0002, 'learning_rate': 6.3461538461538466e-06, 'epoch': 36.54}
{'loss': 0.0001, 'learning_rate': 6.153846153846155e-06, 'epoch': 38.46}
{'eval_loss': 7.90451085777022e-05, 'eval_runtime': 2.9634, 'eval_samples_per_second': 139.705, 'eval_steps_per_second': 17.548, 'epoch': 38.46}
{'loss': 0.0001, 'learning_rate': 5.961538461538462e-06, 'epoch': 40.38}
{'loss': 0.0001, 'learning_rate': 5.769230769230769e-06, 'epoch': 42.31}
{'loss': 0.0001, 'learning_rate': 5.576923076923077e-06, 'epoch': 44.23}
{'loss': 0.0001, 'learning_rate': 5.384615384615385e-06, 'epoch': 46.15}
{'loss': 0.0001, 'learning_rate': 5.192307692307693e-06, 'epoch': 48.08}
{'eval_loss': 5.372813029680401e-05, 'eval_runtime': 2.9566, 'eval_samples_per_second': 140.024, 'eval_steps_per_second': 17.588, 'epoch': 48.08}
{'loss': 0.0001, 'learning_rate': 5e-06, 'epoch': 50.0}
{'loss': 0.0001, 'learning_rate': 4.807692307692308e-06, 'epoch': 51.92}
{'loss': 0.0001, 'learning_rate': 4.615384615384616e-06, 'epoch': 53.85}
{'loss': 0.0001, 'learning_rate': 4.423076923076924e-06, 'epoch': 55.77}
{'loss': 0.0, 'learning_rate': 4.230769230769231e-06, 'epoch': 57.69}
{'eval_loss': 3.962092159781605e-05, 'eval_runtime': 2.9565, 'eval_samples_per_second': 140.033, 'eval_steps_per_second': 17.589, 'epoch': 57.69}
{'loss': 0.0001, 'learning_rate': 4.0384615384615385e-06, 'epoch': 59.62}
{'loss': 0.0001, 'learning_rate': 3.846153846153847e-06, 'epoch': 61.54}
{'loss': 0.0, 'learning_rate': 3.653846153846154e-06, 'epoch': 63.46}
{'loss': 0.0001, 'learning_rate': 3.4615384615384617e-06, 'epoch': 65.38}
{'loss': 0.0, 'learning_rate': 3.2692307692307696e-06, 'epoch': 67.31}
{'eval_loss': 3.0220830012694933e-05, 'eval_runtime': 2.9523, 'eval_samples_per_second': 140.23, 'eval_steps_per_second': 17.613, 'epoch': 67.31}
{'loss': 0.0, 'learning_rate': 3.0769230769230774e-06, 'epoch': 69.23}
{'loss': 0.0001, 'learning_rate': 2.8846153846153845e-06, 'epoch': 71.15}
{'loss': 0.0, 'learning_rate': 2.6923076923076923e-06, 'epoch': 73.08}
{'loss': 0.0, 'learning_rate': 2.5e-06, 'epoch': 75.0}
{'loss': 0.0, 'learning_rate': 2.307692307692308e-06, 'epoch': 76.92}
{'eval_loss': 2.4674203814356588e-05, 'eval_runtime': 2.9613, 'eval_samples_per_second': 139.804, 'eval_steps_per_second': 17.56, 'epoch': 76.92}
{'loss': 0.0, 'learning_rate': 2.1153846153846155e-06, 'epoch': 78.85}
{'loss': 0.0, 'learning_rate': 1.9230769230769234e-06, 'epoch': 80.77}
{'loss': 0.0, 'learning_rate': 1.7307692307692308e-06, 'epoch': 82.69}
{'loss': 0.0, 'learning_rate': 1.5384615384615387e-06, 'epoch': 84.62}
{'loss': 0.0, 'learning_rate': 1.3461538461538462e-06, 'epoch': 86.54}
{'eval_loss': 2.142228913726285e-05, 'eval_runtime': 2.9594, 'eval_samples_per_second': 139.893, 'eval_steps_per_second': 17.571, 'epoch': 86.54}
{'loss': 0.0, 'learning_rate': 1.153846153846154e-06, 'epoch': 88.46}
{'loss': 0.0, 'learning_rate': 9.615384615384617e-07, 'epoch': 90.38}
{'loss': 0.0, 'learning_rate': 7.692307692307694e-07, 'epoch': 92.31}
{'loss': 0.0, 'learning_rate': 5.76923076923077e-07, 'epoch': 94.23}
{'loss': 0.0, 'learning_rate': 3.846153846153847e-07, 'epoch': 96.15}
{'eval_loss': 1.9981962395831943e-05, 'eval_runtime': 2.9612, 'eval_samples_per_second': 139.809, 'eval_steps_per_second': 17.561, 'epoch': 96.15}
{'loss': 0.0, 'learning_rate': 1.9230769230769234e-07, 'epoch': 98.08}
{'loss': 0.0, 'learning_rate': 0.0, 'epoch': 100.0}
{'train_runtime': 947.8233, 'train_samples_per_second': 43.679, 'train_steps_per_second': 5.486, 'train_loss': 0.011465159075227207, 'epoch': 100.0}
Some weights of the model checkpoint at ../model/mengzi-bert-base-fin were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'pos_transform.dense.weight', 'cls.predictions.decoder.bias', 'pos_transform.LayerNorm.weight', 'sop.cls.bias', 'cls.predictions.transform.LayerNorm.weight', 'pos_head.bias', 'pos_head.weight', 'pos_transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'pos_transform.dense.bias', 'sop.cls.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ../model/mengzi-bert-base-fin and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
{'loss': 0.3397, 'learning_rate': 9.990877771116588e-06, 'epoch': 1.92}
{'loss': 0.1452, 'learning_rate': 9.96354437049027e-06, 'epoch': 3.85}
{'loss': 0.0172, 'learning_rate': 9.91809953473572e-06, 'epoch': 5.77}
{'loss': 0.0164, 'learning_rate': 9.854709087130261e-06, 'epoch': 7.69}
{'loss': 0.0012, 'learning_rate': 9.77360433254273e-06, 'epoch': 9.62}
{'eval_loss': 0.000602597719989717, 'eval_runtime': 2.9636, 'eval_samples_per_second': 139.696, 'eval_steps_per_second': 17.546, 'epoch': 9.62}
{'loss': 0.0006, 'learning_rate': 9.675081213427076e-06, 'epoch': 11.54}
{'loss': 0.0004, 'learning_rate': 9.55949922996045e-06, 'epoch': 13.46}
{'loss': 0.0004, 'learning_rate': 9.427280128266049e-06, 'epoch': 15.38}
{'loss': 0.0003, 'learning_rate': 9.278906361507238e-06, 'epoch': 17.31}
{'loss': 0.0003, 'learning_rate': 9.114919329468283e-06, 'epoch': 19.23}
{'eval_loss': 0.00020836909243371338, 'eval_runtime': 2.9681, 'eval_samples_per_second': 139.484, 'eval_steps_per_second': 17.52, 'epoch': 19.23}
{'loss': 0.0002, 'learning_rate': 8.935917403045251e-06, 'epoch': 21.15}
{'loss': 0.0002, 'learning_rate': 8.742553740855507e-06, 'epoch': 23.08}
{'loss': 0.0002, 'learning_rate': 8.535533905932739e-06, 'epoch': 25.0}
{'loss': 0.0002, 'learning_rate': 8.315613291203977e-06, 'epoch': 26.92}
{'loss': 0.0001, 'learning_rate': 8.083594363142717e-06, 'epoch': 28.85}
{'eval_loss': 0.00010396825382485986, 'eval_runtime': 2.9512, 'eval_samples_per_second': 140.281, 'eval_steps_per_second': 17.62, 'epoch': 28.85}
{'loss': 0.0001, 'learning_rate': 7.84032373365578e-06, 'epoch': 30.77}
{'loss': 0.0001, 'learning_rate': 7.586689070888284e-06, 'epoch': 32.69}
{'loss': 0.0001, 'learning_rate': 7.323615860218844e-06, 'epoch': 34.62}
{'loss': 0.0001, 'learning_rate': 7.052064027263785e-06, 'epoch': 36.54}
{'loss': 0.0001, 'learning_rate': 6.773024435212678e-06, 'epoch': 38.46}
{'eval_loss': 6.452308298321441e-05, 'eval_runtime': 2.9652, 'eval_samples_per_second': 139.62, 'eval_steps_per_second': 17.537, 'epoch': 38.46}
{'loss': 0.0001, 'learning_rate': 6.487515269276015e-06, 'epoch': 40.38}
{'loss': 0.0001, 'learning_rate': 6.1965783214377895e-06, 'epoch': 42.31}
{'loss': 0.0001, 'learning_rate': 5.90127518906953e-06, 'epoch': 44.23}
{'loss': 0.0001, 'learning_rate': 5.6026834012766155e-06, 'epoch': 46.15}
{'loss': 0.0001, 'learning_rate': 5.301892487111431e-06, 'epoch': 48.08}
{'eval_loss': 4.359666854725219e-05, 'eval_runtime': 2.9689, 'eval_samples_per_second': 139.447, 'eval_steps_per_second': 17.515, 'epoch': 48.08}
{'loss': 0.0001, 'learning_rate': 5e-06, 'epoch': 50.0}
{'loss': 0.0001, 'learning_rate': 4.69810751288857e-06, 'epoch': 51.92}
{'loss': 0.0, 'learning_rate': 4.397316598723385e-06, 'epoch': 53.85}
{'loss': 0.0001, 'learning_rate': 4.098724810930472e-06, 'epoch': 55.77}
{'loss': 0.0, 'learning_rate': 3.803421678562213e-06, 'epoch': 57.69}
{'eval_loss': 3.2825257221702486e-05, 'eval_runtime': 2.9659, 'eval_samples_per_second': 139.586, 'eval_steps_per_second': 17.533, 'epoch': 57.69}
{'loss': 0.0, 'learning_rate': 3.5124847307239863e-06, 'epoch': 59.62}
{'loss': 0.0, 'learning_rate': 3.226975564787322e-06, 'epoch': 61.54}
{'loss': 0.0, 'learning_rate': 2.947935972736217e-06, 'epoch': 63.46}
{'loss': 0.0, 'learning_rate': 2.6763841397811576e-06, 'epoch': 65.38}
{'loss': 0.0, 'learning_rate': 2.4133109291117156e-06, 'epoch': 67.31}
{'eval_loss': 2.7153253540745936e-05, 'eval_runtime': 2.9646, 'eval_samples_per_second': 139.648, 'eval_steps_per_second': 17.54, 'epoch': 67.31}
{'loss': 0.0, 'learning_rate': 2.159676266344222e-06, 'epoch': 69.23}
{'loss': 0.0, 'learning_rate': 1.9164056368572847e-06, 'epoch': 71.15}
{'loss': 0.0, 'learning_rate': 1.6843867087960252e-06, 'epoch': 73.08}
{'loss': 0.0, 'learning_rate': 1.4644660940672628e-06, 'epoch': 75.0}
{'loss': 0.0, 'learning_rate': 1.257446259144494e-06, 'epoch': 76.92}
{'eval_loss': 2.385649895586539e-05, 'eval_runtime': 2.9621, 'eval_samples_per_second': 139.767, 'eval_steps_per_second': 17.555, 'epoch': 76.92}
{'loss': 0.0, 'learning_rate': 1.0640825969547498e-06, 'epoch': 78.85}
{'loss': 0.0, 'learning_rate': 8.850806705317183e-07, 'epoch': 80.77}
{'loss': 0.0, 'learning_rate': 7.210936384927631e-07, 'epoch': 82.69}
{'loss': 0.0, 'learning_rate': 5.727198717339511e-07, 'epoch': 84.62}
{'loss': 0.0, 'learning_rate': 4.405007700395497e-07, 'epoch': 86.54}
{'eval_loss': 2.2409234588849358e-05, 'eval_runtime': 2.9654, 'eval_samples_per_second': 139.611, 'eval_steps_per_second': 17.536, 'epoch': 86.54}
{'loss': 0.0, 'learning_rate': 3.2491878657292643e-07, 'epoch': 88.46}
{'loss': 0.0, 'learning_rate': 2.2639566745727203e-07, 'epoch': 90.38}
{'loss': 0.0, 'learning_rate': 1.4529091286973994e-07, 'epoch': 92.31}
{'loss': 0.0, 'learning_rate': 8.190046526428241e-08, 'epoch': 94.23}
{'loss': 0.0, 'learning_rate': 3.645562950973014e-08, 'epoch': 96.15}
{'eval_loss': 2.2028685634722933e-05, 'eval_runtime': 2.9649, 'eval_samples_per_second': 139.632, 'eval_steps_per_second': 17.538, 'epoch': 96.15}
{'loss': 0.0, 'learning_rate': 9.12222888341252e-09, 'epoch': 98.08}
{'loss': 0.0, 'learning_rate': 0.0, 'epoch': 100.0}
{'train_runtime': 947.9424, 'train_samples_per_second': 43.674, 'train_steps_per_second': 5.486, 'train_loss': 0.0100869054229742, 'epoch': 100.0}
Some weights of the model checkpoint at ../model/mengzi-bert-base-fin were not used when initializing BertForSequenceClassification: ['pos_transform.dense.bias', 'cls.predictions.bias', 'pos_transform.dense.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'pos_transform.LayerNorm.bias', 'pos_head.bias', 'pos_head.weight', 'pos_transform.LayerNorm.weight', 'sop.cls.weight', 'sop.cls.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ../model/mengzi-bert-base-fin and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
{'loss': 0.3322, 'learning_rate': 9.990877771116588e-06, 'epoch': 1.92}
{'loss': 0.1303, 'learning_rate': 9.96354437049027e-06, 'epoch': 3.85}
{'loss': 0.0179, 'learning_rate': 9.91809953473572e-06, 'epoch': 5.77}
{'loss': 0.0037, 'learning_rate': 9.854709087130261e-06, 'epoch': 7.69}
{'loss': 0.0017, 'learning_rate': 9.77360433254273e-06, 'epoch': 9.62}
{'eval_loss': 0.0010980241931974888, 'eval_runtime': 2.9708, 'eval_samples_per_second': 139.356, 'eval_steps_per_second': 17.504, 'epoch': 9.62}
{'loss': 0.001, 'learning_rate': 9.675081213427076e-06, 'epoch': 11.54}
{'loss': 0.0012, 'learning_rate': 9.55949922996045e-06, 'epoch': 13.46}
{'loss': 0.0007, 'learning_rate': 9.427280128266049e-06, 'epoch': 15.38}
{'loss': 0.0005, 'learning_rate': 9.278906361507238e-06, 'epoch': 17.31}
{'loss': 0.0003, 'learning_rate': 9.114919329468283e-06, 'epoch': 19.23}
{'eval_loss': 0.0002951935457531363, 'eval_runtime': 2.9757, 'eval_samples_per_second': 139.127, 'eval_steps_per_second': 17.475, 'epoch': 19.23}
{'loss': 0.0003, 'learning_rate': 8.935917403045251e-06, 'epoch': 21.15}
{'loss': 0.0003, 'learning_rate': 8.742553740855507e-06, 'epoch': 23.08}
{'loss': 0.0003, 'learning_rate': 8.535533905932739e-06, 'epoch': 25.0}
{'loss': 0.0002, 'learning_rate': 8.315613291203977e-06, 'epoch': 26.92}
{'loss': 0.0001, 'learning_rate': 8.083594363142717e-06, 'epoch': 28.85}
{'eval_loss': 0.0001247788459295407, 'eval_runtime': 2.9709, 'eval_samples_per_second': 139.351, 'eval_steps_per_second': 17.503, 'epoch': 28.85}
{'loss': 0.0002, 'learning_rate': 7.84032373365578e-06, 'epoch': 30.77}
{'loss': 0.0001, 'learning_rate': 7.586689070888284e-06, 'epoch': 32.69}
{'loss': 0.0001, 'learning_rate': 7.323615860218844e-06, 'epoch': 34.62}
{'loss': 0.0001, 'learning_rate': 7.052064027263785e-06, 'epoch': 36.54}
{'loss': 0.0001, 'learning_rate': 6.773024435212678e-06, 'epoch': 38.46}
{'eval_loss': 6.897640560055152e-05, 'eval_runtime': 2.9707, 'eval_samples_per_second': 139.361, 'eval_steps_per_second': 17.504, 'epoch': 38.46}
{'loss': 0.0001, 'learning_rate': 6.487515269276015e-06, 'epoch': 40.38}
{'loss': 0.0001, 'learning_rate': 6.1965783214377895e-06, 'epoch': 42.31}
{'loss': 0.0001, 'learning_rate': 5.90127518906953e-06, 'epoch': 44.23}
{'loss': 0.0001, 'learning_rate': 5.6026834012766155e-06, 'epoch': 46.15}
{'loss': 0.0001, 'learning_rate': 5.301892487111431e-06, 'epoch': 48.08}
{'eval_loss': 4.235930464346893e-05, 'eval_runtime': 2.9716, 'eval_samples_per_second': 139.319, 'eval_steps_per_second': 17.499, 'epoch': 48.08}
{'loss': 0.0001, 'learning_rate': 5e-06, 'epoch': 50.0}
{'loss': 0.0001, 'learning_rate': 4.69810751288857e-06, 'epoch': 51.92}
{'loss': 0.0, 'learning_rate': 4.397316598723385e-06, 'epoch': 53.85}
{'loss': 0.0001, 'learning_rate': 4.098724810930472e-06, 'epoch': 55.77}
{'loss': 0.0, 'learning_rate': 3.803421678562213e-06, 'epoch': 57.69}
{'eval_loss': 3.069176818826236e-05, 'eval_runtime': 2.9661, 'eval_samples_per_second': 139.579, 'eval_steps_per_second': 17.532, 'epoch': 57.69}
{'loss': 0.0, 'learning_rate': 3.5124847307239863e-06, 'epoch': 59.62}
{'loss': 0.0, 'learning_rate': 3.226975564787322e-06, 'epoch': 61.54}
{'loss': 0.0, 'learning_rate': 2.947935972736217e-06, 'epoch': 63.46}
{'loss': 0.0, 'learning_rate': 2.6763841397811576e-06, 'epoch': 65.38}
{'loss': 0.0, 'learning_rate': 2.4133109291117156e-06, 'epoch': 67.31}
{'eval_loss': 2.4543363906559534e-05, 'eval_runtime': 2.9642, 'eval_samples_per_second': 139.668, 'eval_steps_per_second': 17.543, 'epoch': 67.31}
{'loss': 0.0, 'learning_rate': 2.159676266344222e-06, 'epoch': 69.23}
{'loss': 0.0, 'learning_rate': 1.9164056368572847e-06, 'epoch': 71.15}
{'loss': 0.0, 'learning_rate': 1.6843867087960252e-06, 'epoch': 73.08}
{'loss': 0.0, 'learning_rate': 1.4644660940672628e-06, 'epoch': 75.0}
{'loss': 0.0, 'learning_rate': 1.257446259144494e-06, 'epoch': 76.92}
{'eval_loss': 2.160994517907966e-05, 'eval_runtime': 2.9704, 'eval_samples_per_second': 139.374, 'eval_steps_per_second': 17.506, 'epoch': 76.92}
{'loss': 0.0, 'learning_rate': 1.0640825969547498e-06, 'epoch': 78.85}
{'loss': 0.0, 'learning_rate': 8.850806705317183e-07, 'epoch': 80.77}
{'loss': 0.0, 'learning_rate': 7.210936384927631e-07, 'epoch': 82.69}
{'loss': 0.0, 'learning_rate': 5.727198717339511e-07, 'epoch': 84.62}
{'loss': 0.0, 'learning_rate': 4.405007700395497e-07, 'epoch': 86.54}
{'eval_loss': 2.0054376363987103e-05, 'eval_runtime': 2.9757, 'eval_samples_per_second': 139.127, 'eval_steps_per_second': 17.475, 'epoch': 86.54}
{'loss': 0.0, 'learning_rate': 3.2491878657292643e-07, 'epoch': 88.46}
{'loss': 0.0, 'learning_rate': 2.2639566745727203e-07, 'epoch': 90.38}
{'loss': 0.0, 'learning_rate': 1.4529091286973994e-07, 'epoch': 92.31}
{'loss': 0.0, 'learning_rate': 8.190046526428241e-08, 'epoch': 94.23}
{'loss': 0.0, 'learning_rate': 3.645562950973014e-08, 'epoch': 96.15}
{'eval_loss': 1.9665521904244088e-05, 'eval_runtime': 2.9748, 'eval_samples_per_second': 139.17, 'eval_steps_per_second': 17.48, 'epoch': 96.15}
{'loss': 0.0, 'learning_rate': 9.12222888341252e-09, 'epoch': 98.08}
{'loss': 0.0, 'learning_rate': 0.0, 'epoch': 100.0}
{'train_runtime': 947.9079, 'train_samples_per_second': 43.675, 'train_steps_per_second': 5.486, 'train_loss': 0.009468996008620776, 'epoch': 100.0}
Some weights of the model checkpoint at ../model/mengzi-bert-base-fin were not used when initializing BertForSequenceClassification: ['pos_head.bias', 'cls.predictions.transform.LayerNorm.weight', 'sop.cls.bias', 'sop.cls.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'pos_head.weight', 'cls.predictions.transform.dense.bias', 'pos_transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'pos_transform.dense.bias', 'pos_transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'pos_transform.dense.weight']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ../model/mengzi-bert-base-fin and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
{'loss': 0.359, 'learning_rate': 9.809615384615385e-06, 'epoch': 1.92}
{'loss': 0.1198, 'learning_rate': 9.61923076923077e-06, 'epoch': 3.85}
{'loss': 0.0154, 'learning_rate': 9.428846153846155e-06, 'epoch': 5.77}
{'loss': 0.0023, 'learning_rate': 9.23846153846154e-06, 'epoch': 7.69}
{'loss': 0.0015, 'learning_rate': 9.048076923076924e-06, 'epoch': 9.62}
{'eval_loss': 0.00113777257502079, 'eval_runtime': 2.9956, 'eval_samples_per_second': 138.204, 'eval_steps_per_second': 17.359, 'epoch': 9.62}
{'loss': 0.0007, 'learning_rate': 8.857692307692309e-06, 'epoch': 11.54}
{'loss': 0.0009, 'learning_rate': 8.667307692307694e-06, 'epoch': 13.46}
{'loss': 0.0005, 'learning_rate': 8.476923076923078e-06, 'epoch': 15.38}
{'loss': 0.0007, 'learning_rate': 8.286538461538461e-06, 'epoch': 17.31}
{'loss': 0.0003, 'learning_rate': 8.096153846153848e-06, 'epoch': 19.23}
{'eval_loss': 0.0003262966056354344, 'eval_runtime': 2.9803, 'eval_samples_per_second': 138.912, 'eval_steps_per_second': 17.448, 'epoch': 19.23}
{'loss': 0.0004, 'learning_rate': 7.90576923076923e-06, 'epoch': 21.15}
{'loss': 0.0003, 'learning_rate': 7.715384615384615e-06, 'epoch': 23.08}
{'loss': 0.0002, 'learning_rate': 7.525e-06, 'epoch': 25.0}
{'loss': 0.0002, 'learning_rate': 7.3346153846153855e-06, 'epoch': 26.92}
{'loss': 0.0002, 'learning_rate': 7.1442307692307685e-06, 'epoch': 28.85}
{'eval_loss': 0.00014067698793951422, 'eval_runtime': 2.972, 'eval_samples_per_second': 139.301, 'eval_steps_per_second': 17.497, 'epoch': 28.85}
{'loss': 0.0002, 'learning_rate': 6.953846153846154e-06, 'epoch': 30.77}
{'loss': 0.0002, 'learning_rate': 6.763461538461539e-06, 'epoch': 32.69}
{'loss': 0.0001, 'learning_rate': 6.573076923076923e-06, 'epoch': 34.62}
{'loss': 0.0001, 'learning_rate': 6.382692307692307e-06, 'epoch': 36.54}
{'loss': 0.0002, 'learning_rate': 6.192307692307693e-06, 'epoch': 38.46}
{'eval_loss': 7.589144661324099e-05, 'eval_runtime': 2.9712, 'eval_samples_per_second': 139.337, 'eval_steps_per_second': 17.501, 'epoch': 38.46}
{'loss': 0.0001, 'learning_rate': 6.0019230769230765e-06, 'epoch': 40.38}
{'loss': 0.0001, 'learning_rate': 5.81153846153846e-06, 'epoch': 42.31}
{'loss': 0.0001, 'learning_rate': 5.621153846153846e-06, 'epoch': 44.23}
{'loss': 0.0001, 'learning_rate': 5.4307692307692306e-06, 'epoch': 46.15}
{'loss': 0.0001, 'learning_rate': 5.240384615384615e-06, 'epoch': 48.08}
{'eval_loss': 4.820184767595492e-05, 'eval_runtime': 2.9719, 'eval_samples_per_second': 139.305, 'eval_steps_per_second': 17.497, 'epoch': 48.08}
{'loss': 0.0001, 'learning_rate': 5.050000000000001e-06, 'epoch': 50.0}
{'loss': 0.0001, 'learning_rate': 4.859615384615384e-06, 'epoch': 51.92}
{'loss': 0.0, 'learning_rate': 4.669230769230769e-06, 'epoch': 53.85}
{'loss': 0.0001, 'learning_rate': 4.478846153846154e-06, 'epoch': 55.77}
{'loss': 0.0, 'learning_rate': 4.288461538461539e-06, 'epoch': 57.69}
{'eval_loss': 3.424828901188448e-05, 'eval_runtime': 2.9802, 'eval_samples_per_second': 138.919, 'eval_steps_per_second': 17.449, 'epoch': 57.69}
{'loss': 0.0, 'learning_rate': 4.098076923076923e-06, 'epoch': 59.62}
{'loss': 0.0001, 'learning_rate': 3.907692307692307e-06, 'epoch': 61.54}
{'loss': 0.0, 'learning_rate': 3.7173076923076927e-06, 'epoch': 63.46}
{'loss': 0.0, 'learning_rate': 3.5269230769230765e-06, 'epoch': 65.38}
{'loss': 0.0, 'learning_rate': 3.3365384615384603e-06, 'epoch': 67.31}
{'eval_loss': 2.6718695153249428e-05, 'eval_runtime': 2.9788, 'eval_samples_per_second': 138.981, 'eval_steps_per_second': 17.457, 'epoch': 67.31}
{'loss': 0.0, 'learning_rate': 3.1461538461538467e-06, 'epoch': 69.23}
{'loss': 0.0, 'learning_rate': 2.9557692307692306e-06, 'epoch': 71.15}
{'loss': 0.0, 'learning_rate': 2.765384615384616e-06, 'epoch': 73.08}
{'loss': 0.0, 'learning_rate': 2.575e-06, 'epoch': 75.0}
{'loss': 0.0, 'learning_rate': 2.384615384615384e-06, 'epoch': 76.92}
{'eval_loss': 2.1700454453821294e-05, 'eval_runtime': 2.9776, 'eval_samples_per_second': 139.039, 'eval_steps_per_second': 17.464, 'epoch': 76.92}
{'loss': 0.0, 'learning_rate': 2.1942307692307693e-06, 'epoch': 78.85}
{'loss': 0.0, 'learning_rate': 2.0038461538461535e-06, 'epoch': 80.77}
{'loss': 0.0, 'learning_rate': 1.8134615384615388e-06, 'epoch': 82.69}
{'loss': 0.0, 'learning_rate': 1.6230769230769233e-06, 'epoch': 84.62}
{'loss': 0.0, 'learning_rate': 1.4326923076923073e-06, 'epoch': 86.54}
{'eval_loss': 1.879209594335407e-05, 'eval_runtime': 2.9811, 'eval_samples_per_second': 138.876, 'eval_steps_per_second': 17.443, 'epoch': 86.54}
{'loss': 0.0, 'learning_rate': 1.2423076923076927e-06, 'epoch': 88.46}
{'loss': 0.0, 'learning_rate': 1.051923076923077e-06, 'epoch': 90.38}
{'loss': 0.0, 'learning_rate': 8.615384615384611e-07, 'epoch': 92.31}
{'loss': 0.0, 'learning_rate': 6.711538461538463e-07, 'epoch': 94.23}
{'loss': 0.0, 'learning_rate': 4.807692307692305e-07, 'epoch': 96.15}
{'eval_loss': 1.7372367437928915e-05, 'eval_runtime': 2.9747, 'eval_samples_per_second': 139.173, 'eval_steps_per_second': 17.481, 'epoch': 96.15}
{'loss': 0.0, 'learning_rate': 2.903846153846158e-07, 'epoch': 98.08}
{'loss': 0.0, 'learning_rate': 1e-07, 'epoch': 100.0}
{'train_runtime': 948.5997, 'train_samples_per_second': 43.643, 'train_steps_per_second': 5.482, 'train_loss': 0.009703288739144156, 'epoch': 100.0}
Some weights of the model checkpoint at ../model/mengzi-bert-base-fin were not used when initializing BertForSequenceClassification: ['pos_transform.LayerNorm.bias', 'pos_transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'sop.cls.weight', 'cls.predictions.transform.LayerNorm.bias', 'pos_transform.dense.weight', 'sop.cls.bias', 'cls.predictions.transform.dense.bias', 'pos_head.bias', 'cls.predictions.transform.dense.weight', 'pos_transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'pos_head.weight', 'cls.predictions.bias']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ../model/mengzi-bert-base-fin and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
{'loss': 0.3114, 'learning_rate': 1e-05, 'epoch': 1.92}
{'loss': 0.1219, 'learning_rate': 1e-05, 'epoch': 3.85}
{'loss': 0.0381, 'learning_rate': 1e-05, 'epoch': 5.77}
{'loss': 0.0018, 'learning_rate': 1e-05, 'epoch': 7.69}
{'loss': 0.001, 'learning_rate': 1e-05, 'epoch': 9.62}
{'eval_loss': 0.0005609119543805718, 'eval_runtime': 2.9709, 'eval_samples_per_second': 139.354, 'eval_steps_per_second': 17.503, 'epoch': 9.62}
{'loss': 0.0006, 'learning_rate': 1e-05, 'epoch': 11.54}
{'loss': 0.0007, 'learning_rate': 1e-05, 'epoch': 13.46}
{'loss': 0.0003, 'learning_rate': 1e-05, 'epoch': 15.38}
{'loss': 0.0003, 'learning_rate': 1e-05, 'epoch': 17.31}
{'loss': 0.0003, 'learning_rate': 1e-05, 'epoch': 19.23}
{'eval_loss': 0.0001707997580524534, 'eval_runtime': 2.9771, 'eval_samples_per_second': 139.061, 'eval_steps_per_second': 17.467, 'epoch': 19.23}
{'loss': 0.0002, 'learning_rate': 1e-05, 'epoch': 21.15}
{'loss': 0.0002, 'learning_rate': 1e-05, 'epoch': 23.08}
{'loss': 0.0001, 'learning_rate': 1e-05, 'epoch': 25.0}
{'loss': 0.0002, 'learning_rate': 1e-05, 'epoch': 26.92}
{'loss': 0.0001, 'learning_rate': 1e-05, 'epoch': 28.85}
{'eval_loss': 8.293914288515225e-05, 'eval_runtime': 2.9784, 'eval_samples_per_second': 138.999, 'eval_steps_per_second': 17.459, 'epoch': 28.85}
{'loss': 0.0001, 'learning_rate': 1e-05, 'epoch': 30.77}
{'loss': 0.0001, 'learning_rate': 1e-05, 'epoch': 32.69}
{'loss': 0.0001, 'learning_rate': 1e-05, 'epoch': 34.62}
{'loss': 0.0001, 'learning_rate': 1e-05, 'epoch': 36.54}
{'loss': 0.0001, 'learning_rate': 1e-05, 'epoch': 38.46}
{'eval_loss': 4.497769623412751e-05, 'eval_runtime': 2.9805, 'eval_samples_per_second': 138.901, 'eval_steps_per_second': 17.447, 'epoch': 38.46}
{'loss': 0.0001, 'learning_rate': 1e-05, 'epoch': 40.38}
{'loss': 0.0001, 'learning_rate': 1e-05, 'epoch': 42.31}
{'loss': 0.0, 'learning_rate': 1e-05, 'epoch': 44.23}
{'loss': 0.0, 'learning_rate': 1e-05, 'epoch': 46.15}
{'loss': 0.0, 'learning_rate': 1e-05, 'epoch': 48.08}
{'eval_loss': 2.7115676857647486e-05, 'eval_runtime': 2.9762, 'eval_samples_per_second': 139.104, 'eval_steps_per_second': 17.472, 'epoch': 48.08}
{'loss': 0.0, 'learning_rate': 1e-05, 'epoch': 50.0}
{'loss': 0.0, 'learning_rate': 1e-05, 'epoch': 51.92}
{'loss': 0.1835, 'learning_rate': 1e-05, 'epoch': 53.85}
{'loss': 0.074, 'learning_rate': 1e-05, 'epoch': 55.77}
{'loss': 0.0002, 'learning_rate': 1e-05, 'epoch': 57.69}
{'eval_loss': 6.941134779481217e-05, 'eval_runtime': 2.9741, 'eval_samples_per_second': 139.2, 'eval_steps_per_second': 17.484, 'epoch': 57.69}
{'loss': 0.0001, 'learning_rate': 1e-05, 'epoch': 59.62}
{'loss': 0.0001, 'learning_rate': 1e-05, 'epoch': 61.54}
{'loss': 0.0001, 'learning_rate': 1e-05, 'epoch': 63.46}
{'loss': 0.0001, 'learning_rate': 1e-05, 'epoch': 65.38}
{'loss': 0.0, 'learning_rate': 1e-05, 'epoch': 67.31}
{'eval_loss': 2.6919258743873797e-05, 'eval_runtime': 2.9731, 'eval_samples_per_second': 139.247, 'eval_steps_per_second': 17.49, 'epoch': 67.31}
{'loss': 0.0, 'learning_rate': 1e-05, 'epoch': 69.23}
{'loss': 0.0, 'learning_rate': 1e-05, 'epoch': 71.15}
{'loss': 0.0, 'learning_rate': 1e-05, 'epoch': 73.08}
{'loss': 0.0, 'learning_rate': 1e-05, 'epoch': 75.0}
{'loss': 0.0, 'learning_rate': 1e-05, 'epoch': 76.92}
{'eval_loss': 1.7048716472345404e-05, 'eval_runtime': 2.9682, 'eval_samples_per_second': 139.48, 'eval_steps_per_second': 17.519, 'epoch': 76.92}
{'loss': 0.0, 'learning_rate': 1e-05, 'epoch': 78.85}
{'loss': 0.0, 'learning_rate': 1e-05, 'epoch': 80.77}
{'loss': 0.0, 'learning_rate': 1e-05, 'epoch': 82.69}
{'loss': 0.0, 'learning_rate': 1e-05, 'epoch': 84.62}
{'loss': 0.0, 'learning_rate': 1e-05, 'epoch': 86.54}
{'eval_loss': 1.1944544894504361e-05, 'eval_runtime': 2.9684, 'eval_samples_per_second': 139.467, 'eval_steps_per_second': 17.518, 'epoch': 86.54}
{'loss': 0.0, 'learning_rate': 1e-05, 'epoch': 88.46}
{'loss': 0.0, 'learning_rate': 1e-05, 'epoch': 90.38}
{'loss': 0.0, 'learning_rate': 1e-05, 'epoch': 92.31}
{'loss': 0.0, 'learning_rate': 1e-05, 'epoch': 94.23}
{'loss': 0.0, 'learning_rate': 1e-05, 'epoch': 96.15}
{'eval_loss': 8.558189620089252e-06, 'eval_runtime': 2.96, 'eval_samples_per_second': 139.863, 'eval_steps_per_second': 17.567, 'epoch': 96.15}
{'loss': 0.0, 'learning_rate': 1e-05, 'epoch': 98.08}
{'loss': 0.0, 'learning_rate': 1e-05, 'epoch': 100.0}
{'train_runtime': 949.9516, 'train_samples_per_second': 43.581, 'train_steps_per_second': 5.474, 'train_loss': 0.014158362387209378, 'epoch': 100.0}
Some weights of the model checkpoint at ../model/bert_FinBERT were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ../model/bert_FinBERT and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
{'loss': 0.2553, 'learning_rate': 1e-05, 'epoch': 1.92}
{'loss': 0.033, 'learning_rate': 1e-05, 'epoch': 3.85}
{'loss': 0.0085, 'learning_rate': 1e-05, 'epoch': 5.77}
{'loss': 0.0006, 'learning_rate': 1e-05, 'epoch': 7.69}
{'loss': 0.0004, 'learning_rate': 1e-05, 'epoch': 9.62}
{'eval_loss': 0.0002320320636499673, 'eval_runtime': 2.974, 'eval_samples_per_second': 139.205, 'eval_steps_per_second': 17.485, 'epoch': 9.62}
{'loss': 0.0003, 'learning_rate': 1e-05, 'epoch': 11.54}
{'loss': 0.0002, 'learning_rate': 1e-05, 'epoch': 13.46}
{'loss': 0.0002, 'learning_rate': 1e-05, 'epoch': 15.38}
{'loss': 0.0001, 'learning_rate': 1e-05, 'epoch': 17.31}
{'loss': 0.0001, 'learning_rate': 1e-05, 'epoch': 19.23}
{'eval_loss': 7.330214430112392e-05, 'eval_runtime': 2.9776, 'eval_samples_per_second': 139.037, 'eval_steps_per_second': 17.464, 'epoch': 19.23}
{'loss': 0.0001, 'learning_rate': 1e-05, 'epoch': 21.15}
{'loss': 0.0001, 'learning_rate': 1e-05, 'epoch': 23.08}
{'loss': 0.0001, 'learning_rate': 1e-05, 'epoch': 25.0}
{'loss': 0.0001, 'learning_rate': 1e-05, 'epoch': 26.92}
{'loss': 0.0001, 'learning_rate': 1e-05, 'epoch': 28.85}
{'eval_loss': 3.538344753906131e-05, 'eval_runtime': 3.9856, 'eval_samples_per_second': 103.875, 'eval_steps_per_second': 13.047, 'epoch': 28.85}
{'loss': 0.0, 'learning_rate': 1e-05, 'epoch': 30.77}
{'loss': 0.0, 'learning_rate': 1e-05, 'epoch': 32.69}
{'loss': 0.0, 'learning_rate': 1e-05, 'epoch': 34.62}
{'loss': 0.0, 'learning_rate': 1e-05, 'epoch': 36.54}
{'loss': 0.0, 'learning_rate': 1e-05, 'epoch': 38.46}
{'eval_loss': 2.1187335732975043e-05, 'eval_runtime': 6.2325, 'eval_samples_per_second': 66.425, 'eval_steps_per_second': 8.343, 'epoch': 38.46}
{'loss': 0.0, 'learning_rate': 1e-05, 'epoch': 40.38}
{'loss': 0.0, 'learning_rate': 1e-05, 'epoch': 42.31}
{'loss': 0.0, 'learning_rate': 1e-05, 'epoch': 44.23}
{'loss': 0.0, 'learning_rate': 1e-05, 'epoch': 46.15}
{'loss': 0.0, 'learning_rate': 1e-05, 'epoch': 48.08}
{'eval_loss': 1.3958901035948656e-05, 'eval_runtime': 6.2338, 'eval_samples_per_second': 66.412, 'eval_steps_per_second': 8.342, 'epoch': 48.08}
{'loss': 0.0, 'learning_rate': 1e-05, 'epoch': 50.0}
{'loss': 0.0, 'learning_rate': 1e-05, 'epoch': 51.92}
{'loss': 0.0, 'learning_rate': 1e-05, 'epoch': 53.85}
{'loss': 0.0, 'learning_rate': 1e-05, 'epoch': 55.77}
{'loss': 0.0, 'learning_rate': 1e-05, 'epoch': 57.69}
{'eval_loss': 9.728177246870473e-06, 'eval_runtime': 6.3006, 'eval_samples_per_second': 65.708, 'eval_steps_per_second': 8.253, 'epoch': 57.69}
{'loss': 0.0, 'learning_rate': 1e-05, 'epoch': 59.62}
{'loss': 0.0, 'learning_rate': 1e-05, 'epoch': 61.54}
{'loss': 0.0, 'learning_rate': 1e-05, 'epoch': 63.46}
{'loss': 0.0, 'learning_rate': 1e-05, 'epoch': 65.38}
{'loss': 0.0, 'learning_rate': 1e-05, 'epoch': 67.31}
{'eval_loss': 7.03044270267128e-06, 'eval_runtime': 6.2263, 'eval_samples_per_second': 66.492, 'eval_steps_per_second': 8.352, 'epoch': 67.31}
{'loss': 0.0, 'learning_rate': 1e-05, 'epoch': 69.23}
{'loss': 0.0, 'learning_rate': 1e-05, 'epoch': 71.15}
{'loss': 0.0, 'learning_rate': 1e-05, 'epoch': 73.08}
{'loss': 0.0, 'learning_rate': 1e-05, 'epoch': 75.0}
{'loss': 0.0, 'learning_rate': 1e-05, 'epoch': 76.92}
{'eval_loss': 5.206898094911594e-06, 'eval_runtime': 6.2292, 'eval_samples_per_second': 66.461, 'eval_steps_per_second': 8.348, 'epoch': 76.92}
{'loss': 0.0, 'learning_rate': 1e-05, 'epoch': 78.85}
{'loss': 0.0, 'learning_rate': 1e-05, 'epoch': 80.77}
{'loss': 0.0, 'learning_rate': 1e-05, 'epoch': 82.69}
{'loss': 0.0, 'learning_rate': 1e-05, 'epoch': 84.62}
{'loss': 0.0, 'learning_rate': 1e-05, 'epoch': 86.54}
{'eval_loss': 3.918061793228844e-06, 'eval_runtime': 6.2322, 'eval_samples_per_second': 66.429, 'eval_steps_per_second': 8.344, 'epoch': 86.54}
{'loss': 0.0, 'learning_rate': 1e-05, 'epoch': 88.46}
{'loss': 0.0, 'learning_rate': 1e-05, 'epoch': 90.38}
{'loss': 0.0, 'learning_rate': 1e-05, 'epoch': 92.31}
{'loss': 0.0, 'learning_rate': 1e-05, 'epoch': 94.23}
{'loss': 0.0, 'learning_rate': 1e-05, 'epoch': 96.15}
{'eval_loss': 2.977636313516996e-06, 'eval_runtime': 6.2234, 'eval_samples_per_second': 66.523, 'eval_steps_per_second': 8.356, 'epoch': 96.15}
{'loss': 0.0, 'learning_rate': 1e-05, 'epoch': 98.08}
{'loss': 0.0, 'learning_rate': 1e-05, 'epoch': 100.0}
{'train_runtime': 1688.46, 'train_samples_per_second': 24.519, 'train_steps_per_second': 3.08, 'train_loss': 0.005759951748237095, 'epoch': 100.0}
Some weights of the model checkpoint at ../model/bert_FinBERT were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ../model/bert_FinBERT and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
{'loss': 0.2704, 'learning_rate': 9.807692307692308e-06, 'epoch': 1.92}
{'loss': 0.0448, 'learning_rate': 9.615384615384616e-06, 'epoch': 3.85}
{'loss': 0.0012, 'learning_rate': 9.423076923076923e-06, 'epoch': 5.77}
{'loss': 0.0005, 'learning_rate': 9.230769230769232e-06, 'epoch': 7.69}
{'loss': 0.0004, 'learning_rate': 9.03846153846154e-06, 'epoch': 9.62}
{'eval_loss': 0.00022183980036061257, 'eval_runtime': 6.215, 'eval_samples_per_second': 66.613, 'eval_steps_per_second': 8.367, 'epoch': 9.62}
{'loss': 0.0003, 'learning_rate': 8.846153846153847e-06, 'epoch': 11.54}
{'loss': 0.0002, 'learning_rate': 8.653846153846155e-06, 'epoch': 13.46}
{'loss': 0.0002, 'learning_rate': 8.461538461538462e-06, 'epoch': 15.38}
{'loss': 0.0001, 'learning_rate': 8.26923076923077e-06, 'epoch': 17.31}
{'loss': 0.0001, 'learning_rate': 8.076923076923077e-06, 'epoch': 19.23}
{'eval_loss': 7.706851465627551e-05, 'eval_runtime': 6.2223, 'eval_samples_per_second': 66.534, 'eval_steps_per_second': 8.357, 'epoch': 19.23}
{'loss': 0.0001, 'learning_rate': 7.884615384615384e-06, 'epoch': 21.15}
{'loss': 0.0001, 'learning_rate': 7.692307692307694e-06, 'epoch': 23.08}
{'loss': 0.0001, 'learning_rate': 7.500000000000001e-06, 'epoch': 25.0}
{'loss': 0.0001, 'learning_rate': 7.307692307692308e-06, 'epoch': 26.92}
{'loss': 0.0001, 'learning_rate': 7.115384615384616e-06, 'epoch': 28.85}
{'eval_loss': 4.260283458279446e-05, 'eval_runtime': 6.2329, 'eval_samples_per_second': 66.422, 'eval_steps_per_second': 8.343, 'epoch': 28.85}
{'loss': 0.0001, 'learning_rate': 6.923076923076923e-06, 'epoch': 30.77}
{'loss': 0.0001, 'learning_rate': 6.730769230769232e-06, 'epoch': 32.69}
{'loss': 0.0, 'learning_rate': 6.538461538461539e-06, 'epoch': 34.62}
{'loss': 0.0, 'learning_rate': 6.3461538461538466e-06, 'epoch': 36.54}
{'loss': 0.0, 'learning_rate': 6.153846153846155e-06, 'epoch': 38.46}
{'eval_loss': 2.8279542675591074e-05, 'eval_runtime': 6.2285, 'eval_samples_per_second': 66.469, 'eval_steps_per_second': 8.349, 'epoch': 38.46}
{'loss': 0.0, 'learning_rate': 5.961538461538462e-06, 'epoch': 40.38}
{'loss': 0.0, 'learning_rate': 5.769230769230769e-06, 'epoch': 42.31}
{'loss': 0.0, 'learning_rate': 5.576923076923077e-06, 'epoch': 44.23}
{'loss': 0.0, 'learning_rate': 5.384615384615385e-06, 'epoch': 46.15}
{'loss': 0.0, 'learning_rate': 5.192307692307693e-06, 'epoch': 48.08}
{'eval_loss': 2.0531133486656472e-05, 'eval_runtime': 6.2326, 'eval_samples_per_second': 66.424, 'eval_steps_per_second': 8.343, 'epoch': 48.08}
{'loss': 0.0, 'learning_rate': 5e-06, 'epoch': 50.0}
{'loss': 0.0, 'learning_rate': 4.807692307692308e-06, 'epoch': 51.92}
{'loss': 0.0, 'learning_rate': 4.615384615384616e-06, 'epoch': 53.85}
{'loss': 0.0, 'learning_rate': 4.423076923076924e-06, 'epoch': 55.77}
{'loss': 0.0, 'learning_rate': 4.230769230769231e-06, 'epoch': 57.69}
{'eval_loss': 1.5920068108243868e-05, 'eval_runtime': 6.2234, 'eval_samples_per_second': 66.523, 'eval_steps_per_second': 8.356, 'epoch': 57.69}
{'loss': 0.0, 'learning_rate': 4.0384615384615385e-06, 'epoch': 59.62}
{'loss': 0.0, 'learning_rate': 3.846153846153847e-06, 'epoch': 61.54}
{'loss': 0.0, 'learning_rate': 3.653846153846154e-06, 'epoch': 63.46}
{'loss': 0.0, 'learning_rate': 3.4615384615384617e-06, 'epoch': 65.38}
{'loss': 0.0, 'learning_rate': 3.2692307692307696e-06, 'epoch': 67.31}
{'eval_loss': 1.2964354027644731e-05, 'eval_runtime': 6.2396, 'eval_samples_per_second': 66.351, 'eval_steps_per_second': 8.334, 'epoch': 67.31}
{'loss': 0.0, 'learning_rate': 3.0769230769230774e-06, 'epoch': 69.23}
{'loss': 0.0, 'learning_rate': 2.8846153846153845e-06, 'epoch': 71.15}
{'loss': 0.0, 'learning_rate': 2.6923076923076923e-06, 'epoch': 73.08}
{'loss': 0.0, 'learning_rate': 2.5e-06, 'epoch': 75.0}
{'loss': 0.0, 'learning_rate': 2.307692307692308e-06, 'epoch': 76.92}
{'eval_loss': 1.1063651072618086e-05, 'eval_runtime': 6.2242, 'eval_samples_per_second': 66.515, 'eval_steps_per_second': 8.354, 'epoch': 76.92}
{'loss': 0.0, 'learning_rate': 2.1153846153846155e-06, 'epoch': 78.85}
{'loss': 0.0, 'learning_rate': 1.9230769230769234e-06, 'epoch': 80.77}
{'loss': 0.0, 'learning_rate': 1.7307692307692308e-06, 'epoch': 82.69}
{'loss': 0.0, 'learning_rate': 1.5384615384615387e-06, 'epoch': 84.62}
{'loss': 0.0, 'learning_rate': 1.3461538461538462e-06, 'epoch': 86.54}
{'eval_loss': 9.858616067504045e-06, 'eval_runtime': 6.2338, 'eval_samples_per_second': 66.413, 'eval_steps_per_second': 8.342, 'epoch': 86.54}
{'loss': 0.0, 'learning_rate': 1.153846153846154e-06, 'epoch': 88.46}
{'loss': 0.0, 'learning_rate': 9.615384615384617e-07, 'epoch': 90.38}
{'loss': 0.0, 'learning_rate': 7.692307692307694e-07, 'epoch': 92.31}
{'loss': 0.0, 'learning_rate': 5.76923076923077e-07, 'epoch': 94.23}
{'loss': 0.0, 'learning_rate': 3.846153846153847e-07, 'epoch': 96.15}
{'eval_loss': 9.254800716007594e-06, 'eval_runtime': 6.2219, 'eval_samples_per_second': 66.539, 'eval_steps_per_second': 8.358, 'epoch': 96.15}
{'loss': 0.0, 'learning_rate': 1.9230769230769234e-07, 'epoch': 98.08}
{'loss': 0.0, 'learning_rate': 0.0, 'epoch': 100.0}
{'train_runtime': 1979.0778, 'train_samples_per_second': 20.919, 'train_steps_per_second': 2.627, 'train_loss': 0.006142826007361202, 'epoch': 100.0}
Some weights of the model checkpoint at ../model/bert_FinBERT were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ../model/bert_FinBERT and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
{'loss': 0.275, 'learning_rate': 9.990877771116588e-06, 'epoch': 1.92}
{'loss': 0.0466, 'learning_rate': 9.96354437049027e-06, 'epoch': 3.85}
{'loss': 0.0135, 'learning_rate': 9.91809953473572e-06, 'epoch': 5.77}
{'loss': 0.0005, 'learning_rate': 9.854709087130261e-06, 'epoch': 7.69}
{'loss': 0.0003, 'learning_rate': 9.77360433254273e-06, 'epoch': 9.62}
{'eval_loss': 0.00015313697804231197, 'eval_runtime': 6.2233, 'eval_samples_per_second': 66.524, 'eval_steps_per_second': 8.356, 'epoch': 9.62}
{'loss': 0.0002, 'learning_rate': 9.675081213427076e-06, 'epoch': 11.54}
{'loss': 0.0002, 'learning_rate': 9.55949922996045e-06, 'epoch': 13.46}
{'loss': 0.0001, 'learning_rate': 9.427280128266049e-06, 'epoch': 15.38}
{'loss': 0.0001, 'learning_rate': 9.278906361507238e-06, 'epoch': 17.31}
{'loss': 0.0001, 'learning_rate': 9.114919329468283e-06, 'epoch': 19.23}
{'eval_loss': 5.341903306543827e-05, 'eval_runtime': 6.2394, 'eval_samples_per_second': 66.353, 'eval_steps_per_second': 8.334, 'epoch': 19.23}
{'loss': 0.0001, 'learning_rate': 8.935917403045251e-06, 'epoch': 21.15}
{'loss': 0.0001, 'learning_rate': 8.742553740855507e-06, 'epoch': 23.08}
{'loss': 0.0001, 'learning_rate': 8.535533905932739e-06, 'epoch': 25.0}
{'loss': 0.0001, 'learning_rate': 8.315613291203977e-06, 'epoch': 26.92}
{'loss': 0.0, 'learning_rate': 8.083594363142717e-06, 'epoch': 28.85}
{'eval_loss': 2.9785414881189354e-05, 'eval_runtime': 6.234, 'eval_samples_per_second': 66.409, 'eval_steps_per_second': 8.341, 'epoch': 28.85}
{'loss': 0.0, 'learning_rate': 7.84032373365578e-06, 'epoch': 30.77}
{'loss': 0.0, 'learning_rate': 7.586689070888284e-06, 'epoch': 32.69}
{'loss': 0.0, 'learning_rate': 7.323615860218844e-06, 'epoch': 34.62}
{'loss': 0.0, 'learning_rate': 7.052064027263785e-06, 'epoch': 36.54}
{'loss': 0.0, 'learning_rate': 6.773024435212678e-06, 'epoch': 38.46}
{'eval_loss': 2.038312413787935e-05, 'eval_runtime': 6.2305, 'eval_samples_per_second': 66.447, 'eval_steps_per_second': 8.346, 'epoch': 38.46}
{'loss': 0.0, 'learning_rate': 6.487515269276015e-06, 'epoch': 40.38}
{'loss': 0.0, 'learning_rate': 6.1965783214377895e-06, 'epoch': 42.31}
{'loss': 0.0, 'learning_rate': 5.90127518906953e-06, 'epoch': 44.23}
{'loss': 0.0, 'learning_rate': 5.6026834012766155e-06, 'epoch': 46.15}
{'loss': 0.0, 'learning_rate': 5.301892487111431e-06, 'epoch': 48.08}
{'eval_loss': 1.5495925254072063e-05, 'eval_runtime': 6.2417, 'eval_samples_per_second': 66.329, 'eval_steps_per_second': 8.331, 'epoch': 48.08}
{'loss': 0.0, 'learning_rate': 5e-06, 'epoch': 50.0}
{'loss': 0.0, 'learning_rate': 4.69810751288857e-06, 'epoch': 51.92}
{'loss': 0.0, 'learning_rate': 4.397316598723385e-06, 'epoch': 53.85}
{'loss': 0.0, 'learning_rate': 4.098724810930472e-06, 'epoch': 55.77}
{'loss': 0.0, 'learning_rate': 3.803421678562213e-06, 'epoch': 57.69}
{'eval_loss': 1.2574478205351625e-05, 'eval_runtime': 6.2399, 'eval_samples_per_second': 66.347, 'eval_steps_per_second': 8.333, 'epoch': 57.69}
{'loss': 0.0, 'learning_rate': 3.5124847307239863e-06, 'epoch': 59.62}
{'loss': 0.0, 'learning_rate': 3.226975564787322e-06, 'epoch': 61.54}
{'loss': 0.0, 'learning_rate': 2.947935972736217e-06, 'epoch': 63.46}
{'loss': 0.0, 'learning_rate': 2.6763841397811576e-06, 'epoch': 65.38}
{'loss': 0.0, 'learning_rate': 2.4133109291117156e-06, 'epoch': 67.31}
{'eval_loss': 1.0871305676118936e-05, 'eval_runtime': 6.2262, 'eval_samples_per_second': 66.493, 'eval_steps_per_second': 8.352, 'epoch': 67.31}
{'loss': 0.0, 'learning_rate': 2.159676266344222e-06, 'epoch': 69.23}
{'loss': 0.0, 'learning_rate': 1.9164056368572847e-06, 'epoch': 71.15}
{'loss': 0.0, 'learning_rate': 1.6843867087960252e-06, 'epoch': 73.08}
{'loss': 0.0, 'learning_rate': 1.4644660940672628e-06, 'epoch': 75.0}
{'loss': 0.0, 'learning_rate': 1.257446259144494e-06, 'epoch': 76.92}
{'eval_loss': 9.88107240118552e-06, 'eval_runtime': 6.2306, 'eval_samples_per_second': 66.447, 'eval_steps_per_second': 8.346, 'epoch': 76.92}
{'loss': 0.0, 'learning_rate': 1.0640825969547498e-06, 'epoch': 78.85}
{'loss': 0.0, 'learning_rate': 8.850806705317183e-07, 'epoch': 80.77}
{'loss': 0.0, 'learning_rate': 7.210936384927631e-07, 'epoch': 82.69}
{'loss': 0.0, 'learning_rate': 5.727198717339511e-07, 'epoch': 84.62}
{'loss': 0.0, 'learning_rate': 4.405007700395497e-07, 'epoch': 86.54}
{'eval_loss': 9.397617759532295e-06, 'eval_runtime': 6.2353, 'eval_samples_per_second': 66.397, 'eval_steps_per_second': 8.34, 'epoch': 86.54}
{'loss': 0.0, 'learning_rate': 3.2491878657292643e-07, 'epoch': 88.46}
{'loss': 0.0, 'learning_rate': 2.2639566745727203e-07, 'epoch': 90.38}
{'loss': 0.0, 'learning_rate': 1.4529091286973994e-07, 'epoch': 92.31}
{'loss': 0.0, 'learning_rate': 8.190046526428241e-08, 'epoch': 94.23}
{'loss': 0.0, 'learning_rate': 3.645562950973014e-08, 'epoch': 96.15}
{'eval_loss': 9.257965757569764e-06, 'eval_runtime': 6.2298, 'eval_samples_per_second': 66.454, 'eval_steps_per_second': 8.347, 'epoch': 96.15}
{'loss': 0.0, 'learning_rate': 9.12222888341252e-09, 'epoch': 98.08}
{'loss': 0.0, 'learning_rate': 0.0, 'epoch': 100.0}
{'train_runtime': 1980.0217, 'train_samples_per_second': 20.909, 'train_steps_per_second': 2.626, 'train_loss': 0.006493571467379717, 'epoch': 100.0}
Some weights of the model checkpoint at ../model/bert_FinBERT were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ../model/bert_FinBERT and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
{'loss': 0.2668, 'learning_rate': 9.990877771116588e-06, 'epoch': 1.92}
{'loss': 0.0702, 'learning_rate': 9.96354437049027e-06, 'epoch': 3.85}
{'loss': 0.0097, 'learning_rate': 9.91809953473572e-06, 'epoch': 5.77}
{'loss': 0.0093, 'learning_rate': 9.854709087130261e-06, 'epoch': 7.69}
{'loss': 0.0004, 'learning_rate': 9.77360433254273e-06, 'epoch': 9.62}
{'eval_loss': 0.0002433751942589879, 'eval_runtime': 6.2275, 'eval_samples_per_second': 66.479, 'eval_steps_per_second': 8.35, 'epoch': 9.62}
{'loss': 0.0005, 'learning_rate': 9.675081213427076e-06, 'epoch': 11.54}
{'loss': 0.0003, 'learning_rate': 9.55949922996045e-06, 'epoch': 13.46}
{'loss': 0.0002, 'learning_rate': 9.427280128266049e-06, 'epoch': 15.38}
{'loss': 0.0001, 'learning_rate': 9.278906361507238e-06, 'epoch': 17.31}
{'loss': 0.0002, 'learning_rate': 9.114919329468283e-06, 'epoch': 19.23}
{'eval_loss': 7.384453783743083e-05, 'eval_runtime': 6.237, 'eval_samples_per_second': 66.378, 'eval_steps_per_second': 8.337, 'epoch': 19.23}
{'loss': 0.0001, 'learning_rate': 8.935917403045251e-06, 'epoch': 21.15}
{'loss': 0.0001, 'learning_rate': 8.742553740855507e-06, 'epoch': 23.08}
{'loss': 0.0001, 'learning_rate': 8.535533905932739e-06, 'epoch': 25.0}
{'loss': 0.0001, 'learning_rate': 8.315613291203977e-06, 'epoch': 26.92}
{'loss': 0.0001, 'learning_rate': 8.083594363142717e-06, 'epoch': 28.85}
{'eval_loss': 3.8323832995956764e-05, 'eval_runtime': 6.2361, 'eval_samples_per_second': 66.387, 'eval_steps_per_second': 8.339, 'epoch': 28.85}
{'loss': 0.0001, 'learning_rate': 7.84032373365578e-06, 'epoch': 30.77}
{'loss': 0.0001, 'learning_rate': 7.586689070888284e-06, 'epoch': 32.69}
{'loss': 0.0, 'learning_rate': 7.323615860218844e-06, 'epoch': 34.62}
{'loss': 0.0, 'learning_rate': 7.052064027263785e-06, 'epoch': 36.54}
{'loss': 0.0, 'learning_rate': 6.773024435212678e-06, 'epoch': 38.46}
{'eval_loss': 2.4426639356533997e-05, 'eval_runtime': 6.2376, 'eval_samples_per_second': 66.372, 'eval_steps_per_second': 8.337, 'epoch': 38.46}
{'loss': 0.0, 'learning_rate': 6.487515269276015e-06, 'epoch': 40.38}
{'loss': 0.0, 'learning_rate': 6.1965783214377895e-06, 'epoch': 42.31}
{'loss': 0.0, 'learning_rate': 5.90127518906953e-06, 'epoch': 44.23}
{'loss': 0.0, 'learning_rate': 5.6026834012766155e-06, 'epoch': 46.15}
{'loss': 0.0, 'learning_rate': 5.301892487111431e-06, 'epoch': 48.08}
{'eval_loss': 1.7702697732602246e-05, 'eval_runtime': 6.221, 'eval_samples_per_second': 66.549, 'eval_steps_per_second': 8.359, 'epoch': 48.08}
{'loss': 0.0, 'learning_rate': 5e-06, 'epoch': 50.0}
{'loss': 0.0, 'learning_rate': 4.69810751288857e-06, 'epoch': 51.92}
{'loss': 0.0, 'learning_rate': 4.397316598723385e-06, 'epoch': 53.85}
{'loss': 0.0, 'learning_rate': 4.098724810930472e-06, 'epoch': 55.77}
{'loss': 0.0, 'learning_rate': 3.803421678562213e-06, 'epoch': 57.69}
{'eval_loss': 1.3892961760575417e-05, 'eval_runtime': 6.2288, 'eval_samples_per_second': 66.466, 'eval_steps_per_second': 8.348, 'epoch': 57.69}
{'loss': 0.0, 'learning_rate': 3.5124847307239863e-06, 'epoch': 59.62}
{'loss': 0.0, 'learning_rate': 3.226975564787322e-06, 'epoch': 61.54}
{'loss': 0.0, 'learning_rate': 2.947935972736217e-06, 'epoch': 63.46}
{'loss': 0.0, 'learning_rate': 2.6763841397811576e-06, 'epoch': 65.38}
{'loss': 0.0, 'learning_rate': 2.4133109291117156e-06, 'epoch': 67.31}
{'eval_loss': 1.1745496522053145e-05, 'eval_runtime': 6.2383, 'eval_samples_per_second': 66.364, 'eval_steps_per_second': 8.336, 'epoch': 67.31}
{'loss': 0.0, 'learning_rate': 2.159676266344222e-06, 'epoch': 69.23}
{'loss': 0.0, 'learning_rate': 1.9164056368572847e-06, 'epoch': 71.15}
{'loss': 0.0, 'learning_rate': 1.6843867087960252e-06, 'epoch': 73.08}
{'loss': 0.0, 'learning_rate': 1.4644660940672628e-06, 'epoch': 75.0}
{'loss': 0.0, 'learning_rate': 1.257446259144494e-06, 'epoch': 76.92}
{'eval_loss': 1.0545355507929344e-05, 'eval_runtime': 6.232, 'eval_samples_per_second': 66.432, 'eval_steps_per_second': 8.344, 'epoch': 76.92}
{'loss': 0.0, 'learning_rate': 1.0640825969547498e-06, 'epoch': 78.85}
{'loss': 0.0, 'learning_rate': 8.850806705317183e-07, 'epoch': 80.77}
{'loss': 0.0, 'learning_rate': 7.210936384927631e-07, 'epoch': 82.69}
{'loss': 0.0, 'learning_rate': 5.727198717339511e-07, 'epoch': 84.62}
{'loss': 0.0, 'learning_rate': 4.405007700395497e-07, 'epoch': 86.54}
{'eval_loss': 9.968031008611433e-06, 'eval_runtime': 6.2212, 'eval_samples_per_second': 66.546, 'eval_steps_per_second': 8.358, 'epoch': 86.54}
{'loss': 0.0, 'learning_rate': 3.2491878657292643e-07, 'epoch': 88.46}
{'loss': 0.0, 'learning_rate': 2.2639566745727203e-07, 'epoch': 90.38}
{'loss': 0.0, 'learning_rate': 1.4529091286973994e-07, 'epoch': 92.31}
{'loss': 0.0, 'learning_rate': 8.190046526428241e-08, 'epoch': 94.23}
{'loss': 0.0, 'learning_rate': 3.645562950973014e-08, 'epoch': 96.15}
{'eval_loss': 9.802176464290824e-06, 'eval_runtime': 6.2333, 'eval_samples_per_second': 66.417, 'eval_steps_per_second': 8.342, 'epoch': 96.15}
{'loss': 0.0, 'learning_rate': 9.12222888341252e-09, 'epoch': 98.08}
{'loss': 0.0, 'learning_rate': 0.0, 'epoch': 100.0}
{'train_runtime': 1980.6456, 'train_samples_per_second': 20.902, 'train_steps_per_second': 2.625, 'train_loss': 0.006902309744865096, 'epoch': 100.0}
Some weights of the model checkpoint at ../model/bert_FinBERT were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ../model/bert_FinBERT and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
{'loss': 0.2782, 'learning_rate': 9.809615384615385e-06, 'epoch': 1.92}
{'loss': 0.06, 'learning_rate': 9.61923076923077e-06, 'epoch': 3.85}
{'loss': 0.0016, 'learning_rate': 9.428846153846155e-06, 'epoch': 5.77}
{'loss': 0.0005, 'learning_rate': 9.23846153846154e-06, 'epoch': 7.69}
{'loss': 0.0004, 'learning_rate': 9.048076923076924e-06, 'epoch': 9.62}
{'eval_loss': 0.0002079959522234276, 'eval_runtime': 6.2335, 'eval_samples_per_second': 66.415, 'eval_steps_per_second': 8.342, 'epoch': 9.62}
{'loss': 0.0003, 'learning_rate': 8.857692307692309e-06, 'epoch': 11.54}
{'loss': 0.0002, 'learning_rate': 8.667307692307694e-06, 'epoch': 13.46}
{'loss': 0.0002, 'learning_rate': 8.476923076923078e-06, 'epoch': 15.38}
{'loss': 0.0001, 'learning_rate': 8.286538461538461e-06, 'epoch': 17.31}
{'loss': 0.0001, 'learning_rate': 8.096153846153848e-06, 'epoch': 19.23}
{'eval_loss': 7.420154724968597e-05, 'eval_runtime': 6.231, 'eval_samples_per_second': 66.442, 'eval_steps_per_second': 8.345, 'epoch': 19.23}
{'loss': 0.0001, 'learning_rate': 7.90576923076923e-06, 'epoch': 21.15}
{'loss': 0.0001, 'learning_rate': 7.715384615384615e-06, 'epoch': 23.08}
{'loss': 0.0001, 'learning_rate': 7.525e-06, 'epoch': 25.0}
{'loss': 0.0001, 'learning_rate': 7.3346153846153855e-06, 'epoch': 26.92}
{'loss': 0.0001, 'learning_rate': 7.1442307692307685e-06, 'epoch': 28.85}
{'eval_loss': 4.140385863138363e-05, 'eval_runtime': 6.2379, 'eval_samples_per_second': 66.368, 'eval_steps_per_second': 8.336, 'epoch': 28.85}
{'loss': 0.0001, 'learning_rate': 6.953846153846154e-06, 'epoch': 30.77}
{'loss': 0.0001, 'learning_rate': 6.763461538461539e-06, 'epoch': 32.69}
{'loss': 0.0, 'learning_rate': 6.573076923076923e-06, 'epoch': 34.62}
{'loss': 0.0, 'learning_rate': 6.382692307692307e-06, 'epoch': 36.54}
{'loss': 0.0, 'learning_rate': 6.192307692307693e-06, 'epoch': 38.46}
{'eval_loss': 2.737656541285105e-05, 'eval_runtime': 6.2264, 'eval_samples_per_second': 66.491, 'eval_steps_per_second': 8.352, 'epoch': 38.46}
{'loss': 0.0, 'learning_rate': 6.0019230769230765e-06, 'epoch': 40.38}
{'loss': 0.0, 'learning_rate': 5.81153846153846e-06, 'epoch': 42.31}
{'loss': 0.0, 'learning_rate': 5.621153846153846e-06, 'epoch': 44.23}
{'loss': 0.0, 'learning_rate': 5.4307692307692306e-06, 'epoch': 46.15}
{'loss': 0.0, 'learning_rate': 5.240384615384615e-06, 'epoch': 48.08}
{'eval_loss': 2.0019751900690608e-05, 'eval_runtime': 6.2303, 'eval_samples_per_second': 66.449, 'eval_steps_per_second': 8.346, 'epoch': 48.08}
{'loss': 0.0, 'learning_rate': 5.050000000000001e-06, 'epoch': 50.0}
{'loss': 0.0, 'learning_rate': 4.859615384615384e-06, 'epoch': 51.92}
{'loss': 0.0, 'learning_rate': 4.669230769230769e-06, 'epoch': 53.85}
{'loss': 0.0, 'learning_rate': 4.478846153846154e-06, 'epoch': 55.77}
{'loss': 0.0, 'learning_rate': 4.288461538461539e-06, 'epoch': 57.69}
{'eval_loss': 1.5564748537144624e-05, 'eval_runtime': 6.234, 'eval_samples_per_second': 66.41, 'eval_steps_per_second': 8.341, 'epoch': 57.69}
{'loss': 0.0, 'learning_rate': 4.098076923076923e-06, 'epoch': 59.62}
{'loss': 0.0, 'learning_rate': 3.907692307692307e-06, 'epoch': 61.54}
{'loss': 0.0, 'learning_rate': 3.7173076923076927e-06, 'epoch': 63.46}
{'loss': 0.0, 'learning_rate': 3.5269230769230765e-06, 'epoch': 65.38}
{'loss': 0.0, 'learning_rate': 3.3365384615384603e-06, 'epoch': 67.31}
{'eval_loss': 1.2713559044641443e-05, 'eval_runtime': 6.234, 'eval_samples_per_second': 66.409, 'eval_steps_per_second': 8.341, 'epoch': 67.31}
{'loss': 0.0, 'learning_rate': 3.1461538461538467e-06, 'epoch': 69.23}
{'loss': 0.0, 'learning_rate': 2.9557692307692306e-06, 'epoch': 71.15}
{'loss': 0.0, 'learning_rate': 2.765384615384616e-06, 'epoch': 73.08}
{'loss': 0.0, 'learning_rate': 2.575e-06, 'epoch': 75.0}
{'loss': 0.0, 'learning_rate': 2.384615384615384e-06, 'epoch': 76.92}
{'eval_loss': 1.0816310350492131e-05, 'eval_runtime': 6.2236, 'eval_samples_per_second': 66.521, 'eval_steps_per_second': 8.355, 'epoch': 76.92}
{'loss': 0.0, 'learning_rate': 2.1942307692307693e-06, 'epoch': 78.85}
{'loss': 0.0, 'learning_rate': 2.0038461538461535e-06, 'epoch': 80.77}
{'loss': 0.0, 'learning_rate': 1.8134615384615388e-06, 'epoch': 82.69}
{'loss': 0.0, 'learning_rate': 1.6230769230769233e-06, 'epoch': 84.62}
{'loss': 0.0, 'learning_rate': 1.4326923076923073e-06, 'epoch': 86.54}
{'eval_loss': 9.613865586288739e-06, 'eval_runtime': 6.2348, 'eval_samples_per_second': 66.402, 'eval_steps_per_second': 8.34, 'epoch': 86.54}
{'loss': 0.0, 'learning_rate': 1.2423076923076927e-06, 'epoch': 88.46}
{'loss': 0.0, 'learning_rate': 1.051923076923077e-06, 'epoch': 90.38}
{'loss': 0.0, 'learning_rate': 8.615384615384611e-07, 'epoch': 92.31}
{'loss': 0.0, 'learning_rate': 6.711538461538463e-07, 'epoch': 94.23}
{'loss': 0.0, 'learning_rate': 4.807692307692305e-07, 'epoch': 96.15}
{'eval_loss': 8.980966413218994e-06, 'eval_runtime': 6.2389, 'eval_samples_per_second': 66.358, 'eval_steps_per_second': 8.335, 'epoch': 96.15}
{'loss': 0.0, 'learning_rate': 2.903846153846158e-07, 'epoch': 98.08}
{'loss': 0.0, 'learning_rate': 1e-07, 'epoch': 100.0}
{'train_runtime': 1979.0038, 'train_samples_per_second': 20.92, 'train_steps_per_second': 2.628, 'train_loss': 0.006591627017970985, 'epoch': 100.0}
Some weights of the model checkpoint at ../model/bert_FinBERT were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ../model/bert_FinBERT and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
{'loss': 0.2674, 'learning_rate': 1e-05, 'epoch': 1.92}
{'loss': 0.0471, 'learning_rate': 1e-05, 'epoch': 3.85}
{'loss': 0.0018, 'learning_rate': 1e-05, 'epoch': 5.77}
{'loss': 0.0005, 'learning_rate': 1e-05, 'epoch': 7.69}
{'loss': 0.0004, 'learning_rate': 1e-05, 'epoch': 9.62}
{'eval_loss': 0.00015355166397057474, 'eval_runtime': 6.226, 'eval_samples_per_second': 66.495, 'eval_steps_per_second': 8.352, 'epoch': 9.62}
{'loss': 0.0002, 'learning_rate': 1e-05, 'epoch': 11.54}
{'loss': 0.0001, 'learning_rate': 1e-05, 'epoch': 13.46}
{'loss': 0.0001, 'learning_rate': 1e-05, 'epoch': 15.38}
{'loss': 0.0001, 'learning_rate': 1e-05, 'epoch': 17.31}
{'loss': 0.0001, 'learning_rate': 1e-05, 'epoch': 19.23}
{'eval_loss': 5.454688653117046e-05, 'eval_runtime': 6.2274, 'eval_samples_per_second': 66.481, 'eval_steps_per_second': 8.35, 'epoch': 19.23}
{'loss': 0.0001, 'learning_rate': 1e-05, 'epoch': 21.15}
{'loss': 0.0001, 'learning_rate': 1e-05, 'epoch': 23.08}
{'loss': 0.0001, 'learning_rate': 1e-05, 'epoch': 25.0}
{'loss': 0.0001, 'learning_rate': 1e-05, 'epoch': 26.92}
{'loss': 0.0, 'learning_rate': 1e-05, 'epoch': 28.85}
{'eval_loss': 2.9167806133045815e-05, 'eval_runtime': 6.2329, 'eval_samples_per_second': 66.422, 'eval_steps_per_second': 8.343, 'epoch': 28.85}
{'loss': 0.0, 'learning_rate': 1e-05, 'epoch': 30.77}
{'loss': 0.0, 'learning_rate': 1e-05, 'epoch': 32.69}
{'loss': 0.0, 'learning_rate': 1e-05, 'epoch': 34.62}
{'loss': 0.0, 'learning_rate': 1e-05, 'epoch': 36.54}
{'loss': 0.0, 'learning_rate': 1e-05, 'epoch': 38.46}
{'eval_loss': 1.824028186092619e-05, 'eval_runtime': 6.2315, 'eval_samples_per_second': 66.437, 'eval_steps_per_second': 8.345, 'epoch': 38.46}
{'loss': 0.0, 'learning_rate': 1e-05, 'epoch': 40.38}
{'loss': 0.0, 'learning_rate': 1e-05, 'epoch': 42.31}
{'loss': 0.0, 'learning_rate': 1e-05, 'epoch': 44.23}
{'loss': 0.0, 'learning_rate': 1e-05, 'epoch': 46.15}
{'loss': 0.0, 'learning_rate': 1e-05, 'epoch': 48.08}
{'eval_loss': 1.2376953236525878e-05, 'eval_runtime': 6.226, 'eval_samples_per_second': 66.496, 'eval_steps_per_second': 8.352, 'epoch': 48.08}
{'loss': 0.0, 'learning_rate': 1e-05, 'epoch': 50.0}
{'loss': 0.0, 'learning_rate': 1e-05, 'epoch': 51.92}
{'loss': 0.0, 'learning_rate': 1e-05, 'epoch': 53.85}
{'loss': 0.0, 'learning_rate': 1e-05, 'epoch': 55.77}
{'loss': 0.0, 'learning_rate': 1e-05, 'epoch': 57.69}
{'eval_loss': 8.80503466760274e-06, 'eval_runtime': 6.234, 'eval_samples_per_second': 66.411, 'eval_steps_per_second': 8.341, 'epoch': 57.69}
{'loss': 0.0, 'learning_rate': 1e-05, 'epoch': 59.62}
{'loss': 0.0, 'learning_rate': 1e-05, 'epoch': 61.54}
{'loss': 0.0, 'learning_rate': 1e-05, 'epoch': 63.46}
{'loss': 0.0, 'learning_rate': 1e-05, 'epoch': 65.38}
{'loss': 0.0, 'learning_rate': 1e-05, 'epoch': 67.31}
{'eval_loss': 6.478455816250062e-06, 'eval_runtime': 6.2364, 'eval_samples_per_second': 66.384, 'eval_steps_per_second': 8.338, 'epoch': 67.31}
{'loss': 0.0, 'learning_rate': 1e-05, 'epoch': 69.23}
{'loss': 0.0, 'learning_rate': 1e-05, 'epoch': 71.15}
{'loss': 0.0, 'learning_rate': 1e-05, 'epoch': 73.08}
{'loss': 0.0, 'learning_rate': 1e-05, 'epoch': 75.0}
{'loss': 0.0, 'learning_rate': 1e-05, 'epoch': 76.92}
{'eval_loss': 4.887568593403557e-06, 'eval_runtime': 6.2279, 'eval_samples_per_second': 66.475, 'eval_steps_per_second': 8.35, 'epoch': 76.92}
{'loss': 0.0, 'learning_rate': 1e-05, 'epoch': 78.85}
{'loss': 0.0, 'learning_rate': 1e-05, 'epoch': 80.77}
{'loss': 0.0, 'learning_rate': 1e-05, 'epoch': 82.69}
{'loss': 0.0, 'learning_rate': 1e-05, 'epoch': 84.62}
{'loss': 0.0, 'learning_rate': 1e-05, 'epoch': 86.54}
{'eval_loss': 3.736944790944108e-06, 'eval_runtime': 6.2258, 'eval_samples_per_second': 66.498, 'eval_steps_per_second': 8.352, 'epoch': 86.54}
{'loss': 0.0, 'learning_rate': 1e-05, 'epoch': 88.46}
{'loss': 0.0, 'learning_rate': 1e-05, 'epoch': 90.38}
{'loss': 0.0, 'learning_rate': 1e-05, 'epoch': 92.31}
{'loss': 0.0, 'learning_rate': 1e-05, 'epoch': 94.23}
{'loss': 0.0, 'learning_rate': 1e-05, 'epoch': 96.15}
{'eval_loss': 2.7487203624332324e-06, 'eval_runtime': 6.226, 'eval_samples_per_second': 66.495, 'eval_steps_per_second': 8.352, 'epoch': 96.15}
{'loss': 0.0, 'learning_rate': 1e-05, 'epoch': 98.08}
{'loss': 0.0, 'learning_rate': 1e-05, 'epoch': 100.0}
{'train_runtime': 1978.8988, 'train_samples_per_second': 20.921, 'train_steps_per_second': 2.628, 'train_loss': 0.0061273958731908355, 'epoch': 100.0}
Traceback (most recent call last):
  File "/home/asus/intelligent-test/ours/sequence_classification.py", line 78, in <module>
    eval_model("../data//json/.json", saved_path, training_args)
  File "/home/asus/intelligent-test/ours/sequence_classification.py", line 42, in eval_model
    eval_dataset = read_json_for_sequence_classification(eval_dataset)
  File "/home/asus/intelligent-test/utils/data_loader.py", line 187, in read_json_for_sequence_classification
    ds = json.load(open(file, "r", encoding="utf-8"))
FileNotFoundError: [Errno 2] No such file or directory: '../data//json/.json'
