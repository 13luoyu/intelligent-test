04/02/2024 00:12:46 - WARNING - decoder_fine_tuning.log - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: False
04/02/2024 00:12:46 - INFO - decoder_fine_tuning.log - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=True,
bf16_full_eval=True,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=False,
ddp_timeout=18000000,
debug=[],
deepspeed=None,
disable_tqdm=True,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=100,
evaluation_strategy=steps,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=8,
gradient_checkpointing=True,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=True,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=1e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=./output/logs,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=10,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=20.0,
optim=adamw_torch,
optim_args=None,
output_dir=./output,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=1,
per_device_train_batch_size=1,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['tensorboard'],
resume_from_checkpoint=None,
run_name=./output,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=100,
save_strategy=steps,
save_total_limit=10,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=400,
weight_decay=0.0,
)
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/transformers/models/auto/configuration_auto.py:1096: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
[INFO|configuration_utils.py:726] 2024-04-02 00:12:46,897 >> loading configuration file ../model/pretrained/Atom-7B/config.json
[INFO|configuration_utils.py:791] 2024-04-02 00:12:46,898 >> Model config LlamaConfig {
  "_name_or_path": "../model/pretrained/Atom-7B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "auto_map": {
    "AutoConfig": "configuration_atom.LlamaConfig",
    "AutoModel": "model_atom.LlamaForCausalLM",
    "AutoModelForCausalLM": "model_atom.LlamaForCausalLM",
    "AutoModelForSequenceClassification": "model_atom.LlamaForSequenceClassification"
  },
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_length": 4096,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 2,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "type": "dynamic"
  },
  "rope_theta": 500000,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.38.2",
  "use_cache": true,
  "vocab_size": 65000
}

/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/transformers/models/auto/tokenization_auto.py:720: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
[INFO|tokenization_utils_base.py:2044] 2024-04-02 00:12:46,898 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:2044] 2024-04-02 00:12:46,898 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2044] 2024-04-02 00:12:46,898 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2044] 2024-04-02 00:12:46,898 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2044] 2024-04-02 00:12:46,898 >> loading file tokenizer.json
04/02/2024 00:12:46 - INFO - decoder_fine_tuning.log - torch_dtype: torch.float16
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py:466: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
[INFO|modeling_utils.py:3254] 2024-04-02 00:12:46,978 >> loading weights file ../model/pretrained/Atom-7B/model.safetensors.index.json
[INFO|modeling_utils.py:1400] 2024-04-02 00:12:46,978 >> Instantiating LlamaForCausalLM model under default dtype torch.float16.
[WARNING|logging.py:329] 2024-04-02 00:12:46,978 >> The model was loaded with use_flash_attention_2=True, which is deprecated and may be removed in a future release. Please use `attn_implementation="flash_attention_2"` instead.
[INFO|configuration_utils.py:845] 2024-04-02 00:12:46,980 >> Generate config GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2,
  "max_length": 4096,
  "pad_token_id": 2
}

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:01<00:03,  1.68s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:02<00:01,  1.35s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:03<00:00,  1.15s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:03<00:00,  1.24s/it]
[INFO|modeling_utils.py:3992] 2024-04-02 00:12:50,801 >> All model checkpoint weights were used when initializing LlamaForCausalLM.

[INFO|modeling_utils.py:4000] 2024-04-02 00:12:50,801 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at ../model/pretrained/Atom-7B.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.
[INFO|configuration_utils.py:798] 2024-04-02 00:12:50,802 >> loading configuration file ../model/pretrained/Atom-7B/generation_config.json
[INFO|configuration_utils.py:845] 2024-04-02 00:12:50,803 >> Generate config GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2,
  "max_length": 4096,
  "pad_token_id": 2
}

/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/datasets/load.py:2516: FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.
You can remove this warning by passing 'token=<use_auth_token>' instead.
  warnings.warn(
Using custom data configuration default-ac318976b409afc9
04/02/2024 00:12:52 - INFO - datasets.builder - Using custom data configuration default-ac318976b409afc9
Loading Dataset Infos from /home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/datasets/packaged_modules/csv
04/02/2024 00:12:52 - INFO - datasets.info - Loading Dataset Infos from /home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/datasets/packaged_modules/csv
Generating dataset csv (/home/asus/intelligent-test/decoder_fine_tuning/output/dataset_cache/csv/default-ac318976b409afc9/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6)
04/02/2024 00:12:52 - INFO - datasets.builder - Generating dataset csv (/home/asus/intelligent-test/decoder_fine_tuning/output/dataset_cache/csv/default-ac318976b409afc9/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6)
Downloading and preparing dataset csv/default to /home/asus/intelligent-test/decoder_fine_tuning/output/dataset_cache/csv/default-ac318976b409afc9/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6...
04/02/2024 00:12:52 - INFO - datasets.builder - Downloading and preparing dataset csv/default to /home/asus/intelligent-test/decoder_fine_tuning/output/dataset_cache/csv/default-ac318976b409afc9/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6...
Downloading took 0.0 min
04/02/2024 00:12:52 - INFO - datasets.download.download_manager - Downloading took 0.0 min
Checksum Computation took 0.0 min
04/02/2024 00:12:52 - INFO - datasets.download.download_manager - Checksum Computation took 0.0 min
Generating train split
04/02/2024 00:12:52 - INFO - datasets.builder - Generating train split
Generating train split: 0 examples [00:00, ? examples/s]Generating train split: 575 examples [00:00, 79937.85 examples/s]
Generating validation split
04/02/2024 00:12:52 - INFO - datasets.builder - Generating validation split
Generating validation split: 0 examples [00:00, ? examples/s]Generating validation split: 64 examples [00:00, 38763.24 examples/s]
Unable to verify splits sizes.
04/02/2024 00:12:52 - INFO - datasets.utils.info_utils - Unable to verify splits sizes.
Dataset csv downloaded and prepared to /home/asus/intelligent-test/decoder_fine_tuning/output/dataset_cache/csv/default-ac318976b409afc9/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6. Subsequent calls will reuse this data.
04/02/2024 00:12:52 - INFO - datasets.builder - Dataset csv downloaded and prepared to /home/asus/intelligent-test/decoder_fine_tuning/output/dataset_cache/csv/default-ac318976b409afc9/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6. Subsequent calls will reuse this data.
Process #0 will write at /home/asus/intelligent-test/decoder_fine_tuning/output/dataset_cache/csv/default-ac318976b409afc9/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-b5170c3771f3ef3d_00000_of_00010.arrow
数据集中是否输入输出在同一列: True
04/02/2024 00:12:52 - INFO - datasets.arrow_dataset - Process #0 will write at /home/asus/intelligent-test/decoder_fine_tuning/output/dataset_cache/csv/default-ac318976b409afc9/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-b5170c3771f3ef3d_00000_of_00010.arrow
Process #1 will write at /home/asus/intelligent-test/decoder_fine_tuning/output/dataset_cache/csv/default-ac318976b409afc9/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-b5170c3771f3ef3d_00001_of_00010.arrow
04/02/2024 00:12:52 - INFO - datasets.arrow_dataset - Process #1 will write at /home/asus/intelligent-test/decoder_fine_tuning/output/dataset_cache/csv/default-ac318976b409afc9/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-b5170c3771f3ef3d_00001_of_00010.arrow
Process #2 will write at /home/asus/intelligent-test/decoder_fine_tuning/output/dataset_cache/csv/default-ac318976b409afc9/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-b5170c3771f3ef3d_00002_of_00010.arrow
04/02/2024 00:12:52 - INFO - datasets.arrow_dataset - Process #2 will write at /home/asus/intelligent-test/decoder_fine_tuning/output/dataset_cache/csv/default-ac318976b409afc9/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-b5170c3771f3ef3d_00002_of_00010.arrow
Process #3 will write at /home/asus/intelligent-test/decoder_fine_tuning/output/dataset_cache/csv/default-ac318976b409afc9/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-b5170c3771f3ef3d_00003_of_00010.arrow
04/02/2024 00:12:52 - INFO - datasets.arrow_dataset - Process #3 will write at /home/asus/intelligent-test/decoder_fine_tuning/output/dataset_cache/csv/default-ac318976b409afc9/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-b5170c3771f3ef3d_00003_of_00010.arrow
Process #4 will write at /home/asus/intelligent-test/decoder_fine_tuning/output/dataset_cache/csv/default-ac318976b409afc9/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-b5170c3771f3ef3d_00004_of_00010.arrow
04/02/2024 00:12:52 - INFO - datasets.arrow_dataset - Process #4 will write at /home/asus/intelligent-test/decoder_fine_tuning/output/dataset_cache/csv/default-ac318976b409afc9/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-b5170c3771f3ef3d_00004_of_00010.arrow
Process #5 will write at /home/asus/intelligent-test/decoder_fine_tuning/output/dataset_cache/csv/default-ac318976b409afc9/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-b5170c3771f3ef3d_00005_of_00010.arrow
04/02/2024 00:12:52 - INFO - datasets.arrow_dataset - Process #5 will write at /home/asus/intelligent-test/decoder_fine_tuning/output/dataset_cache/csv/default-ac318976b409afc9/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-b5170c3771f3ef3d_00005_of_00010.arrow
Process #6 will write at /home/asus/intelligent-test/decoder_fine_tuning/output/dataset_cache/csv/default-ac318976b409afc9/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-b5170c3771f3ef3d_00006_of_00010.arrow
04/02/2024 00:12:52 - INFO - datasets.arrow_dataset - Process #6 will write at /home/asus/intelligent-test/decoder_fine_tuning/output/dataset_cache/csv/default-ac318976b409afc9/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-b5170c3771f3ef3d_00006_of_00010.arrow
Process #7 will write at /home/asus/intelligent-test/decoder_fine_tuning/output/dataset_cache/csv/default-ac318976b409afc9/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-b5170c3771f3ef3d_00007_of_00010.arrow
04/02/2024 00:12:52 - INFO - datasets.arrow_dataset - Process #7 will write at /home/asus/intelligent-test/decoder_fine_tuning/output/dataset_cache/csv/default-ac318976b409afc9/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-b5170c3771f3ef3d_00007_of_00010.arrow
Process #8 will write at /home/asus/intelligent-test/decoder_fine_tuning/output/dataset_cache/csv/default-ac318976b409afc9/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-b5170c3771f3ef3d_00008_of_00010.arrow
04/02/2024 00:12:52 - INFO - datasets.arrow_dataset - Process #8 will write at /home/asus/intelligent-test/decoder_fine_tuning/output/dataset_cache/csv/default-ac318976b409afc9/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-b5170c3771f3ef3d_00008_of_00010.arrow
Process #9 will write at /home/asus/intelligent-test/decoder_fine_tuning/output/dataset_cache/csv/default-ac318976b409afc9/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-b5170c3771f3ef3d_00009_of_00010.arrow
04/02/2024 00:12:52 - INFO - datasets.arrow_dataset - Process #9 will write at /home/asus/intelligent-test/decoder_fine_tuning/output/dataset_cache/csv/default-ac318976b409afc9/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-b5170c3771f3ef3d_00009_of_00010.arrow
Spawning 10 processes
04/02/2024 00:12:52 - INFO - datasets.arrow_dataset - Spawning 10 processes
Running tokenizer on dataset (num_proc=10):   0%|          | 0/575 [00:00<?, ? examples/s]Caching processed dataset at /home/asus/intelligent-test/decoder_fine_tuning/output/dataset_cache/csv/default-ac318976b409afc9/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-b5170c3771f3ef3d_00000_of_00010.arrow
04/02/2024 00:12:52 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/asus/intelligent-test/decoder_fine_tuning/output/dataset_cache/csv/default-ac318976b409afc9/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-b5170c3771f3ef3d_00000_of_00010.arrow
Caching processed dataset at /home/asus/intelligent-test/decoder_fine_tuning/output/dataset_cache/csv/default-ac318976b409afc9/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-b5170c3771f3ef3d_00001_of_00010.arrow
04/02/2024 00:12:52 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/asus/intelligent-test/decoder_fine_tuning/output/dataset_cache/csv/default-ac318976b409afc9/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-b5170c3771f3ef3d_00001_of_00010.arrow
Caching processed dataset at /home/asus/intelligent-test/decoder_fine_tuning/output/dataset_cache/csv/default-ac318976b409afc9/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-b5170c3771f3ef3d_00002_of_00010.arrow
04/02/2024 00:12:52 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/asus/intelligent-test/decoder_fine_tuning/output/dataset_cache/csv/default-ac318976b409afc9/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-b5170c3771f3ef3d_00002_of_00010.arrow
Caching processed dataset at /home/asus/intelligent-test/decoder_fine_tuning/output/dataset_cache/csv/default-ac318976b409afc9/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-b5170c3771f3ef3d_00003_of_00010.arrow
04/02/2024 00:12:52 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/asus/intelligent-test/decoder_fine_tuning/output/dataset_cache/csv/default-ac318976b409afc9/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-b5170c3771f3ef3d_00003_of_00010.arrow
Caching processed dataset at /home/asus/intelligent-test/decoder_fine_tuning/output/dataset_cache/csv/default-ac318976b409afc9/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-b5170c3771f3ef3d_00004_of_00010.arrow
04/02/2024 00:12:52 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/asus/intelligent-test/decoder_fine_tuning/output/dataset_cache/csv/default-ac318976b409afc9/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-b5170c3771f3ef3d_00004_of_00010.arrow
Running tokenizer on dataset (num_proc=10):  40%|████      | 232/575 [00:00<00:00, 2124.94 examples/s]Caching processed dataset at /home/asus/intelligent-test/decoder_fine_tuning/output/dataset_cache/csv/default-ac318976b409afc9/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-b5170c3771f3ef3d_00005_of_00010.arrow
04/02/2024 00:12:52 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/asus/intelligent-test/decoder_fine_tuning/output/dataset_cache/csv/default-ac318976b409afc9/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-b5170c3771f3ef3d_00005_of_00010.arrow
Caching processed dataset at /home/asus/intelligent-test/decoder_fine_tuning/output/dataset_cache/csv/default-ac318976b409afc9/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-b5170c3771f3ef3d_00006_of_00010.arrow
04/02/2024 00:12:52 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/asus/intelligent-test/decoder_fine_tuning/output/dataset_cache/csv/default-ac318976b409afc9/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-b5170c3771f3ef3d_00006_of_00010.arrow
Caching processed dataset at /home/asus/intelligent-test/decoder_fine_tuning/output/dataset_cache/csv/default-ac318976b409afc9/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-b5170c3771f3ef3d_00007_of_00010.arrow
04/02/2024 00:12:52 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/asus/intelligent-test/decoder_fine_tuning/output/dataset_cache/csv/default-ac318976b409afc9/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-b5170c3771f3ef3d_00007_of_00010.arrow
Caching processed dataset at /home/asus/intelligent-test/decoder_fine_tuning/output/dataset_cache/csv/default-ac318976b409afc9/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-b5170c3771f3ef3d_00008_of_00010.arrow
04/02/2024 00:12:52 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/asus/intelligent-test/decoder_fine_tuning/output/dataset_cache/csv/default-ac318976b409afc9/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-b5170c3771f3ef3d_00008_of_00010.arrow
Caching processed dataset at /home/asus/intelligent-test/decoder_fine_tuning/output/dataset_cache/csv/default-ac318976b409afc9/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-b5170c3771f3ef3d_00009_of_00010.arrow
04/02/2024 00:12:52 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/asus/intelligent-test/decoder_fine_tuning/output/dataset_cache/csv/default-ac318976b409afc9/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-b5170c3771f3ef3d_00009_of_00010.arrow
Running tokenizer on dataset (num_proc=10): 100%|██████████| 575/575 [00:00<00:00, 2810.10 examples/s]
Concatenating 10 shards
04/02/2024 00:12:52 - INFO - datasets.arrow_dataset - Concatenating 10 shards
Process #0 will write at /home/asus/intelligent-test/decoder_fine_tuning/output/dataset_cache/csv/default-ac318976b409afc9/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-8978e4342494117e_00000_of_00010.arrow
04/02/2024 00:12:52 - INFO - datasets.arrow_dataset - Process #0 will write at /home/asus/intelligent-test/decoder_fine_tuning/output/dataset_cache/csv/default-ac318976b409afc9/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-8978e4342494117e_00000_of_00010.arrow
Process #1 will write at /home/asus/intelligent-test/decoder_fine_tuning/output/dataset_cache/csv/default-ac318976b409afc9/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-8978e4342494117e_00001_of_00010.arrow
04/02/2024 00:12:52 - INFO - datasets.arrow_dataset - Process #1 will write at /home/asus/intelligent-test/decoder_fine_tuning/output/dataset_cache/csv/default-ac318976b409afc9/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-8978e4342494117e_00001_of_00010.arrow
Process #2 will write at /home/asus/intelligent-test/decoder_fine_tuning/output/dataset_cache/csv/default-ac318976b409afc9/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-8978e4342494117e_00002_of_00010.arrow
04/02/2024 00:12:52 - INFO - datasets.arrow_dataset - Process #2 will write at /home/asus/intelligent-test/decoder_fine_tuning/output/dataset_cache/csv/default-ac318976b409afc9/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-8978e4342494117e_00002_of_00010.arrow
Process #3 will write at /home/asus/intelligent-test/decoder_fine_tuning/output/dataset_cache/csv/default-ac318976b409afc9/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-8978e4342494117e_00003_of_00010.arrow
04/02/2024 00:12:52 - INFO - datasets.arrow_dataset - Process #3 will write at /home/asus/intelligent-test/decoder_fine_tuning/output/dataset_cache/csv/default-ac318976b409afc9/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-8978e4342494117e_00003_of_00010.arrow
Process #4 will write at /home/asus/intelligent-test/decoder_fine_tuning/output/dataset_cache/csv/default-ac318976b409afc9/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-8978e4342494117e_00004_of_00010.arrow
04/02/2024 00:12:52 - INFO - datasets.arrow_dataset - Process #4 will write at /home/asus/intelligent-test/decoder_fine_tuning/output/dataset_cache/csv/default-ac318976b409afc9/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-8978e4342494117e_00004_of_00010.arrow
Process #5 will write at /home/asus/intelligent-test/decoder_fine_tuning/output/dataset_cache/csv/default-ac318976b409afc9/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-8978e4342494117e_00005_of_00010.arrow
04/02/2024 00:12:52 - INFO - datasets.arrow_dataset - Process #5 will write at /home/asus/intelligent-test/decoder_fine_tuning/output/dataset_cache/csv/default-ac318976b409afc9/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-8978e4342494117e_00005_of_00010.arrow
Process #6 will write at /home/asus/intelligent-test/decoder_fine_tuning/output/dataset_cache/csv/default-ac318976b409afc9/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-8978e4342494117e_00006_of_00010.arrow
04/02/2024 00:12:52 - INFO - datasets.arrow_dataset - Process #6 will write at /home/asus/intelligent-test/decoder_fine_tuning/output/dataset_cache/csv/default-ac318976b409afc9/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-8978e4342494117e_00006_of_00010.arrow
Process #7 will write at /home/asus/intelligent-test/decoder_fine_tuning/output/dataset_cache/csv/default-ac318976b409afc9/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-8978e4342494117e_00007_of_00010.arrow
04/02/2024 00:12:52 - INFO - datasets.arrow_dataset - Process #7 will write at /home/asus/intelligent-test/decoder_fine_tuning/output/dataset_cache/csv/default-ac318976b409afc9/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-8978e4342494117e_00007_of_00010.arrow
Process #8 will write at /home/asus/intelligent-test/decoder_fine_tuning/output/dataset_cache/csv/default-ac318976b409afc9/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-8978e4342494117e_00008_of_00010.arrow
04/02/2024 00:12:52 - INFO - datasets.arrow_dataset - Process #8 will write at /home/asus/intelligent-test/decoder_fine_tuning/output/dataset_cache/csv/default-ac318976b409afc9/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-8978e4342494117e_00008_of_00010.arrow
Process #9 will write at /home/asus/intelligent-test/decoder_fine_tuning/output/dataset_cache/csv/default-ac318976b409afc9/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-8978e4342494117e_00009_of_00010.arrow
04/02/2024 00:12:52 - INFO - datasets.arrow_dataset - Process #9 will write at /home/asus/intelligent-test/decoder_fine_tuning/output/dataset_cache/csv/default-ac318976b409afc9/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-8978e4342494117e_00009_of_00010.arrow
Spawning 10 processes
04/02/2024 00:12:52 - INFO - datasets.arrow_dataset - Spawning 10 processes
Running tokenizer on dataset (num_proc=10):   0%|          | 0/64 [00:00<?, ? examples/s]Caching processed dataset at /home/asus/intelligent-test/decoder_fine_tuning/output/dataset_cache/csv/default-ac318976b409afc9/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-8978e4342494117e_00000_of_00010.arrow
04/02/2024 00:12:52 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/asus/intelligent-test/decoder_fine_tuning/output/dataset_cache/csv/default-ac318976b409afc9/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-8978e4342494117e_00000_of_00010.arrow
Caching processed dataset at /home/asus/intelligent-test/decoder_fine_tuning/output/dataset_cache/csv/default-ac318976b409afc9/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-8978e4342494117e_00001_of_00010.arrow
04/02/2024 00:12:52 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/asus/intelligent-test/decoder_fine_tuning/output/dataset_cache/csv/default-ac318976b409afc9/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-8978e4342494117e_00001_of_00010.arrow
Caching processed dataset at /home/asus/intelligent-test/decoder_fine_tuning/output/dataset_cache/csv/default-ac318976b409afc9/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-8978e4342494117e_00002_of_00010.arrow
04/02/2024 00:12:52 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/asus/intelligent-test/decoder_fine_tuning/output/dataset_cache/csv/default-ac318976b409afc9/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-8978e4342494117e_00002_of_00010.arrow
Caching processed dataset at /home/asus/intelligent-test/decoder_fine_tuning/output/dataset_cache/csv/default-ac318976b409afc9/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-8978e4342494117e_00003_of_00010.arrow
04/02/2024 00:12:52 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/asus/intelligent-test/decoder_fine_tuning/output/dataset_cache/csv/default-ac318976b409afc9/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-8978e4342494117e_00003_of_00010.arrow
Caching processed dataset at /home/asus/intelligent-test/decoder_fine_tuning/output/dataset_cache/csv/default-ac318976b409afc9/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-8978e4342494117e_00004_of_00010.arrow
04/02/2024 00:12:52 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/asus/intelligent-test/decoder_fine_tuning/output/dataset_cache/csv/default-ac318976b409afc9/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-8978e4342494117e_00004_of_00010.arrow
Caching processed dataset at /home/asus/intelligent-test/decoder_fine_tuning/output/dataset_cache/csv/default-ac318976b409afc9/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-8978e4342494117e_00005_of_00010.arrow
04/02/2024 00:12:52 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/asus/intelligent-test/decoder_fine_tuning/output/dataset_cache/csv/default-ac318976b409afc9/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-8978e4342494117e_00005_of_00010.arrow
Caching processed dataset at /home/asus/intelligent-test/decoder_fine_tuning/output/dataset_cache/csv/default-ac318976b409afc9/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-8978e4342494117e_00006_of_00010.arrow
04/02/2024 00:12:52 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/asus/intelligent-test/decoder_fine_tuning/output/dataset_cache/csv/default-ac318976b409afc9/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-8978e4342494117e_00006_of_00010.arrow
Caching processed dataset at /home/asus/intelligent-test/decoder_fine_tuning/output/dataset_cache/csv/default-ac318976b409afc9/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-8978e4342494117e_00007_of_00010.arrow
04/02/2024 00:12:52 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/asus/intelligent-test/decoder_fine_tuning/output/dataset_cache/csv/default-ac318976b409afc9/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-8978e4342494117e_00007_of_00010.arrow
Caching processed dataset at /home/asus/intelligent-test/decoder_fine_tuning/output/dataset_cache/csv/default-ac318976b409afc9/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-8978e4342494117e_00008_of_00010.arrow
04/02/2024 00:12:52 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/asus/intelligent-test/decoder_fine_tuning/output/dataset_cache/csv/default-ac318976b409afc9/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-8978e4342494117e_00008_of_00010.arrow
Running tokenizer on dataset (num_proc=10):  91%|█████████ | 58/64 [00:00<00:00, 574.12 examples/s]Caching processed dataset at /home/asus/intelligent-test/decoder_fine_tuning/output/dataset_cache/csv/default-ac318976b409afc9/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-8978e4342494117e_00009_of_00010.arrow
04/02/2024 00:12:52 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/asus/intelligent-test/decoder_fine_tuning/output/dataset_cache/csv/default-ac318976b409afc9/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-8978e4342494117e_00009_of_00010.arrow
Running tokenizer on dataset (num_proc=10): 100%|██████████| 64/64 [00:00<00:00, 373.56 examples/s]
Concatenating 10 shards
04/02/2024 00:12:52 - INFO - datasets.arrow_dataset - Concatenating 10 shards
04/02/2024 00:12:52 - INFO - decoder_fine_tuning.log - 训练集的采样114: {'input_ids': [1, 12968, 29901, 38857, 32949, 33304, 30214, 40339, 38276, 32471, 53824, 33304, 32135, 33690, 32066, 52072, 32513, 30267, 13, 33304, 29901, 32405, 34608, 34353, 34827, 36092, 34353, 30214, 32147, 31407, 31268, 30544, 30682, 36735, 35959, 42515, 34353, 32696, 30330, 31407, 36912, 32473, 30330, 31407, 32886, 32407, 41575, 31184, 33562, 39005, 32039, 31570, 36065, 32198, 31184, 32451, 35506, 56868, 30210, 30505, 36662, 34353, 30214, 33201, 30505, 32686, 32405, 33911, 35748, 33371, 30594, 30214, 32007, 32110, 53180, 57058, 32236, 30539, 39552, 32128, 32039, 32115, 37356, 32114, 32686, 31149, 39708, 30267, 13, 2, 1, 4007, 22137, 29901, 32406, 33602, 29901, 32405, 34608, 34353, 34827, 36092, 34353, 29892, 32810, 29901, 32147, 31407, 31268, 30544, 30682, 36735, 35959, 42515, 34353, 32696, 30330, 31407, 36912, 32473, 30330, 31407, 32886, 32407, 41575, 31184, 33562, 39005, 29892, 272, 29901, 32039, 29892, 32810, 29901, 31570, 36065, 32198, 31184, 32451, 35506, 56868, 30210, 30505, 36662, 34353, 29892, 32406, 33602, 29901, 33201, 29892, 32406, 29901, 32686, 29892, 32406, 32141, 29901, 32405, 33911, 35748, 33371, 29892, 1767, 29901, 53180, 57058, 29892, 1767, 29901, 30539, 39552, 32128, 29892, 272, 29901, 32039, 29892, 1767, 29901, 32115, 37356, 32114, 29892, 32406, 29901, 32686, 29892, 32406, 32141, 29901, 31149, 39708, 13, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [1, 12968, 29901, 38857, 32949, 33304, 30214, 40339, 38276, 32471, 53824, 33304, 32135, 33690, 32066, 52072, 32513, 30267, 13, 33304, 29901, 32405, 34608, 34353, 34827, 36092, 34353, 30214, 32147, 31407, 31268, 30544, 30682, 36735, 35959, 42515, 34353, 32696, 30330, 31407, 36912, 32473, 30330, 31407, 32886, 32407, 41575, 31184, 33562, 39005, 32039, 31570, 36065, 32198, 31184, 32451, 35506, 56868, 30210, 30505, 36662, 34353, 30214, 33201, 30505, 32686, 32405, 33911, 35748, 33371, 30594, 30214, 32007, 32110, 53180, 57058, 32236, 30539, 39552, 32128, 32039, 32115, 37356, 32114, 32686, 31149, 39708, 30267, 13, 2, 1, 4007, 22137, 29901, 32406, 33602, 29901, 32405, 34608, 34353, 34827, 36092, 34353, 29892, 32810, 29901, 32147, 31407, 31268, 30544, 30682, 36735, 35959, 42515, 34353, 32696, 30330, 31407, 36912, 32473, 30330, 31407, 32886, 32407, 41575, 31184, 33562, 39005, 29892, 272, 29901, 32039, 29892, 32810, 29901, 31570, 36065, 32198, 31184, 32451, 35506, 56868, 30210, 30505, 36662, 34353, 29892, 32406, 33602, 29901, 33201, 29892, 32406, 29901, 32686, 29892, 32406, 32141, 29901, 32405, 33911, 35748, 33371, 29892, 1767, 29901, 53180, 57058, 29892, 1767, 29901, 30539, 39552, 32128, 29892, 272, 29901, 32039, 29892, 1767, 29901, 32115, 37356, 32114, 29892, 32406, 29901, 32686, 29892, 32406, 32141, 29901, 31149, 39708, 13, 2]}
04/02/2024 00:12:52 - INFO - decoder_fine_tuning.log - 训练集的采样25: {'input_ids': [1, 12968, 29901, 38857, 32949, 33304, 30214, 40339, 38276, 32471, 53824, 33304, 32135, 33690, 32066, 52072, 32513, 30267, 13, 33304, 29901, 33664, 32651, 34769, 32665, 30313, 32196, 36065, 36207, 33105, 32846, 42150, 32330, 32039, 32447, 32367, 33105, 32846, 42150, 30210, 29941, 29900, 29995, 62226, 32330, 29945, 29900, 29995, 30210, 30214, 32501, 37186, 30866, 33472, 32013, 34666, 32045, 33264, 30843, 62594, 39100, 34188, 36065, 37856, 32364, 31900, 30267, 13, 2, 1, 4007, 22137, 29901, 32406, 33602, 29901, 33664, 32651, 34769, 32665, 30313, 29892, 1989, 29901, 32196, 36065, 36207, 33105, 32846, 42150, 29892, 459, 29901, 32330, 29892, 272, 29901, 32039, 29892, 459, 29901, 32447, 29892, 1767, 29901, 32367, 33105, 32846, 42150, 30210, 29941, 29900, 13667, 459, 29901, 59414, 29892, 1767, 29901, 29945, 29900, 13667, 37214, 29901, 30866, 33472, 32013, 34666, 32045, 33264, 30843, 62594, 39100, 29892, 32406, 29901, 34188, 29892, 32406, 32141, 29901, 36065, 37856, 32364, 31900, 13, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [1, 12968, 29901, 38857, 32949, 33304, 30214, 40339, 38276, 32471, 53824, 33304, 32135, 33690, 32066, 52072, 32513, 30267, 13, 33304, 29901, 33664, 32651, 34769, 32665, 30313, 32196, 36065, 36207, 33105, 32846, 42150, 32330, 32039, 32447, 32367, 33105, 32846, 42150, 30210, 29941, 29900, 29995, 62226, 32330, 29945, 29900, 29995, 30210, 30214, 32501, 37186, 30866, 33472, 32013, 34666, 32045, 33264, 30843, 62594, 39100, 34188, 36065, 37856, 32364, 31900, 30267, 13, 2, 1, 4007, 22137, 29901, 32406, 33602, 29901, 33664, 32651, 34769, 32665, 30313, 29892, 1989, 29901, 32196, 36065, 36207, 33105, 32846, 42150, 29892, 459, 29901, 32330, 29892, 272, 29901, 32039, 29892, 459, 29901, 32447, 29892, 1767, 29901, 32367, 33105, 32846, 42150, 30210, 29941, 29900, 13667, 459, 29901, 59414, 29892, 1767, 29901, 29945, 29900, 13667, 37214, 29901, 30866, 33472, 32013, 34666, 32045, 33264, 30843, 62594, 39100, 29892, 32406, 29901, 34188, 29892, 32406, 32141, 29901, 36065, 37856, 32364, 31900, 13, 2]}
04/02/2024 00:12:52 - INFO - decoder_fine_tuning.log - 训练集的采样281: {'input_ids': [1, 12968, 29901, 38857, 32949, 33304, 30214, 40339, 38276, 32471, 53824, 33304, 32135, 33690, 32066, 52072, 32513, 30267, 13, 33304, 29901, 36596, 47483, 30651, 33706, 33826, 39142, 34353, 33707, 49560, 30744, 33472, 32005, 30946, 33099, 37783, 30210, 30214, 33472, 63477, 42332, 35157, 32908, 30210, 30658, 63994, 31168, 39635, 33417, 32587, 30214, 30346, 30744, 39581, 35905, 35847, 30267, 13, 2, 1, 4007, 22137, 29901, 32406, 33602, 29901, 36596, 47483, 29892, 1767, 29901, 33706, 33826, 29892, 1989, 29901, 32367, 34353, 29892, 1767, 29901, 33707, 29892, 32406, 33053, 29901, 30346, 30744, 29892, 32406, 29901, 33472, 29892, 1767, 29901, 32005, 30946, 33099, 37783, 29892, 32026, 29901, 33472, 63477, 29892, 1989, 29901, 42332, 35157, 32908, 30210, 30658, 63994, 31168, 29892, 32128, 29901, 33417, 32587, 13, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [1, 12968, 29901, 38857, 32949, 33304, 30214, 40339, 38276, 32471, 53824, 33304, 32135, 33690, 32066, 52072, 32513, 30267, 13, 33304, 29901, 36596, 47483, 30651, 33706, 33826, 39142, 34353, 33707, 49560, 30744, 33472, 32005, 30946, 33099, 37783, 30210, 30214, 33472, 63477, 42332, 35157, 32908, 30210, 30658, 63994, 31168, 39635, 33417, 32587, 30214, 30346, 30744, 39581, 35905, 35847, 30267, 13, 2, 1, 4007, 22137, 29901, 32406, 33602, 29901, 36596, 47483, 29892, 1767, 29901, 33706, 33826, 29892, 1989, 29901, 32367, 34353, 29892, 1767, 29901, 33707, 29892, 32406, 33053, 29901, 30346, 30744, 29892, 32406, 29901, 33472, 29892, 1767, 29901, 32005, 30946, 33099, 37783, 29892, 32026, 29901, 33472, 63477, 29892, 1989, 29901, 42332, 35157, 32908, 30210, 30658, 63994, 31168, 29892, 32128, 29901, 33417, 32587, 13, 2]}
Caching indices mapping at /home/asus/intelligent-test/decoder_fine_tuning/output/dataset_cache/csv/default-ac318976b409afc9/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-b16f44208a3b0212.arrow
04/02/2024 00:12:52 - INFO - datasets.arrow_dataset - Caching indices mapping at /home/asus/intelligent-test/decoder_fine_tuning/output/dataset_cache/csv/default-ac318976b409afc9/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-b16f44208a3b0212.arrow
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
  warnings.warn(
[INFO|trainer.py:601] 2024-04-02 00:12:52,713 >> Using auto half precision backend
04/02/2024 00:12:52 - INFO - decoder_fine_tuning.log - 0 start train
[INFO|trainer.py:1812] 2024-04-02 00:12:52,910 >> ***** Running training *****
[INFO|trainer.py:1813] 2024-04-02 00:12:52,910 >>   Num examples = 575
[INFO|trainer.py:1814] 2024-04-02 00:12:52,910 >>   Num Epochs = 20
[INFO|trainer.py:1815] 2024-04-02 00:12:52,910 >>   Instantaneous batch size per device = 1
[INFO|trainer.py:1818] 2024-04-02 00:12:52,910 >>   Total train batch size (w. parallel, distributed & accumulation) = 8
[INFO|trainer.py:1819] 2024-04-02 00:12:52,910 >>   Gradient Accumulation steps = 8
[INFO|trainer.py:1820] 2024-04-02 00:12:52,910 >>   Total optimization steps = 1,420
[INFO|trainer.py:1821] 2024-04-02 00:12:52,910 >>   Number of trainable parameters = 7,008,751,616
04/02/2024 00:12:52 - WARNING - transformers_modules.Atom-7B.model_atom - `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
[INFO|trainer.py:3376] 2024-04-02 01:31:30,283 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-04-02 01:31:30,283 >>   Num examples = 64
[INFO|trainer.py:3381] 2024-04-02 01:31:30,283 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-04-02 01:31:40,896 >> Saving model checkpoint to ./output/tmp-checkpoint-100
[WARNING|configuration_utils.py:449] 2024-04-02 01:31:40,898 >> Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.
Non-default generation parameters: {'max_length': 4096}
[INFO|configuration_utils.py:473] 2024-04-02 01:31:40,898 >> Configuration saved in ./output/tmp-checkpoint-100/config.json
[INFO|configuration_utils.py:845] 2024-04-02 01:31:40,898 >> Generate config GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2,
  "max_length": 4096,
  "pad_token_id": 2
}

[INFO|configuration_utils.py:614] 2024-04-02 01:31:40,899 >> Configuration saved in ./output/tmp-checkpoint-100/generation_config.json
[INFO|modeling_utils.py:2462] 2024-04-02 01:31:54,902 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 3 checkpoint shards. You can find where each parameters has been saved in the index located at ./output/tmp-checkpoint-100/model.safetensors.index.json.
[INFO|tokenization_utils_base.py:2459] 2024-04-02 01:31:54,903 >> tokenizer config file saved in ./output/tmp-checkpoint-100/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-04-02 01:31:54,903 >> Special tokens file saved in ./output/tmp-checkpoint-100/special_tokens_map.json
[INFO|trainer.py:3159] 2024-04-02 01:32:09,384 >> Deleting older checkpoint [output/checkpoint-100] due to args.save_total_limit
[INFO|trainer.py:3376] 2024-04-02 02:51:15,520 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-04-02 02:51:15,520 >>   Num examples = 64
[INFO|trainer.py:3381] 2024-04-02 02:51:15,520 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-04-02 02:51:26,150 >> Saving model checkpoint to ./output/tmp-checkpoint-200
[WARNING|configuration_utils.py:449] 2024-04-02 02:51:26,152 >> Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.
Non-default generation parameters: {'max_length': 4096}
[INFO|configuration_utils.py:473] 2024-04-02 02:51:26,152 >> Configuration saved in ./output/tmp-checkpoint-200/config.json
[INFO|configuration_utils.py:845] 2024-04-02 02:51:26,152 >> Generate config GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2,
  "max_length": 4096,
  "pad_token_id": 2
}

[INFO|configuration_utils.py:614] 2024-04-02 02:51:26,153 >> Configuration saved in ./output/tmp-checkpoint-200/generation_config.json
[INFO|modeling_utils.py:2462] 2024-04-02 02:51:36,023 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 3 checkpoint shards. You can find where each parameters has been saved in the index located at ./output/tmp-checkpoint-200/model.safetensors.index.json.
[INFO|tokenization_utils_base.py:2459] 2024-04-02 02:51:36,023 >> tokenizer config file saved in ./output/tmp-checkpoint-200/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-04-02 02:51:36,023 >> Special tokens file saved in ./output/tmp-checkpoint-200/special_tokens_map.json
[INFO|trainer.py:3159] 2024-04-02 02:51:48,867 >> Deleting older checkpoint [output/checkpoint-200] due to args.save_total_limit
[INFO|trainer.py:3376] 2024-04-02 04:10:28,707 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-04-02 04:10:28,707 >>   Num examples = 64
[INFO|trainer.py:3381] 2024-04-02 04:10:28,707 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-04-02 04:10:39,330 >> Saving model checkpoint to ./output/tmp-checkpoint-300
[WARNING|configuration_utils.py:449] 2024-04-02 04:10:39,332 >> Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.
Non-default generation parameters: {'max_length': 4096}
[INFO|configuration_utils.py:473] 2024-04-02 04:10:39,332 >> Configuration saved in ./output/tmp-checkpoint-300/config.json
[INFO|configuration_utils.py:845] 2024-04-02 04:10:39,332 >> Generate config GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2,
  "max_length": 4096,
  "pad_token_id": 2
}

[INFO|configuration_utils.py:614] 2024-04-02 04:10:39,333 >> Configuration saved in ./output/tmp-checkpoint-300/generation_config.json
[INFO|modeling_utils.py:2462] 2024-04-02 04:10:49,103 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 3 checkpoint shards. You can find where each parameters has been saved in the index located at ./output/tmp-checkpoint-300/model.safetensors.index.json.
[INFO|tokenization_utils_base.py:2459] 2024-04-02 04:10:49,103 >> tokenizer config file saved in ./output/tmp-checkpoint-300/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-04-02 04:10:49,103 >> Special tokens file saved in ./output/tmp-checkpoint-300/special_tokens_map.json
[INFO|trainer.py:3159] 2024-04-02 04:11:02,350 >> Deleting older checkpoint [output/checkpoint-300] due to args.save_total_limit
[INFO|trainer.py:3376] 2024-04-02 05:30:12,374 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-04-02 05:30:12,374 >>   Num examples = 64
[INFO|trainer.py:3381] 2024-04-02 05:30:12,374 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-04-02 05:30:23,039 >> Saving model checkpoint to ./output/tmp-checkpoint-400
[WARNING|configuration_utils.py:449] 2024-04-02 05:30:23,041 >> Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.
Non-default generation parameters: {'max_length': 4096}
[INFO|configuration_utils.py:473] 2024-04-02 05:30:23,042 >> Configuration saved in ./output/tmp-checkpoint-400/config.json
[INFO|configuration_utils.py:845] 2024-04-02 05:30:23,042 >> Generate config GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2,
  "max_length": 4096,
  "pad_token_id": 2
}

[INFO|configuration_utils.py:614] 2024-04-02 05:30:23,042 >> Configuration saved in ./output/tmp-checkpoint-400/generation_config.json
[INFO|modeling_utils.py:2462] 2024-04-02 05:30:33,724 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 3 checkpoint shards. You can find where each parameters has been saved in the index located at ./output/tmp-checkpoint-400/model.safetensors.index.json.
[INFO|tokenization_utils_base.py:2459] 2024-04-02 05:30:33,724 >> tokenizer config file saved in ./output/tmp-checkpoint-400/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-04-02 05:30:33,725 >> Special tokens file saved in ./output/tmp-checkpoint-400/special_tokens_map.json
[INFO|trainer.py:3159] 2024-04-02 05:30:47,196 >> Deleting older checkpoint [output/checkpoint-400] due to args.save_total_limit
********************on step end call back********************
Step 10 finish
{'loss': 2.8706, 'grad_norm': 38.375, 'learning_rate': 2.5000000000000004e-07, 'epoch': 0.14}
********************on step end call back********************
Step 20 finish
{'loss': 2.6912, 'grad_norm': 31.953125, 'learning_rate': 5.000000000000001e-07, 'epoch': 0.28}
********************on step end call back********************
Step 30 finish
{'loss': 2.4717, 'grad_norm': 24.578125, 'learning_rate': 7.5e-07, 'epoch': 0.42}
********************on step end call back********************
Step 40 finish
{'loss': 2.2109, 'grad_norm': 22.46875, 'learning_rate': 1.0000000000000002e-06, 'epoch': 0.56}
********************on step end call back********************
Step 50 finish
{'loss': 1.7029, 'grad_norm': 18.765625, 'learning_rate': 1.25e-06, 'epoch': 0.7}
********************on step end call back********************
Step 60 finish
{'loss': 1.2703, 'grad_norm': 16.953125, 'learning_rate': 1.5e-06, 'epoch': 0.83}
********************on step end call back********************
Step 70 finish
{'loss': 1.0602, 'grad_norm': 18.421875, 'learning_rate': 1.75e-06, 'epoch': 0.97}
********************on epoch end call back********************
Epoch 0.9878260869565217 finish
********************on step end call back********************
Step 80 finish
{'loss': 0.9604, 'grad_norm': 15.296875, 'learning_rate': 2.0000000000000003e-06, 'epoch': 1.11}
********************on step end call back********************
Step 90 finish
{'loss': 0.9161, 'grad_norm': 12.8203125, 'learning_rate': 2.25e-06, 'epoch': 1.25}
********************on step end call back********************
Step 100 finish
{'loss': 0.8331, 'grad_norm': 12.71875, 'learning_rate': 2.5e-06, 'epoch': 1.39}
{'eval_loss': 0.8128097653388977, 'eval_accuracy': 0.8392857142857143, 'eval_runtime': 10.6131, 'eval_samples_per_second': 6.03, 'eval_steps_per_second': 6.03, 'epoch': 1.39}
********************on step end call back********************
Step 110 finish
{'loss': 0.8004, 'grad_norm': 11.2578125, 'learning_rate': 2.7500000000000004e-06, 'epoch': 1.53}
********************on step end call back********************
Step 120 finish
{'loss': 0.7708, 'grad_norm': 9.7421875, 'learning_rate': 3e-06, 'epoch': 1.67}
********************on step end call back********************
Step 130 finish
{'loss': 0.7442, 'grad_norm': 11.265625, 'learning_rate': 3.2500000000000002e-06, 'epoch': 1.81}
********************on step end call back********************
Step 140 finish
{'loss': 0.728, 'grad_norm': 9.8515625, 'learning_rate': 3.5e-06, 'epoch': 1.95}
********************on epoch end call back********************
Epoch 1.9895652173913043 finish
********************on step end call back********************
Step 150 finish
{'loss': 0.5915, 'grad_norm': 12.03125, 'learning_rate': 3.7500000000000005e-06, 'epoch': 2.09}
********************on step end call back********************
Step 160 finish
{'loss': 0.5331, 'grad_norm': 11.6875, 'learning_rate': 4.000000000000001e-06, 'epoch': 2.23}
********************on step end call back********************
Step 170 finish
{'loss': 0.5397, 'grad_norm': 11.3671875, 'learning_rate': 4.25e-06, 'epoch': 2.37}
********************on step end call back********************
Step 180 finish
{'loss': 0.5584, 'grad_norm': 12.921875, 'learning_rate': 4.5e-06, 'epoch': 2.5}
********************on step end call back********************
Step 190 finish
{'loss': 0.5382, 'grad_norm': 10.984375, 'learning_rate': 4.75e-06, 'epoch': 2.64}
********************on step end call back********************
Step 200 finish
{'loss': 0.5287, 'grad_norm': 9.6640625, 'learning_rate': 5e-06, 'epoch': 2.78}
{'eval_loss': 0.636144757270813, 'eval_accuracy': 0.8571428571428571, 'eval_runtime': 10.6295, 'eval_samples_per_second': 6.021, 'eval_steps_per_second': 6.021, 'epoch': 2.78}
********************on step end call back********************
Step 210 finish
{'loss': 0.5161, 'grad_norm': 10.0703125, 'learning_rate': 5.2500000000000006e-06, 'epoch': 2.92}
********************on epoch end call back********************
Epoch 2.991304347826087 finish
********************on step end call back********************
Step 220 finish
{'loss': 0.4018, 'grad_norm': 7.39453125, 'learning_rate': 5.500000000000001e-06, 'epoch': 3.06}
********************on step end call back********************
Step 230 finish
{'loss': 0.2994, 'grad_norm': 9.90625, 'learning_rate': 5.75e-06, 'epoch': 3.2}
********************on step end call back********************
Step 240 finish
{'loss': 0.3018, 'grad_norm': 10.9296875, 'learning_rate': 6e-06, 'epoch': 3.34}
********************on step end call back********************
Step 250 finish
{'loss': 0.2902, 'grad_norm': 11.171875, 'learning_rate': 6.25e-06, 'epoch': 3.48}
********************on step end call back********************
Step 260 finish
{'loss': 0.3441, 'grad_norm': 11.359375, 'learning_rate': 6.5000000000000004e-06, 'epoch': 3.62}
********************on step end call back********************
Step 270 finish
{'loss': 0.2928, 'grad_norm': 10.890625, 'learning_rate': 6.750000000000001e-06, 'epoch': 3.76}
********************on step end call back********************
Step 280 finish
{'loss': 0.3389, 'grad_norm': 10.7890625, 'learning_rate': 7e-06, 'epoch': 3.9}
********************on epoch end call back********************
Epoch 3.9930434782608697 finish
********************on step end call back********************
Step 290 finish
{'loss': 0.3147, 'grad_norm': 8.5078125, 'learning_rate': 7.25e-06, 'epoch': 4.03}
********************on step end call back********************
Step 300 finish
{'loss': 0.1868, 'grad_norm': 11.234375, 'learning_rate': 7.500000000000001e-06, 'epoch': 4.17}
{'eval_loss': 0.7159405946731567, 'eval_accuracy': 0.8571428571428571, 'eval_runtime': 10.6222, 'eval_samples_per_second': 6.025, 'eval_steps_per_second': 6.025, 'epoch': 4.17}
********************on step end call back********************
Step 310 finish
{'loss': 0.2387, 'grad_norm': 11.84375, 'learning_rate': 7.75e-06, 'epoch': 4.31}
********************on step end call back********************
Step 320 finish
{'loss': 0.2349, 'grad_norm': 15.2109375, 'learning_rate': 8.000000000000001e-06, 'epoch': 4.45}
********************on step end call back********************
Step 330 finish
{'loss': 0.2384, 'grad_norm': 10.359375, 'learning_rate': 8.25e-06, 'epoch': 4.59}
********************on step end call back********************
Step 340 finish
{'loss': 0.258, 'grad_norm': 8.3828125, 'learning_rate': 8.5e-06, 'epoch': 4.73}
********************on step end call back********************
Step 350 finish
{'loss': 0.2621, 'grad_norm': 10.0, 'learning_rate': 8.750000000000001e-06, 'epoch': 4.87}
********************on epoch end call back********************
Epoch 4.994782608695652 finish
********************on step end call back********************
Step 360 finish
{'loss': 0.2764, 'grad_norm': 7.49609375, 'learning_rate': 9e-06, 'epoch': 5.01}
********************on step end call back********************
Step 370 finish
{'loss': 0.1912, 'grad_norm': 11.046875, 'learning_rate': 9.250000000000001e-06, 'epoch': 5.15}
********************on step end call back********************
Step 380 finish
{'loss': 0.2081, 'grad_norm': 7.9296875, 'learning_rate': 9.5e-06, 'epoch': 5.29}
********************on step end call back********************
Step 390 finish
{'loss': 0.2146, 'grad_norm': 7.3671875, 'learning_rate': 9.75e-06, 'epoch': 5.43}
********************on step end call back********************
Step 400 finish
{'loss': 0.2159, 'grad_norm': 8.28125, 'learning_rate': 1e-05, 'epoch': 5.57}
{'eval_loss': 0.6719131469726562, 'eval_accuracy': 0.8392857142857143, 'eval_runtime': 10.6645, 'eval_samples_per_second': 6.001, 'eval_steps_per_second': 6.001, 'epoch': 5.57}
********************on step end call back********************
Step 410 finish
{'loss': 0.235, 'grad_norm': 7.4140625, 'learning_rate': 9.901960784313727e-06, 'epoch': 5.7}
********************on step end call back********************
Step 420 finish
{'loss': 0.2389, 'grad_norm': 10.546875, 'learning_rate': 9.803921568627451e-06, 'epoch': 5.84}
********************on step end call back********************[INFO|trainer.py:3376] 2024-04-02 06:48:52,980 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-04-02 06:48:52,980 >>   Num examples = 64
[INFO|trainer.py:3381] 2024-04-02 06:48:52,980 >>   Batch size = 1
[WARNING|trainer.py:2492] 2024-04-02 06:49:03,580 >> Checkpoint destination directory ./output/checkpoint-500 already exists and is non-empty. Saving will proceed but saved results may be invalid.
[INFO|trainer.py:3067] 2024-04-02 06:49:03,580 >> Saving model checkpoint to ./output/checkpoint-500
[WARNING|configuration_utils.py:449] 2024-04-02 06:49:03,582 >> Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.
Non-default generation parameters: {'max_length': 4096}
[INFO|configuration_utils.py:473] 2024-04-02 06:49:03,582 >> Configuration saved in ./output/checkpoint-500/config.json
[INFO|configuration_utils.py:845] 2024-04-02 06:49:03,583 >> Generate config GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2,
  "max_length": 4096,
  "pad_token_id": 2
}

[INFO|configuration_utils.py:614] 2024-04-02 06:49:03,583 >> Configuration saved in ./output/checkpoint-500/generation_config.json
[INFO|modeling_utils.py:2462] 2024-04-02 06:49:15,873 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 3 checkpoint shards. You can find where each parameters has been saved in the index located at ./output/checkpoint-500/model.safetensors.index.json.
[INFO|tokenization_utils_base.py:2459] 2024-04-02 06:49:15,873 >> tokenizer config file saved in ./output/checkpoint-500/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-04-02 06:49:15,874 >> Special tokens file saved in ./output/checkpoint-500/special_tokens_map.json
[INFO|trainer.py:3376] 2024-04-02 08:08:11,392 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-04-02 08:08:11,392 >>   Num examples = 64
[INFO|trainer.py:3381] 2024-04-02 08:08:11,392 >>   Batch size = 1
[WARNING|trainer.py:2492] 2024-04-02 08:08:22,007 >> Checkpoint destination directory ./output/checkpoint-600 already exists and is non-empty. Saving will proceed but saved results may be invalid.
[INFO|trainer.py:3067] 2024-04-02 08:08:22,007 >> Saving model checkpoint to ./output/checkpoint-600
[WARNING|configuration_utils.py:449] 2024-04-02 08:08:22,008 >> Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.
Non-default generation parameters: {'max_length': 4096}
[INFO|configuration_utils.py:473] 2024-04-02 08:08:22,009 >> Configuration saved in ./output/checkpoint-600/config.json
[INFO|configuration_utils.py:845] 2024-04-02 08:08:22,009 >> Generate config GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2,
  "max_length": 4096,
  "pad_token_id": 2
}

[INFO|configuration_utils.py:614] 2024-04-02 08:08:22,010 >> Configuration saved in ./output/checkpoint-600/generation_config.json
[INFO|modeling_utils.py:2462] 2024-04-02 08:09:00,146 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 3 checkpoint shards. You can find where each parameters has been saved in the index located at ./output/checkpoint-600/model.safetensors.index.json.
[INFO|tokenization_utils_base.py:2459] 2024-04-02 08:09:00,146 >> tokenizer config file saved in ./output/checkpoint-600/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-04-02 08:09:00,146 >> Special tokens file saved in ./output/checkpoint-600/special_tokens_map.json
[INFO|trainer.py:3376] 2024-04-02 09:29:11,751 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-04-02 09:29:11,751 >>   Num examples = 64
[INFO|trainer.py:3381] 2024-04-02 09:29:11,751 >>   Batch size = 1
[WARNING|trainer.py:2492] 2024-04-02 09:29:22,377 >> Checkpoint destination directory ./output/checkpoint-700 already exists and is non-empty. Saving will proceed but saved results may be invalid.
[INFO|trainer.py:3067] 2024-04-02 09:29:22,377 >> Saving model checkpoint to ./output/checkpoint-700
[WARNING|configuration_utils.py:449] 2024-04-02 09:29:22,380 >> Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.
Non-default generation parameters: {'max_length': 4096}
[INFO|configuration_utils.py:473] 2024-04-02 09:29:22,380 >> Configuration saved in ./output/checkpoint-700/config.json
[INFO|configuration_utils.py:845] 2024-04-02 09:29:22,381 >> Generate config GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2,
  "max_length": 4096,
  "pad_token_id": 2
}

[INFO|configuration_utils.py:614] 2024-04-02 09:29:22,381 >> Configuration saved in ./output/checkpoint-700/generation_config.json
[INFO|modeling_utils.py:2462] 2024-04-02 09:30:05,245 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 3 checkpoint shards. You can find where each parameters has been saved in the index located at ./output/checkpoint-700/model.safetensors.index.json.
[INFO|tokenization_utils_base.py:2459] 2024-04-02 09:30:05,246 >> tokenizer config file saved in ./output/checkpoint-700/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-04-02 09:30:05,246 >> Special tokens file saved in ./output/checkpoint-700/special_tokens_map.json
[INFO|trainer.py:3376] 2024-04-02 10:49:04,295 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-04-02 10:49:04,295 >>   Num examples = 64
[INFO|trainer.py:3381] 2024-04-02 10:49:04,295 >>   Batch size = 1
[WARNING|trainer.py:2492] 2024-04-02 10:49:14,914 >> Checkpoint destination directory ./output/checkpoint-800 already exists and is non-empty. Saving will proceed but saved results may be invalid.
[INFO|trainer.py:3067] 2024-04-02 10:49:14,914 >> Saving model checkpoint to ./output/checkpoint-800
[WARNING|configuration_utils.py:449] 2024-04-02 10:49:14,917 >> Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.
Non-default generation parameters: {'max_length': 4096}
[INFO|configuration_utils.py:473] 2024-04-02 10:49:14,918 >> Configuration saved in ./output/checkpoint-800/config.json
[INFO|configuration_utils.py:845] 2024-04-02 10:49:14,918 >> Generate config GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2,
  "max_length": 4096,
  "pad_token_id": 2
}

[INFO|configuration_utils.py:614] 2024-04-02 10:49:14,919 >> Configuration saved in ./output/checkpoint-800/generation_config.json
[INFO|modeling_utils.py:2462] 2024-04-02 10:50:00,887 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 3 checkpoint shards. You can find where each parameters has been saved in the index located at ./output/checkpoint-800/model.safetensors.index.json.
[INFO|tokenization_utils_base.py:2459] 2024-04-02 10:50:00,888 >> tokenizer config file saved in ./output/checkpoint-800/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-04-02 10:50:00,888 >> Special tokens file saved in ./output/checkpoint-800/special_tokens_map.json

Step 430 finish
{'loss': 0.2444, 'grad_norm': 7.0703125, 'learning_rate': 9.705882352941177e-06, 'epoch': 5.98}
********************on epoch end call back********************
Epoch 5.996521739130435 finish
********************on step end call back********************
Step 440 finish
{'loss': 0.2062, 'grad_norm': 7.8984375, 'learning_rate': 9.607843137254903e-06, 'epoch': 6.12}
********************on step end call back********************
Step 450 finish
{'loss': 0.1971, 'grad_norm': 6.98046875, 'learning_rate': 9.509803921568628e-06, 'epoch': 6.26}
********************on step end call back********************
Step 460 finish
{'loss': 0.1825, 'grad_norm': 4.984375, 'learning_rate': 9.411764705882354e-06, 'epoch': 6.4}
********************on step end call back********************
Step 470 finish
{'loss': 0.2, 'grad_norm': 5.9765625, 'learning_rate': 9.31372549019608e-06, 'epoch': 6.54}
********************on step end call back********************
Step 480 finish
{'loss': 0.2108, 'grad_norm': 5.72265625, 'learning_rate': 9.215686274509804e-06, 'epoch': 6.68}
********************on step end call back********************
Step 490 finish
{'loss': 0.2013, 'grad_norm': 6.40625, 'learning_rate': 9.11764705882353e-06, 'epoch': 6.82}
********************on step end call back********************
Step 500 finish
{'loss': 0.1845, 'grad_norm': 7.24609375, 'learning_rate': 9.019607843137256e-06, 'epoch': 6.96}
{'eval_loss': 0.6955714821815491, 'eval_accuracy': 0.8392857142857143, 'eval_runtime': 10.5996, 'eval_samples_per_second': 6.038, 'eval_steps_per_second': 6.038, 'epoch': 6.96}
********************on epoch end call back********************
Epoch 6.998260869565217 finish
********************on step end call back********************
Step 510 finish
{'loss': 0.1532, 'grad_norm': 3.908203125, 'learning_rate': 8.921568627450982e-06, 'epoch': 7.1}
********************on step end call back********************
Step 520 finish
{'loss': 0.1467, 'grad_norm': 3.81640625, 'learning_rate': 8.823529411764707e-06, 'epoch': 7.23}
********************on step end call back********************
Step 530 finish
{'loss': 0.1516, 'grad_norm': 5.50390625, 'learning_rate': 8.725490196078433e-06, 'epoch': 7.37}
********************on step end call back********************
Step 540 finish
{'loss': 0.1552, 'grad_norm': 6.6953125, 'learning_rate': 8.627450980392157e-06, 'epoch': 7.51}
********************on step end call back********************
Step 550 finish
{'loss': 0.1531, 'grad_norm': 4.58203125, 'learning_rate': 8.529411764705883e-06, 'epoch': 7.65}
********************on step end call back********************
Step 560 finish
{'loss': 0.1671, 'grad_norm': 6.55078125, 'learning_rate': 8.43137254901961e-06, 'epoch': 7.79}
********************on step end call back********************
Step 570 finish
{'loss': 0.1701, 'grad_norm': 5.734375, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.93}
********************on epoch end call back********************
Epoch 8.0 finish
********************on step end call back********************
Step 580 finish
{'loss': 0.1444, 'grad_norm': 3.158203125, 'learning_rate': 8.23529411764706e-06, 'epoch': 8.07}
********************on step end call back********************
Step 590 finish
{'loss': 0.1451, 'grad_norm': 4.484375, 'learning_rate': 8.137254901960784e-06, 'epoch': 8.21}
********************on step end call back********************
Step 600 finish
{'loss': 0.1357, 'grad_norm': 2.234375, 'learning_rate': 8.03921568627451e-06, 'epoch': 8.35}
{'eval_loss': 0.67198646068573, 'eval_accuracy': 0.8928571428571429, 'eval_runtime': 10.614, 'eval_samples_per_second': 6.03, 'eval_steps_per_second': 6.03, 'epoch': 8.35}
********************on step end call back********************
Step 610 finish
{'loss': 0.1271, 'grad_norm': 6.97265625, 'learning_rate': 7.941176470588236e-06, 'epoch': 8.49}
********************on step end call back********************
Step 620 finish
{'loss': 0.1374, 'grad_norm': 5.0703125, 'learning_rate': 7.84313725490196e-06, 'epoch': 8.63}
********************on step end call back********************
Step 630 finish
{'loss': 0.1434, 'grad_norm': 4.546875, 'learning_rate': 7.745098039215687e-06, 'epoch': 8.77}
********************on step end call back********************
Step 640 finish
{'loss': 0.1342, 'grad_norm': 5.34375, 'learning_rate': 7.647058823529411e-06, 'epoch': 8.9}
********************on epoch end call back********************
Epoch 8.987826086956522 finish
********************on step end call back********************
Step 650 finish
{'loss': 0.131, 'grad_norm': 2.451171875, 'learning_rate': 7.549019607843138e-06, 'epoch': 9.04}
********************on step end call back********************
Step 660 finish
{'loss': 0.1084, 'grad_norm': 3.681640625, 'learning_rate': 7.450980392156863e-06, 'epoch': 9.18}
********************on step end call back********************
Step 670 finish
{'loss': 0.1203, 'grad_norm': 4.171875, 'learning_rate': 7.352941176470589e-06, 'epoch': 9.32}
********************on step end call back********************
Step 680 finish
{'loss': 0.1057, 'grad_norm': 5.06640625, 'learning_rate': 7.2549019607843145e-06, 'epoch': 9.46}
********************on step end call back********************
Step 690 finish
{'loss': 0.1139, 'grad_norm': 1.6904296875, 'learning_rate': 7.15686274509804e-06, 'epoch': 9.6}
********************on step end call back********************
Step 700 finish
{'loss': 0.1185, 'grad_norm': 1.9833984375, 'learning_rate': 7.058823529411766e-06, 'epoch': 9.74}
{'eval_loss': 0.7086129784584045, 'eval_accuracy': 0.875, 'eval_runtime': 10.6253, 'eval_samples_per_second': 6.023, 'eval_steps_per_second': 6.023, 'epoch': 9.74}
********************on step end call back********************
Step 710 finish
{'loss': 0.1146, 'grad_norm': 2.783203125, 'learning_rate': 6.96078431372549e-06, 'epoch': 9.88}
********************on epoch end call back********************
Epoch 9.989565217391304 finish
********************on step end call back********************
Step 720 finish
{'loss': 0.1138, 'grad_norm': 2.390625, 'learning_rate': 6.862745098039216e-06, 'epoch': 10.02}
********************on step end call back********************
Step 730 finish
{'loss': 0.0912, 'grad_norm': 1.853515625, 'learning_rate': 6.764705882352942e-06, 'epoch': 10.16}
********************on step end call back********************
Step 740 finish
{'loss': 0.1049, 'grad_norm': 1.4638671875, 'learning_rate': 6.666666666666667e-06, 'epoch': 10.3}
********************on step end call back********************
Step 750 finish
{'loss': 0.1057, 'grad_norm': 1.119140625, 'learning_rate': 6.568627450980393e-06, 'epoch': 10.43}
********************on step end call back********************
Step 760 finish
{'loss': 0.1061, 'grad_norm': 1.8115234375, 'learning_rate': 6.470588235294119e-06, 'epoch': 10.57}
********************on step end call back********************
Step 770 finish
{'loss': 0.1072, 'grad_norm': 4.671875, 'learning_rate': 6.372549019607843e-06, 'epoch': 10.71}
********************on step end call back********************
Step 780 finish
{'loss': 0.1247, 'grad_norm': 4.23046875, 'learning_rate': 6.274509803921569e-06, 'epoch': 10.85}
********************on step end call back********************
Step 790 finish
{'loss': 0.1196, 'grad_norm': 3.697265625, 'learning_rate': 6.176470588235295e-06, 'epoch': 10.99}
********************on epoch end call back********************
Epoch 10.991304347826087 finish
********************on step end call back********************
Step 800 finish
{'loss': 0.0833, 'grad_norm': 2.560546875, 'learning_rate': 6.07843137254902e-06, 'epoch': 11.13}
{'eval_loss': 0.7572137713432312, 'eval_accuracy': 0.8571428571428571, 'eval_runtime': 10.6181, 'eval_samples_per_second': 6.027, 'eval_steps_per_second': 6.027, 'epoch': 11.13}
********************on step end call back********************
Step 810 finish
{'loss': 0.0941, 'grad_norm': 1.2509765625, 'learning_rate': 5.980392156862746e-06, 'epoch': 11.27}
********************on step end call back********************
Step 820 finish
{'loss': 0.0941, 'grad_norm': 1.19921875, 'learning_rate': 5.882352941176471e-06, 'epoch': 11.41}
[INFO|trainer.py:3376] 2024-04-02 12:08:04,682 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-04-02 12:08:04,682 >>   Num examples = 64
[INFO|trainer.py:3381] 2024-04-02 12:08:04,682 >>   Batch size = 1
[WARNING|trainer.py:2492] 2024-04-02 12:08:15,386 >> Checkpoint destination directory ./output/checkpoint-900 already exists and is non-empty. Saving will proceed but saved results may be invalid.
[INFO|trainer.py:3067] 2024-04-02 12:08:15,386 >> Saving model checkpoint to ./output/checkpoint-900
[WARNING|configuration_utils.py:449] 2024-04-02 12:08:15,388 >> Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.
Non-default generation parameters: {'max_length': 4096}
[INFO|configuration_utils.py:473] 2024-04-02 12:08:15,388 >> Configuration saved in ./output/checkpoint-900/config.json
[INFO|configuration_utils.py:845] 2024-04-02 12:08:15,389 >> Generate config GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2,
  "max_length": 4096,
  "pad_token_id": 2
}

[INFO|configuration_utils.py:614] 2024-04-02 12:08:15,389 >> Configuration saved in ./output/checkpoint-900/generation_config.json
[INFO|modeling_utils.py:2462] 2024-04-02 12:08:53,557 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 3 checkpoint shards. You can find where each parameters has been saved in the index located at ./output/checkpoint-900/model.safetensors.index.json.
[INFO|tokenization_utils_base.py:2459] 2024-04-02 12:08:53,558 >> tokenizer config file saved in ./output/checkpoint-900/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-04-02 12:08:53,558 >> Special tokens file saved in ./output/checkpoint-900/special_tokens_map.json
[INFO|trainer.py:3376] 2024-04-02 13:30:26,711 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-04-02 13:30:26,712 >>   Num examples = 64
[INFO|trainer.py:3381] 2024-04-02 13:30:26,712 >>   Batch size = 1
[WARNING|trainer.py:2492] 2024-04-02 13:30:48,078 >> Checkpoint destination directory ./output/checkpoint-1000 already exists and is non-empty. Saving will proceed but saved results may be invalid.
[INFO|trainer.py:3067] 2024-04-02 13:30:48,078 >> Saving model checkpoint to ./output/checkpoint-1000
[WARNING|configuration_utils.py:449] 2024-04-02 13:30:48,080 >> Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.
Non-default generation parameters: {'max_length': 4096}
[INFO|configuration_utils.py:473] 2024-04-02 13:30:48,080 >> Configuration saved in ./output/checkpoint-1000/config.json
[INFO|configuration_utils.py:845] 2024-04-02 13:30:48,081 >> Generate config GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2,
  "max_length": 4096,
  "pad_token_id": 2
}

[INFO|configuration_utils.py:614] 2024-04-02 13:30:48,081 >> Configuration saved in ./output/checkpoint-1000/generation_config.json
[INFO|modeling_utils.py:2462] 2024-04-02 13:31:16,328 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 3 checkpoint shards. You can find where each parameters has been saved in the index located at ./output/checkpoint-1000/model.safetensors.index.json.
[INFO|tokenization_utils_base.py:2459] 2024-04-02 13:31:16,329 >> tokenizer config file saved in ./output/checkpoint-1000/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-04-02 13:31:16,329 >> Special tokens file saved in ./output/checkpoint-1000/special_tokens_map.json
[INFO|trainer.py:3376] 2024-04-02 14:59:10,633 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-04-02 14:59:10,633 >>   Num examples = 64
[INFO|trainer.py:3381] 2024-04-02 14:59:10,633 >>   Batch size = 1
[WARNING|trainer.py:2492] 2024-04-02 14:59:32,003 >> Checkpoint destination directory ./output/checkpoint-1100 already exists and is non-empty. Saving will proceed but saved results may be invalid.
[INFO|trainer.py:3067] 2024-04-02 14:59:32,003 >> Saving model checkpoint to ./output/checkpoint-1100
[WARNING|configuration_utils.py:449] 2024-04-02 14:59:32,004 >> Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.
Non-default generation parameters: {'max_length': 4096}
[INFO|configuration_utils.py:473] 2024-04-02 14:59:32,005 >> Configuration saved in ./output/checkpoint-1100/config.json
[INFO|configuration_utils.py:845] 2024-04-02 14:59:32,005 >> Generate config GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2,
  "max_length": 4096,
  "pad_token_id": 2
}

[INFO|configuration_utils.py:614] 2024-04-02 14:59:32,006 >> Configuration saved in ./output/checkpoint-1100/generation_config.json
[INFO|modeling_utils.py:2462] 2024-04-02 15:00:05,064 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 3 checkpoint shards. You can find where each parameters has been saved in the index located at ./output/checkpoint-1100/model.safetensors.index.json.
[INFO|tokenization_utils_base.py:2459] 2024-04-02 15:00:05,065 >> tokenizer config file saved in ./output/checkpoint-1100/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-04-02 15:00:05,065 >> Special tokens file saved in ./output/checkpoint-1100/special_tokens_map.json
[INFO|trainer.py:3376] 2024-04-02 16:27:15,952 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-04-02 16:27:15,952 >>   Num examples = 64
[INFO|trainer.py:3381] 2024-04-02 16:27:15,952 >>   Batch size = 1
[WARNING|trainer.py:2492] 2024-04-02 16:27:37,367 >> Checkpoint destination directory ./output/checkpoint-1200 already exists and is non-empty. Saving will proceed but saved results may be invalid.
[INFO|trainer.py:3067] 2024-04-02 16:27:37,367 >> Saving model checkpoint to ./output/checkpoint-1200
[WARNING|configuration_utils.py:449] 2024-04-02 16:27:37,369 >> Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.
Non-default generation parameters: {'max_length': 4096}
[INFO|configuration_utils.py:473] 2024-04-02 16:27:37,370 >> Configuration saved in ./output/checkpoint-1200/config.json
[INFO|configuration_utils.py:845] 2024-04-02 16:27:37,370 >> Generate config GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2,
  "max_length": 4096,
  "pad_token_id": 2
}

[INFO|configuration_utils.py:614] 2024-04-02 16:27:37,370 >> Configuration saved in ./output/checkpoint-1200/generation_config.json
[INFO|modeling_utils.py:2462] 2024-04-02 16:28:08,884 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 3 checkpoint shards. You can find where each parameters has been saved in the index located at ./output/checkpoint-1200/model.safetensors.index.json.
[INFO|tokenization_utils_base.py:2459] 2024-04-02 16:28:08,885 >> tokenizer config file saved in ./output/checkpoint-1200/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-04-02 16:28:08,885 >> Special tokens file saved in ./output/checkpoint-1200/special_tokens_map.json
********************on step end call back********************
Step 830 finish
{'loss': 0.0995, 'grad_norm': 1.2333984375, 'learning_rate': 5.784313725490197e-06, 'epoch': 11.55}
********************on step end call back********************
Step 840 finish
{'loss': 0.099, 'grad_norm': 1.8095703125, 'learning_rate': 5.686274509803922e-06, 'epoch': 11.69}
********************on step end call back********************
Step 850 finish
{'loss': 0.1023, 'grad_norm': 2.79296875, 'learning_rate': 5.588235294117647e-06, 'epoch': 11.83}
********************on step end call back********************
Step 860 finish
{'loss': 0.1033, 'grad_norm': 2.669921875, 'learning_rate': 5.4901960784313735e-06, 'epoch': 11.97}
********************on epoch end call back********************
Epoch 11.99304347826087 finish
********************on step end call back********************
Step 870 finish
{'loss': 0.0837, 'grad_norm': 1.78515625, 'learning_rate': 5.392156862745098e-06, 'epoch': 12.1}
********************on step end call back********************
Step 880 finish
{'loss': 0.085, 'grad_norm': 1.1259765625, 'learning_rate': 5.294117647058824e-06, 'epoch': 12.24}
********************on step end call back********************
Step 890 finish
{'loss': 0.0907, 'grad_norm': 2.064453125, 'learning_rate': 5.19607843137255e-06, 'epoch': 12.38}
********************on step end call back********************
Step 900 finish
{'loss': 0.0841, 'grad_norm': 1.4921875, 'learning_rate': 5.098039215686274e-06, 'epoch': 12.52}
{'eval_loss': 0.7740086317062378, 'eval_accuracy': 0.875, 'eval_runtime': 10.7037, 'eval_samples_per_second': 5.979, 'eval_steps_per_second': 5.979, 'epoch': 12.52}
********************on step end call back********************
Step 910 finish
{'loss': 0.0896, 'grad_norm': 2.04296875, 'learning_rate': 5e-06, 'epoch': 12.66}
********************on step end call back********************
Step 920 finish
{'loss': 0.0993, 'grad_norm': 0.84326171875, 'learning_rate': 4.901960784313726e-06, 'epoch': 12.8}
********************on step end call back********************
Step 930 finish
{'loss': 0.092, 'grad_norm': 1.017578125, 'learning_rate': 4.803921568627452e-06, 'epoch': 12.94}
********************on epoch end call back********************
Epoch 12.994782608695653 finish
********************on step end call back********************
Step 940 finish
{'loss': 0.0798, 'grad_norm': 0.7958984375, 'learning_rate': 4.705882352941177e-06, 'epoch': 13.08}
********************on step end call back********************
Step 950 finish
{'loss': 0.0806, 'grad_norm': 1.173828125, 'learning_rate': 4.607843137254902e-06, 'epoch': 13.22}
********************on step end call back********************
Step 960 finish
{'loss': 0.079, 'grad_norm': 0.7666015625, 'learning_rate': 4.509803921568628e-06, 'epoch': 13.36}
********************on step end call back********************
Step 970 finish
{'loss': 0.0857, 'grad_norm': 1.2607421875, 'learning_rate': 4.411764705882353e-06, 'epoch': 13.5}
********************on step end call back********************
Step 980 finish
{'loss': 0.0882, 'grad_norm': 0.96337890625, 'learning_rate': 4.313725490196079e-06, 'epoch': 13.63}
********************on step end call back********************
Step 990 finish
{'loss': 0.0803, 'grad_norm': 0.84228515625, 'learning_rate': 4.215686274509805e-06, 'epoch': 13.77}
********************on step end call back********************
Step 1000 finish
{'loss': 0.0822, 'grad_norm': 0.81640625, 'learning_rate': 4.11764705882353e-06, 'epoch': 13.91}
{'eval_loss': 0.7747099995613098, 'eval_accuracy': 0.875, 'eval_runtime': 21.3658, 'eval_samples_per_second': 2.995, 'eval_steps_per_second': 2.995, 'epoch': 13.91}
********************on epoch end call back********************
Epoch 13.996521739130435 finish
********************on step end call back********************
Step 1010 finish
{'loss': 0.0798, 'grad_norm': 0.53271484375, 'learning_rate': 4.019607843137255e-06, 'epoch': 14.05}
********************on step end call back********************
Step 1020 finish
{'loss': 0.0719, 'grad_norm': 0.75634765625, 'learning_rate': 3.92156862745098e-06, 'epoch': 14.19}
********************on step end call back********************
Step 1030 finish
{'loss': 0.0764, 'grad_norm': 0.70068359375, 'learning_rate': 3.8235294117647055e-06, 'epoch': 14.33}
********************on step end call back********************
Step 1040 finish
{'loss': 0.0742, 'grad_norm': 0.51806640625, 'learning_rate': 3.7254901960784316e-06, 'epoch': 14.47}
********************on step end call back********************
Step 1050 finish
{'loss': 0.0798, 'grad_norm': 0.79833984375, 'learning_rate': 3.6274509803921573e-06, 'epoch': 14.61}
********************on step end call back********************
Step 1060 finish
{'loss': 0.0764, 'grad_norm': 0.83203125, 'learning_rate': 3.529411764705883e-06, 'epoch': 14.75}
********************on step end call back********************
Step 1070 finish
{'loss': 0.0806, 'grad_norm': 0.7685546875, 'learning_rate': 3.431372549019608e-06, 'epoch': 14.89}
********************on epoch end call back********************
Epoch 14.998260869565218 finish
********************on step end call back********************
Step 1080 finish
{'loss': 0.0743, 'grad_norm': 0.7236328125, 'learning_rate': 3.3333333333333333e-06, 'epoch': 15.03}
********************on step end call back********************
Step 1090 finish
{'loss': 0.07, 'grad_norm': 0.76416015625, 'learning_rate': 3.2352941176470594e-06, 'epoch': 15.17}
********************on step end call back********************
Step 1100 finish
{'loss': 0.0687, 'grad_norm': 0.73046875, 'learning_rate': 3.1372549019607846e-06, 'epoch': 15.3}
{'eval_loss': 0.8547166585922241, 'eval_accuracy': 0.875, 'eval_runtime': 21.3688, 'eval_samples_per_second': 2.995, 'eval_steps_per_second': 2.995, 'epoch': 15.3}
********************on step end call back********************
Step 1110 finish
{'loss': 0.0687, 'grad_norm': 0.81689453125, 'learning_rate': 3.03921568627451e-06, 'epoch': 15.44}
********************on step end call back********************
Step 1120 finish
{'loss': 0.0736, 'grad_norm': 0.9052734375, 'learning_rate': 2.9411764705882355e-06, 'epoch': 15.58}
********************on step end call back********************
Step 1130 finish
{'loss': 0.0742, 'grad_norm': 0.71875, 'learning_rate': 2.843137254901961e-06, 'epoch': 15.72}
********************on step end call back********************
Step 1140 finish
{'loss': 0.0732, 'grad_norm': 1.01953125, 'learning_rate': 2.7450980392156867e-06, 'epoch': 15.86}
********************on step end call back********************
Step 1150 finish
{'loss': 0.0742, 'grad_norm': 0.990234375, 'learning_rate': 2.647058823529412e-06, 'epoch': 16.0}
********************on epoch end call back********************
Epoch 16.0 finish
********************on step end call back********************
Step 1160 finish
{'loss': 0.0672, 'grad_norm': 0.64794921875, 'learning_rate': 2.549019607843137e-06, 'epoch': 16.14}
********************on step end call back********************
Step 1170 finish
{'loss': 0.0678, 'grad_norm': 0.96337890625, 'learning_rate': 2.450980392156863e-06, 'epoch': 16.28}
********************on step end call back********************
Step 1180 finish
{'loss': 0.0672, 'grad_norm': 0.791015625, 'learning_rate': 2.3529411764705885e-06, 'epoch': 16.42}
********************on step end call back********************
Step 1190 finish
{'loss': 0.0711, 'grad_norm': 0.80419921875, 'learning_rate': 2.254901960784314e-06, 'epoch': 16.56}
********************on step end call back********************
Step 1200 finish
{'loss': 0.0693, 'grad_norm': 0.771484375, 'learning_rate': 2.1568627450980393e-06, 'epoch': 16.7}
{'eval_loss': 0.8744620084762573, 'eval_accuracy': 0.875, 'eval_runtime': 21.4147, 'eval_samples_per_second': 2.989, 'eval_steps_per_second': 2.989, 'epoch': 16.7}
********************on step end call back********************
Step 1210 finish
{'loss': 0.0727, 'grad_norm': 0.8310546875, 'learning_rate': 2.058823529411765e-06, 'epoch': 16.83}
********************on step end call back********************
Step 1220 finish
[INFO|trainer.py:3376] 2024-04-02 17:57:54,762 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-04-02 17:57:54,762 >>   Num examples = 64
[INFO|trainer.py:3381] 2024-04-02 17:57:54,762 >>   Batch size = 1
[WARNING|trainer.py:2492] 2024-04-02 17:58:15,375 >> Checkpoint destination directory ./output/checkpoint-1300 already exists and is non-empty. Saving will proceed but saved results may be invalid.
[INFO|trainer.py:3067] 2024-04-02 17:58:15,376 >> Saving model checkpoint to ./output/checkpoint-1300
[WARNING|configuration_utils.py:449] 2024-04-02 17:58:15,377 >> Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.
Non-default generation parameters: {'max_length': 4096}
[INFO|configuration_utils.py:473] 2024-04-02 17:58:15,378 >> Configuration saved in ./output/checkpoint-1300/config.json
[INFO|configuration_utils.py:845] 2024-04-02 17:58:15,378 >> Generate config GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2,
  "max_length": 4096,
  "pad_token_id": 2
}

[INFO|configuration_utils.py:614] 2024-04-02 17:58:15,378 >> Configuration saved in ./output/checkpoint-1300/generation_config.json
[INFO|modeling_utils.py:2462] 2024-04-02 17:58:40,480 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 3 checkpoint shards. You can find where each parameters has been saved in the index located at ./output/checkpoint-1300/model.safetensors.index.json.
[INFO|tokenization_utils_base.py:2459] 2024-04-02 17:58:40,481 >> tokenizer config file saved in ./output/checkpoint-1300/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-04-02 17:58:40,481 >> Special tokens file saved in ./output/checkpoint-1300/special_tokens_map.json
[INFO|trainer.py:3376] 2024-04-02 19:27:10,953 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-04-02 19:27:10,954 >>   Num examples = 64
[INFO|trainer.py:3381] 2024-04-02 19:27:10,954 >>   Batch size = 1
[WARNING|trainer.py:2492] 2024-04-02 19:27:32,740 >> Checkpoint destination directory ./output/checkpoint-1400 already exists and is non-empty. Saving will proceed but saved results may be invalid.
[INFO|trainer.py:3067] 2024-04-02 19:27:32,740 >> Saving model checkpoint to ./output/checkpoint-1400
[WARNING|configuration_utils.py:449] 2024-04-02 19:27:32,742 >> Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.
Non-default generation parameters: {'max_length': 4096}
[INFO|configuration_utils.py:473] 2024-04-02 19:27:32,743 >> Configuration saved in ./output/checkpoint-1400/config.json
[INFO|configuration_utils.py:845] 2024-04-02 19:27:32,743 >> Generate config GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2,
  "max_length": 4096,
  "pad_token_id": 2
}

[INFO|configuration_utils.py:614] 2024-04-02 19:27:32,743 >> Configuration saved in ./output/checkpoint-1400/generation_config.json
[INFO|modeling_utils.py:2462] 2024-04-02 19:28:12,510 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 3 checkpoint shards. You can find where each parameters has been saved in the index located at ./output/checkpoint-1400/model.safetensors.index.json.
[INFO|tokenization_utils_base.py:2459] 2024-04-02 19:28:12,511 >> tokenizer config file saved in ./output/checkpoint-1400/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-04-02 19:28:12,511 >> Special tokens file saved in ./output/checkpoint-1400/special_tokens_map.json
[INFO|trainer.py:2067] 2024-04-02 19:46:43,102 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:3067] 2024-04-02 19:46:43,104 >> Saving model checkpoint to ./output/best_model
[WARNING|configuration_utils.py:449] 2024-04-02 19:46:43,106 >> Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.
Non-default generation parameters: {'max_length': 4096}
[INFO|configuration_utils.py:473] 2024-04-02 19:46:43,107 >> Configuration saved in ./output/best_model/config.json
[INFO|configuration_utils.py:845] 2024-04-02 19:46:43,108 >> Generate config GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2,
  "max_length": 4096,
  "pad_token_id": 2
}

[INFO|configuration_utils.py:614] 2024-04-02 19:46:43,108 >> Configuration saved in ./output/best_model/generation_config.json
[INFO|modeling_utils.py:2462] 2024-04-02 19:47:22,494 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 3 checkpoint shards. You can find where each parameters has been saved in the index located at ./output/best_model/model.safetensors.index.json.
[INFO|tokenization_utils_base.py:2459] 2024-04-02 19:47:22,495 >> tokenizer config file saved in ./output/best_model/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-04-02 19:47:22,495 >> Special tokens file saved in ./output/best_model/special_tokens_map.json
{'loss': 0.0673, 'grad_norm': 0.8134765625, 'learning_rate': 1.96078431372549e-06, 'epoch': 16.97}
********************on epoch end call back********************
Epoch 16.98782608695652 finish
********************on step end call back********************
Step 1230 finish
{'loss': 0.0694, 'grad_norm': 0.80126953125, 'learning_rate': 1.8627450980392158e-06, 'epoch': 17.11}
********************on step end call back********************
Step 1240 finish
{'loss': 0.0675, 'grad_norm': 0.740234375, 'learning_rate': 1.7647058823529414e-06, 'epoch': 17.25}
********************on step end call back********************
Step 1250 finish
{'loss': 0.0673, 'grad_norm': 0.7724609375, 'learning_rate': 1.6666666666666667e-06, 'epoch': 17.39}
********************on step end call back********************
Step 1260 finish
{'loss': 0.0658, 'grad_norm': 0.64013671875, 'learning_rate': 1.5686274509803923e-06, 'epoch': 17.53}
********************on step end call back********************
Step 1270 finish
{'loss': 0.0673, 'grad_norm': 0.767578125, 'learning_rate': 1.4705882352941177e-06, 'epoch': 17.67}
********************on step end call back********************
Step 1280 finish
{'loss': 0.0693, 'grad_norm': 0.86474609375, 'learning_rate': 1.3725490196078434e-06, 'epoch': 17.81}
********************on step end call back********************
Step 1290 finish
{'loss': 0.0673, 'grad_norm': 0.88671875, 'learning_rate': 1.2745098039215686e-06, 'epoch': 17.95}
********************on epoch end call back********************
Epoch 17.989565217391306 finish
********************on step end call back********************
Step 1300 finish
{'loss': 0.07, 'grad_norm': 0.75, 'learning_rate': 1.1764705882352942e-06, 'epoch': 18.09}
{'eval_loss': 0.8904364109039307, 'eval_accuracy': 0.875, 'eval_runtime': 20.6131, 'eval_samples_per_second': 3.105, 'eval_steps_per_second': 3.105, 'epoch': 18.09}
********************on step end call back********************
Step 1310 finish
{'loss': 0.0705, 'grad_norm': 0.83740234375, 'learning_rate': 1.0784313725490197e-06, 'epoch': 18.23}
********************on step end call back********************
Step 1320 finish
{'loss': 0.0644, 'grad_norm': 0.80419921875, 'learning_rate': 9.80392156862745e-07, 'epoch': 18.37}
********************on step end call back********************
Step 1330 finish
{'loss': 0.0651, 'grad_norm': 0.7197265625, 'learning_rate': 8.823529411764707e-07, 'epoch': 18.5}
********************on step end call back********************
Step 1340 finish
{'loss': 0.0637, 'grad_norm': 0.837890625, 'learning_rate': 7.843137254901962e-07, 'epoch': 18.64}
********************on step end call back********************
Step 1350 finish
{'loss': 0.0692, 'grad_norm': 0.67138671875, 'learning_rate': 6.862745098039217e-07, 'epoch': 18.78}
********************on step end call back********************
Step 1360 finish
{'loss': 0.0666, 'grad_norm': 0.779296875, 'learning_rate': 5.882352941176471e-07, 'epoch': 18.92}
********************on epoch end call back********************
Epoch 18.991304347826087 finish
********************on step end call back********************
Step 1370 finish
{'loss': 0.0641, 'grad_norm': 0.779296875, 'learning_rate': 4.901960784313725e-07, 'epoch': 19.06}
********************on step end call back********************
Step 1380 finish
{'loss': 0.0651, 'grad_norm': 0.80712890625, 'learning_rate': 3.921568627450981e-07, 'epoch': 19.2}
********************on step end call back********************
Step 1390 finish
{'loss': 0.0693, 'grad_norm': 0.8486328125, 'learning_rate': 2.9411764705882356e-07, 'epoch': 19.34}
********************on step end call back********************
Step 1400 finish
{'loss': 0.0643, 'grad_norm': 0.9267578125, 'learning_rate': 1.9607843137254904e-07, 'epoch': 19.48}
{'eval_loss': 0.893712043762207, 'eval_accuracy': 0.875, 'eval_runtime': 21.7856, 'eval_samples_per_second': 2.938, 'eval_steps_per_second': 2.938, 'epoch': 19.48}
********************on step end call back********************
Step 1410 finish
{'loss': 0.0659, 'grad_norm': 0.69384765625, 'learning_rate': 9.803921568627452e-08, 'epoch': 19.62}
********************on step end call back********************
Step 1420 finish
{'loss': 0.0664, 'grad_norm': 0.87060546875, 'learning_rate': 0.0, 'epoch': 19.76}
********************on epoch end call back********************
Epoch 19.756521739130434 finish
{'train_runtime': 70430.1923, 'train_samples_per_second': 0.163, 'train_steps_per_second': 0.02, 'train_loss': 0.278932189017954, 'epoch': 19.76}
***** train metrics *****
  epoch                    =       19.76
  train_loss               =      0.2789
  train_runtime            = 19:33:50.19
  train_samples            =         575
  train_samples_per_second =       0.163
  train_steps_per_second   =        0.02
04/02/2024 19:47:22 - INFO - decoder_fine_tuning.log - *** Evaluate ***
[INFO|trainer.py:3376] 2024-04-02 19:47:22,531 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-04-02 19:47:22,531 >>   Num examples = 64
[INFO|trainer.py:3381] 2024-04-02 19:47:22,531 >>   Batch size = 1
{'eval_loss': 0.8943244814872742, 'eval_accuracy': 0.875, 'eval_runtime': 18.9056, 'eval_samples_per_second': 3.385, 'eval_steps_per_second': 3.385, 'epoch': 19.76}
***** eval metrics *****
  epoch                   =      19.76
  eval_accuracy           =      0.875
  eval_loss               =     0.8943
  eval_runtime            = 0:00:18.90
  eval_samples            =         64
  eval_samples_per_second =      3.385
  eval_steps_per_second   =      3.385
  perplexity              =     2.4457
04/02/2024 19:47:41 - INFO - decoder_fine_tuning.log - *** Predict ***
[INFO|trainer.py:3376] 2024-04-02 19:47:41,435 >> ***** Running Prediction *****
[INFO|trainer.py:3378] 2024-04-02 19:47:41,436 >>   Num examples = 64
[INFO|trainer.py:3381] 2024-04-02 19:47:41,436 >>   Batch size = 1
The model was loaded with use_flash_attention_2=True, which is deprecated and may be removed in a future release. Please use `attn_implementation="flash_attention_2"` instead.
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:01,  1.27it/s]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:01<00:00,  1.18it/s]Loading checkpoint shards: 100%|██████████| 3/3 [00:02<00:00,  1.33it/s]Loading checkpoint shards: 100%|██████████| 3/3 [00:02<00:00,  1.29it/s]
模型结构 LlamaForCausalLM
Traceback (most recent call last):
  File "/home/asus/intelligent-test/decoder_fine_tuning/predict.py", line 99, in <module>
    inputs, targets = get_datas(args.eval_dataset)
  File "/home/asus/intelligent-test/decoder_fine_tuning/predict.py", line 50, in get_datas
    with open(file_path, "r", encoding="utf-8") as f:
FileNotFoundError: [Errno 2] No such file or directory: '../data/ir_validate.csv'
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
The model was loaded with use_flash_attention_2=True, which is deprecated and may be removed in a future release. Please use `attn_implementation="flash_attention_2"` instead.
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:01,  1.16it/s]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:01<00:00,  1.12it/s]Loading checkpoint shards: 100%|██████████| 3/3 [00:02<00:00,  1.24it/s]Loading checkpoint shards: 100%|██████████| 3/3 [00:02<00:00,  1.21it/s]
模型结构 LlamaForCausalLM
Traceback (most recent call last):
  File "/home/asus/intelligent-test/decoder_fine_tuning/predict.py", line 99, in <module>
    inputs, targets = get_datas(args.eval_dataset)
  File "/home/asus/intelligent-test/decoder_fine_tuning/predict.py", line 50, in get_datas
    with open(file_path, "r", encoding="utf-8") as f:
FileNotFoundError: [Errno 2] No such file or directory: '../data/ir_validate.csv'
