04/02/2024 23:05:14 - WARNING - decoder_lora.log - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: False
04/02/2024 23:05:14 - INFO - decoder_lora.log - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=True,
bf16_full_eval=True,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=False,
ddp_timeout=18000000,
debug=[],
deepspeed=None,
disable_tqdm=True,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=100,
evaluation_strategy=steps,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=8,
gradient_checkpointing=True,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=True,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=0.0001,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=./output/logs,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=10,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=10.0,
optim=adamw_torch,
optim_args=None,
output_dir=./output,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=1,
per_device_train_batch_size=1,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['tensorboard'],
resume_from_checkpoint=None,
run_name=./output,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=100,
save_strategy=steps,
save_total_limit=100,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=400,
weight_decay=0.0,
)
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/transformers/models/auto/configuration_auto.py:1096: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
[INFO|configuration_utils.py:726] 2024-04-02 23:05:14,533 >> loading configuration file ../model/pretrained/Atom-7B/config.json
[INFO|configuration_utils.py:791] 2024-04-02 23:05:14,533 >> Model config LlamaConfig {
  "_name_or_path": "../model/pretrained/Atom-7B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "auto_map": {
    "AutoConfig": "configuration_atom.LlamaConfig",
    "AutoModel": "model_atom.LlamaForCausalLM",
    "AutoModelForCausalLM": "model_atom.LlamaForCausalLM",
    "AutoModelForSequenceClassification": "model_atom.LlamaForSequenceClassification"
  },
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_length": 4096,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 2,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "type": "dynamic"
  },
  "rope_theta": 500000,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.38.2",
  "use_cache": true,
  "vocab_size": 65000
}

/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/transformers/models/auto/tokenization_auto.py:720: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
[INFO|tokenization_utils_base.py:2044] 2024-04-02 23:05:14,534 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:2044] 2024-04-02 23:05:14,534 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2044] 2024-04-02 23:05:14,534 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2044] 2024-04-02 23:05:14,534 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2044] 2024-04-02 23:05:14,535 >> loading file tokenizer.json
</s>
04/02/2024 23:05:14 - INFO - decoder_lora.log - lora配置: LoraConfig(peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, task_type='CAUSAL_LM', inference_mode=False, r=8, target_modules={'q_proj', 'o_proj', 'gate_proj', 'up_proj', 'v_proj', 'k_proj', 'down_proj'}, lora_alpha=16, lora_dropout=0.05, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', loftq_config={}, use_dora=False)
04/02/2024 23:05:14 - INFO - decoder_lora.log - torch_dtype: torch.float16
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py:466: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
[INFO|modeling_utils.py:3254] 2024-04-02 23:05:14,621 >> loading weights file ../model/pretrained/Atom-7B/model.safetensors.index.json
[INFO|modeling_utils.py:1400] 2024-04-02 23:05:14,621 >> Instantiating LlamaForCausalLM model under default dtype torch.float16.
[WARNING|logging.py:329] 2024-04-02 23:05:14,622 >> The model was loaded with use_flash_attention_2=True, which is deprecated and may be removed in a future release. Please use `attn_implementation="flash_attention_2"` instead.
[INFO|configuration_utils.py:845] 2024-04-02 23:05:14,624 >> Generate config GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2,
  "max_length": 4096,
  "pad_token_id": 2
}

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:03<00:07,  3.55s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:06<00:03,  3.08s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:08<00:00,  2.75s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:08<00:00,  2.89s/it]
[INFO|modeling_utils.py:3992] 2024-04-02 23:05:23,517 >> All model checkpoint weights were used when initializing LlamaForCausalLM.

[INFO|modeling_utils.py:4000] 2024-04-02 23:05:23,517 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at ../model/pretrained/Atom-7B.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.
[INFO|configuration_utils.py:798] 2024-04-02 23:05:23,519 >> loading configuration file ../model/pretrained/Atom-7B/generation_config.json
[INFO|configuration_utils.py:845] 2024-04-02 23:05:23,519 >> Generate config GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2,
  "max_length": 4096,
  "pad_token_id": 2
}

/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/datasets/load.py:2516: FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.
You can remove this warning by passing 'token=<use_auth_token>' instead.
  warnings.warn(
Using custom data configuration default-eefa1f3370c37a64
trainable params: 19,988,480 || all params: 7,028,740,096 || trainable%: 0.284382118658439
04/02/2024 23:05:25 - INFO - datasets.builder - Using custom data configuration default-eefa1f3370c37a64
Loading Dataset Infos from /home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/datasets/packaged_modules/csv
04/02/2024 23:05:25 - INFO - datasets.info - Loading Dataset Infos from /home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/datasets/packaged_modules/csv
Overwrite dataset info from restored data version if exists.
04/02/2024 23:05:25 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.
Loading Dataset info from ./output/dataset_cache/csv/default-eefa1f3370c37a64/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6
04/02/2024 23:05:25 - INFO - datasets.info - Loading Dataset info from ./output/dataset_cache/csv/default-eefa1f3370c37a64/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6
Found cached dataset csv (/home/asus/intelligent-test/decoder_lora/output/dataset_cache/csv/default-eefa1f3370c37a64/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6)
04/02/2024 23:05:25 - INFO - datasets.builder - Found cached dataset csv (/home/asus/intelligent-test/decoder_lora/output/dataset_cache/csv/default-eefa1f3370c37a64/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6)
Loading Dataset info from /home/asus/intelligent-test/decoder_lora/output/dataset_cache/csv/default-eefa1f3370c37a64/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6
04/02/2024 23:05:25 - INFO - datasets.info - Loading Dataset info from /home/asus/intelligent-test/decoder_lora/output/dataset_cache/csv/default-eefa1f3370c37a64/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6
Process #0 will write at /home/asus/intelligent-test/decoder_lora/output/dataset_cache/csv/default-eefa1f3370c37a64/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-4f4936303bb7a144_00000_of_00010.arrow
数据集中是否输入输出在同一列: True
04/02/2024 23:05:25 - INFO - datasets.arrow_dataset - Process #0 will write at /home/asus/intelligent-test/decoder_lora/output/dataset_cache/csv/default-eefa1f3370c37a64/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-4f4936303bb7a144_00000_of_00010.arrow
Process #1 will write at /home/asus/intelligent-test/decoder_lora/output/dataset_cache/csv/default-eefa1f3370c37a64/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-4f4936303bb7a144_00001_of_00010.arrow
04/02/2024 23:05:25 - INFO - datasets.arrow_dataset - Process #1 will write at /home/asus/intelligent-test/decoder_lora/output/dataset_cache/csv/default-eefa1f3370c37a64/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-4f4936303bb7a144_00001_of_00010.arrow
Process #2 will write at /home/asus/intelligent-test/decoder_lora/output/dataset_cache/csv/default-eefa1f3370c37a64/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-4f4936303bb7a144_00002_of_00010.arrow
04/02/2024 23:05:25 - INFO - datasets.arrow_dataset - Process #2 will write at /home/asus/intelligent-test/decoder_lora/output/dataset_cache/csv/default-eefa1f3370c37a64/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-4f4936303bb7a144_00002_of_00010.arrow
Process #3 will write at /home/asus/intelligent-test/decoder_lora/output/dataset_cache/csv/default-eefa1f3370c37a64/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-4f4936303bb7a144_00003_of_00010.arrow
04/02/2024 23:05:25 - INFO - datasets.arrow_dataset - Process #3 will write at /home/asus/intelligent-test/decoder_lora/output/dataset_cache/csv/default-eefa1f3370c37a64/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-4f4936303bb7a144_00003_of_00010.arrow
Process #4 will write at /home/asus/intelligent-test/decoder_lora/output/dataset_cache/csv/default-eefa1f3370c37a64/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-4f4936303bb7a144_00004_of_00010.arrow
04/02/2024 23:05:25 - INFO - datasets.arrow_dataset - Process #4 will write at /home/asus/intelligent-test/decoder_lora/output/dataset_cache/csv/default-eefa1f3370c37a64/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-4f4936303bb7a144_00004_of_00010.arrow
Process #5 will write at /home/asus/intelligent-test/decoder_lora/output/dataset_cache/csv/default-eefa1f3370c37a64/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-4f4936303bb7a144_00005_of_00010.arrow
04/02/2024 23:05:25 - INFO - datasets.arrow_dataset - Process #5 will write at /home/asus/intelligent-test/decoder_lora/output/dataset_cache/csv/default-eefa1f3370c37a64/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-4f4936303bb7a144_00005_of_00010.arrow
Process #6 will write at /home/asus/intelligent-test/decoder_lora/output/dataset_cache/csv/default-eefa1f3370c37a64/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-4f4936303bb7a144_00006_of_00010.arrow
04/02/2024 23:05:25 - INFO - datasets.arrow_dataset - Process #6 will write at /home/asus/intelligent-test/decoder_lora/output/dataset_cache/csv/default-eefa1f3370c37a64/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-4f4936303bb7a144_00006_of_00010.arrow
Process #7 will write at /home/asus/intelligent-test/decoder_lora/output/dataset_cache/csv/default-eefa1f3370c37a64/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-4f4936303bb7a144_00007_of_00010.arrow
04/02/2024 23:05:25 - INFO - datasets.arrow_dataset - Process #7 will write at /home/asus/intelligent-test/decoder_lora/output/dataset_cache/csv/default-eefa1f3370c37a64/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-4f4936303bb7a144_00007_of_00010.arrow
Process #8 will write at /home/asus/intelligent-test/decoder_lora/output/dataset_cache/csv/default-eefa1f3370c37a64/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-4f4936303bb7a144_00008_of_00010.arrow
04/02/2024 23:05:25 - INFO - datasets.arrow_dataset - Process #8 will write at /home/asus/intelligent-test/decoder_lora/output/dataset_cache/csv/default-eefa1f3370c37a64/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-4f4936303bb7a144_00008_of_00010.arrow
Process #9 will write at /home/asus/intelligent-test/decoder_lora/output/dataset_cache/csv/default-eefa1f3370c37a64/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-4f4936303bb7a144_00009_of_00010.arrow
04/02/2024 23:05:25 - INFO - datasets.arrow_dataset - Process #9 will write at /home/asus/intelligent-test/decoder_lora/output/dataset_cache/csv/default-eefa1f3370c37a64/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-4f4936303bb7a144_00009_of_00010.arrow
Loading cached processed dataset at /home/asus/intelligent-test/decoder_lora/output/dataset_cache/csv/default-eefa1f3370c37a64/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-4f4936303bb7a144_*_of_00010.arrow
04/02/2024 23:05:25 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /home/asus/intelligent-test/decoder_lora/output/dataset_cache/csv/default-eefa1f3370c37a64/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-4f4936303bb7a144_*_of_00010.arrow
Concatenating 10 shards
04/02/2024 23:05:25 - INFO - datasets.arrow_dataset - Concatenating 10 shards
Process #0 will write at /home/asus/intelligent-test/decoder_lora/output/dataset_cache/csv/default-eefa1f3370c37a64/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-67294efdfe8c36b2_00000_of_00010.arrow
04/02/2024 23:05:25 - INFO - datasets.arrow_dataset - Process #0 will write at /home/asus/intelligent-test/decoder_lora/output/dataset_cache/csv/default-eefa1f3370c37a64/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-67294efdfe8c36b2_00000_of_00010.arrow
Process #1 will write at /home/asus/intelligent-test/decoder_lora/output/dataset_cache/csv/default-eefa1f3370c37a64/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-67294efdfe8c36b2_00001_of_00010.arrow
04/02/2024 23:05:25 - INFO - datasets.arrow_dataset - Process #1 will write at /home/asus/intelligent-test/decoder_lora/output/dataset_cache/csv/default-eefa1f3370c37a64/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-67294efdfe8c36b2_00001_of_00010.arrow
Process #2 will write at /home/asus/intelligent-test/decoder_lora/output/dataset_cache/csv/default-eefa1f3370c37a64/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-67294efdfe8c36b2_00002_of_00010.arrow
04/02/2024 23:05:25 - INFO - datasets.arrow_dataset - Process #2 will write at /home/asus/intelligent-test/decoder_lora/output/dataset_cache/csv/default-eefa1f3370c37a64/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-67294efdfe8c36b2_00002_of_00010.arrow
Process #3 will write at /home/asus/intelligent-test/decoder_lora/output/dataset_cache/csv/default-eefa1f3370c37a64/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-67294efdfe8c36b2_00003_of_00010.arrow
04/02/2024 23:05:25 - INFO - datasets.arrow_dataset - Process #3 will write at /home/asus/intelligent-test/decoder_lora/output/dataset_cache/csv/default-eefa1f3370c37a64/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-67294efdfe8c36b2_00003_of_00010.arrow
Process #4 will write at /home/asus/intelligent-test/decoder_lora/output/dataset_cache/csv/default-eefa1f3370c37a64/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-67294efdfe8c36b2_00004_of_00010.arrow
04/02/2024 23:05:25 - INFO - datasets.arrow_dataset - Process #4 will write at /home/asus/intelligent-test/decoder_lora/output/dataset_cache/csv/default-eefa1f3370c37a64/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-67294efdfe8c36b2_00004_of_00010.arrow
Process #5 will write at /home/asus/intelligent-test/decoder_lora/output/dataset_cache/csv/default-eefa1f3370c37a64/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-67294efdfe8c36b2_00005_of_00010.arrow
04/02/2024 23:05:25 - INFO - datasets.arrow_dataset - Process #5 will write at /home/asus/intelligent-test/decoder_lora/output/dataset_cache/csv/default-eefa1f3370c37a64/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-67294efdfe8c36b2_00005_of_00010.arrow
Process #6 will write at /home/asus/intelligent-test/decoder_lora/output/dataset_cache/csv/default-eefa1f3370c37a64/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-67294efdfe8c36b2_00006_of_00010.arrow
04/02/2024 23:05:25 - INFO - datasets.arrow_dataset - Process #6 will write at /home/asus/intelligent-test/decoder_lora/output/dataset_cache/csv/default-eefa1f3370c37a64/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-67294efdfe8c36b2_00006_of_00010.arrow
Process #7 will write at /home/asus/intelligent-test/decoder_lora/output/dataset_cache/csv/default-eefa1f3370c37a64/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-67294efdfe8c36b2_00007_of_00010.arrow
04/02/2024 23:05:25 - INFO - datasets.arrow_dataset - Process #7 will write at /home/asus/intelligent-test/decoder_lora/output/dataset_cache/csv/default-eefa1f3370c37a64/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-67294efdfe8c36b2_00007_of_00010.arrow
Process #8 will write at /home/asus/intelligent-test/decoder_lora/output/dataset_cache/csv/default-eefa1f3370c37a64/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-67294efdfe8c36b2_00008_of_00010.arrow
04/02/2024 23:05:25 - INFO - datasets.arrow_dataset - Process #8 will write at /home/asus/intelligent-test/decoder_lora/output/dataset_cache/csv/default-eefa1f3370c37a64/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-67294efdfe8c36b2_00008_of_00010.arrow
Process #9 will write at /home/asus/intelligent-test/decoder_lora/output/dataset_cache/csv/default-eefa1f3370c37a64/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-67294efdfe8c36b2_00009_of_00010.arrow
04/02/2024 23:05:25 - INFO - datasets.arrow_dataset - Process #9 will write at /home/asus/intelligent-test/decoder_lora/output/dataset_cache/csv/default-eefa1f3370c37a64/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-67294efdfe8c36b2_00009_of_00010.arrow
Loading cached processed dataset at /home/asus/intelligent-test/decoder_lora/output/dataset_cache/csv/default-eefa1f3370c37a64/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-67294efdfe8c36b2_*_of_00010.arrow
04/02/2024 23:05:25 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /home/asus/intelligent-test/decoder_lora/output/dataset_cache/csv/default-eefa1f3370c37a64/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-67294efdfe8c36b2_*_of_00010.arrow
Concatenating 10 shards
04/02/2024 23:05:25 - INFO - datasets.arrow_dataset - Concatenating 10 shards
04/02/2024 23:05:25 - INFO - decoder_lora.log - 训练集的采样114: {'input_ids': [1, 12968, 29901, 38857, 32949, 33304, 30214, 40339, 38276, 32471, 53824, 33304, 32135, 33690, 32066, 52072, 32513, 30267, 13, 33304, 29901, 32405, 34608, 34353, 34827, 36092, 34353, 30214, 32147, 31407, 31268, 30544, 30682, 36735, 35959, 42515, 34353, 32696, 30330, 31407, 36912, 32473, 30330, 31407, 32886, 32407, 41575, 31184, 33562, 39005, 32039, 31570, 36065, 32198, 31184, 32451, 35506, 56868, 30210, 30505, 36662, 34353, 30214, 33201, 30505, 32686, 32405, 33911, 35748, 33371, 30594, 30214, 32007, 32110, 53180, 57058, 32236, 30539, 39552, 32128, 32039, 32115, 37356, 32114, 32686, 31149, 39708, 30267, 13, 2, 1, 4007, 22137, 29901, 32406, 33602, 29901, 32405, 34608, 34353, 34827, 36092, 34353, 29892, 32810, 29901, 32147, 31407, 31268, 30544, 30682, 36735, 35959, 42515, 34353, 32696, 30330, 31407, 36912, 32473, 30330, 31407, 32886, 32407, 41575, 31184, 33562, 39005, 29892, 272, 29901, 32039, 29892, 32810, 29901, 31570, 36065, 32198, 31184, 32451, 35506, 56868, 30210, 30505, 36662, 34353, 29892, 32406, 33602, 29901, 33201, 29892, 32406, 29901, 32686, 29892, 32406, 32141, 29901, 32405, 33911, 35748, 33371, 29892, 1767, 29901, 53180, 57058, 29892, 1767, 29901, 30539, 39552, 32128, 29892, 272, 29901, 32039, 29892, 1767, 29901, 32115, 37356, 32114, 29892, 32406, 29901, 32686, 29892, 32406, 32141, 29901, 31149, 39708, 13, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [1, 12968, 29901, 38857, 32949, 33304, 30214, 40339, 38276, 32471, 53824, 33304, 32135, 33690, 32066, 52072, 32513, 30267, 13, 33304, 29901, 32405, 34608, 34353, 34827, 36092, 34353, 30214, 32147, 31407, 31268, 30544, 30682, 36735, 35959, 42515, 34353, 32696, 30330, 31407, 36912, 32473, 30330, 31407, 32886, 32407, 41575, 31184, 33562, 39005, 32039, 31570, 36065, 32198, 31184, 32451, 35506, 56868, 30210, 30505, 36662, 34353, 30214, 33201, 30505, 32686, 32405, 33911, 35748, 33371, 30594, 30214, 32007, 32110, 53180, 57058, 32236, 30539, 39552, 32128, 32039, 32115, 37356, 32114, 32686, 31149, 39708, 30267, 13, 2, 1, 4007, 22137, 29901, 32406, 33602, 29901, 32405, 34608, 34353, 34827, 36092, 34353, 29892, 32810, 29901, 32147, 31407, 31268, 30544, 30682, 36735, 35959, 42515, 34353, 32696, 30330, 31407, 36912, 32473, 30330, 31407, 32886, 32407, 41575, 31184, 33562, 39005, 29892, 272, 29901, 32039, 29892, 32810, 29901, 31570, 36065, 32198, 31184, 32451, 35506, 56868, 30210, 30505, 36662, 34353, 29892, 32406, 33602, 29901, 33201, 29892, 32406, 29901, 32686, 29892, 32406, 32141, 29901, 32405, 33911, 35748, 33371, 29892, 1767, 29901, 53180, 57058, 29892, 1767, 29901, 30539, 39552, 32128, 29892, 272, 29901, 32039, 29892, 1767, 29901, 32115, 37356, 32114, 29892, 32406, 29901, 32686, 29892, 32406, 32141, 29901, 31149, 39708, 13, 2]}
Loading cached shuffled indices for dataset at /home/asus/intelligent-test/decoder_lora/output/dataset_cache/csv/default-eefa1f3370c37a64/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-7041622b03318dbc.arrow
04/02/2024 23:05:25 - INFO - datasets.arrow_dataset - Loading cached shuffled indices for dataset at /home/asus/intelligent-test/decoder_lora/output/dataset_cache/csv/default-eefa1f3370c37a64/0.0.0/2c2bba169a7e870eb2bcc07f2c69b72b171db4933b1c2e2833d2b48aeaf2f6c6/cache-7041622b03318dbc.arrow
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
  warnings.warn(
[INFO|trainer.py:601] 2024-04-02 23:05:25,637 >> Using auto half precision backend
[INFO|trainer.py:1812] 2024-04-02 23:05:25,866 >> ***** Running training *****
[INFO|trainer.py:1813] 2024-04-02 23:05:25,867 >>   Num examples = 575
[INFO|trainer.py:1814] 2024-04-02 23:05:25,867 >>   Num Epochs = 10
[INFO|trainer.py:1815] 2024-04-02 23:05:25,867 >>   Instantaneous batch size per device = 1
[INFO|trainer.py:1818] 2024-04-02 23:05:25,867 >>   Total train batch size (w. parallel, distributed & accumulation) = 8
[INFO|trainer.py:1819] 2024-04-02 23:05:25,867 >>   Gradient Accumulation steps = 8
[INFO|trainer.py:1820] 2024-04-02 23:05:25,867 >>   Total optimization steps = 710
[INFO|trainer.py:1821] 2024-04-02 23:05:25,870 >>   Number of trainable parameters = 19,988,480
04/02/2024 23:05:25 - WARNING - transformers_modules.Atom-7B.model_atom - `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
04/02/2024 23:05:26 - WARNING - transformers_modules.Atom-7B.model_atom - The input hidden states seems to be silently casted in float32, this might be related to the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in torch.float16.
[INFO|trainer.py:3376] 2024-04-02 23:12:47,461 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-04-02 23:12:47,461 >>   Num examples = 64
[INFO|trainer.py:3381] 2024-04-02 23:12:47,461 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-04-02 23:13:00,143 >> Saving model checkpoint to ./output/tmp-checkpoint-100
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/pretrained/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-04-02 23:13:00,342 >> tokenizer config file saved in ./output/tmp-checkpoint-100/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-04-02 23:13:00,342 >> Special tokens file saved in ./output/tmp-checkpoint-100/special_tokens_map.json
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/pretrained/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-04-02 23:20:22,642 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-04-02 23:20:22,642 >>   Num examples = 64
[INFO|trainer.py:3381] 2024-04-02 23:20:22,642 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-04-02 23:20:35,319 >> Saving model checkpoint to ./output/tmp-checkpoint-200
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/pretrained/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-04-02 23:20:35,467 >> tokenizer config file saved in ./output/tmp-checkpoint-200/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-04-02 23:20:35,467 >> Special tokens file saved in ./output/tmp-checkpoint-200/special_tokens_map.json
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/pretrained/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-04-02 23:27:57,381 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-04-02 23:27:57,381 >>   Num examples = 64
[INFO|trainer.py:3381] 2024-04-02 23:27:57,381 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-04-02 23:28:10,056 >> Saving model checkpoint to ./output/tmp-checkpoint-300
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/pretrained/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-04-02 23:28:10,202 >> tokenizer config file saved in ./output/tmp-checkpoint-300/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-04-02 23:28:10,202 >> Special tokens file saved in ./output/tmp-checkpoint-300/special_tokens_map.json
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/pretrained/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-04-02 23:35:33,081 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-04-02 23:35:33,081 >>   Num examples = 64
[INFO|trainer.py:3381] 2024-04-02 23:35:33,081 >>   Batch size = 1
********************on step end call back********************
Step 10 finish
{'loss': 3.0631, 'grad_norm': 2.859712839126587, 'learning_rate': 2.5e-06, 'epoch': 0.14}
********************on step end call back********************
Step 20 finish
{'loss': 2.9037, 'grad_norm': 2.7162375450134277, 'learning_rate': 5e-06, 'epoch': 0.28}
********************on step end call back********************
Step 30 finish
{'loss': 2.7708, 'grad_norm': 1.849366545677185, 'learning_rate': 7.5e-06, 'epoch': 0.42}
********************on step end call back********************
Step 40 finish
{'loss': 2.7027, 'grad_norm': 1.7962532043457031, 'learning_rate': 1e-05, 'epoch': 0.56}
********************on step end call back********************
Step 50 finish
{'loss': 2.4362, 'grad_norm': 1.976690649986267, 'learning_rate': 1.25e-05, 'epoch': 0.7}
********************on step end call back********************
Step 60 finish
{'loss': 1.987, 'grad_norm': 1.422349452972412, 'learning_rate': 1.5e-05, 'epoch': 0.83}
********************on step end call back********************
Step 70 finish
{'loss': 1.6432, 'grad_norm': 1.5921663045883179, 'learning_rate': 1.75e-05, 'epoch': 0.97}
********************on epoch end call back********************
Epoch 0.9878260869565217 finish
********************on step end call back********************
Step 80 finish
{'loss': 1.4422, 'grad_norm': 2.845750331878662, 'learning_rate': 2e-05, 'epoch': 1.11}
********************on step end call back********************
Step 90 finish
{'loss': 1.2554, 'grad_norm': 1.2579582929611206, 'learning_rate': 2.25e-05, 'epoch': 1.25}
********************on step end call back********************
Step 100 finish
{'loss': 1.1053, 'grad_norm': 1.9780117273330688, 'learning_rate': 2.5e-05, 'epoch': 1.39}
{'eval_loss': 1.0434225797653198, 'eval_accuracy': 0.8571428571428571, 'eval_runtime': 12.6815, 'eval_samples_per_second': 5.047, 'eval_steps_per_second': 5.047, 'epoch': 1.39}
********************save call back********************
********************on step end call back********************
Step 110 finish
{'loss': 1.0118, 'grad_norm': 1.9080674648284912, 'learning_rate': 2.7500000000000004e-05, 'epoch': 1.53}
********************on step end call back********************
Step 120 finish
{'loss': 0.9479, 'grad_norm': 1.1627719402313232, 'learning_rate': 3e-05, 'epoch': 1.67}
********************on step end call back********************
Step 130 finish
{'loss': 0.8938, 'grad_norm': 1.2046570777893066, 'learning_rate': 3.2500000000000004e-05, 'epoch': 1.81}
********************on step end call back********************
Step 140 finish
{'loss': 0.8665, 'grad_norm': 1.3168202638626099, 'learning_rate': 3.5e-05, 'epoch': 1.95}
********************on epoch end call back********************
Epoch 1.9895652173913043 finish
********************on step end call back********************
Step 150 finish
{'loss': 0.8033, 'grad_norm': 1.454275369644165, 'learning_rate': 3.7500000000000003e-05, 'epoch': 2.09}
********************on step end call back********************
Step 160 finish
{'loss': 0.7899, 'grad_norm': 1.7494210004806519, 'learning_rate': 4e-05, 'epoch': 2.23}
********************on step end call back********************
Step 170 finish
{'loss': 0.7871, 'grad_norm': 1.5997915267944336, 'learning_rate': 4.25e-05, 'epoch': 2.37}
********************on step end call back********************
Step 180 finish
{'loss': 0.7996, 'grad_norm': 1.4510523080825806, 'learning_rate': 4.5e-05, 'epoch': 2.5}
********************on step end call back********************
Step 190 finish
{'loss': 0.7683, 'grad_norm': 1.5662349462509155, 'learning_rate': 4.75e-05, 'epoch': 2.64}
********************on step end call back********************
Step 200 finish
{'loss': 0.7638, 'grad_norm': 1.3859082460403442, 'learning_rate': 5e-05, 'epoch': 2.78}
{'eval_loss': 0.7300271987915039, 'eval_accuracy': 0.8035714285714286, 'eval_runtime': 12.677, 'eval_samples_per_second': 5.048, 'eval_steps_per_second': 5.048, 'epoch': 2.78}
********************save call back********************
********************on step end call back********************
Step 210 finish
{'loss': 0.7214, 'grad_norm': 1.3770976066589355, 'learning_rate': 5.25e-05, 'epoch': 2.92}
********************on epoch end call back********************
Epoch 2.991304347826087 finish
********************on step end call back********************
Step 220 finish
{'loss': 0.6233, 'grad_norm': 1.4246264696121216, 'learning_rate': 5.500000000000001e-05, 'epoch': 3.06}
********************on step end call back********************
Step 230 finish
{'loss': 0.5902, 'grad_norm': 2.3493425846099854, 'learning_rate': 5.7499999999999995e-05, 'epoch': 3.2}
********************on step end call back********************
Step 240 finish
{'loss': 0.6284, 'grad_norm': 1.795153260231018, 'learning_rate': 6e-05, 'epoch': 3.34}
********************on step end call back********************
Step 250 finish
{'loss': 0.5859, 'grad_norm': 1.8697211742401123, 'learning_rate': 6.25e-05, 'epoch': 3.48}
********************on step end call back********************
Step 260 finish
{'loss': 0.6018, 'grad_norm': 1.9105411767959595, 'learning_rate': 6.500000000000001e-05, 'epoch': 3.62}
********************on step end call back********************
Step 270 finish
{'loss': 0.5322, 'grad_norm': 2.512072801589966, 'learning_rate': 6.750000000000001e-05, 'epoch': 3.76}
********************on step end call back********************
Step 280 finish
{'loss': 0.5854, 'grad_norm': 1.8122440576553345, 'learning_rate': 7e-05, 'epoch': 3.9}
********************on epoch end call back********************
Epoch 3.9930434782608697 finish
********************on step end call back********************
Step 290 finish
{'loss': 0.5485, 'grad_norm': 1.8046960830688477, 'learning_rate': 7.25e-05, 'epoch': 4.03}
********************on step end call back********************
Step 300 finish
{'loss': 0.4111, 'grad_norm': 2.3910679817199707, 'learning_rate': 7.500000000000001e-05, 'epoch': 4.17}
{'eval_loss': 0.6975645422935486, 'eval_accuracy': 0.8035714285714286, 'eval_runtime': 12.6736, 'eval_samples_per_second': 5.05, 'eval_steps_per_second': 5.05, 'epoch': 4.17}
********************save call back********************
********************on step end call back********************
Step 310 finish
{'loss': 0.3979, 'grad_norm': 3.4262328147888184, 'learning_rate': 7.75e-05, 'epoch': 4.31}
********************on step end call back********************
Step 320 finish
{'loss': 0.4382, 'grad_norm': 2.581285238265991, 'learning_rate': 8e-05, 'epoch': 4.45}
********************on step end call back********************
Step 330 finish
{'loss': 0.3956, 'grad_norm': 2.653935194015503, 'learning_rate': 8.25e-05, 'epoch': 4.59}
********************on step end call back********************
Step 340 finish
{'loss': 0.4536, 'grad_norm': 2.32533597946167, 'learning_rate': 8.5e-05, 'epoch': 4.73}
********************on step end call back********************
Step 350 finish
{'loss': 0.4303, 'grad_norm': 2.294361114501953, 'learning_rate': 8.75e-05, 'epoch': 4.87}
********************on epoch end call back********************
Epoch 4.994782608695652 finish
********************on step end call back********************
Step 360 finish
{'loss': 0.4248, 'grad_norm': 1.9284838438034058, 'learning_rate': 9e-05, 'epoch': 5.01}
********************on step end call back********************
Step 370 finish
{'loss': 0.2919, 'grad_norm': 2.4348220825195312, 'learning_rate': 9.250000000000001e-05, 'epoch': 5.15}
********************on step end call back********************
Step 380 finish
{'loss': 0.2795, 'grad_norm': 2.1543099880218506, 'learning_rate': 9.5e-05, 'epoch': 5.29}
********************on step end call back********************
Step 390 finish
{'loss': 0.3044, 'grad_norm': 2.416865110397339, 'learning_rate': 9.75e-05, 'epoch': 5.43}
********************on step end call back********************
Step 400 finish
{'loss': 0.3015, 'grad_norm': 2.3030738830566406, 'learning_rate': 0.0001, 'epoch': 5.57}
[INFO|trainer.py:3067] 2024-04-02 23:35:45,744 >> Saving model checkpoint to ./output/tmp-checkpoint-400
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/pretrained/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-04-02 23:35:45,909 >> tokenizer config file saved in ./output/tmp-checkpoint-400/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-04-02 23:35:45,909 >> Special tokens file saved in ./output/tmp-checkpoint-400/special_tokens_map.json
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/pretrained/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-04-02 23:43:04,219 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-04-02 23:43:04,219 >>   Num examples = 64
[INFO|trainer.py:3381] 2024-04-02 23:43:04,219 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-04-02 23:43:16,835 >> Saving model checkpoint to ./output/tmp-checkpoint-500
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/pretrained/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-04-02 23:43:16,978 >> tokenizer config file saved in ./output/tmp-checkpoint-500/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-04-02 23:43:16,979 >> Special tokens file saved in ./output/tmp-checkpoint-500/special_tokens_map.json
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/pretrained/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-04-02 23:50:36,257 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-04-02 23:50:36,257 >>   Num examples = 64
[INFO|trainer.py:3381] 2024-04-02 23:50:36,257 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-04-02 23:50:48,895 >> Saving model checkpoint to ./output/tmp-checkpoint-600
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/pretrained/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-04-02 23:50:49,048 >> tokenizer config file saved in ./output/tmp-checkpoint-600/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-04-02 23:50:49,049 >> Special tokens file saved in ./output/tmp-checkpoint-600/special_tokens_map.json
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/pretrained/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:3376] 2024-04-02 23:58:09,144 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-04-02 23:58:09,144 >>   Num examples = 64
[INFO|trainer.py:3381] 2024-04-02 23:58:09,144 >>   Batch size = 1
[INFO|trainer.py:3067] 2024-04-02 23:58:21,787 >> Saving model checkpoint to ./output/tmp-checkpoint-700
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/pretrained/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2459] 2024-04-02 23:58:21,963 >> tokenizer config file saved in ./output/tmp-checkpoint-700/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-04-02 23:58:21,964 >> Special tokens file saved in ./output/tmp-checkpoint-700/special_tokens_map.json
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../model/pretrained/Atom-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|trainer.py:2067] 2024-04-02 23:59:05,606 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:3067] 2024-04-02 23:59:05,607 >> Saving model checkpoint to ./output/best_lora_model
[INFO|tokenization_utils_base.py:2459] 2024-04-02 23:59:05,752 >> tokenizer config file saved in ./output/best_lora_model/tokenizer_config.json
[INFO|tokenization_utils_base.py:2468] 2024-04-02 23:59:05,752 >> Special tokens file saved in ./output/best_lora_model/special_tokens_map.json
{'eval_loss': 0.6565937995910645, 'eval_accuracy': 0.8571428571428571, 'eval_runtime': 12.6623, 'eval_samples_per_second': 5.054, 'eval_steps_per_second': 5.054, 'epoch': 5.57}
********************save call back********************
********************on step end call back********************
Step 410 finish
{'loss': 0.2888, 'grad_norm': 2.1623451709747314, 'learning_rate': 9.677419354838711e-05, 'epoch': 5.7}
********************on step end call back********************
Step 420 finish
{'loss': 0.2988, 'grad_norm': 2.356062650680542, 'learning_rate': 9.35483870967742e-05, 'epoch': 5.84}
********************on step end call back********************
Step 430 finish
{'loss': 0.3035, 'grad_norm': 2.1412863731384277, 'learning_rate': 9.032258064516129e-05, 'epoch': 5.98}
********************on epoch end call back********************
Epoch 5.996521739130435 finish
********************on step end call back********************
Step 440 finish
{'loss': 0.2044, 'grad_norm': 2.0903167724609375, 'learning_rate': 8.709677419354839e-05, 'epoch': 6.12}
********************on step end call back********************
Step 450 finish
{'loss': 0.1951, 'grad_norm': 2.0998589992523193, 'learning_rate': 8.387096774193549e-05, 'epoch': 6.26}
********************on step end call back********************
Step 460 finish
{'loss': 0.1914, 'grad_norm': 1.6086217164993286, 'learning_rate': 8.064516129032258e-05, 'epoch': 6.4}
********************on step end call back********************
Step 470 finish
{'loss': 0.1968, 'grad_norm': 1.790846586227417, 'learning_rate': 7.741935483870968e-05, 'epoch': 6.54}
********************on step end call back********************
Step 480 finish
{'loss': 0.2104, 'grad_norm': 1.7010107040405273, 'learning_rate': 7.419354838709677e-05, 'epoch': 6.68}
********************on step end call back********************
Step 490 finish
{'loss': 0.2196, 'grad_norm': 2.293870449066162, 'learning_rate': 7.096774193548388e-05, 'epoch': 6.82}
********************on step end call back********************
Step 500 finish
{'loss': 0.2146, 'grad_norm': 2.236758232116699, 'learning_rate': 6.774193548387096e-05, 'epoch': 6.96}
{'eval_loss': 0.6830773949623108, 'eval_accuracy': 0.8214285714285714, 'eval_runtime': 12.6152, 'eval_samples_per_second': 5.073, 'eval_steps_per_second': 5.073, 'epoch': 6.96}
********************save call back********************
********************on epoch end call back********************
Epoch 6.998260869565217 finish
********************on step end call back********************
Step 510 finish
{'loss': 0.1782, 'grad_norm': 1.659891963005066, 'learning_rate': 6.451612903225807e-05, 'epoch': 7.1}
********************on step end call back********************
Step 520 finish
{'loss': 0.1454, 'grad_norm': 1.701828122138977, 'learning_rate': 6.129032258064517e-05, 'epoch': 7.23}
********************on step end call back********************
Step 530 finish
{'loss': 0.1494, 'grad_norm': 1.4962682723999023, 'learning_rate': 5.8064516129032266e-05, 'epoch': 7.37}
********************on step end call back********************
Step 540 finish
{'loss': 0.1354, 'grad_norm': 1.487865924835205, 'learning_rate': 5.4838709677419355e-05, 'epoch': 7.51}
********************on step end call back********************
Step 550 finish
{'loss': 0.1247, 'grad_norm': 1.4125134944915771, 'learning_rate': 5.161290322580645e-05, 'epoch': 7.65}
********************on step end call back********************
Step 560 finish
{'loss': 0.1491, 'grad_norm': 1.5871635675430298, 'learning_rate': 4.8387096774193554e-05, 'epoch': 7.79}
********************on step end call back********************
Step 570 finish
{'loss': 0.1514, 'grad_norm': 2.042567729949951, 'learning_rate': 4.516129032258064e-05, 'epoch': 7.93}
********************on epoch end call back********************
Epoch 8.0 finish
********************on step end call back********************
Step 580 finish
{'loss': 0.1298, 'grad_norm': 0.9436544179916382, 'learning_rate': 4.1935483870967746e-05, 'epoch': 8.07}
********************on step end call back********************
Step 590 finish
{'loss': 0.1069, 'grad_norm': 0.994381308555603, 'learning_rate': 3.870967741935484e-05, 'epoch': 8.21}
********************on step end call back********************
Step 600 finish
{'loss': 0.1132, 'grad_norm': 1.3578882217407227, 'learning_rate': 3.548387096774194e-05, 'epoch': 8.35}
{'eval_loss': 0.7133622765541077, 'eval_accuracy': 0.875, 'eval_runtime': 12.6379, 'eval_samples_per_second': 5.064, 'eval_steps_per_second': 5.064, 'epoch': 8.35}
********************save call back********************
********************on step end call back********************
Step 610 finish
{'loss': 0.1034, 'grad_norm': 1.0430424213409424, 'learning_rate': 3.2258064516129034e-05, 'epoch': 8.49}
********************on step end call back********************
Step 620 finish
{'loss': 0.1146, 'grad_norm': 1.0928212404251099, 'learning_rate': 2.9032258064516133e-05, 'epoch': 8.63}
********************on step end call back********************
Step 630 finish
{'loss': 0.1146, 'grad_norm': 1.2532871961593628, 'learning_rate': 2.5806451612903226e-05, 'epoch': 8.77}
********************on step end call back********************
Step 640 finish
{'loss': 0.1081, 'grad_norm': 1.255405068397522, 'learning_rate': 2.258064516129032e-05, 'epoch': 8.9}
********************on epoch end call back********************
Epoch 8.987826086956522 finish
********************on step end call back********************
Step 650 finish
{'loss': 0.1044, 'grad_norm': 0.7302931547164917, 'learning_rate': 1.935483870967742e-05, 'epoch': 9.04}
********************on step end call back********************
Step 660 finish
{'loss': 0.0896, 'grad_norm': 0.7751162052154541, 'learning_rate': 1.6129032258064517e-05, 'epoch': 9.18}
********************on step end call back********************
Step 670 finish
{'loss': 0.087, 'grad_norm': 0.8283815383911133, 'learning_rate': 1.2903225806451613e-05, 'epoch': 9.32}
********************on step end call back********************
Step 680 finish
{'loss': 0.088, 'grad_norm': 0.885718584060669, 'learning_rate': 9.67741935483871e-06, 'epoch': 9.46}
********************on step end call back********************
Step 690 finish
{'loss': 0.0867, 'grad_norm': 0.7675573229789734, 'learning_rate': 6.451612903225806e-06, 'epoch': 9.6}
********************on step end call back********************
Step 700 finish
{'loss': 0.0845, 'grad_norm': 1.0279146432876587, 'learning_rate': 3.225806451612903e-06, 'epoch': 9.74}
{'eval_loss': 0.7129284143447876, 'eval_accuracy': 0.875, 'eval_runtime': 12.6426, 'eval_samples_per_second': 5.062, 'eval_steps_per_second': 5.062, 'epoch': 9.74}
********************save call back********************
********************on step end call back********************
Step 710 finish
{'loss': 0.0863, 'grad_norm': 0.901833176612854, 'learning_rate': 0.0, 'epoch': 9.88}
********************on epoch end call back********************
Epoch 9.878260869565217 finish
{'train_runtime': 3219.7364, 'train_samples_per_second': 1.786, 'train_steps_per_second': 0.221, 'train_loss': 0.6205866514796942, 'epoch': 9.88}
***** train metrics *****
  epoch                    =       9.88
  train_loss               =     0.6206
  train_runtime            = 0:53:39.73
  train_samples            =        575
  train_samples_per_second =      1.786
  train_steps_per_second   =      0.221
04/02/2024 23:59:05 - INFO - decoder_lora.log - *** Evaluate ***
[INFO|trainer.py:3376] 2024-04-02 23:59:05,779 >> ***** Running Evaluation *****
[INFO|trainer.py:3378] 2024-04-02 23:59:05,779 >>   Num examples = 64
[INFO|trainer.py:3381] 2024-04-02 23:59:05,779 >>   Batch size = 1
{'eval_loss': 0.7118671536445618, 'eval_accuracy': 0.875, 'eval_runtime': 12.0352, 'eval_samples_per_second': 5.318, 'eval_steps_per_second': 5.318, 'epoch': 9.88}
***** eval metrics *****
  epoch                   =       9.88
  eval_accuracy           =      0.875
  eval_loss               =     0.7119
  eval_runtime            = 0:00:12.03
  eval_samples            =         64
  eval_samples_per_second =      5.318
  eval_steps_per_second   =      5.318
  perplexity              =     2.0378
04/02/2024 23:59:17 - INFO - decoder_lora.log - *** Predict ***
[INFO|trainer.py:3376] 2024-04-02 23:59:17,805 >> ***** Running Prediction *****
[INFO|trainer.py:3378] 2024-04-02 23:59:17,805 >>   Num examples = 64
[INFO|trainer.py:3381] 2024-04-02 23:59:17,805 >>   Batch size = 1
The model was loaded with use_flash_attention_2=True, which is deprecated and may be removed in a future release. Please use `attn_implementation="flash_attention_2"` instead.
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:01<00:03,  1.60s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:02<00:01,  1.34s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:03<00:00,  1.12s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:03<00:00,  1.21s/it]
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/peft/tuners/lora/bnb.py:272: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.
  warnings.warn(
Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.
Non-default generation parameters: {'max_length': 4096}
模型结构 LlamaForCausalLM
The model was loaded with use_flash_attention_2=True, which is deprecated and may be removed in a future release. Please use `attn_implementation="flash_attention_2"` instead.
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:01<00:03,  1.67s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:02<00:01,  1.33s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:03<00:00,  1.13s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:03<00:00,  1.22s/it]
Traceback (most recent call last):
  File "/home/asus/intelligent-test/decoder_lora/predict.py", line 135, in <module>
    inputs, targets = get_datas(args.eval_dataset)
  File "/home/asus/intelligent-test/decoder_lora/predict.py", line 67, in get_datas
    with open(file_path, "r", encoding="utf-8") as f:
FileNotFoundError: [Errno 2] No such file or directory: '../data/ir_validate.csv'
The model was loaded with use_flash_attention_2=True, which is deprecated and may be removed in a future release. Please use `attn_implementation="flash_attention_2"` instead.
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:01<00:03,  1.74s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:02<00:01,  1.34s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:03<00:00,  1.12s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:03<00:00,  1.22s/it]
Traceback (most recent call last):
  File "/home/asus/intelligent-test/decoder_lora/predict.py", line 135, in <module>
    inputs, targets = get_datas(args.eval_dataset)
  File "/home/asus/intelligent-test/decoder_lora/predict.py", line 67, in get_datas
    with open(file_path, "r", encoding="utf-8") as f:
FileNotFoundError: [Errno 2] No such file or directory: '../data/ir_validate.csv'
