Some weights of the model checkpoint at ../model/mengzi-bert-base-fin were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.bias', 'pos_transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'pos_head.bias', 'sop.cls.bias', 'cls.predictions.transform.dense.bias', 'sop.cls.weight', 'pos_transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'pos_transform.LayerNorm.weight', 'pos_transform.LayerNorm.bias', 'pos_head.weight']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ../model/mengzi-bert-base-fin and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
{'loss': 0.7704, 'learning_rate': 1e-05, 'epoch': 0.02}
{'loss': 0.3806, 'learning_rate': 1e-05, 'epoch': 0.05}
{'loss': 0.3203, 'learning_rate': 1e-05, 'epoch': 0.07}
{'loss': 0.2311, 'learning_rate': 1e-05, 'epoch': 0.1}
{'loss': 0.2397, 'learning_rate': 1e-05, 'epoch': 0.12}
{'loss': 0.3087, 'learning_rate': 1e-05, 'epoch': 0.15}
{'loss': 0.2196, 'learning_rate': 1e-05, 'epoch': 0.17}
{'loss': 0.2342, 'learning_rate': 1e-05, 'epoch': 0.19}
{'loss': 0.1766, 'learning_rate': 1e-05, 'epoch': 0.22}
{'loss': 0.1771, 'learning_rate': 1e-05, 'epoch': 0.24}
{'loss': 0.2284, 'learning_rate': 1e-05, 'epoch': 0.27}
{'loss': 0.1891, 'learning_rate': 1e-05, 'epoch': 0.29}
{'loss': 0.227, 'learning_rate': 1e-05, 'epoch': 0.32}
{'loss': 0.2043, 'learning_rate': 1e-05, 'epoch': 0.34}
{'loss': 0.1409, 'learning_rate': 1e-05, 'epoch': 0.36}
{'loss': 0.1522, 'learning_rate': 1e-05, 'epoch': 0.39}
{'loss': 0.1553, 'learning_rate': 1e-05, 'epoch': 0.41}
{'loss': 0.1747, 'learning_rate': 1e-05, 'epoch': 0.44}
{'loss': 0.1272, 'learning_rate': 1e-05, 'epoch': 0.46}
{'loss': 0.1702, 'learning_rate': 1e-05, 'epoch': 0.48}
{'loss': 0.0978, 'learning_rate': 1e-05, 'epoch': 0.51}
{'loss': 0.1264, 'learning_rate': 1e-05, 'epoch': 0.53}
{'loss': 0.1074, 'learning_rate': 1e-05, 'epoch': 0.56}
{'loss': 0.1214, 'learning_rate': 1e-05, 'epoch': 0.58}
{'loss': 0.1174, 'learning_rate': 1e-05, 'epoch': 0.61}
{'loss': 0.0981, 'learning_rate': 1e-05, 'epoch': 0.63}
{'loss': 0.0672, 'learning_rate': 1e-05, 'epoch': 0.65}
{'loss': 0.0865, 'learning_rate': 1e-05, 'epoch': 0.68}
{'loss': 0.0656, 'learning_rate': 1e-05, 'epoch': 0.7}
{'loss': 0.0604, 'learning_rate': 1e-05, 'epoch': 0.73}
{'loss': 0.078, 'learning_rate': 1e-05, 'epoch': 0.75}
{'loss': 0.105, 'learning_rate': 1e-05, 'epoch': 0.78}
{'loss': 0.0787, 'learning_rate': 1e-05, 'epoch': 0.8}
{'loss': 0.0344, 'learning_rate': 1e-05, 'epoch': 0.82}
{'loss': 0.0523, 'learning_rate': 1e-05, 'epoch': 0.85}
{'loss': 0.056, 'learning_rate': 1e-05, 'epoch': 0.87}
{'loss': 0.0871, 'learning_rate': 1e-05, 'epoch': 0.9}
{'loss': 0.0523, 'learning_rate': 1e-05, 'epoch': 0.92}
{'loss': 0.029, 'learning_rate': 1e-05, 'epoch': 0.95}
{'loss': 0.0471, 'learning_rate': 1e-05, 'epoch': 0.97}
{'loss': 0.0501, 'learning_rate': 1e-05, 'epoch': 0.99}
{'eval_loss': 0.4970512092113495, 'eval_runtime': 2.408, 'eval_samples_per_second': 138.707, 'eval_steps_per_second': 17.442, 'epoch': 1.0}
{'loss': 0.0476, 'learning_rate': 1e-05, 'epoch': 1.02}
{'loss': 0.0411, 'learning_rate': 1e-05, 'epoch': 1.04}
{'loss': 0.0284, 'learning_rate': 1e-05, 'epoch': 1.07}
{'loss': 0.0329, 'learning_rate': 1e-05, 'epoch': 1.09}
{'loss': 0.0168, 'learning_rate': 1e-05, 'epoch': 1.12}
{'loss': 0.0395, 'learning_rate': 1e-05, 'epoch': 1.14}
{'loss': 0.057, 'learning_rate': 1e-05, 'epoch': 1.16}
{'loss': 0.0397, 'learning_rate': 1e-05, 'epoch': 1.19}
{'loss': 0.0214, 'learning_rate': 1e-05, 'epoch': 1.21}
{'loss': 0.058, 'learning_rate': 1e-05, 'epoch': 1.24}
{'loss': 0.0564, 'learning_rate': 1e-05, 'epoch': 1.26}
{'loss': 0.0492, 'learning_rate': 1e-05, 'epoch': 1.28}
{'loss': 0.0625, 'learning_rate': 1e-05, 'epoch': 1.31}
{'loss': 0.034, 'learning_rate': 1e-05, 'epoch': 1.33}
{'loss': 0.0369, 'learning_rate': 1e-05, 'epoch': 1.36}
{'loss': 0.042, 'learning_rate': 1e-05, 'epoch': 1.38}
{'loss': 0.0453, 'learning_rate': 1e-05, 'epoch': 1.41}
{'loss': 0.0083, 'learning_rate': 1e-05, 'epoch': 1.43}
{'loss': 0.0462, 'learning_rate': 1e-05, 'epoch': 1.45}
{'loss': 0.0677, 'learning_rate': 1e-05, 'epoch': 1.48}
{'loss': 0.0178, 'learning_rate': 1e-05, 'epoch': 1.5}
{'loss': 0.0535, 'learning_rate': 1e-05, 'epoch': 1.53}
{'loss': 0.0507, 'learning_rate': 1e-05, 'epoch': 1.55}
{'loss': 0.0234, 'learning_rate': 1e-05, 'epoch': 1.58}
{'loss': 0.0447, 'learning_rate': 1e-05, 'epoch': 1.6}
{'loss': 0.0374, 'learning_rate': 1e-05, 'epoch': 1.62}
{'loss': 0.0248, 'learning_rate': 1e-05, 'epoch': 1.65}
{'loss': 0.0252, 'learning_rate': 1e-05, 'epoch': 1.67}
{'loss': 0.0321, 'learning_rate': 1e-05, 'epoch': 1.7}
{'loss': 0.0325, 'learning_rate': 1e-05, 'epoch': 1.72}
{'loss': 0.0133, 'learning_rate': 1e-05, 'epoch': 1.75}
{'loss': 0.0186, 'learning_rate': 1e-05, 'epoch': 1.77}
{'loss': 0.0571, 'learning_rate': 1e-05, 'epoch': 1.79}
{'loss': 0.0378, 'learning_rate': 1e-05, 'epoch': 1.82}
{'loss': 0.0203, 'learning_rate': 1e-05, 'epoch': 1.84}
{'loss': 0.0377, 'learning_rate': 1e-05, 'epoch': 1.87}
{'loss': 0.0523, 'learning_rate': 1e-05, 'epoch': 1.89}
{'loss': 0.0422, 'learning_rate': 1e-05, 'epoch': 1.92}
{'loss': 0.0502, 'learning_rate': 1e-05, 'epoch': 1.94}
{'loss': 0.0097, 'learning_rate': 1e-05, 'epoch': 1.96}
{'loss': 0.0393, 'learning_rate': 1e-05, 'epoch': 1.99}
{'eval_loss': 0.628233015537262, 'eval_runtime': 2.4383, 'eval_samples_per_second': 136.978, 'eval_steps_per_second': 17.225, 'epoch': 2.0}
{'loss': 0.0269, 'learning_rate': 1e-05, 'epoch': 2.01}
{'loss': 0.0205, 'learning_rate': 1e-05, 'epoch': 2.04}
{'loss': 0.0341, 'learning_rate': 1e-05, 'epoch': 2.06}
{'loss': 0.0151, 'learning_rate': 1e-05, 'epoch': 2.08}
{'loss': 0.0169, 'learning_rate': 1e-05, 'epoch': 2.11}
{'loss': 0.0114, 'learning_rate': 1e-05, 'epoch': 2.13}
{'loss': 0.0172, 'learning_rate': 1e-05, 'epoch': 2.16}
{'loss': 0.0233, 'learning_rate': 1e-05, 'epoch': 2.18}
{'loss': 0.0224, 'learning_rate': 1e-05, 'epoch': 2.21}
{'loss': 0.0102, 'learning_rate': 1e-05, 'epoch': 2.23}
{'loss': 0.0041, 'learning_rate': 1e-05, 'epoch': 2.25}
{'loss': 0.0091, 'learning_rate': 1e-05, 'epoch': 2.28}
{'loss': 0.0035, 'learning_rate': 1e-05, 'epoch': 2.3}
{'loss': 0.0135, 'learning_rate': 1e-05, 'epoch': 2.33}
{'loss': 0.0196, 'learning_rate': 1e-05, 'epoch': 2.35}
{'loss': 0.0177, 'learning_rate': 1e-05, 'epoch': 2.38}
{'loss': 0.0357, 'learning_rate': 1e-05, 'epoch': 2.4}
{'loss': 0.0343, 'learning_rate': 1e-05, 'epoch': 2.42}
{'loss': 0.0451, 'learning_rate': 1e-05, 'epoch': 2.45}
{'loss': 0.0264, 'learning_rate': 1e-05, 'epoch': 2.47}
{'loss': 0.0427, 'learning_rate': 1e-05, 'epoch': 2.5}
{'loss': 0.0454, 'learning_rate': 1e-05, 'epoch': 2.52}
{'loss': 0.0335, 'learning_rate': 1e-05, 'epoch': 2.55}
{'loss': 0.0244, 'learning_rate': 1e-05, 'epoch': 2.57}
{'loss': 0.0379, 'learning_rate': 1e-05, 'epoch': 2.59}
{'loss': 0.0166, 'learning_rate': 1e-05, 'epoch': 2.62}
{'loss': 0.0166, 'learning_rate': 1e-05, 'epoch': 2.64}
{'loss': 0.036, 'learning_rate': 1e-05, 'epoch': 2.67}
{'loss': 0.0033, 'learning_rate': 1e-05, 'epoch': 2.69}
{'loss': 0.0, 'learning_rate': 1e-05, 'epoch': 2.72}
{'loss': 0.0447, 'learning_rate': 1e-05, 'epoch': 2.74}
{'loss': 0.0217, 'learning_rate': 1e-05, 'epoch': 2.76}
{'loss': 0.0134, 'learning_rate': 1e-05, 'epoch': 2.79}
{'loss': 0.0462, 'learning_rate': 1e-05, 'epoch': 2.81}
{'loss': 0.0217, 'learning_rate': 1e-05, 'epoch': 2.84}
{'loss': 0.0368, 'learning_rate': 1e-05, 'epoch': 2.86}
{'loss': 0.0322, 'learning_rate': 1e-05, 'epoch': 2.88}
{'loss': 0.0036, 'learning_rate': 1e-05, 'epoch': 2.91}
{'loss': 0.0168, 'learning_rate': 1e-05, 'epoch': 2.93}
{'loss': 0.0323, 'learning_rate': 1e-05, 'epoch': 2.96}
{'loss': 0.0071, 'learning_rate': 1e-05, 'epoch': 2.98}
{'eval_loss': 0.6883644461631775, 'eval_runtime': 2.4283, 'eval_samples_per_second': 137.547, 'eval_steps_per_second': 17.296, 'epoch': 3.0}
