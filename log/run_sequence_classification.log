原始数据（规则筛选或信息抽取）：训练集有数据3000条，验证集有数据334条。
原始数据（规则筛选或信息抽取）：训练集有数据1199条，验证集有数据134条。
规则：训练集有数据574条，验证集有数据64条。
Building prefix dict from /home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/synonyms/data/vocab.txt ...
Loading model from cache /tmp/jieba.u2936b6aa2669ea7ea53bb471c04c76db.cache
Loading model cost 0.882 seconds.
Prefix dict has been built successfully.
Simbert不能正常使用，除非你安装：bert4keras、tensorflow ，为了安装快捷，没有默认安装.... No module named 'bert4keras'
smart_open library not found; falling back to local-filesystem-only

 Synonyms: v3.18.0, Project home: https://github.com/chatopera/Synonyms/

 Project Sponsored by Chatopera

  deliver your chatbots with Chatopera Cloud Services --> https://bot.chatopera.com

>> Synonyms load wordseg dict [/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/synonyms/data/vocab.txt] ... 
>> Synonyms on loading stopwords [/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/synonyms/data/stopwords.txt] ...
>> Synonyms on loading vectors [/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/synonyms/data/words.vector.gz] ...
Some weights of the model checkpoint at ../model/mengzi-bert-base-fin were not used when initializing BertForSequenceClassification: ['sop.cls.bias', 'pos_transform.dense.weight', 'pos_transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'pos_transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.weight', 'pos_transform.dense.bias', 'cls.predictions.decoder.weight', 'sop.cls.weight', 'pos_head.bias', 'pos_head.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ../model/mengzi-bert-base-fin and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
{'loss': 0.6515, 'learning_rate': 1e-05, 'epoch': 0.27}
{'loss': 0.3829, 'learning_rate': 1e-05, 'epoch': 0.53}
{'loss': 0.3665, 'learning_rate': 1e-05, 'epoch': 0.8}
{'eval_loss': 0.3359506130218506, 'eval_runtime': 2.4007, 'eval_samples_per_second': 139.125, 'eval_steps_per_second': 17.495, 'epoch': 1.0}
{'loss': 0.3143, 'learning_rate': 1e-05, 'epoch': 1.07}
{'loss': 0.1764, 'learning_rate': 1e-05, 'epoch': 1.33}
{'loss': 0.2651, 'learning_rate': 1e-05, 'epoch': 1.6}
{'loss': 0.2249, 'learning_rate': 1e-05, 'epoch': 1.87}
{'eval_loss': 0.3377167582511902, 'eval_runtime': 2.3844, 'eval_samples_per_second': 140.076, 'eval_steps_per_second': 17.614, 'epoch': 2.0}
{'loss': 0.2067, 'learning_rate': 1e-05, 'epoch': 2.13}
{'loss': 0.1373, 'learning_rate': 1e-05, 'epoch': 2.4}
{'loss': 0.139, 'learning_rate': 1e-05, 'epoch': 2.67}
{'loss': 0.1816, 'learning_rate': 1e-05, 'epoch': 2.93}
{'eval_loss': 0.3611915409564972, 'eval_runtime': 2.3856, 'eval_samples_per_second': 140.004, 'eval_steps_per_second': 17.605, 'epoch': 3.0}
{'loss': 0.1342, 'learning_rate': 1e-05, 'epoch': 3.2}
{'loss': 0.1042, 'learning_rate': 1e-05, 'epoch': 3.47}
{'loss': 0.0853, 'learning_rate': 1e-05, 'epoch': 3.73}
{'loss': 0.1303, 'learning_rate': 1e-05, 'epoch': 4.0}
{'eval_loss': 0.37345361709594727, 'eval_runtime': 2.394, 'eval_samples_per_second': 139.513, 'eval_steps_per_second': 17.544, 'epoch': 4.0}
{'loss': 0.0565, 'learning_rate': 1e-05, 'epoch': 4.27}
{'loss': 0.0803, 'learning_rate': 1e-05, 'epoch': 4.53}
{'loss': 0.072, 'learning_rate': 1e-05, 'epoch': 4.8}
{'eval_loss': 0.4082319438457489, 'eval_runtime': 2.3865, 'eval_samples_per_second': 139.954, 'eval_steps_per_second': 17.599, 'epoch': 5.0}
{'loss': 0.056, 'learning_rate': 1e-05, 'epoch': 5.07}
{'loss': 0.0414, 'learning_rate': 1e-05, 'epoch': 5.33}
{'loss': 0.0501, 'learning_rate': 1e-05, 'epoch': 5.6}
{'loss': 0.0798, 'learning_rate': 1e-05, 'epoch': 5.87}
{'eval_loss': 0.42824211716651917, 'eval_runtime': 2.3881, 'eval_samples_per_second': 139.858, 'eval_steps_per_second': 17.587, 'epoch': 6.0}
{'loss': 0.0621, 'learning_rate': 1e-05, 'epoch': 6.13}
{'loss': 0.0327, 'learning_rate': 1e-05, 'epoch': 6.4}
{'loss': 0.0212, 'learning_rate': 1e-05, 'epoch': 6.67}
{'loss': 0.0427, 'learning_rate': 1e-05, 'epoch': 6.93}
{'eval_loss': 0.43293264508247375, 'eval_runtime': 2.3829, 'eval_samples_per_second': 140.164, 'eval_steps_per_second': 17.625, 'epoch': 7.0}
{'loss': 0.0287, 'learning_rate': 1e-05, 'epoch': 7.2}
{'loss': 0.0339, 'learning_rate': 1e-05, 'epoch': 7.47}
{'loss': 0.0277, 'learning_rate': 1e-05, 'epoch': 7.73}
{'loss': 0.0654, 'learning_rate': 1e-05, 'epoch': 8.0}
{'eval_loss': 0.4367697834968567, 'eval_runtime': 2.3843, 'eval_samples_per_second': 140.086, 'eval_steps_per_second': 17.616, 'epoch': 8.0}
{'loss': 0.0261, 'learning_rate': 1e-05, 'epoch': 8.27}
{'loss': 0.0481, 'learning_rate': 1e-05, 'epoch': 8.53}
{'loss': 0.0185, 'learning_rate': 1e-05, 'epoch': 8.8}
{'eval_loss': 0.4876740872859955, 'eval_runtime': 2.3944, 'eval_samples_per_second': 139.489, 'eval_steps_per_second': 17.541, 'epoch': 9.0}
{'loss': 0.0199, 'learning_rate': 1e-05, 'epoch': 9.07}
{'loss': 0.0103, 'learning_rate': 1e-05, 'epoch': 9.33}
{'loss': 0.0349, 'learning_rate': 1e-05, 'epoch': 9.6}
{'loss': 0.0304, 'learning_rate': 1e-05, 'epoch': 9.87}
{'eval_loss': 0.4633738398551941, 'eval_runtime': 2.3892, 'eval_samples_per_second': 139.795, 'eval_steps_per_second': 17.579, 'epoch': 10.0}
{'train_runtime': 683.7735, 'train_samples_per_second': 43.874, 'train_steps_per_second': 5.484, 'train_loss': 0.11840537610054017, 'epoch': 10.0}
Some weights of the model checkpoint at ../model/mengzi-bert-base-fin were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'sop.cls.weight', 'sop.cls.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'pos_transform.LayerNorm.bias', 'pos_transform.dense.weight', 'pos_transform.LayerNorm.weight', 'pos_head.bias', 'pos_head.weight', 'cls.predictions.transform.dense.weight', 'pos_transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ../model/mengzi-bert-base-fin and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
{'loss': 0.7205, 'learning_rate': 9.733333333333334e-06, 'epoch': 0.27}
{'loss': 0.4144, 'learning_rate': 9.466666666666667e-06, 'epoch': 0.53}
{'loss': 0.3251, 'learning_rate': 9.200000000000002e-06, 'epoch': 0.8}
{'eval_loss': 0.3179309368133545, 'eval_runtime': 2.3868, 'eval_samples_per_second': 139.934, 'eval_steps_per_second': 17.596, 'epoch': 1.0}
{'loss': 0.2731, 'learning_rate': 8.933333333333333e-06, 'epoch': 1.07}
{'loss': 0.2316, 'learning_rate': 8.666666666666668e-06, 'epoch': 1.33}
{'loss': 0.2268, 'learning_rate': 8.400000000000001e-06, 'epoch': 1.6}
{'loss': 0.2031, 'learning_rate': 8.133333333333334e-06, 'epoch': 1.87}
{'eval_loss': 0.29639434814453125, 'eval_runtime': 2.3841, 'eval_samples_per_second': 140.097, 'eval_steps_per_second': 17.617, 'epoch': 2.0}
{'loss': 0.1771, 'learning_rate': 7.866666666666667e-06, 'epoch': 2.13}
{'loss': 0.1598, 'learning_rate': 7.600000000000001e-06, 'epoch': 2.4}
{'loss': 0.1305, 'learning_rate': 7.333333333333333e-06, 'epoch': 2.67}
{'loss': 0.1231, 'learning_rate': 7.066666666666667e-06, 'epoch': 2.93}
{'eval_loss': 0.369750440120697, 'eval_runtime': 2.3942, 'eval_samples_per_second': 139.506, 'eval_steps_per_second': 17.543, 'epoch': 3.0}
{'loss': 0.0966, 'learning_rate': 6.800000000000001e-06, 'epoch': 3.2}
{'loss': 0.1041, 'learning_rate': 6.533333333333334e-06, 'epoch': 3.47}
{'loss': 0.1042, 'learning_rate': 6.266666666666668e-06, 'epoch': 3.73}
{'loss': 0.0901, 'learning_rate': 6e-06, 'epoch': 4.0}
{'eval_loss': 0.3588193953037262, 'eval_runtime': 2.3875, 'eval_samples_per_second': 139.896, 'eval_steps_per_second': 17.592, 'epoch': 4.0}
{'loss': 0.0561, 'learning_rate': 5.733333333333334e-06, 'epoch': 4.27}
{'loss': 0.0481, 'learning_rate': 5.466666666666667e-06, 'epoch': 4.53}
{'loss': 0.0743, 'learning_rate': 5.2e-06, 'epoch': 4.8}
{'eval_loss': 0.34718698263168335, 'eval_runtime': 2.3915, 'eval_samples_per_second': 139.661, 'eval_steps_per_second': 17.562, 'epoch': 5.0}
{'loss': 0.0761, 'learning_rate': 4.933333333333334e-06, 'epoch': 5.07}
{'loss': 0.0251, 'learning_rate': 4.666666666666667e-06, 'epoch': 5.33}
{'loss': 0.0448, 'learning_rate': 4.4e-06, 'epoch': 5.6}
{'loss': 0.0488, 'learning_rate': 4.133333333333333e-06, 'epoch': 5.87}
{'eval_loss': 0.3968544602394104, 'eval_runtime': 2.3947, 'eval_samples_per_second': 139.476, 'eval_steps_per_second': 17.539, 'epoch': 6.0}
{'loss': 0.0403, 'learning_rate': 3.866666666666667e-06, 'epoch': 6.13}
{'loss': 0.0226, 'learning_rate': 3.6000000000000003e-06, 'epoch': 6.4}
{'loss': 0.0311, 'learning_rate': 3.3333333333333333e-06, 'epoch': 6.67}
{'loss': 0.0143, 'learning_rate': 3.066666666666667e-06, 'epoch': 6.93}
{'eval_loss': 0.4136994779109955, 'eval_runtime': 2.3895, 'eval_samples_per_second': 139.778, 'eval_steps_per_second': 17.577, 'epoch': 7.0}
{'loss': 0.0534, 'learning_rate': 2.8000000000000003e-06, 'epoch': 7.2}
{'loss': 0.0192, 'learning_rate': 2.5333333333333338e-06, 'epoch': 7.47}
{'loss': 0.0084, 'learning_rate': 2.266666666666667e-06, 'epoch': 7.73}
{'loss': 0.0067, 'learning_rate': 2.0000000000000003e-06, 'epoch': 8.0}
{'eval_loss': 0.460371732711792, 'eval_runtime': 2.3985, 'eval_samples_per_second': 139.252, 'eval_steps_per_second': 17.511, 'epoch': 8.0}
{'loss': 0.0183, 'learning_rate': 1.7333333333333336e-06, 'epoch': 8.27}
{'loss': 0.0155, 'learning_rate': 1.4666666666666669e-06, 'epoch': 8.53}
{'loss': 0.0086, 'learning_rate': 1.2000000000000002e-06, 'epoch': 8.8}
{'eval_loss': 0.466385155916214, 'eval_runtime': 2.3889, 'eval_samples_per_second': 139.811, 'eval_steps_per_second': 17.581, 'epoch': 9.0}
{'loss': 0.0073, 'learning_rate': 9.333333333333334e-07, 'epoch': 9.07}
{'loss': 0.0015, 'learning_rate': 6.666666666666667e-07, 'epoch': 9.33}
{'loss': 0.0306, 'learning_rate': 4.0000000000000003e-07, 'epoch': 9.6}
{'loss': 0.0014, 'learning_rate': 1.3333333333333336e-07, 'epoch': 9.87}
{'eval_loss': 0.47165465354919434, 'eval_runtime': 2.3896, 'eval_samples_per_second': 139.769, 'eval_steps_per_second': 17.576, 'epoch': 10.0}
{'train_runtime': 685.4144, 'train_samples_per_second': 43.769, 'train_steps_per_second': 5.471, 'train_loss': 0.10754351119548082, 'epoch': 10.0}
Some weights of the model checkpoint at ../model/mengzi-bert-base-fin were not used when initializing BertForSequenceClassification: ['pos_head.weight', 'sop.cls.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'pos_transform.dense.bias', 'pos_transform.dense.weight', 'pos_transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'pos_head.bias', 'cls.predictions.transform.LayerNorm.bias', 'pos_transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'sop.cls.bias']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ../model/mengzi-bert-base-fin and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
{'loss': 0.6511, 'learning_rate': 9.982464296247523e-06, 'epoch': 0.27}
{'loss': 0.3677, 'learning_rate': 9.929980185352525e-06, 'epoch': 0.53}
{'loss': 0.2999, 'learning_rate': 9.842915805643156e-06, 'epoch': 0.8}
{'eval_loss': 0.2763832211494446, 'eval_runtime': 2.3942, 'eval_samples_per_second': 139.503, 'eval_steps_per_second': 17.542, 'epoch': 1.0}
{'loss': 0.2774, 'learning_rate': 9.721881851187406e-06, 'epoch': 1.07}
{'loss': 0.2114, 'learning_rate': 9.567727288213005e-06, 'epoch': 1.33}
{'loss': 0.2126, 'learning_rate': 9.381533400219319e-06, 'epoch': 1.6}
{'loss': 0.2174, 'learning_rate': 9.164606203550498e-06, 'epoch': 1.87}
{'eval_loss': 0.278418630361557, 'eval_runtime': 2.3875, 'eval_samples_per_second': 139.895, 'eval_steps_per_second': 17.592, 'epoch': 2.0}
{'loss': 0.2361, 'learning_rate': 8.9184672866292e-06, 'epoch': 2.13}
{'loss': 0.1292, 'learning_rate': 8.644843137107058e-06, 'epoch': 2.4}
{'loss': 0.1547, 'learning_rate': 8.345653031794292e-06, 'epoch': 2.67}
{'loss': 0.1532, 'learning_rate': 8.022995574311876e-06, 'epoch': 2.93}
{'eval_loss': 0.3763638734817505, 'eval_runtime': 2.3952, 'eval_samples_per_second': 139.447, 'eval_steps_per_second': 17.535, 'epoch': 3.0}
{'loss': 0.093, 'learning_rate': 7.679133974894984e-06, 'epoch': 3.2}
{'loss': 0.1051, 'learning_rate': 7.31648017559931e-06, 'epoch': 3.47}
{'loss': 0.0748, 'learning_rate': 6.9375779322605154e-06, 'epoch': 3.73}
{'loss': 0.0987, 'learning_rate': 6.545084971874738e-06, 'epoch': 4.0}
{'eval_loss': 0.3510044515132904, 'eval_runtime': 2.3866, 'eval_samples_per_second': 139.949, 'eval_steps_per_second': 17.598, 'epoch': 4.0}
{'loss': 0.0258, 'learning_rate': 6.141754350553279e-06, 'epoch': 4.27}
{'loss': 0.0499, 'learning_rate': 5.730415142812059e-06, 'epoch': 4.53}
{'loss': 0.0661, 'learning_rate': 5.3139525976465675e-06, 'epoch': 4.8}
{'eval_loss': 0.40545979142189026, 'eval_runtime': 2.3892, 'eval_samples_per_second': 139.798, 'eval_steps_per_second': 17.579, 'epoch': 5.0}
{'loss': 0.0752, 'learning_rate': 4.895287900583216e-06, 'epoch': 5.07}
{'loss': 0.0487, 'learning_rate': 4.477357683661734e-06, 'epoch': 5.33}
{'loss': 0.0411, 'learning_rate': 4.063093427071376e-06, 'epoch': 5.6}
{'loss': 0.0449, 'learning_rate': 3.655400896923672e-06, 'epoch': 5.87}
{'eval_loss': 0.43934082984924316, 'eval_runtime': 2.3934, 'eval_samples_per_second': 139.55, 'eval_steps_per_second': 17.548, 'epoch': 6.0}
{'loss': 0.0513, 'learning_rate': 3.2571397633909252e-06, 'epoch': 6.13}
{'loss': 0.0435, 'learning_rate': 2.871103542174637e-06, 'epoch': 6.4}
{'loss': 0.0101, 'learning_rate': 2.5000000000000015e-06, 'epoch': 6.67}
{'loss': 0.0069, 'learning_rate': 2.146432161577842e-06, 'epoch': 6.93}
{'eval_loss': 0.4707590341567993, 'eval_runtime': 2.387, 'eval_samples_per_second': 139.927, 'eval_steps_per_second': 17.596, 'epoch': 7.0}
{'loss': 0.0228, 'learning_rate': 1.8128800512565514e-06, 'epoch': 7.2}
{'loss': 0.019, 'learning_rate': 1.5016832974331725e-06, 'epoch': 7.47}
{'loss': 0.0038, 'learning_rate': 1.2150247217412186e-06, 'epoch': 7.73}
{'loss': 0.016, 'learning_rate': 9.549150281252633e-07, 'epoch': 8.0}
{'eval_loss': 0.47635725140571594, 'eval_runtime': 2.3848, 'eval_samples_per_second': 140.056, 'eval_steps_per_second': 17.612, 'epoch': 8.0}
{'loss': 0.0074, 'learning_rate': 7.23178699197467e-07, 'epoch': 8.27}
{'loss': 0.0144, 'learning_rate': 5.214411988029355e-07, 'epoch': 8.53}
{'loss': 0.0028, 'learning_rate': 3.511175705587433e-07, 'epoch': 8.8}
{'eval_loss': 0.49084052443504333, 'eval_runtime': 2.3906, 'eval_samples_per_second': 139.714, 'eval_steps_per_second': 17.569, 'epoch': 9.0}
{'loss': 0.0319, 'learning_rate': 2.134025123396638e-07, 'epoch': 9.07}
{'loss': 0.001, 'learning_rate': 1.0926199633097156e-07, 'epoch': 9.33}
{'loss': 0.0157, 'learning_rate': 3.9426493427611177e-08, 'epoch': 9.6}
{'loss': 0.0007, 'learning_rate': 4.385849505708084e-09, 'epoch': 9.87}
{'eval_loss': 0.48772069811820984, 'eval_runtime': 2.3884, 'eval_samples_per_second': 139.845, 'eval_steps_per_second': 17.585, 'epoch': 10.0}
{'train_runtime': 685.9624, 'train_samples_per_second': 43.734, 'train_steps_per_second': 5.467, 'train_loss': 0.1037064179122448, 'epoch': 10.0}
Some weights of the model checkpoint at ../model/mengzi-bert-base-fin were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'pos_transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'sop.cls.weight', 'pos_transform.dense.weight', 'pos_head.weight', 'pos_head.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'pos_transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight', 'pos_transform.LayerNorm.bias', 'sop.cls.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ../model/mengzi-bert-base-fin and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
{'loss': 0.745, 'learning_rate': 1e-05, 'epoch': 0.02}
{'loss': 0.4044, 'learning_rate': 1e-05, 'epoch': 0.05}
{'loss': 0.3227, 'learning_rate': 1e-05, 'epoch': 0.07}
{'loss': 0.2249, 'learning_rate': 1e-05, 'epoch': 0.1}
{'loss': 0.2907, 'learning_rate': 1e-05, 'epoch': 0.12}
{'loss': 0.2478, 'learning_rate': 1e-05, 'epoch': 0.15}
{'loss': 0.2598, 'learning_rate': 1e-05, 'epoch': 0.17}
{'loss': 0.254, 'learning_rate': 1e-05, 'epoch': 0.19}
{'loss': 0.2184, 'learning_rate': 1e-05, 'epoch': 0.22}
{'loss': 0.2295, 'learning_rate': 1e-05, 'epoch': 0.24}
{'loss': 0.1798, 'learning_rate': 1e-05, 'epoch': 0.27}
{'loss': 0.1859, 'learning_rate': 1e-05, 'epoch': 0.29}
{'loss': 0.1726, 'learning_rate': 1e-05, 'epoch': 0.32}
{'loss': 0.1962, 'learning_rate': 1e-05, 'epoch': 0.34}
{'loss': 0.1561, 'learning_rate': 1e-05, 'epoch': 0.36}
{'loss': 0.1572, 'learning_rate': 1e-05, 'epoch': 0.39}
{'loss': 0.1088, 'learning_rate': 1e-05, 'epoch': 0.41}
{'loss': 0.1695, 'learning_rate': 1e-05, 'epoch': 0.44}
{'loss': 0.1576, 'learning_rate': 1e-05, 'epoch': 0.46}
{'loss': 0.1251, 'learning_rate': 1e-05, 'epoch': 0.48}
{'loss': 0.1406, 'learning_rate': 1e-05, 'epoch': 0.51}
{'loss': 0.1146, 'learning_rate': 1e-05, 'epoch': 0.53}
{'loss': 0.0848, 'learning_rate': 1e-05, 'epoch': 0.56}
{'loss': 0.0863, 'learning_rate': 1e-05, 'epoch': 0.58}
{'loss': 0.1062, 'learning_rate': 1e-05, 'epoch': 0.61}
{'loss': 0.0933, 'learning_rate': 1e-05, 'epoch': 0.63}
{'loss': 0.0581, 'learning_rate': 1e-05, 'epoch': 0.65}
{'loss': 0.0954, 'learning_rate': 1e-05, 'epoch': 0.68}
{'loss': 0.114, 'learning_rate': 1e-05, 'epoch': 0.7}
{'loss': 0.0749, 'learning_rate': 1e-05, 'epoch': 0.73}
{'loss': 0.0905, 'learning_rate': 1e-05, 'epoch': 0.75}
{'loss': 0.0589, 'learning_rate': 1e-05, 'epoch': 0.78}
{'loss': 0.0803, 'learning_rate': 1e-05, 'epoch': 0.8}
{'loss': 0.0907, 'learning_rate': 1e-05, 'epoch': 0.82}
{'loss': 0.1029, 'learning_rate': 1e-05, 'epoch': 0.85}
{'loss': 0.0352, 'learning_rate': 1e-05, 'epoch': 0.87}
{'loss': 0.0882, 'learning_rate': 1e-05, 'epoch': 0.9}
{'loss': 0.0343, 'learning_rate': 1e-05, 'epoch': 0.92}
{'loss': 0.0323, 'learning_rate': 1e-05, 'epoch': 0.95}
{'loss': 0.0826, 'learning_rate': 1e-05, 'epoch': 0.97}
{'loss': 0.0543, 'learning_rate': 1e-05, 'epoch': 0.99}
{'eval_loss': 0.5431655049324036, 'eval_runtime': 2.3835, 'eval_samples_per_second': 140.128, 'eval_steps_per_second': 17.621, 'epoch': 1.0}
{'loss': 0.0403, 'learning_rate': 1e-05, 'epoch': 1.02}
{'loss': 0.0315, 'learning_rate': 1e-05, 'epoch': 1.04}
{'loss': 0.0239, 'learning_rate': 1e-05, 'epoch': 1.07}
{'loss': 0.0163, 'learning_rate': 1e-05, 'epoch': 1.09}
{'loss': 0.0401, 'learning_rate': 1e-05, 'epoch': 1.12}
{'loss': 0.0222, 'learning_rate': 1e-05, 'epoch': 1.14}
{'loss': 0.0737, 'learning_rate': 1e-05, 'epoch': 1.16}
{'loss': 0.0315, 'learning_rate': 1e-05, 'epoch': 1.19}
{'loss': 0.038, 'learning_rate': 1e-05, 'epoch': 1.21}
{'loss': 0.0668, 'learning_rate': 1e-05, 'epoch': 1.24}
{'loss': 0.0259, 'learning_rate': 1e-05, 'epoch': 1.26}
{'loss': 0.017, 'learning_rate': 1e-05, 'epoch': 1.28}
{'loss': 0.0526, 'learning_rate': 1e-05, 'epoch': 1.31}
{'loss': 0.0446, 'learning_rate': 1e-05, 'epoch': 1.33}
{'loss': 0.0325, 'learning_rate': 1e-05, 'epoch': 1.36}
{'loss': 0.069, 'learning_rate': 1e-05, 'epoch': 1.38}
{'loss': 0.0309, 'learning_rate': 1e-05, 'epoch': 1.41}
{'loss': 0.0293, 'learning_rate': 1e-05, 'epoch': 1.43}
{'loss': 0.0503, 'learning_rate': 1e-05, 'epoch': 1.45}
{'loss': 0.0392, 'learning_rate': 1e-05, 'epoch': 1.48}
{'loss': 0.0586, 'learning_rate': 1e-05, 'epoch': 1.5}
{'loss': 0.0455, 'learning_rate': 1e-05, 'epoch': 1.53}
{'loss': 0.0605, 'learning_rate': 1e-05, 'epoch': 1.55}
{'loss': 0.0267, 'learning_rate': 1e-05, 'epoch': 1.58}
{'loss': 0.0496, 'learning_rate': 1e-05, 'epoch': 1.6}
{'loss': 0.0259, 'learning_rate': 1e-05, 'epoch': 1.62}
{'loss': 0.0672, 'learning_rate': 1e-05, 'epoch': 1.65}
{'loss': 0.0496, 'learning_rate': 1e-05, 'epoch': 1.67}
{'loss': 0.0209, 'learning_rate': 1e-05, 'epoch': 1.7}
{'loss': 0.0338, 'learning_rate': 1e-05, 'epoch': 1.72}
{'loss': 0.021, 'learning_rate': 1e-05, 'epoch': 1.75}
{'loss': 0.03, 'learning_rate': 1e-05, 'epoch': 1.77}
{'loss': 0.0208, 'learning_rate': 1e-05, 'epoch': 1.79}
{'loss': 0.0206, 'learning_rate': 1e-05, 'epoch': 1.82}
{'loss': 0.0179, 'learning_rate': 1e-05, 'epoch': 1.84}
{'loss': 0.0211, 'learning_rate': 1e-05, 'epoch': 1.87}
{'loss': 0.0243, 'learning_rate': 1e-05, 'epoch': 1.89}
{'loss': 0.052, 'learning_rate': 1e-05, 'epoch': 1.92}
{'loss': 0.0415, 'learning_rate': 1e-05, 'epoch': 1.94}
{'loss': 0.0504, 'learning_rate': 1e-05, 'epoch': 1.96}
{'loss': 0.0131, 'learning_rate': 1e-05, 'epoch': 1.99}
{'eval_loss': 0.5250467658042908, 'eval_runtime': 2.3937, 'eval_samples_per_second': 139.532, 'eval_steps_per_second': 17.546, 'epoch': 2.0}
{'loss': 0.0188, 'learning_rate': 1e-05, 'epoch': 2.01}
{'loss': 0.0336, 'learning_rate': 1e-05, 'epoch': 2.04}
{'loss': 0.0172, 'learning_rate': 1e-05, 'epoch': 2.06}
{'loss': 0.0019, 'learning_rate': 1e-05, 'epoch': 2.08}
{'loss': 0.063, 'learning_rate': 1e-05, 'epoch': 2.11}
{'loss': 0.0299, 'learning_rate': 1e-05, 'epoch': 2.13}
{'loss': 0.0224, 'learning_rate': 1e-05, 'epoch': 2.16}
{'loss': 0.0139, 'learning_rate': 1e-05, 'epoch': 2.18}
{'loss': 0.005, 'learning_rate': 1e-05, 'epoch': 2.21}
{'loss': 0.0384, 'learning_rate': 1e-05, 'epoch': 2.23}
{'loss': 0.0199, 'learning_rate': 1e-05, 'epoch': 2.25}
{'loss': 0.0391, 'learning_rate': 1e-05, 'epoch': 2.28}
{'loss': 0.0271, 'learning_rate': 1e-05, 'epoch': 2.3}
{'loss': 0.0289, 'learning_rate': 1e-05, 'epoch': 2.33}
{'loss': 0.0233, 'learning_rate': 1e-05, 'epoch': 2.35}
{'loss': 0.0132, 'learning_rate': 1e-05, 'epoch': 2.38}
{'loss': 0.0368, 'learning_rate': 1e-05, 'epoch': 2.4}
{'loss': 0.0372, 'learning_rate': 1e-05, 'epoch': 2.42}
{'loss': 0.0309, 'learning_rate': 1e-05, 'epoch': 2.45}
{'loss': 0.0338, 'learning_rate': 1e-05, 'epoch': 2.47}
{'loss': 0.0338, 'learning_rate': 1e-05, 'epoch': 2.5}
{'loss': 0.0226, 'learning_rate': 1e-05, 'epoch': 2.52}
{'loss': 0.0384, 'learning_rate': 1e-05, 'epoch': 2.55}
{'loss': 0.02, 'learning_rate': 1e-05, 'epoch': 2.57}
{'loss': 0.0328, 'learning_rate': 1e-05, 'epoch': 2.59}
{'loss': 0.0371, 'learning_rate': 1e-05, 'epoch': 2.62}
{'loss': 0.0398, 'learning_rate': 1e-05, 'epoch': 2.64}
{'loss': 0.0143, 'learning_rate': 1e-05, 'epoch': 2.67}
{'loss': 0.0158, 'learning_rate': 1e-05, 'epoch': 2.69}
{'loss': 0.0209, 'learning_rate': 1e-05, 'epoch': 2.72}
{'loss': 0.0183, 'learning_rate': 1e-05, 'epoch': 2.74}
{'loss': 0.0158, 'learning_rate': 1e-05, 'epoch': 2.76}
{'loss': 0.0318, 'learning_rate': 1e-05, 'epoch': 2.79}
{'loss': 0.0292, 'learning_rate': 1e-05, 'epoch': 2.81}
{'loss': 0.0186, 'learning_rate': 1e-05, 'epoch': 2.84}
{'loss': 0.0183, 'learning_rate': 1e-05, 'epoch': 2.86}
{'loss': 0.0003, 'learning_rate': 1e-05, 'epoch': 2.88}
{'loss': 0.0057, 'learning_rate': 1e-05, 'epoch': 2.91}
{'loss': 0.02, 'learning_rate': 1e-05, 'epoch': 2.93}
{'loss': 0.0579, 'learning_rate': 1e-05, 'epoch': 2.96}
{'loss': 0.0257, 'learning_rate': 1e-05, 'epoch': 2.98}
{'eval_loss': 0.557702898979187, 'eval_runtime': 2.399, 'eval_samples_per_second': 139.224, 'eval_steps_per_second': 17.507, 'epoch': 3.0}
{'loss': 0.0147, 'learning_rate': 1e-05, 'epoch': 3.01}
{'loss': 0.0191, 'learning_rate': 1e-05, 'epoch': 3.03}
{'loss': 0.0071, 'learning_rate': 1e-05, 'epoch': 3.05}
{'loss': 0.0002, 'learning_rate': 1e-05, 'epoch': 3.08}
{'loss': 0.0113, 'learning_rate': 1e-05, 'epoch': 3.1}
{'loss': 0.0137, 'learning_rate': 1e-05, 'epoch': 3.13}
{'loss': 0.021, 'learning_rate': 1e-05, 'epoch': 3.15}
{'loss': 0.0274, 'learning_rate': 1e-05, 'epoch': 3.18}
{'loss': 0.0173, 'learning_rate': 1e-05, 'epoch': 3.2}
{'loss': 0.0254, 'learning_rate': 1e-05, 'epoch': 3.22}
{'loss': 0.0368, 'learning_rate': 1e-05, 'epoch': 3.25}
{'loss': 0.0219, 'learning_rate': 1e-05, 'epoch': 3.27}
{'loss': 0.0189, 'learning_rate': 1e-05, 'epoch': 3.3}
{'loss': 0.0082, 'learning_rate': 1e-05, 'epoch': 3.32}
{'loss': 0.0447, 'learning_rate': 1e-05, 'epoch': 3.35}
{'loss': 0.0344, 'learning_rate': 1e-05, 'epoch': 3.37}
{'loss': 0.0137, 'learning_rate': 1e-05, 'epoch': 3.39}
{'loss': 0.0091, 'learning_rate': 1e-05, 'epoch': 3.42}
{'loss': 0.023, 'learning_rate': 1e-05, 'epoch': 3.44}
{'loss': 0.0079, 'learning_rate': 1e-05, 'epoch': 3.47}
{'loss': 0.0042, 'learning_rate': 1e-05, 'epoch': 3.49}
{'loss': 0.0221, 'learning_rate': 1e-05, 'epoch': 3.52}
{'loss': 0.048, 'learning_rate': 1e-05, 'epoch': 3.54}
{'loss': 0.0266, 'learning_rate': 1e-05, 'epoch': 3.56}
{'loss': 0.0444, 'learning_rate': 1e-05, 'epoch': 3.59}
{'loss': 0.0071, 'learning_rate': 1e-05, 'epoch': 3.61}
{'loss': 0.0001, 'learning_rate': 1e-05, 'epoch': 3.64}
{'loss': 0.0003, 'learning_rate': 1e-05, 'epoch': 3.66}
{'loss': 0.0114, 'learning_rate': 1e-05, 'epoch': 3.68}
{'loss': 0.0263, 'learning_rate': 1e-05, 'epoch': 3.71}
{'loss': 0.0165, 'learning_rate': 1e-05, 'epoch': 3.73}
{'loss': 0.0162, 'learning_rate': 1e-05, 'epoch': 3.76}
{'loss': 0.0118, 'learning_rate': 1e-05, 'epoch': 3.78}
{'loss': 0.0036, 'learning_rate': 1e-05, 'epoch': 3.81}
{'loss': 0.0, 'learning_rate': 1e-05, 'epoch': 3.83}
{'loss': 0.0012, 'learning_rate': 1e-05, 'epoch': 3.85}
{'loss': 0.0427, 'learning_rate': 1e-05, 'epoch': 3.88}
{'loss': 0.0001, 'learning_rate': 1e-05, 'epoch': 3.9}
{'loss': 0.0444, 'learning_rate': 1e-05, 'epoch': 3.93}
{'loss': 0.018, 'learning_rate': 1e-05, 'epoch': 3.95}
{'loss': 0.0198, 'learning_rate': 1e-05, 'epoch': 3.98}
{'loss': 0.0156, 'learning_rate': 1e-05, 'epoch': 4.0}
{'eval_loss': 0.5338266491889954, 'eval_runtime': 2.3974, 'eval_samples_per_second': 139.318, 'eval_steps_per_second': 17.519, 'epoch': 4.0}
{'loss': 0.0399, 'learning_rate': 1e-05, 'epoch': 4.02}
{'loss': 0.0382, 'learning_rate': 1e-05, 'epoch': 4.05}
{'loss': 0.0217, 'learning_rate': 1e-05, 'epoch': 4.07}
{'loss': 0.0232, 'learning_rate': 1e-05, 'epoch': 4.1}
{'loss': 0.012, 'learning_rate': 1e-05, 'epoch': 4.12}
{'loss': 0.0082, 'learning_rate': 1e-05, 'epoch': 4.15}
{'loss': 0.0275, 'learning_rate': 1e-05, 'epoch': 4.17}
{'loss': 0.0, 'learning_rate': 1e-05, 'epoch': 4.19}
{'loss': 0.0002, 'learning_rate': 1e-05, 'epoch': 4.22}
{'loss': 0.0001, 'learning_rate': 1e-05, 'epoch': 4.24}
{'loss': 0.0347, 'learning_rate': 1e-05, 'epoch': 4.27}
{'loss': 0.0001, 'learning_rate': 1e-05, 'epoch': 4.29}
{'loss': 0.0008, 'learning_rate': 1e-05, 'epoch': 4.32}
{'loss': 0.0173, 'learning_rate': 1e-05, 'epoch': 4.34}
{'loss': 0.0159, 'learning_rate': 1e-05, 'epoch': 4.36}
{'loss': 0.0, 'learning_rate': 1e-05, 'epoch': 4.39}
{'loss': 0.0168, 'learning_rate': 1e-05, 'epoch': 4.41}
{'loss': 0.0004, 'learning_rate': 1e-05, 'epoch': 4.44}
{'loss': 0.0008, 'learning_rate': 1e-05, 'epoch': 4.46}
{'loss': 0.0003, 'learning_rate': 1e-05, 'epoch': 4.48}
{'loss': 0.0422, 'learning_rate': 1e-05, 'epoch': 4.51}
{'loss': 0.0223, 'learning_rate': 1e-05, 'epoch': 4.53}
{'loss': 0.0004, 'learning_rate': 1e-05, 'epoch': 4.56}
{'loss': 0.0239, 'learning_rate': 1e-05, 'epoch': 4.58}
{'loss': 0.0288, 'learning_rate': 1e-05, 'epoch': 4.61}
{'loss': 0.0393, 'learning_rate': 1e-05, 'epoch': 4.63}
{'loss': 0.0165, 'learning_rate': 1e-05, 'epoch': 4.65}
{'loss': 0.0093, 'learning_rate': 1e-05, 'epoch': 4.68}
{'loss': 0.0502, 'learning_rate': 1e-05, 'epoch': 4.7}
{'loss': 0.0017, 'learning_rate': 1e-05, 'epoch': 4.73}
{'loss': 0.0246, 'learning_rate': 1e-05, 'epoch': 4.75}
{'loss': 0.016, 'learning_rate': 1e-05, 'epoch': 4.78}
{'loss': 0.0319, 'learning_rate': 1e-05, 'epoch': 4.8}
{'loss': 0.0067, 'learning_rate': 1e-05, 'epoch': 4.82}
{'loss': 0.0001, 'learning_rate': 1e-05, 'epoch': 4.85}
{'loss': 0.0023, 'learning_rate': 1e-05, 'epoch': 4.87}
{'loss': 0.0119, 'learning_rate': 1e-05, 'epoch': 4.9}
{'loss': 0.0247, 'learning_rate': 1e-05, 'epoch': 4.92}
{'loss': 0.0001, 'learning_rate': 1e-05, 'epoch': 4.95}
{'loss': 0.0455, 'learning_rate': 1e-05, 'epoch': 4.97}
{'loss': 0.0115, 'learning_rate': 1e-05, 'epoch': 4.99}
{'eval_loss': 0.6278807520866394, 'eval_runtime': 2.3977, 'eval_samples_per_second': 139.302, 'eval_steps_per_second': 17.517, 'epoch': 5.0}
{'train_runtime': 3595.6098, 'train_samples_per_second': 45.889, 'train_steps_per_second': 5.736, 'train_loss': 0.051031945459993386, 'epoch': 5.0}
Some weights of the model checkpoint at ../model/mengzi-bert-base-fin were not used when initializing BertForSequenceClassification: ['pos_transform.LayerNorm.weight', 'pos_transform.dense.bias', 'sop.cls.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'pos_transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'pos_head.weight', 'sop.cls.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'pos_transform.dense.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'pos_head.bias']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ../model/mengzi-bert-base-fin and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
{'loss': 0.702, 'learning_rate': 9.951515151515152e-06, 'epoch': 0.02}
{'loss': 0.4155, 'learning_rate': 9.903030303030305e-06, 'epoch': 0.05}
{'loss': 0.3237, 'learning_rate': 9.854545454545456e-06, 'epoch': 0.07}
{'loss': 0.2058, 'learning_rate': 9.806060606060607e-06, 'epoch': 0.1}
{'loss': 0.3023, 'learning_rate': 9.757575757575758e-06, 'epoch': 0.12}
{'loss': 0.2606, 'learning_rate': 9.70909090909091e-06, 'epoch': 0.15}
{'loss': 0.2737, 'learning_rate': 9.660606060606061e-06, 'epoch': 0.17}
{'loss': 0.2814, 'learning_rate': 9.612121212121212e-06, 'epoch': 0.19}
{'loss': 0.2027, 'learning_rate': 9.563636363636365e-06, 'epoch': 0.22}
{'loss': 0.2078, 'learning_rate': 9.515151515151516e-06, 'epoch': 0.24}
{'loss': 0.157, 'learning_rate': 9.466666666666667e-06, 'epoch': 0.27}
{'loss': 0.1749, 'learning_rate': 9.418181818181818e-06, 'epoch': 0.29}
{'loss': 0.1718, 'learning_rate': 9.36969696969697e-06, 'epoch': 0.32}
{'loss': 0.1749, 'learning_rate': 9.321212121212122e-06, 'epoch': 0.34}
{'loss': 0.1816, 'learning_rate': 9.272727272727273e-06, 'epoch': 0.36}
{'loss': 0.1564, 'learning_rate': 9.224242424242424e-06, 'epoch': 0.39}
{'loss': 0.1124, 'learning_rate': 9.175757575757576e-06, 'epoch': 0.41}
{'loss': 0.1154, 'learning_rate': 9.127272727272727e-06, 'epoch': 0.44}
{'loss': 0.1356, 'learning_rate': 9.078787878787878e-06, 'epoch': 0.46}
{'loss': 0.1669, 'learning_rate': 9.030303030303031e-06, 'epoch': 0.48}
{'loss': 0.1439, 'learning_rate': 8.981818181818182e-06, 'epoch': 0.51}
{'loss': 0.0639, 'learning_rate': 8.933333333333333e-06, 'epoch': 0.53}
{'loss': 0.0565, 'learning_rate': 8.884848484848486e-06, 'epoch': 0.56}
{'loss': 0.0499, 'learning_rate': 8.836363636363637e-06, 'epoch': 0.58}
{'loss': 0.0721, 'learning_rate': 8.787878787878788e-06, 'epoch': 0.61}
{'loss': 0.1013, 'learning_rate': 8.73939393939394e-06, 'epoch': 0.63}
{'loss': 0.1095, 'learning_rate': 8.690909090909091e-06, 'epoch': 0.65}
{'loss': 0.0987, 'learning_rate': 8.642424242424242e-06, 'epoch': 0.68}
{'loss': 0.0589, 'learning_rate': 8.593939393939395e-06, 'epoch': 0.7}
{'loss': 0.1012, 'learning_rate': 8.545454545454546e-06, 'epoch': 0.73}
{'loss': 0.0472, 'learning_rate': 8.496969696969697e-06, 'epoch': 0.75}
{'loss': 0.0282, 'learning_rate': 8.44848484848485e-06, 'epoch': 0.78}
{'loss': 0.0631, 'learning_rate': 8.400000000000001e-06, 'epoch': 0.8}
{'loss': 0.0624, 'learning_rate': 8.351515151515152e-06, 'epoch': 0.82}
{'loss': 0.0587, 'learning_rate': 8.303030303030305e-06, 'epoch': 0.85}
{'loss': 0.0541, 'learning_rate': 8.254545454545456e-06, 'epoch': 0.87}
{'loss': 0.0553, 'learning_rate': 8.206060606060607e-06, 'epoch': 0.9}
{'loss': 0.0582, 'learning_rate': 8.15757575757576e-06, 'epoch': 0.92}
{'loss': 0.044, 'learning_rate': 8.10909090909091e-06, 'epoch': 0.95}
{'loss': 0.0666, 'learning_rate': 8.060606060606061e-06, 'epoch': 0.97}
{'loss': 0.1014, 'learning_rate': 8.012121212121214e-06, 'epoch': 0.99}
{'eval_loss': 0.4500557780265808, 'eval_runtime': 2.3833, 'eval_samples_per_second': 140.142, 'eval_steps_per_second': 17.623, 'epoch': 1.0}
{'loss': 0.0385, 'learning_rate': 7.963636363636365e-06, 'epoch': 1.02}
{'loss': 0.0441, 'learning_rate': 7.915151515151516e-06, 'epoch': 1.04}
{'loss': 0.0452, 'learning_rate': 7.866666666666667e-06, 'epoch': 1.07}
{'loss': 0.0607, 'learning_rate': 7.81818181818182e-06, 'epoch': 1.09}
{'loss': 0.0172, 'learning_rate': 7.76969696969697e-06, 'epoch': 1.12}
{'loss': 0.0228, 'learning_rate': 7.721212121212122e-06, 'epoch': 1.14}
{'loss': 0.0388, 'learning_rate': 7.672727272727273e-06, 'epoch': 1.16}
{'loss': 0.0399, 'learning_rate': 7.6242424242424254e-06, 'epoch': 1.19}
{'loss': 0.028, 'learning_rate': 7.5757575757575764e-06, 'epoch': 1.21}
{'loss': 0.0189, 'learning_rate': 7.5272727272727274e-06, 'epoch': 1.24}
{'loss': 0.0166, 'learning_rate': 7.47878787878788e-06, 'epoch': 1.26}
{'loss': 0.0371, 'learning_rate': 7.430303030303031e-06, 'epoch': 1.28}
{'loss': 0.0429, 'learning_rate': 7.381818181818182e-06, 'epoch': 1.31}
{'loss': 0.017, 'learning_rate': 7.333333333333333e-06, 'epoch': 1.33}
{'loss': 0.0281, 'learning_rate': 7.284848484848486e-06, 'epoch': 1.36}
{'loss': 0.0462, 'learning_rate': 7.236363636363637e-06, 'epoch': 1.38}
{'loss': 0.0249, 'learning_rate': 7.187878787878788e-06, 'epoch': 1.41}
{'loss': 0.0183, 'learning_rate': 7.1393939393939405e-06, 'epoch': 1.43}
{'loss': 0.0209, 'learning_rate': 7.0909090909090916e-06, 'epoch': 1.45}
{'loss': 0.0795, 'learning_rate': 7.0424242424242426e-06, 'epoch': 1.48}
{'loss': 0.035, 'learning_rate': 6.993939393939394e-06, 'epoch': 1.5}
{'loss': 0.0398, 'learning_rate': 6.945454545454546e-06, 'epoch': 1.53}
{'loss': 0.0495, 'learning_rate': 6.896969696969697e-06, 'epoch': 1.55}
{'loss': 0.0159, 'learning_rate': 6.848484848484849e-06, 'epoch': 1.58}
{'loss': 0.0444, 'learning_rate': 6.800000000000001e-06, 'epoch': 1.6}
{'loss': 0.0087, 'learning_rate': 6.751515151515152e-06, 'epoch': 1.62}
{'loss': 0.0065, 'learning_rate': 6.703030303030304e-06, 'epoch': 1.65}
{'loss': 0.0554, 'learning_rate': 6.654545454545455e-06, 'epoch': 1.67}
{'loss': 0.0189, 'learning_rate': 6.606060606060607e-06, 'epoch': 1.7}
{'loss': 0.0277, 'learning_rate': 6.5575757575757585e-06, 'epoch': 1.72}
{'loss': 0.0555, 'learning_rate': 6.5090909090909095e-06, 'epoch': 1.75}
{'loss': 0.0178, 'learning_rate': 6.460606060606061e-06, 'epoch': 1.77}
{'loss': 0.0147, 'learning_rate': 6.412121212121213e-06, 'epoch': 1.79}
{'loss': 0.0316, 'learning_rate': 6.363636363636364e-06, 'epoch': 1.82}
{'loss': 0.0689, 'learning_rate': 6.315151515151515e-06, 'epoch': 1.84}
{'loss': 0.0213, 'learning_rate': 6.266666666666668e-06, 'epoch': 1.87}
{'loss': 0.0182, 'learning_rate': 6.218181818181819e-06, 'epoch': 1.89}
{'loss': 0.006, 'learning_rate': 6.16969696969697e-06, 'epoch': 1.92}
{'loss': 0.0141, 'learning_rate': 6.121212121212121e-06, 'epoch': 1.94}
{'loss': 0.0576, 'learning_rate': 6.072727272727274e-06, 'epoch': 1.96}
{'loss': 0.0214, 'learning_rate': 6.024242424242425e-06, 'epoch': 1.99}
{'eval_loss': 0.6397887468338013, 'eval_runtime': 2.3975, 'eval_samples_per_second': 139.31, 'eval_steps_per_second': 17.518, 'epoch': 2.0}
{'loss': 0.0035, 'learning_rate': 5.975757575757576e-06, 'epoch': 2.01}
{'loss': 0.0113, 'learning_rate': 5.927272727272728e-06, 'epoch': 2.04}
{'loss': 0.0013, 'learning_rate': 5.878787878787879e-06, 'epoch': 2.06}
{'loss': 0.0115, 'learning_rate': 5.83030303030303e-06, 'epoch': 2.08}
{'loss': 0.0244, 'learning_rate': 5.781818181818181e-06, 'epoch': 2.11}
{'loss': 0.0001, 'learning_rate': 5.733333333333334e-06, 'epoch': 2.13}
{'loss': 0.0173, 'learning_rate': 5.684848484848485e-06, 'epoch': 2.16}
{'loss': 0.0357, 'learning_rate': 5.636363636363636e-06, 'epoch': 2.18}
{'loss': 0.0096, 'learning_rate': 5.587878787878789e-06, 'epoch': 2.21}
{'loss': 0.011, 'learning_rate': 5.53939393939394e-06, 'epoch': 2.23}
{'loss': 0.0227, 'learning_rate': 5.490909090909091e-06, 'epoch': 2.25}
{'loss': 0.0301, 'learning_rate': 5.442424242424243e-06, 'epoch': 2.28}
{'loss': 0.0002, 'learning_rate': 5.3939393939393945e-06, 'epoch': 2.3}
{'loss': 0.0184, 'learning_rate': 5.3454545454545455e-06, 'epoch': 2.33}
{'loss': 0.0148, 'learning_rate': 5.296969696969697e-06, 'epoch': 2.35}
{'loss': 0.0014, 'learning_rate': 5.248484848484849e-06, 'epoch': 2.38}
{'loss': 0.0098, 'learning_rate': 5.2e-06, 'epoch': 2.4}
{'loss': 0.0026, 'learning_rate': 5.151515151515152e-06, 'epoch': 2.42}
{'loss': 0.0105, 'learning_rate': 5.103030303030303e-06, 'epoch': 2.45}
{'loss': 0.0115, 'learning_rate': 5.054545454545455e-06, 'epoch': 2.47}
{'loss': 0.0109, 'learning_rate': 5.006060606060607e-06, 'epoch': 2.5}
{'loss': 0.0192, 'learning_rate': 4.957575757575758e-06, 'epoch': 2.52}
{'loss': 0.0022, 'learning_rate': 4.90909090909091e-06, 'epoch': 2.55}
{'loss': 0.0097, 'learning_rate': 4.8606060606060615e-06, 'epoch': 2.57}
{'loss': 0.0235, 'learning_rate': 4.8121212121212125e-06, 'epoch': 2.59}
{'loss': 0.0026, 'learning_rate': 4.763636363636364e-06, 'epoch': 2.62}
{'loss': 0.0023, 'learning_rate': 4.715151515151515e-06, 'epoch': 2.64}
{'loss': 0.0171, 'learning_rate': 4.666666666666667e-06, 'epoch': 2.67}
{'loss': 0.0101, 'learning_rate': 4.618181818181818e-06, 'epoch': 2.69}
{'loss': 0.0332, 'learning_rate': 4.56969696969697e-06, 'epoch': 2.72}
{'loss': 0.0151, 'learning_rate': 4.521212121212122e-06, 'epoch': 2.74}
{'loss': 0.0045, 'learning_rate': 4.472727272727273e-06, 'epoch': 2.76}
{'loss': 0.0249, 'learning_rate': 4.424242424242425e-06, 'epoch': 2.79}
{'loss': 0.0039, 'learning_rate': 4.375757575757576e-06, 'epoch': 2.81}
{'loss': 0.0116, 'learning_rate': 4.327272727272728e-06, 'epoch': 2.84}
{'loss': 0.0006, 'learning_rate': 4.278787878787879e-06, 'epoch': 2.86}
{'loss': 0.0, 'learning_rate': 4.2303030303030304e-06, 'epoch': 2.88}
{'loss': 0.0, 'learning_rate': 4.181818181818182e-06, 'epoch': 2.91}
{'loss': 0.0074, 'learning_rate': 4.133333333333333e-06, 'epoch': 2.93}
{'loss': 0.0063, 'learning_rate': 4.084848484848485e-06, 'epoch': 2.96}
{'loss': 0.0025, 'learning_rate': 4.036363636363637e-06, 'epoch': 2.98}
{'eval_loss': 0.6837357878684998, 'eval_runtime': 2.3902, 'eval_samples_per_second': 139.735, 'eval_steps_per_second': 17.571, 'epoch': 3.0}
{'loss': 0.0104, 'learning_rate': 3.987878787878788e-06, 'epoch': 3.01}
{'loss': 0.0001, 'learning_rate': 3.93939393939394e-06, 'epoch': 3.03}
{'loss': 0.0082, 'learning_rate': 3.890909090909092e-06, 'epoch': 3.05}
{'loss': 0.0039, 'learning_rate': 3.842424242424243e-06, 'epoch': 3.08}
{'loss': 0.0101, 'learning_rate': 3.793939393939394e-06, 'epoch': 3.1}
{'loss': 0.0, 'learning_rate': 3.745454545454546e-06, 'epoch': 3.13}
{'loss': 0.0, 'learning_rate': 3.6969696969696974e-06, 'epoch': 3.15}
{'loss': 0.0, 'learning_rate': 3.648484848484849e-06, 'epoch': 3.18}
{'loss': 0.0001, 'learning_rate': 3.6000000000000003e-06, 'epoch': 3.2}
{'loss': 0.0001, 'learning_rate': 3.551515151515152e-06, 'epoch': 3.22}
{'loss': 0.0201, 'learning_rate': 3.503030303030303e-06, 'epoch': 3.25}
{'loss': 0.0, 'learning_rate': 3.454545454545455e-06, 'epoch': 3.27}
{'loss': 0.0099, 'learning_rate': 3.406060606060606e-06, 'epoch': 3.3}
{'loss': 0.0, 'learning_rate': 3.357575757575758e-06, 'epoch': 3.32}
{'loss': 0.006, 'learning_rate': 3.3090909090909097e-06, 'epoch': 3.35}
{'loss': 0.0121, 'learning_rate': 3.2606060606060607e-06, 'epoch': 3.37}
{'loss': 0.0035, 'learning_rate': 3.2121212121212125e-06, 'epoch': 3.39}
{'loss': 0.0086, 'learning_rate': 3.1636363636363635e-06, 'epoch': 3.42}
{'loss': 0.0053, 'learning_rate': 3.1151515151515154e-06, 'epoch': 3.44}
{'loss': 0.0032, 'learning_rate': 3.066666666666667e-06, 'epoch': 3.47}
{'loss': 0.0096, 'learning_rate': 3.0181818181818182e-06, 'epoch': 3.49}
{'loss': 0.0105, 'learning_rate': 2.96969696969697e-06, 'epoch': 3.52}
{'loss': 0.0048, 'learning_rate': 2.9212121212121215e-06, 'epoch': 3.54}
{'loss': 0.0088, 'learning_rate': 2.872727272727273e-06, 'epoch': 3.56}
{'loss': 0.0228, 'learning_rate': 2.8242424242424244e-06, 'epoch': 3.59}
{'loss': 0.0134, 'learning_rate': 2.7757575757575762e-06, 'epoch': 3.61}
{'loss': 0.0094, 'learning_rate': 2.7272727272727272e-06, 'epoch': 3.64}
{'loss': 0.0006, 'learning_rate': 2.678787878787879e-06, 'epoch': 3.66}
{'loss': 0.0, 'learning_rate': 2.63030303030303e-06, 'epoch': 3.68}
{'loss': 0.025, 'learning_rate': 2.581818181818182e-06, 'epoch': 3.71}
{'loss': 0.0042, 'learning_rate': 2.5333333333333338e-06, 'epoch': 3.73}
{'loss': 0.0001, 'learning_rate': 2.4848484848484848e-06, 'epoch': 3.76}
{'loss': 0.0125, 'learning_rate': 2.4363636363636366e-06, 'epoch': 3.78}
{'loss': 0.0069, 'learning_rate': 2.387878787878788e-06, 'epoch': 3.81}
{'loss': 0.0017, 'learning_rate': 2.3393939393939395e-06, 'epoch': 3.83}
{'loss': 0.0, 'learning_rate': 2.2909090909090913e-06, 'epoch': 3.85}
{'loss': 0.0075, 'learning_rate': 2.2424242424242428e-06, 'epoch': 3.88}
{'loss': 0.0161, 'learning_rate': 2.193939393939394e-06, 'epoch': 3.9}
{'loss': 0.0, 'learning_rate': 2.1454545454545456e-06, 'epoch': 3.93}
{'loss': 0.0025, 'learning_rate': 2.096969696969697e-06, 'epoch': 3.95}
{'loss': 0.0125, 'learning_rate': 2.0484848484848485e-06, 'epoch': 3.98}
{'loss': 0.0009, 'learning_rate': 2.0000000000000003e-06, 'epoch': 4.0}
{'eval_loss': 0.7778927087783813, 'eval_runtime': 2.3925, 'eval_samples_per_second': 139.601, 'eval_steps_per_second': 17.555, 'epoch': 4.0}
{'loss': 0.0007, 'learning_rate': 1.9515151515151518e-06, 'epoch': 4.02}
{'loss': 0.0, 'learning_rate': 1.9030303030303032e-06, 'epoch': 4.05}
{'loss': 0.0, 'learning_rate': 1.8545454545454546e-06, 'epoch': 4.07}
{'loss': 0.0, 'learning_rate': 1.8060606060606063e-06, 'epoch': 4.1}
{'loss': 0.0, 'learning_rate': 1.7575757575757577e-06, 'epoch': 4.12}
{'loss': 0.0031, 'learning_rate': 1.7090909090909091e-06, 'epoch': 4.15}
{'loss': 0.0, 'learning_rate': 1.6606060606060605e-06, 'epoch': 4.17}
{'loss': 0.0003, 'learning_rate': 1.6121212121212124e-06, 'epoch': 4.19}
{'loss': 0.0, 'learning_rate': 1.5636363636363638e-06, 'epoch': 4.22}
{'loss': 0.0001, 'learning_rate': 1.5151515151515152e-06, 'epoch': 4.24}
{'loss': 0.0, 'learning_rate': 1.4666666666666669e-06, 'epoch': 4.27}
{'loss': 0.0, 'learning_rate': 1.4181818181818183e-06, 'epoch': 4.29}
{'loss': 0.0051, 'learning_rate': 1.3696969696969697e-06, 'epoch': 4.32}
{'loss': 0.0132, 'learning_rate': 1.3212121212121212e-06, 'epoch': 4.34}
{'loss': 0.0001, 'learning_rate': 1.2727272727272728e-06, 'epoch': 4.36}
{'loss': 0.0, 'learning_rate': 1.2242424242424242e-06, 'epoch': 4.39}
{'loss': 0.0, 'learning_rate': 1.1757575757575759e-06, 'epoch': 4.41}
{'loss': 0.0, 'learning_rate': 1.1272727272727275e-06, 'epoch': 4.44}
{'loss': 0.0, 'learning_rate': 1.078787878787879e-06, 'epoch': 4.46}
{'loss': 0.0036, 'learning_rate': 1.0303030303030304e-06, 'epoch': 4.48}
{'loss': 0.0, 'learning_rate': 9.818181818181818e-07, 'epoch': 4.51}
{'loss': 0.0037, 'learning_rate': 9.333333333333334e-07, 'epoch': 4.53}
{'loss': 0.0001, 'learning_rate': 8.84848484848485e-07, 'epoch': 4.56}
{'loss': 0.0, 'learning_rate': 8.363636363636364e-07, 'epoch': 4.58}
{'loss': 0.0, 'learning_rate': 7.878787878787879e-07, 'epoch': 4.61}
{'loss': 0.0, 'learning_rate': 7.393939393939395e-07, 'epoch': 4.63}
{'loss': 0.0033, 'learning_rate': 6.90909090909091e-07, 'epoch': 4.65}
{'loss': 0.0, 'learning_rate': 6.424242424242424e-07, 'epoch': 4.68}
{'loss': 0.0062, 'learning_rate': 5.93939393939394e-07, 'epoch': 4.7}
{'loss': 0.0028, 'learning_rate': 5.454545454545455e-07, 'epoch': 4.73}
{'loss': 0.0217, 'learning_rate': 4.96969696969697e-07, 'epoch': 4.75}
{'loss': 0.0002, 'learning_rate': 4.484848484848485e-07, 'epoch': 4.78}
{'loss': 0.0031, 'learning_rate': 4.0000000000000003e-07, 'epoch': 4.8}
{'loss': 0.0032, 'learning_rate': 3.515151515151515e-07, 'epoch': 4.82}
{'loss': 0.0, 'learning_rate': 3.0303030303030305e-07, 'epoch': 4.85}
{'loss': 0.0003, 'learning_rate': 2.545454545454546e-07, 'epoch': 4.87}
{'loss': 0.0, 'learning_rate': 2.060606060606061e-07, 'epoch': 4.9}
{'loss': 0.0, 'learning_rate': 1.575757575757576e-07, 'epoch': 4.92}
{'loss': 0.0019, 'learning_rate': 1.090909090909091e-07, 'epoch': 4.95}
{'loss': 0.0, 'learning_rate': 6.060606060606061e-08, 'epoch': 4.97}
{'loss': 0.0086, 'learning_rate': 1.2121212121212122e-08, 'epoch': 4.99}
{'eval_loss': 0.7662100195884705, 'eval_runtime': 2.3919, 'eval_samples_per_second': 139.637, 'eval_steps_per_second': 17.559, 'epoch': 5.0}
{'train_runtime': 3602.7673, 'train_samples_per_second': 45.798, 'train_steps_per_second': 5.725, 'train_loss': 0.0404383896392756, 'epoch': 5.0}
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/scipy/__init__.py:132: UserWarning: A NumPy version >=1.21.6 and <1.28.0 is required for this version of SciPy (detected version 1.21.5)
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
Some weights of the model checkpoint at ../model/mengzi-bert-base-fin were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'pos_transform.dense.weight', 'sop.cls.weight', 'cls.predictions.transform.LayerNorm.bias', 'pos_head.weight', 'pos_transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'pos_transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'sop.cls.bias', 'pos_head.bias', 'pos_transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ../model/mengzi-bert-base-fin and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/home/asus/miniconda3/envs/intelligent-test/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
{'loss': 0.7173, 'learning_rate': 9.999419979367215e-06, 'epoch': 0.02}
{'loss': 0.4765, 'learning_rate': 9.997680052038434e-06, 'epoch': 0.05}
{'loss': 0.3842, 'learning_rate': 9.994780621691156e-06, 'epoch': 0.07}
{'loss': 0.327, 'learning_rate': 9.99072236101715e-06, 'epoch': 0.1}
{'loss': 0.2427, 'learning_rate': 9.985506211566388e-06, 'epoch': 0.12}
{'loss': 0.2388, 'learning_rate': 9.979133383528591e-06, 'epoch': 0.15}
{'loss': 0.2388, 'learning_rate': 9.97160535545246e-06, 'epoch': 0.17}
{'loss': 0.2193, 'learning_rate': 9.962923873902637e-06, 'epoch': 0.19}
{'loss': 0.2099, 'learning_rate': 9.953090953054491e-06, 'epoch': 0.22}
{'loss': 0.1835, 'learning_rate': 9.942108874226812e-06, 'epoch': 0.24}
{'loss': 0.2186, 'learning_rate': 9.929980185352525e-06, 'epoch': 0.27}
{'loss': 0.2047, 'learning_rate': 9.916707700387546e-06, 'epoch': 0.29}
{'loss': 0.2447, 'learning_rate': 9.902294498657929e-06, 'epoch': 0.32}
{'loss': 0.1448, 'learning_rate': 9.886743924145428e-06, 'epoch': 0.34}
{'loss': 0.1658, 'learning_rate': 9.870059584711668e-06, 'epoch': 0.36}
{'loss': 0.1722, 'learning_rate': 9.852245351261099e-06, 'epoch': 0.39}
{'loss': 0.1564, 'learning_rate': 9.833305356842902e-06, 'epoch': 0.41}
{'loss': 0.1339, 'learning_rate': 9.813243995692097e-06, 'epoch': 0.44}
{'loss': 0.154, 'learning_rate': 9.792065922210042e-06, 'epoch': 0.46}
{'loss': 0.0972, 'learning_rate': 9.769776049884564e-06, 'epoch': 0.48}
{'loss': 0.1333, 'learning_rate': 9.74637955015001e-06, 'epoch': 0.51}
{'loss': 0.1151, 'learning_rate': 9.721881851187406e-06, 'epoch': 0.53}
{'loss': 0.0954, 'learning_rate': 9.6962886366651e-06, 'epoch': 0.56}
{'loss': 0.132, 'learning_rate': 9.66960584442008e-06, 'epoch': 0.58}
{'loss': 0.075, 'learning_rate': 9.641839665080363e-06, 'epoch': 0.61}
{'loss': 0.1367, 'learning_rate': 9.612996540628718e-06, 'epoch': 0.63}
{'loss': 0.1178, 'learning_rate': 9.58308316290806e-06, 'epoch': 0.65}
{'loss': 0.1136, 'learning_rate': 9.552106472068898e-06, 'epoch': 0.68}
{'loss': 0.1032, 'learning_rate': 9.520073654959163e-06, 'epoch': 0.7}
{'loss': 0.0753, 'learning_rate': 9.486992143456792e-06, 'epoch': 0.73}
{'loss': 0.0982, 'learning_rate': 9.452869612745483e-06, 'epoch': 0.75}
{'loss': 0.0784, 'learning_rate': 9.417713979533976e-06, 'epoch': 0.78}
{'loss': 0.0684, 'learning_rate': 9.381533400219319e-06, 'epoch': 0.8}
{'loss': 0.0621, 'learning_rate': 9.344336268994516e-06, 'epoch': 0.82}
{'loss': 0.0725, 'learning_rate': 9.306131215901004e-06, 'epoch': 0.85}
{'loss': 0.0678, 'learning_rate': 9.26692710482641e-06, 'epoch': 0.87}
{'loss': 0.0638, 'learning_rate': 9.226733031448061e-06, 'epoch': 0.9}
{'loss': 0.0662, 'learning_rate': 9.185558321122705e-06, 'epoch': 0.92}
{'loss': 0.0506, 'learning_rate': 9.143412526722958e-06, 'epoch': 0.95}
{'loss': 0.0461, 'learning_rate': 9.100305426420957e-06, 'epoch': 0.97}
{'loss': 0.0666, 'learning_rate': 9.056247021419735e-06, 'epoch': 0.99}
{'eval_loss': 0.5020512938499451, 'eval_runtime': 2.3988, 'eval_samples_per_second': 139.238, 'eval_steps_per_second': 17.509, 'epoch': 1.0}
{'loss': 0.0323, 'learning_rate': 9.011247533632876e-06, 'epoch': 1.02}
{'loss': 0.0402, 'learning_rate': 8.96531740331293e-06, 'epoch': 1.04}
{'loss': 0.0317, 'learning_rate': 8.9184672866292e-06, 'epoch': 1.07}
{'loss': 0.0456, 'learning_rate': 8.870708053195414e-06, 'epoch': 1.09}
{'loss': 0.0314, 'learning_rate': 8.82205078354789e-06, 'epoch': 1.12}
{'loss': 0.0175, 'learning_rate': 8.772506766574762e-06, 'epoch': 1.14}
{'loss': 0.0088, 'learning_rate': 8.72208749689686e-06, 'epoch': 1.16}
{'loss': 0.015, 'learning_rate': 8.670804672200865e-06, 'epoch': 1.19}
{'loss': 0.0502, 'learning_rate': 8.61867019052535e-06, 'epoch': 1.21}
{'loss': 0.0323, 'learning_rate': 8.565696147500338e-06, 'epoch': 1.24}
{'loss': 0.0325, 'learning_rate': 8.511894833541006e-06, 'epoch': 1.26}
{'loss': 0.026, 'learning_rate': 8.457278730996223e-06, 'epoch': 1.28}
{'loss': 0.0186, 'learning_rate': 8.401860511252535e-06, 'epoch': 1.31}
{'loss': 0.0376, 'learning_rate': 8.345653031794292e-06, 'epoch': 1.33}
{'loss': 0.0258, 'learning_rate': 8.288669333220616e-06, 'epoch': 1.36}
{'loss': 0.0467, 'learning_rate': 8.230922636219872e-06, 'epoch': 1.38}
{'loss': 0.0387, 'learning_rate': 8.172426338502351e-06, 'epoch': 1.41}
{'loss': 0.0056, 'learning_rate': 8.1131940116919e-06, 'epoch': 1.43}
{'loss': 0.0246, 'learning_rate': 8.053239398177191e-06, 'epoch': 1.45}
{'loss': 0.0271, 'learning_rate': 7.992576407923373e-06, 'epoch': 1.48}
{'loss': 0.0166, 'learning_rate': 7.931219115244841e-06, 'epoch': 1.5}
{'loss': 0.036, 'learning_rate': 7.869181755539888e-06, 'epoch': 1.53}
{'loss': 0.0324, 'learning_rate': 7.806478721987964e-06, 'epoch': 1.55}
{'loss': 0.0354, 'learning_rate': 7.743124562210351e-06, 'epoch': 1.58}
{'loss': 0.0371, 'learning_rate': 7.679133974894984e-06, 'epoch': 1.6}
{'loss': 0.0206, 'learning_rate': 7.614521806386244e-06, 'epoch': 1.62}
{'loss': 0.0043, 'learning_rate': 7.549303047240475e-06, 'epoch': 1.65}
{'loss': 0.029, 'learning_rate': 7.4834928287480566e-06, 'epoch': 1.67}
{'loss': 0.0489, 'learning_rate': 7.4171064194228196e-06, 'epoch': 1.7}
{'loss': 0.0167, 'learning_rate': 7.350159221459622e-06, 'epoch': 1.72}
{'loss': 0.0215, 'learning_rate': 7.282666767160913e-06, 'epoch': 1.75}
{'loss': 0.0205, 'learning_rate': 7.214644715333114e-06, 'epoch': 1.77}
{'loss': 0.0392, 'learning_rate': 7.146108847653642e-06, 'epoch': 1.79}
{'loss': 0.0142, 'learning_rate': 7.0770750650094335e-06, 'epoch': 1.82}
{'loss': 0.0112, 'learning_rate': 7.007559383807804e-06, 'epoch': 1.84}
{'loss': 0.0134, 'learning_rate': 6.9375779322605154e-06, 'epoch': 1.87}
{'loss': 0.0116, 'learning_rate': 6.8671469466418914e-06, 'epoch': 1.89}
{'loss': 0.0044, 'learning_rate': 6.796282767521869e-06, 'epoch': 1.92}
{'loss': 0.0174, 'learning_rate': 6.725001835974854e-06, 'epoch': 1.94}
{'loss': 0.0471, 'learning_rate': 6.653320689765257e-06, 'epoch': 1.96}
{'loss': 0.0213, 'learning_rate': 6.58125595951059e-06, 'epoch': 1.99}
{'eval_loss': 0.5445639491081238, 'eval_runtime': 2.3892, 'eval_samples_per_second': 139.795, 'eval_steps_per_second': 17.579, 'epoch': 2.0}
{'loss': 0.012, 'learning_rate': 6.508824364823031e-06, 'epoch': 2.01}
{'loss': 0.0176, 'learning_rate': 6.4360427104303326e-06, 'epoch': 2.04}
{'loss': 0.0324, 'learning_rate': 6.362927882276991e-06, 'epoch': 2.06}
{'loss': 0.0252, 'learning_rate': 6.28949684360656e-06, 'epoch': 2.08}
{'loss': 0.0214, 'learning_rate': 6.215766631026049e-06, 'epoch': 2.11}
{'loss': 0.0019, 'learning_rate': 6.141754350553279e-06, 'epoch': 2.13}
{'loss': 0.0, 'learning_rate': 6.067477173648153e-06, 'epoch': 2.16}
{'loss': 0.0078, 'learning_rate': 5.9929523332287275e-06, 'epoch': 2.18}
{'loss': 0.0075, 'learning_rate': 5.918197119673046e-06, 'epoch': 2.21}
{'loss': 0.0196, 'learning_rate': 5.843228876807614e-06, 'epoch': 2.23}
{'loss': 0.0346, 'learning_rate': 5.7680649978834976e-06, 'epoch': 2.25}
{'loss': 0.0116, 'learning_rate': 5.692722921540946e-06, 'epoch': 2.28}
{'loss': 0.0065, 'learning_rate': 5.617220127763474e-06, 'epoch': 2.3}
{'loss': 0.0024, 'learning_rate': 5.541574133822374e-06, 'epoch': 2.33}
{'loss': 0.0061, 'learning_rate': 5.465802490212554e-06, 'epoch': 2.35}
{'loss': 0.0164, 'learning_rate': 5.389922776580682e-06, 'epoch': 2.38}
{'loss': 0.0309, 'learning_rate': 5.3139525976465675e-06, 'epoch': 2.4}
{'loss': 0.0252, 'learning_rate': 5.237909579118713e-06, 'epoch': 2.42}
{'loss': 0.0053, 'learning_rate': 5.161811363605006e-06, 'epoch': 2.45}
{'loss': 0.011, 'learning_rate': 5.085675606519496e-06, 'epoch': 2.47}
{'loss': 0.0, 'learning_rate': 5.009519971986183e-06, 'epoch': 2.5}
{'loss': 0.0051, 'learning_rate': 4.9333621287408005e-06, 'epoch': 2.52}
{'loss': 0.0, 'learning_rate': 4.85721974603152e-06, 'epoch': 2.55}
{'loss': 0.0048, 'learning_rate': 4.781110489519541e-06, 'epoch': 2.57}
{'loss': 0.0261, 'learning_rate': 4.705052017180514e-06, 'epoch': 2.59}
{'loss': 0.0217, 'learning_rate': 4.62906197520774e-06, 'epoch': 2.62}
{'loss': 0.0139, 'learning_rate': 4.553157993918113e-06, 'epoch': 2.64}
{'loss': 0.0057, 'learning_rate': 4.477357683661734e-06, 'epoch': 2.67}
{'loss': 0.0067, 'learning_rate': 4.401678630736172e-06, 'epoch': 2.69}
{'loss': 0.0, 'learning_rate': 4.326138393306292e-06, 'epoch': 2.72}
{'loss': 0.0054, 'learning_rate': 4.250754497330626e-06, 'epoch': 2.74}
{'loss': 0.0092, 'learning_rate': 4.175544432495184e-06, 'epoch': 2.76}
{'loss': 0.0002, 'learning_rate': 4.100525648155731e-06, 'epoch': 2.79}
{'loss': 0.0222, 'learning_rate': 4.025715549289371e-06, 'epoch': 2.81}
{'loss': 0.0122, 'learning_rate': 3.951131492456455e-06, 'epoch': 2.84}
{'loss': 0.021, 'learning_rate': 3.876790781773721e-06, 'epoch': 2.86}
{'loss': 0.0067, 'learning_rate': 3.802710664899588e-06, 'epoch': 2.88}
{'loss': 0.0287, 'learning_rate': 3.7289083290325668e-06, 'epoch': 2.91}
{'loss': 0.0199, 'learning_rate': 3.655400896923672e-06, 'epoch': 2.93}
{'loss': 0.0002, 'learning_rate': 3.5822054229038207e-06, 'epoch': 2.96}
{'loss': 0.0047, 'learning_rate': 3.509338888927079e-06, 'epoch': 2.98}
{'eval_loss': 0.600212037563324, 'eval_runtime': 2.3987, 'eval_samples_per_second': 139.244, 'eval_steps_per_second': 17.51, 'epoch': 3.0}
{'loss': 0.0033, 'learning_rate': 3.4368182006307005e-06, 'epoch': 3.01}
{'loss': 0.0048, 'learning_rate': 3.3646601834128924e-06, 'epoch': 3.03}
{'loss': 0.0068, 'learning_rate': 3.292881578529179e-06, 'epoch': 3.05}
{'loss': 0.0128, 'learning_rate': 3.2214990392082913e-06, 'epoch': 3.08}
{'loss': 0.0063, 'learning_rate': 3.1505291267884773e-06, 'epoch': 3.1}
{'loss': 0.0008, 'learning_rate': 3.0799883068751433e-06, 'epoch': 3.13}
{'loss': 0.0, 'learning_rate': 3.0098929455206905e-06, 'epoch': 3.15}
{'loss': 0.0, 'learning_rate': 2.9402593054274557e-06, 'epoch': 3.18}
{'loss': 0.0037, 'learning_rate': 2.871103542174637e-06, 'epoch': 3.2}
{'loss': 0.0062, 'learning_rate': 2.8024417004700598e-06, 'epoch': 3.22}
{'loss': 0.0052, 'learning_rate': 2.7342897104276732e-06, 'epoch': 3.25}
{'loss': 0.0133, 'learning_rate': 2.6666633838716317e-06, 'epoch': 3.27}
{'loss': 0.0081, 'learning_rate': 2.599578410667827e-06, 'epoch': 3.3}
{'loss': 0.0071, 'learning_rate': 2.5330503550837004e-06, 'epoch': 3.32}
{'loss': 0.003, 'learning_rate': 2.467094652177209e-06, 'epoch': 3.35}
{'loss': 0.0001, 'learning_rate': 2.40172660421577e-06, 'epoch': 3.37}
{'loss': 0.0, 'learning_rate': 2.3369613771260006e-06, 'epoch': 3.39}
{'loss': 0.0062, 'learning_rate': 2.2728139969751005e-06, 'epoch': 3.42}
{'loss': 0.0048, 'learning_rate': 2.2092993464846775e-06, 'epoch': 3.44}
{'loss': 0.0055, 'learning_rate': 2.146432161577842e-06, 'epoch': 3.47}
{'loss': 0.0007, 'learning_rate': 2.0842270279603403e-06, 'epoch': 3.49}
{'loss': 0.0133, 'learning_rate': 2.0226983777365604e-06, 'epoch': 3.52}
{'loss': 0.0022, 'learning_rate': 1.9618604860611556e-06, 'epoch': 3.54}
{'loss': 0.0039, 'learning_rate': 1.9017274678270948e-06, 'epoch': 3.56}
{'loss': 0.0064, 'learning_rate': 1.8423132743908962e-06, 'epoch': 3.59}
{'loss': 0.0069, 'learning_rate': 1.7836316903357881e-06, 'epoch': 3.61}
{'loss': 0.0012, 'learning_rate': 1.7256963302735752e-06, 'epoch': 3.64}
{'loss': 0.0203, 'learning_rate': 1.6685206356859402e-06, 'epoch': 3.66}
{'loss': 0.0, 'learning_rate': 1.612117871805907e-06, 'epoch': 3.68}
{'loss': 0.0, 'learning_rate': 1.5565011245401928e-06, 'epoch': 3.71}
{'loss': 0.0004, 'learning_rate': 1.5016832974331725e-06, 'epoch': 3.73}
{'loss': 0.0, 'learning_rate': 1.4476771086731567e-06, 'epoch': 3.76}
{'loss': 0.0, 'learning_rate': 1.3944950881416541e-06, 'epoch': 3.78}
{'loss': 0.0068, 'learning_rate': 1.3421495745063452e-06, 'epoch': 3.81}
{'loss': 0.0166, 'learning_rate': 1.2906527123584084e-06, 'epoch': 3.83}
{'loss': 0.0086, 'learning_rate': 1.2400164493948713e-06, 'epoch': 3.85}
{'loss': 0.0, 'learning_rate': 1.1902525336466465e-06, 'epoch': 3.88}
{'loss': 0.0004, 'learning_rate': 1.1413725107528956e-06, 'epoch': 3.9}
{'loss': 0.0, 'learning_rate': 1.0933877212823462e-06, 'epoch': 3.93}
{'loss': 0.0, 'learning_rate': 1.0463092981021732e-06, 'epoch': 3.95}
{'loss': 0.0059, 'learning_rate': 1.000148163795101e-06, 'epoch': 3.98}
{'loss': 0.0132, 'learning_rate': 9.549150281252633e-07, 'epoch': 4.0}
{'eval_loss': 0.6145133972167969, 'eval_runtime': 2.3853, 'eval_samples_per_second': 140.026, 'eval_steps_per_second': 17.608, 'epoch': 4.0}
{'loss': 0.0057, 'learning_rate': 9.106203855534479e-07, 'epoch': 4.02}
{'loss': 0.0106, 'learning_rate': 8.672745128022997e-07, 'epoch': 4.05}
{'loss': 0.0029, 'learning_rate': 8.248874664720375e-07, 'epoch': 4.07}
{'loss': 0.0, 'learning_rate': 7.834690807072342e-07, 'epoch': 4.1}
{'loss': 0.0, 'learning_rate': 7.430289649152156e-07, 'epoch': 4.12}
{'loss': 0.0001, 'learning_rate': 7.035765015366047e-07, 'epoch': 4.15}
{'loss': 0.0, 'learning_rate': 6.651208438685119e-07, 'epoch': 4.17}
{'loss': 0.0, 'learning_rate': 6.276709139408937e-07, 'epoch': 4.19}
{'loss': 0.0022, 'learning_rate': 5.912354004465709e-07, 'epoch': 4.22}
{'loss': 0.0013, 'learning_rate': 5.558227567253832e-07, 'epoch': 4.24}
{'loss': 0.0035, 'learning_rate': 5.214411988029355e-07, 'epoch': 4.27}
{'loss': 0.0, 'learning_rate': 4.880987034844231e-07, 'epoch': 4.29}
{'loss': 0.0, 'learning_rate': 4.558030065039387e-07, 'epoch': 4.32}
{'loss': 0.0, 'learning_rate': 4.2456160072972097e-07, 'epoch': 4.34}
{'loss': 0.0006, 'learning_rate': 3.9438173442575e-07, 'epoch': 4.36}
{'loss': 0.0007, 'learning_rate': 3.6527040957008487e-07, 'epoch': 4.39}
{'loss': 0.0008, 'learning_rate': 3.3723438023035073e-07, 'epoch': 4.41}
{'loss': 0.0, 'learning_rate': 3.1028015099673957e-07, 'epoch': 4.44}
{'loss': 0.0, 'learning_rate': 2.844139754728914e-07, 'epoch': 4.46}
{'loss': 0.0025, 'learning_rate': 2.596418548250029e-07, 'epoch': 4.48}
{'loss': 0.0002, 'learning_rate': 2.3596953638951093e-07, 'epoch': 4.51}
{'loss': 0.0, 'learning_rate': 2.134025123396638e-07, 'epoch': 4.53}
{'loss': 0.0, 'learning_rate': 1.9194601841128924e-07, 'epoch': 4.56}
{'loss': 0.0, 'learning_rate': 1.7160503268806084e-07, 'epoch': 4.58}
{'loss': 0.0001, 'learning_rate': 1.5238427444654368e-07, 'epoch': 4.61}
{'loss': 0.0102, 'learning_rate': 1.3428820306128076e-07, 'epoch': 4.63}
{'loss': 0.0096, 'learning_rate': 1.1732101697018161e-07, 'epoch': 4.65}
{'loss': 0.0, 'learning_rate': 1.014866527004521e-07, 'epoch': 4.68}
{'loss': 0.0, 'learning_rate': 8.678878395528667e-08, 'epoch': 4.7}
{'loss': 0.0001, 'learning_rate': 7.32308207615351e-08, 'epoch': 4.73}
{'loss': 0.0, 'learning_rate': 6.081590867855536e-08, 'epoch': 4.75}
{'loss': 0.0, 'learning_rate': 4.9546928068411884e-08, 'epoch': 4.78}
{'loss': 0.0009, 'learning_rate': 3.9426493427611177e-08, 'epoch': 4.8}
{'loss': 0.0, 'learning_rate': 3.0456952780513747e-08, 'epoch': 4.82}
{'loss': 0.0007, 'learning_rate': 2.264038713457706e-08, 'epoch': 4.85}
{'loss': 0.003, 'learning_rate': 1.5978609997542306e-08, 'epoch': 4.87}
{'loss': 0.0091, 'learning_rate': 1.0473166956684322e-08, 'epoch': 4.9}
{'loss': 0.0023, 'learning_rate': 6.125335320227299e-09, 'epoch': 4.92}
{'loss': 0.0014, 'learning_rate': 2.9361238209935085e-09, 'epoch': 4.95}
{'loss': 0.0, 'learning_rate': 9.062723823710651e-10, 'epoch': 4.97}
{'loss': 0.0, 'learning_rate': 3.625194664735876e-11, 'epoch': 4.99}
{'eval_loss': 0.6170605421066284, 'eval_runtime': 2.3844, 'eval_samples_per_second': 140.076, 'eval_steps_per_second': 17.614, 'epoch': 5.0}
{'train_runtime': 3598.4975, 'train_samples_per_second': 45.852, 'train_steps_per_second': 5.732, 'train_loss': 0.04188684411143856, 'epoch': 5.0}
